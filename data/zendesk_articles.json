[
  {
    "title": "DvSum AWS PrivateLink Setup (Customer Guide)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/43220623721876-DvSum-AWS-PrivateLink-Setup-Customer-Guide",
    "content": "In this Article:\nOverview\nService Architecture\nInformation Required from Customer\nInformation Provided by DvSum\nPrerequisites\nStep-by-Step Setup Instructions\nStep 1: Create VPC Endpoint for REST APIs (apis.dvsum.ai)\nStep 2: Create VPC Endpoint for WebSocket (socket-api.dvsum.ai)\nStep 3: Create VPC Endpoint for S3 (s3-storage.dvsum.ai)\nStep 4: Notify DvSum for Endpoint Acceptance\nStep 5: Security Group Configuration\nStep 6: DNS and Connectivity Validation\nCustom DNS Configuration\nAdvanced Troubleshooting\nOverview\nThis document provides comprehensive setup guidance for securely connecting to DvSum SaaS services using AWS PrivateLink. It enables private network access to REST APIs, WebSocket services, and S3 storage using the AWS network. All communication remains inside the AWS private backbone, improving security, reliability, and compliance for enterprise environments. The document also details required customer information, DvSum-provided configurations, step-by-step setup instructions, and validation procedures to establish AWS PrivateLink connectivity successfully.\nService Architecture\nDvSum provides\nthree separate VPC Endpoint Services\nfor comprehensive private access:\nService\nDomain\nPurpose\nProtocol\nPort\nEndpoint Service\nREST APIs\napis.dvsum.ai\nStandard API communication\nHTTPS\n443\nDedicated Service\nWebSocket\nsocket-api.dvsum.ai\nReal-time WebSocket communication\nHTTPS\n443\nDedicated Service\nS3\ns3-storage.dvsum.ai\nS3 Pre-signed URLs Handling\nHTTPS\n443\nDedicated Service\nImportant\n: Each service requires its own VPC Interface Endpoint in your VPC\nInformation Required from Customer\nTo enable PrivateLink connectivity, please provide DvSum with:\n1. Customer Account Information\nParameter\nDescription\nAWS Account ID\nYour 12-digit AWS account number\nPrimary AWS Region\nThe AWS region where your VPC resides\nVPC ID\nThe VPC where interface endpoints will be created\n2. Network Configuration\nParameter\nDescription\nVPC CIDR Blocks\nUsed for security group configuration and network monitoring\nSubnet IDs\nSubnets where you plan to create interface endpoints (recommend private subnets)\nAvailability Zones\nAZs to use for endpoint deployment (recommend all for high availability)\n3. Service Level Requirements\nExpected bandwidth\n: Helps DvSum provision appropriate infrastructure capacity\nInformation Provided by DvSum\nBefore proceeding with setup, DvSum will provide you with the following:\n1. VPC Endpoint Service Names\nService\nExample Service Name\nREST API Service\ncom.amazonaws.vpce.us-east-1.vpce-svc-yyyyyyyyy\nWebSocket Service\ncom.amazonaws.vpce.us-east-1.vpce-svc-zzzzzzzzz\nS3 Service\ncom.amazonaws.vpce.us-east-1.vpce-svc-xxxxxxxxxxxxxx\n2. Health Check Testing Endpoints\nService\nURL\nREST API\nhttps://apis.dvsum.ai/health-check\nWebSocket\nhttps://socket-api.dvsum.ai/health-check\nS3\nhttps://s3-storage.dvsum.ai/health-check\nPrerequisites\nBefore starting the setup, ensure you have:\nAWS Access Requirements\nAWS Console access with permissions for:\nVPC endpoint creation and management\nSecurity group configuration\nRoute 53 private hosted zone management (if needed)\nEC2 instance access (for testing)\nNetwork Requirements\nA VPC with private subnets in multiple Availability Zones\nExisting applications or test environment to validate connectivity\nSecurity groups configured to allow outbound HTTPS (port 443)\nStep-by-Step Setup Instructions\nStep 1: Create VPC Endpoint for REST APIs (apis.dvsum.ai)\nIn the AWS Management Console, go to\nVPC â†’ Endpoints â†’ Create Endpoint.\nUnder\nService Category Type\n, select: â€œ\nEndpoint services that use NLBs and GWLBs\n.â€\nIn\nService Name\n, paste the\nREST API service name\nprovided by DvSum and click\nVerify service\n. It should get successfully verified.\nUnder\nNetwork settings:\nChoose your VPC.\nSelect at least\ntwo private subnets\nacross different AZs for high availability.\nSecurity Group\n: Create or select the security group with the following Inbound and Outbound rules:\nInbound Rules:\nType\nProtocol\nPort\nSource\nHTTPS\nTCP\n443\nSecurity Group ID of the machine or VPC CIDR\nOutbound Rules:\nKeep all outbound traffic allowed by default\nType\nProtocol\nPort Range\nDestination\nAll Traffic\nAll\nAll\n0.0.0.0/0\nClick\nCreate Endpoint\n.\nAfter endpoint is created, its status will be\nPending Acceptance\n, at this point, customers need to inform DvSum to accept the connection request.\nOnce DvSum accepts the connection request, endpoint status will change to\nAvailable.\nAs the connection is in Available state, select the endpoint and from\nActions,\nclick on\nModify private DNS name\nUnder Modify private DNS name settings, select:\nEnable private DNS names\nand click on Save changes\nStep 2: Create VPC Endpoint for WebSocket (socket-api.dvsum.ai)\nIn the AWS Management Console, go to\nVPC â†’ Endpoints â†’ Create Endpoint.\nUnder\nService Category Type\n, select: â€œ\nEndpoint services that use NLBs and GWLBs\n.â€\nIn\nService Name\n, paste the\nSocket service name\nprovided by DvSum and click\nVerify service\n. It should get successfully verified.\nUnder\nNetwork settings:\nChoose your VPC.\nSelect at least\ntwo private subnets\nacross different AZs for high availability.\\\nSecurity Group\n: Create or select the security group with the following Inbound and Outbound rules:\nInbound Rules:\nType\nProtocol\nPort\nSource\nHTTPS\nTCP\n443\nSecurity Group ID of the machine or VPC CIDR\nOutbound Rules:\nKeep all outbound traffic allowed as by default\nType\nProtocol\nPort Range\nDestination\nAll Traffic\nAll\nAll\n0.0.0.0/0\nClick\nCreate Endpoint\n.\nAfter endpoint is created, its status will be\nPending Acceptance\n, at this point, customers need to inform DvSum to accept the connection request.\nOnce DvSum accepts the connection request, endpoint status will change to\nAvailable.\nAs the connection is in Available state, select the endpoint and from\nActions,\nclick on\nModify private DNS name\nUnder Modify private DNS name settings, select:\nEnable private DNS names\nand click on Save changes\nStep 3: Create VPC Endpoint for S3 (s3-storage.dvsum.ai)\nIn the AWS Management Console, go to\nVPC â†’ Endpoints â†’ Create Endpoint.\nUnder\nService Category Type\n, select: â€œ\nEndpoint services that use NLBs and GWLBs\n.â€\nIn\nService Name\n, paste the\nSocket service name\nprovided by DvSum and click\nVerify service\n. It should get successfully verified.\nUnder\nNetwork settings:\nChoose your VPC.\nSelect at least\ntwo private subnets\nacross different AZs for high availability.\nSecurity Group\n: Create or select the security group with the following Inbound and Outbound rules:\nInbound Rules:\nType\nProtocol\nPort\nSource\nHTTPS\nTCP\n443\nSecurity Group ID of the machine or VPC CIDR\nOutbound Rules:\nKeep all outbound traffic allowed as by default\nType\nProtocol\nPort Range\nDestination\nAll Traffic\nAll\nAll\n0.0.0.0/0\nClick\nCreate Endpoint\n.\nAfter endpoint is created, its status will be\nPending Acceptance\n, at this point, customers need to inform DvSum to accept the connection request.\nOnce DvSum accepts the connection request, endpoint status will change to\nAvailable.\nAs the connection is in Available state, select the endpoint and from\nActions,\nclick on\nModify private DNS name\nUnder Modify private DNS name settings, select:\nEnable private DNS names\nand click on Save changes\nStep 4: Notify DvSum for Endpoint Acceptance\nOnce all endpoints are created, their initial status will appear as\nâ€œPending acceptance.â€\nDvSum must approve these requests before they become active.\nContact your DvSum representative and notify them that the VPC endpoints for the REST API, S3 and WebSocket services have been created in your AWS Account.\nAfter DvSum accepts the requests, the endpoint status will change to\nâ€œAvailable.â€\nStep 5: Security Group Configuration\nConfigure your security groups to permit traffic between your applications and the new VPC endpoints.\nMachine Security Group (Outbound Rules):\nKeep all outbound traffic allowed as by default\nType\nProtocol\nPort Range\nDestination\nAll Traffic\nAll\nAll\n0.0.0.0/0\nVPC Endpoint Security Group:\nInbound Rules:\nType\nProtocol\nPort\nSource\nHTTPS\nTCP\n443\nSecurity Group ID of the machine or VPC CIDR\nOutbound Rules:\nKeep all outbound traffic allowed as by default\nType\nProtocol\nPort Range\nDestination\nAll Traffic\nAll\nAll\n0.0.0.0/0\nStep 6: DNS and Connectivity Validation\nPerform these tests from an EC2 instance inside your configured VPC.\n1. DNS Resolution Test:\nnslookup apis.dvsum.ai\nnslookup socket-api.dvsum.ai\nnslookup s3-storage.dvsum.ai\nExpected Result:\nBoth domains should resolve to the private IP addresses of the interface endpoints within your VPC's CIDR range.\n2. REST API and S3 Connectivity Test:\ncurl -v https://apis.dvsum.ai/health-check\ncurl -v https://s3-storage.dvsum.ai/health-check\nExpected Result:\nA JSON response: {\"status\": \"ok\"}.\n3. WebSocket Connectivity Test:\nopenssl s_client -connect socket-api.dvsum.ai:443\nExpected Result:\nA successful TLS handshake, displaying the certificate chain and a \"\nCONNECTED\n\" message without errors.\nCustom DNS Configuration\nNote:\nThis step is to be followed only if\nPrivate DNS is not enabled\non VPC Endpoints.\nThe following are the steps to configure DNS settings for the correct routing of traffic through three VPC endpoints via Private Link.\nStep 1: Create Private Hosted Zone\nIn the AWS Management Console\n, Go to Route 53 â†’ Hosted Zones\nCreate Private Hosted Zone\n:\nDomain name\n: dvsum.ai\nType\n: Private hosted zone\nVPC\n: Associate with your application VPC\nStep 2: Add DNS Records\nAdd A-records pointing to your endpoint private IPs:\nRecord Name\nTarget\napis.dvsum.ai\nPrivate IPs of REST API VPC Interface Endpoint\nsocket-api.dvsum.ai\nPrivate IPs of WebSocket VPC Interface Endpoint\ns3-storage.dvsum.ai\nPrivate IPs of S3 VPC Interface Endpoint\nAdvanced Troubleshooting\nNetwork Connectivity Test\n# Test from EC2 instance in your VPCâ€¯ \ntelnet apis.dvsum.ai 443â€¯â€¯â€¯ \ntelnet socket-api.dvsum.ai 443â€¯\ntelnet s3-storage.dvsum.ai 443\nVPC Flow Logs Analysis\nEnable VPC Flow Logs to monitor traffic patterns:\nMonitor accepted vs. rejected connections\nIdentify security group misconfigurations\nTrack bandwidth utilization\nDNS Resolution Debugging\n# Check DNS resolution chainâ€¯ \ndig apis.dvsum.aiâ€¯ \ndig socket-api.dvsum.aiâ€¯\ndig s3-storage.dvsum.ai",
    "scraped_at": "2026-02-02 15:26:34"
  },
  {
    "title": "IP Allow List",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/42943305742612-IP-Allow-List",
    "content": "In this Article:\nOverview\nNavigation Path\nConfiguration Steps\nAdd IP Address\nEdit, Update, or Delete an IP Address\nOverview\nThe\nIP Allow List\nfeature lets administrators control access to the DvSum environment by specifying the network locations that are permitted to connect.\nOnly the IP addresses or ranges added to this list will be able to sign in or send API requests.\nThis setting is typically used to:\nRestrict access to corporate networks, VPNs, or trusted data centers.\nPrevent unauthorized logins from unknown or public networks.\nStrengthen overall security and compliance posture.\nNavigation Path\nTo access the IP Allow List feature, follow the below path:\nAdministration â†’ Account â†’ Application Security â†’ IP Allow List\nThis section allows administrators to define and manage the list of approved IP addresses or ranges that are permitted to access the DvSum environment.\nConfiguration Steps\nFollow the below steps to configure the IP Allow List.\nAdd IP Address\nSelect the checkbox\nAllow login only from listed IP addresses\nto enable IP-based access control.\nOnce enabled, only users connecting from the listed IP addresses can access the application.\nIn the input field below, enter the IP address or range that needs to be allowed (for example,\n39.43.171.123\n).\nTip:\nClick\nGet Current IP\nto automatically extract your current public IP and insert it in the input field.\nClick\nAdd\nto include the entered IP address in the allow list.\nNote:\nThis setting does\nnot apply to the owner account\n, which can log in regardless of the IP allow list.\nThe added IP appears in the list below and will be considered an approved source for login when enforcement is active.\nEdit, Update, or Delete an IP Address\nOnce IP addresses are added, they appear in a list below the input field.\nEach entry in the list includes options to modify or remove the IP configuration.\nEdit:\nClick the\nEdit\nicon (âœï¸) next to an existing IP address to modify it.\nUpdate the IP address or range as required, then click the\ncheckmark\nicon to save the changes.\nThe updated entry is immediately reflected in the list.\nCancel Edit:\nTo discard an edit before saving, click the\ncross\nicon next to the field.\nDelete:\nClick the\nDelete\nicon (ðŸ—‘ï¸) to remove an IP address from the allow list.\nOnce deleted, login access from that IP address will be restricted.",
    "scraped_at": "2026-02-02 15:26:39"
  },
  {
    "title": "Audit Trail Logs",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/42333972028436-Audit-Trail-Logs",
    "content": "In This Article:\nOverview\nNavigation Path\nAudit Trail Log Details\nFiltering Records\nExporting Data\nManaging Access to Audit Trail Logs\nAudit Trail on Asset Detail Page\nOverview\nThe\nAudit Trail Logs\nfeature provides a comprehensive record of all user and system activities performed within DvSum. It captures key events such as data updates, configuration changes, and user actions across various modules, allowing administrators to maintain visibility into platform usage and modifications.\nEach log entry records what action was performed, on which entity, by whom, and when. This information supports transparency, accountability, and compliance by enabling detailed tracking of platform activity and changes over time.\nNavigation Path\nTo access the\nAudit Trail Logs\n, follow these steps:\nFrom the left navigation panel, click\nAdministration\n.\nSelect\nAudit Trail Logs\n.\nAudit Trail Log Details\nThe\nAudit Trail Logs\npage displays a comprehensive record of all user and system activities within the platform. Each entry captures details about the action performed, the entity affected, the user responsible, and the timestamp of the event.\nThe table contains the following columns:\nEntity ID\nâ€“ Unique identifier of the entity on which the action was performed.\nEntity Name\nâ€“ Name of the specific item (such as a table, glossary term, or data source) affected by the action.\nEntity Type\nâ€“ Type of entity, for example,\nDatabase Table\nor\nGlossary Term\n.\nAction Taken\nâ€“ Description of the operation performed, such as\nCreated\n,\nUpdated\n,\nDeleted\n, or\nSubmitted for Approval\n.\nAction Details\nâ€“ Additional context or system-generated information related to the action, such as notification or approval details.\nProperty Changed\nâ€“ Field or attribute that was modified as part of the action.\nWhen multiple fields are modified during an action, the\nProperty Changed\ncolumn displays a count (for example,\n+7\n). Hovering over this value reveals a tooltip listing all affected attributes.\nFor instance, when a user account is created, the tooltip may show fields such as\nEmail\n,\nUser Group\n,\nStatus\n,\nStart Date\n,\nFirst Name\n, and\nLast Name\nas the attributes that were set or updated.\nModified By\nâ€“ Name of the user who performed the action.\nModified On\nâ€“ Date and time when the action occurred.\nThe log table provides an organized view of all recorded activities, supporting system transparency and data governance tracking.\nFiltering Records\nTo refine the records displayed in the\nAudit Trail Logs\ntable, a set of filters is available. Filters enable focused viewing of specific actions, users, or time periods, making it easier to analyze targeted activity.\nTo open the filter panel:\nClick the\nFilter\nicon on the top-right of the\nAudit Trail Logs\npage.\nA\nFilter By\npanel will slide out from the right side of the screen.\nHere, you can narrow down the logs using the following criteria:\nEntity Type\nâ€“ Select the specific asset type or module (e.g., Glossary Term, Data Source, etc.) to view activities related to it.\nAction\nâ€“ Filter by the type of operation performed, such as Create, Update, or Delete.\nUser\nâ€“ Choose a specific user to view only the actions performed by that user.\nDate\nâ€“ Filter the logs based on when the action occurred. You can select from preset time ranges or define a custom period:\nLast 24 hours\nLast 7 days\nLast month\nLast 6 months\nLast year\nCustom\nâ€“ Allows you to specify your own date range.\nOnce your filters are selected, the list updates to display only the relevant records.\nExporting Data\nThe\nAudit Trail Logs\ncan be exported for offline review or reporting.\nSelecting the\nExport\nicon in the top-right corner generates an\nExcel (.xlsx)\nfile containing all log entries based on the current filters. If no filters are applied, all available records are included.\nAs noted in the\nAudit Trail Log Details\nsection, when an action involves multiple field updates (displayed in the interface as\n+7\n, for example), each updated field is recorded as an individual entry in the exported file. This may result in a higher number of records appearing in the export than displayed on-screen.\nThe exported file retains the same column structure as displayed in the table. A maximum of\n100,000 records\ncan be exported at a time.\nManaging Access to Audit Trail Logs\nAccess to Audit Trail Logs is controlled through user role permissions in the Admin panel. Administrators can assign view or admin-level access to this feature using the Fine-Grained Control settings.\nTo modify access:\nNavigate to\nAdmin > Edit User Role\nIn the\nAudit Trail Logs\nrow, select the appropriate permission level:\nThis allows organizations to tailor visibility and control based on user responsibilities.\nAudit Trail on Asset Detail Page\nAudit Trail information is also available directly on\nAsset Detail Pages\n.\nClicking the\nthree dots menu\nopens a\nright-side drawer\n, similar to the filter panel described earlier\nThis drawer displays a timeline of actions specific to the selected asset, such as creation, edits, approvals, and notifications\nThe drawer includes:\nExport\noption to download asset-specific audit data\nComment field\nfor adding remarks or context; these comments are also shown on the main Audit Trail Logs page\nThis feature supports quick access to audit history and annotations without leaving the asset view.",
    "scraped_at": "2026-02-02 15:26:45"
  },
  {
    "title": "Configure SAML for Okta",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35305937125268-Configure-SAML-for-Okta",
    "content": "In This Article\nIntroduction\nPrerequisites\nSAML Configuration Steps\nStep 1: Configuring Okta\n1.1 Create a SAML App in Okta\n1.2 Configure SAML Settings in Okta\nStep 2: Retrieve Identity Provider Metadata\nStep 3: Setting Up SAML in DvSum\nStep 4: Enable & Test SSO in DvSum\nStep 5: Assign Users to Okta Application\nIntroduction\nThis guide walks you through setting up SAML authentication for Okta in DvSum. By following these steps, you will enable Single Sign-On (SSO), allowing users to log in securely using their corporate credentials.\nPrerequisites\nBefore you begin, ensure the following:\nYou have admin access to your Okta account.\nYou have a DvSum Owner account with access to Manage Account > SSO settings.\nYou have an SP Metadata File from DvSum (will be downloaded during setup).\nPlease use the following steps to configure SAML integration for your Okta application.\nSAML Configuration Steps:\nStep 1: Configuring Okta\n1.1 Create a SAML App in Okta\nLog in\nto your Okta admin account.\nIn the\nmenu bar\n, go to\nApplications > Add Application\n.\nClick\nCreate App Integration\n.\nFor the Sign-on\nmethod\n, select\nSAML 2.0\nand click\nNext\n.\nEnter an app name\n(e.g., \"DvSum SSO\") and click\nNext\n.\n1.2 Configure SAML Settings in Okta\nLog in to DvSum\nfrom your\nOwner account\n.\nGo to\nManage Account > SSO tab\n.\nClick\nAdd IdP\n.\nDownload the\nSP Metadata File\n.\nOpen the XML file in Notepad.\nExtract and enter required values in Okta\n:\nCopy the\nLocation\nvalue inside the <AssertionConsumerService> tag from XML.\nPaste it into a Single\nSign-On URL\nin Okta.\nCopy the\nentityID\nvalue from XML.\nPaste it into\nAudience URI (SP Entity ID)\nin Okta.\nSet\nName ID Format\nto\nEmailAddress\n.\nSet\nApplication Username\nto\nEmail\n.\nUnder\nAttribute Statements (Optional)\n, add a statement:\nName\n: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\nName Format\n: Basic\nValue\n: user.email\nLeave other settings as default and click\nNext\n.\nChoose a feedback response for Okta Support and click\nFinish\n.\nStep 2: Retrieve Identity Provider Metadata\nIn Okta, go to\nApplications > Select your created app\n.\nOpen the\nSign-On\ntab.\nFind the\nIdentity Provider Metadata\nhyperlink.\nRight-click\nthe link and\nCopy the URL\n.\nThis will be used as the\nIdP metadata\nwhen adding the IdP in DvSum.\nStep 3: Setting Up SAML in DvSum\nLog in to\nDvSum\nwith an\nOwner account\n.\nGo to\nManage Account > SSO tab\n.\nClick\nAdd IdP\nand enter the required details:\nProvider Name\n: A unique name for the IdP.\nIdentifier\n: Your company's domain (e.g., mycompany.com).\nIdP Metadata\n: Paste the\nmetadata URL\ncopied from Okta.\nClick\nSave\n.\nStep 4: Enable & Test SSO in DvSum\nEnable SSO\nin DvSum.\nTry logging in with your\ncorporate email address\n.\nStep 5: Assign Users to Okta Application\nIn Okta, go to the\nAssignments\ntab.\nClick\nAssign to People\n.\nClick\nAssign\nnext to the user you want to add.\nIf this is a new account, assign yourself (admin).\n(Optional) Set a custom\nUser Name\n, or leave it as the\nemail address\n.\nClick\nSave and Go Back\nâ†’\nDone\n.\nFor more information\nClick here",
    "scraped_at": "2026-02-02 15:26:50"
  },
  {
    "title": "Enabling SAML-Based Single Sign-On (SSO)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/4403180875156-Enabling-SAML-Based-Single-Sign-On-SSO",
    "content": "In this Article:\nOverview\nDetailed Steps\nStep 1: Log in to Owner Account\nStep 2: Add Identity Provider\nStep 3: Download DvSum SP Metadata\nStep 4: Fill in Required Fields\nStep 5: Save and View IdP Listing\nStep 6: Enable SSO in Test Mode\nStep 7: Enable SSO in Live Mode\nStep 8: Adding Multiple Identity Providers\nStep 9: Disable SSO\nSAML Configuration to Okta\nAdditional Reference: Microsoft Entra\nOverview\nSAML-based single sign on (SSO) gives members access to DvSum through an\n identity provider (IdP) of your choice.\nSSO, or Single Sign-On, is a service for session and user authentication.\n It enables users to use a single set of login credentials to access multiple\n applications. This simplifies the management of various usernames and passwords\n for both enterprises and individuals.\nDvSum enables you to secure your account by providing Web SSO capabilities\n based on popular standards such as SAML-based identity provider, allowing\n your enterprise user directory or third-party IdP to secure your applications\n via standards-based security tokens.\nCertified identity providers:\nADFS\nMicrosoft Entra\nOkta\nOneLogin\nDetailed Steps\nStep 1: Log in to your Owner account\nGo to the Administration tab â†’ Account â†’ Click on SSO\nStep 2: Add Identity Provider\nClick the button \"Add Identity Provider\" to navigate to the following form.\nStep 3: Download DvSum SP Metadata\nDownload the DvSum SP Metadata file and configure your IdP to add DvSum as\n an application.\nStep 4: Fill in Required Fields\nFill the form with all the required fields.\nProvider Name\nâ€“ Give a unique provider name in case you\n have multiple IdP's. The Provider Name cannot be updated once configured.\n Provider Name can only be alphanumeric, must be 3 to 32 characters long,\n must start with a letter, and cannot have special characters or spaces.\nIdentifier\nâ€“ This is your company's domain name, e.g. mycompany.com.\nIdP Metadata\nâ€“ This is the metadata information from your\n Identity Provider. You can either provide a URL, or upload a local copy of\n the metadata file.\nAttributes\nâ€“ DvSum requires one attribute:\nattribute: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\nvalue:\nEmail\nStep 5: Save and View IdP Listing\nClick Save. You will be directed to the IdP Listing page.\nStep 6: Enable SSO in Test Mode\nWhen a user enables the SSO Toggle button, they will be prompted with a confirmation.\n After clicking OK, SSO will be enabled in Test Mode.\nAfter clicking OK, SSO will be enabled in Test Mode.\nAll administrators will receive an email notification informing them of this\n change.\nNote:\nIn Test mode, Single Sign-On applies only to administrators\n who can test login using either corporate email ID or basic authentication\n credentials. Super users and regular users continue to log in with basic\n authentication.\nStep 7: Enable SSO in Live Mode\nAfter the Admin successfully verifies the SSO corporate login in Test mode,\n they can enable SSO in Live mode.\nWhen SSO is made live, all active DvSum users will be required to use their\n SSO credentials to sign in. Their existing DvSum passwords will be deleted.\n An email notification will be sent to all active users.\nAn email will be sent to all active users and they will no longer be able\n to login using basic authentication.\nStep 8: Adding Multiple Identity Providers\nAdd a new identifier and ensure it is different while keeping the Metadata\n URL the same. Save the changes. Both domains should now work with SSO.\nClicking \"Sign in\" will redirect to the identity provider (Okta).\nOnce the user is verified by Okta, they will be logged in to the application.\nAfter Okta verifies the user, they will be logged into the application.\nStep 9: Disable SSO\nIf the Owner turns off the SSO configuration from Manage Account, an email\n will be sent to all users.\nNote:\nWhen Single Sign-On is disabled, all users will be\n authenticated by the Basic Authentication process and will be required to\n set up a password.\nSAML Configuration to Okta\nPlease use the following\nlink\nto configure SAML integration for your Okta application.\nAdditional Reference: Microsoft Entra\nReference:\nhttps://learn.microsoft.com/en-us/entra/identity/enterprise-apps/add-application-portal-setup-sso\nNote: For more details on reset password refer\nHow to Reset Password on the DI Platform\narticle.",
    "scraped_at": "2026-02-02 15:26:55"
  },
  {
    "title": "Power BI Source Permissions",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41264497137684-Power-BI-Source-Permissions",
    "content": "In this Article:\nOverview\nRequired Prerequisites\nAPI Testing from Power BI Admin\nAuthentication Token Acquisition\nTest Connection - Retrieve Workspaces using Auth token Generated in Step 1\nScanning APIs using Auth token Generated in Step 1\nWorkspace Information Request\nScan Status Check\nScan Results Retrieval\nOverview:\nThe Power BI Admin Team needs to complete the required prerequisites and test the specified APIs to resolve the authentication issue. Once these steps are successfully completed, we can proceed with configuring the connection in DvSum.\nNote:\nFor steps to configure Power BI as a source, refer to\nMicrosoft Power BI as a Source.\nRequired Prerequisites\nBefore proceeding to test API, please ensure the following prerequisites are completed:\nMetadata Scanning Setup\nCreate Azure AD Application\nNote: The same links are already available in the Source Detail Page.\nAPI Testing from Power BI Admin\n1. Authentication Token Acquisition\nEndpoint:\nPOST\nhttps://login.microsoftonline.com/\n<tenant-id>/oauth2/v2.0/token\nRequest Body:\n{\n\"client_id\": \"<client-id>\",\n\"scope\": \"\nhttps://analysis.windows.net/powerbi/api/.default\n\",\n\"grant_type\": \"client_credentials\",\n\"client_secret\": \"<client_secret>\"\n}\nExpected Response:\nStatus Code: 200 OK (indicates successful connection)\nResponse Body:\n{\n\"token_type\": \"Bearer\",\n\"expires_in\": 3599,\n\"ext_expires_in\": 3599,\n\"access_token\": \"<token>\"\n}\n2. Test Connection - Retrieve Workspaces using Auth token Generated in Step 1\nEndpoint:\nGET\nhttps://api.powerbi.com/v1.0/myorg/admin/reports?$filter={$filter}\nReference:\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/admin/groups-get-groups-as-admin\nSupported Filters: \"Personal\", \"PersonalGroup\", \"Workspace\"\n3. Scanning APIs using Auth token Generated in Step 1\nThe following APIs will be used during the scanning process:\nWorkspace Information Request:\nPOST\nhttps://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo?lineage={lineage}&datasourceDetails={datasourceDetails}&datasetSchema={datasetSchema}&datasetExpressions={datasetExpressions}&getArtifactUsers={getArtifactUsers}\nDocumentation:\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-post-workspace-info\nScan Status Check:\nGET\nhttps://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scanId}\nDocumentation:\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-status\nScan Results Retrieval:\nGET\nhttps://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scanId}\nDocumentation:\nhttps://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-result",
    "scraped_at": "2026-02-02 15:27:00"
  },
  {
    "title": "Azure SQL & Azure Synapse Analytics User Setup and Query Store Configuration for Metadata Extraction",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41243166846740-Azure-SQL-Azure-Synapse-Analytics-User-Setup-and-Query-Store-Configuration-for-Metadata-Extraction",
    "content": "In this Article:\nOverview\nPrerequisites\nSection 1: Query Store Configuration\nSection 2: User Creation and Permission Setup\nSection 3: Permission Requirements Matrix\nSection 4: Complete Setup Script\nSection 5: Alternative Contained User Setup\nSection 6: Validation and Testing\nSection 7: Troubleshooting Guide\nSection 8: Security Best Practices\nSection 9: Maintenance Recommendations\nConclusion\nOverview\nThis comprehensive guide provides step-by-step instructions for:\nCreating an Azure SQL Database user with required permissions.\nEnabling and configuring Query Store for lineage data collection.\nValidating the setup for metadata extraction queries.\nNote:\nFor steps to configure Azure SQL or Azure Synapse Analytics as a source, refer to\nConfigure Azure SQL or Azure Synapse Analytics\nPrerequisites\nAzure SQL Database with administrative access\nSQL Server Management Studio SSMS) version 16 or later,\nAdministrative privileges on the target database\nSection 1 Query Store Configuration\n-> Understanding Query Store in Azure SQL Database\nQuery Store is automatically enabled by default in Azure SQL Database, but may need configuration adjustments for optimal metadata collection.\nNote:\nQuery Store cannot be disabled in Azure SQL Database, Single Database, and Elastic Pool.\nThe Default configuration is optimized for continuous data collection\nMinimum permission required: VIEW DATABASE STATE for reading, ALTER DATABASE for configuration\n-> Enable Query Store Using SQL Server Management Studio\nSteps:\nConnect to your Azure SQL Database using SSMS\nIn Object Explorer, right-click the target database\nSelect\nProperties\nNavigate to the\nQuery Store\npage\nIn the\nOperation Mode Requested)\nbox, select\nRead Write\nConfigure additional settings if needed:\nData Flush Interval Minutes)\n: 15 (default: 15\nStatistics Collection Interval Minutes\n: 60 (default: 60\nMax Size MB\n: 1000 (default for new databases)\nQuery Store Capture Mode\n: Auto (recommended)\nClick\nOK\nto apply changes\n-> Enable Query Store Using T SQL Commands\nFor Azure SQL Database:\nFor Azure Synapse Analytics:\n-> Verify Query Store Configuration\nCheck Query Store Status:\nValidation Test Query:\nSection 2 User Creation and Permission Setup\n-> Create Server Login Master Database)\nConnect to the\nmaster\ndatabase as administrator:\n-> Create Database User\nConnect to your target database:\n-> Grant Essential Permissions\nApply the minimum required permissions for metadata extraction:\nSection 3 Permission Requirements Matrix\nQuery Component\nRequired Permission\nPurpose\nINFORMATION_SCHEMA.TABLES\ndb_datareader\nTable metadata extraction\nINFORMATION_SCHEMA.COLUMNS\ndb_datareader\nColumn metadata extraction\nINFORMATION_SCHEMA.CONSTRAINTS\ndb_datareader\nConstraint metadata extraction\nsys.extended_properties\nVIEW DEFINITION\nExtended propertie MS_Description)\nsys.query_store_* views\nVIEW DATABASE STATE\nQuery history for lineage\nsys.sql_modules\nVIEW DEFINITION\nView and procedure definitions\nsys.objects\ndb_datareader\nObject metadata\nsys.schemas\ndb_datareader\nSchema information\nSection 4 Complete Setup Script\nSection 5 Alternative Contained User Setup\n-> For enhanced security, use contained database users:\nSection 6 Validation and Testing\n-> Test Metadata Access\nExecute these validation queries with the new user:\n-> Permission Verification Query\nSection 7 Troubleshooting Guide\n-> Common Issues and Solutions\nIssue\nSymptoms\nSolution\nQuery Store not collecting data\nEmpty results from query store views\nVerify OPERATION_MODE is READ_WRITE\nExtended properties return NULL\nMissing descriptions in metadata\nEnsure VIEW DEFINITION permission granted\nINFORMATION_SCHEMA access denied\nPermission errors on schema views\nConfirm db_datareader role membership\nLogin authentication fails\nCannot connect with new user\nVerify login exists in master database\n-> Query Store Troubleshooting\nCheck Query Store Health:\nClear Query Store (if needed):\nSection 8 Security Best Practices\n-> Principle of Least Privilege\nThe configuration provides:\nRead-only access\nto metadata and system catalogs\nNo data modification\ncapabilities\nNo schema changes\nallowed\nLimited to metadata visibility\nonly\n-> Password Security\nUse strong passwords with complexity\nRequirements: Consider password expiration\npolicies for production, implement regular password rotation\n-> Monitoring and Auditing\nMonitor login attempts and failures\nAudit permission changes\nReview Query Store usage patterns\nSection 9 Maintenance Recommendations\n-> Regular Monitoring\nCheck Query Store storage usage monthly\nMonitor permission assignments quarterly\nReview user access patterns regularly\n-> Performance Optimization\nAdjust Query Store retention policies based on needs\nMonitor Query Store's impact on database performance.\nConsider cleanup schedules for large databases\nConclusion\nThis documentation provides a comprehensive setup for extracting Azure SQL Database metadata, including proper Query Store configuration and user permissions. The setup follows security best practices while ensuring all necessary permissions for comprehensive metadata and lineage collection.\nKey Points:\nQuery Store is essential for lineage data collection\nMinimum permissions follow the least-privilege principle\nRegular validation ensures continued functionality\nThe troubleshooting guide addresses common issues\nIf you need additional support, please refer to Microsoft documentation or contact your database administrator.",
    "scraped_at": "2026-02-02 15:27:04"
  },
  {
    "title": "Configure File Upload as a Data Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35169298078356-Configure-File-Upload-as-a-Data-Source",
    "content": "In this Article:\nOverview\nDvSum Configuration\nAdd Source\nAdd Files\nSupported File Formats\nFile Upload and Scan\nScan Summary\nBrowsing the Data Dictionary\nSample Files\nOverview\nThis article describes the steps needed to configure File Upload as a source in DvSum Data Intelligence (DI).\nDetailed Steps\nDvSum configuration\n1. Add Source\nTo create a data source, navigate to Administration â†’ Data Sources â†’ âŠ•Add Source.\nSelect Upload Files.\nGive the folder a name and save it.\n2: Add Files\nDrag and drop a file into the designated area.\nAlternatively, click on the\nUpload Files\nbutton to open your system's file explorer and select the desired file.\nSupported File Formats:\nYou can upload the following file types:\nExcel\n:\n.xlsx\n,\n.xls\n,\n.xlsm\n,\n.xlt\n,\n.xlr\nCSV\n:\n.csv\nNote: You can find the sample files attached at the end of this article.\n3. File Upload and Scan\nOnce a file is selected or dragged and dropped, it is uploaded, and a scan is automatically initiated on the file.\nFile Actions\nClick on the\nthree-dot menu\nnext to the uploaded file to access additional actions. You can choose to\nDelete\nor\nDownload\nthe file as needed.\nFile uploads are manual:\nUnlike other data sources (databases or APIs) that can fetch new data automatically during rescans, file-based sources do not update automatically.\nUser action required:\nUsers must manually upload new files each day to refresh the data; the system will not do this automatically.\nAfter a file is uploaded and scanned, the following tabs are populated accordingly:\nOverview Tab\nDisplays the detected tables and columns extracted from the scanned file.\nScan History Tab\nShows details of the scan, including timestamps and outcomes.\nLogs Tab\nRecords system activities related to the scan for troubleshooting and tracking purposes.\nSettings Tab\nThe\nSettings\ntab displays folder-related information, including:\nFolder Name\nand\nDescription\nAccess Information\non how to download files\nOwner Details\nUsers can\nEdit\nfolder settings or\nDelete\nthe folder if needed.\nNote:\nOnly profiling tables can be scheduled by creating a job.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\n4. Scan Summary\nAfter the scan is completed, click on the\nScan Name\n(e.g., SCN-000060) to open the\nScan Summary\npage.\nThe\nScan Summary\npage provides insights into the scan, including:\nThe number of new tables and columns detected.\nOther key details related to the scan..\n5. Browsing the Data Dictionary\nAfter the scan, you can explore the detected tables and columns. Navigate to\nData Dictionary\nfrom the left sidebar to access the table listing view.\nClick on the\nRecently Refreshed\ntab to view tables discovered in the latest scan.\nSelect a table name to view its details.\nDQ rules:\nData Quality (DQ) rules cannot be created for file upload data sources.\nSample Files\nHere are the sample files for reference:",
    "scraped_at": "2026-02-02 15:27:10"
  },
  {
    "title": "Configure Amazon S3 as a Data Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/32779279948052-Configure-Amazon-S3-as-a-Data-Source",
    "content": "In this Article\nOverview\nConnect Your Amazon S3 Data Source\nStep-by-Step Configuration\nStep 1: Add Amazon S3 as a Source\nStep 2: Configure Connection\nStep 3: Save Connection Information\nStep 4: Scan the Source\nS3 Configurations\nConfiguring Data Retrieval\nScenario 1: Use an Existing Glue Catalog\nScenario 2: Create a New Glue Catalog\nOverview\nThis guide outlines the steps to configure Amazon S3 as a data source within our SaaS application. It details the process of authenticating and adding your Amazon S3 source using one of two distinct methods, followed by running scans to retrieve your data.\nConnect Your Amazon S3 Data Source\nTo integrate your data from Amazon S3, our platform offers two flexible connection methods:\n1. Use Existing Resources (Connect to Your Existing S3 and Glue Setup)\nChoose this option if you have already independently set up your Amazon S3 environment, including the creation of S3 buckets, AWS Glue databases, and crawlers. In this scenario, our platform will connect directly to your existing Glue Data Catalog to access your data. Because the necessary administrative tasks for your S3 data source have already been completed outside of our system, this method requires only limited permissions for our platform to read the metadata defined in your Glue databases.\nAmazon S3 Data Source Connection: Use Existing Resources (Limited Permissions)\n2. Create New Resources (Platform Interface Setup)\nSelect this option to utilize our platform's user interface to create and manage the necessary AWS resources for accessing your S3 data. This allows you to define and build your data infrastructure directly within our application. You will have the ability to create S3 buckets, define AWS Glue databases, and set up crawlers through our intuitive interface. Consequently, this method requires elevated permissions to enable the creation and management of these resources on your AWS account.\nConfigure S3 Data Source in DvSum (Elevated Permissions Required)\nOnce you've selected the connection method that aligns with your current setup and desired level of management, proceed with the relevant instructions to configure the necessary IAM user and assign the appropriate permissions.\nStep-by-Step Configuration\nStep 1: Add Amazon S3 as a Source\nNavigate to\nAdministration â†’ Data Sources â†’ âŠ• Add Source\n.\nSelect\nAmazon S3\nas the source type.\nProvide a name for the source and click\nSave\n.\nStep 2: Configure Connection\nAfter saving, you will be redirected to the connection settings page.\nEnable the checkbox for On-premise Web Service and select the Gateway, or use DvSum Web Services (default).\nIn the Credentials section, enter the AWS Access Key and Secret Key.\nClick Authenticate to verify the connection.\nNote:\nFor more information regarding On-premise Web Servic\ne installation, click\nhere\n.\n1. Note: By default, the Web Service type is set to Cloud. For additional details, refer\nDvSum Web Service vs On-Premises Gateway\n.\nIn the\nCredentials\nsection, provide the correct\nAWS Access Key\nand\nAWS Secret Key\n, then click\nAuthenticate\nto verify the source.\nHow to create an AWS IAM User to configure S3 Data Source in DvSum?\nS3 Configurations\nOnce authenticated, configure the S3 connection settings:\nSelect the\nRegion\n(e.g., US West - Oregon).\nWorkgroup\nand\nCatalog\nsettings align with your AWS Athena configuration. For typical setups, you can leave them blank.\nIf you've created custom catalogs or workgroups, enter their names as defined in your Athena environment.\nUnder\nGlue Database\n, select one or more databases from the\nAvailable\nlist and move them to the\nSelected\nlist.\nThese are fetched from the AWS Glue Data Catalog.\nConfiguring Data Retrieval\nScenario 1: Use an Existing Glue Catalog\nSelect the Glue\nCatalog\nfrom the dropdown menu, and input the\nStaging Bucket\nand\nFolder\n.\nScenario 2: Create a New Glue Catalog\nEnable 'Create New' button\nCreate Source Bucket:\nClick\n+ Create\nnext to\nS3 Source Bucket\n.\nDvSum recommends using the prefix\n\"dvsum-s3-source-\"\nto ensure a unique bucket name.\nEnter a valid bucket name (\nAWS Bucket Naming Rules\n) and click\nSave\n. The bucket will be created and automatically added to the dropdown list.\nHow to Create an AWS S3 Bucket?\nCreate Staging Bucket:\nClick\n+ Create\nnext to\nS3 Staging Bucket\n.\nUse the recommended prefix\n\"dvsum-s3-source-\"\nto create a unique bucket name.\nEnsure that the\nSource Bucket\nand\nStaging Bucket\nhave distinct names.\nEnter a valid bucket name and click\nSave\n. The bucket will be created and added to the dropdown list.\nSet Folders (Optional):\nSpecify folders for the\nS3 Source Bucket\nand\nS3 Staging Bucket\nif required.\nFiles within the specified folder in the\nSource Bucket\nwill be cataloged.\nTemporary staging files will be created in the specified folder in the\nStaging Bucket\n.\nCreate Glue Database:\nClick\n+ Create\nnext to\nGlue Database\n.\nDefine a database name that complies with\nAWS Glue Database naming requirements\n(\nThe name must be in lowercase letters and cannot be longer than 255 characters\n) and provide a description.\nClick\nSave\n.\nHow to Create an AWS Glue Database?\nCreate Glue Crawler:\nClick\n+ Create\nnext to\nGlue Crawler\n.\nDefine a crawler name that follows AWS Glue Crawler naming requirements (\nThe name you choose can be up to 255 characters long, but certain special characters and symbols are not allowed.\n) and provide a description.\nClick\nSave\n.\nHow to Create an AWS Glue Crawler?\nNote:\nIf the action fails, the user should verify and check the following:\nCertain special characters are not supported in column names. Examples of unsupported characters include:\nComma separator in column names (e.g., A,B)\nControl characters (e.g., U+0000)\nEnsure that there are no carriage returns within the column data, and it should be presented in a single line.\nStep 3: Save Connection Information\nAfter the credentials are authenticated, scroll to the top of the page.\nClick the\nDone\nbutton in the top-right corner.\nClick\nSave\nto save the source connection.\nFinally, click\nTest Connection\nto verify the setup.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nStep 4: Scan the Source\nNavigate to the\nScan History Page\nand click the\n\"Scan Now\"\nbutton.\nA job will be created, and once its status shows\nCompleted\n, the scan for the new Amazon S3 source will be finished successfully.\nAfter the scan is complete, click on the\nScan Name\nto open the\nScan Summary\npage for this scan.\nOn the\nScan Summary\npage, you will see insights from the scan, including the number of new tables and columns retrieved from the database that was selected earlier.\nTo gain more insights into the details of the tables, click on\n\"Data Dictionary\"\nfrom the sidebar. A table listing view will appear. Then, click on the\n\"Recently Refreshed\"\ntab. This tab will display all the tables fetched in the recent scan. Click on the table names to view more details on the respective table's detail page.",
    "scraped_at": "2026-02-02 15:27:15"
  },
  {
    "title": "Configure Netezza as a Data Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/31133806684948-Configure-Netezza-as-a-Data-Source",
    "content": "In this Article:\nOverview\nPre-Requisites\nDetailed Steps\nStep 1: Add Netezza as a Source\nStep 2: Configure Connection\nStep 3: Save Connection Information\nStep 4: Scan the Data Source\nOverview:\nThis article outlines the steps required to configure Netezza as a source in DvSum Data Intelligence (DI). It details the process of adding and authenticating the Netezza source, as well as running scans to retrieve data.\nPre-Requisites\nNetezza Performance Server Deployment:\nIBM Cloud or On-Premises\nServer Status:\nNetezza server must be operational\nSAWS Version:\n1.2.9 or above\nRequired Permissions for System Tables:\n_V_TABLES\n_V_RELATION_COLUMN\n_V_DATATYPE\n_V_RELATION_KEYDATA\n_V_DATSBASE\n_V_SCHEMA\nEnabling General Query Log for MySQL\nBefore configuring MySQL as a source, ensure that the general query log is enabled for your MySQL server. This is essential for tracking data lineage and obtaining insights into usage patterns. For more information, refer to the\nEnabling General Query Log for Data Sources\narticle.\nDetailed Steps\nStep-by-Step Configuration\nStep 1: Add Netezza as a Source\nNavigate to Administration > Data Sources > âŠ• Add Source.\nSelect Netezza as the source type.\nProvide a name for the source and click Save.\nStep 2: Configure Connection\nAfter saving, you will be redirected to the connection settings page\nEnable the checkbox for On-premise Web Service and select the Gateway, or use DvSum Web Services (default).\nNote:\nFor more information regarding Cloud Gateway, click\nhere.\n3. Enter the required Host, Port, DB Login, and Password.\n4. Click Authenticate to verify the connection.\nNote:\nFor more information regarding On-premise Web Service installation, click\nhere\n.\n5. After authentication, the Database section will appear, allowing you to select the database forÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  scanning.\n6. Choose whether to scan all schemas or limit the scan to specific schemas.\nIf limiting to specific schemas, select one or multiple schemas from the Available Schemas list and move them to Selected Schemas.\nWhen the option to limit to specific schemas is selected, a list of available schemas will be displayed. The user can choose one or multiple schemas from the Available Schemas list and move them to the Selected Schemas tab on the right.\nStep 3: Save Connection Information\nAfter authentication and schema selection, scroll to the top.\nClick Done > Save.\nClick Test Connection to verify the setup.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nStep 4: Scan the Data Source\nNavigate to the Scan History Page.\nClick Scan Now to initiate a job.\nAfter the scan is complete, click on the Scan Name to open the\nScan Summary\npage. This page will display all the insights from the scan, including the number of new tables and columns retrieved from the schemas selected earlier.\nNext, analyze the tables and columns discovered during the scan by navigating to \"Data Dictionary\" in the left sidebar. The table listing view will appear. Click on the \"Recently Refreshed\" tab to view the tables identified in the recent scan. Click on the table names for more details.",
    "scraped_at": "2026-02-02 15:27:20"
  },
  {
    "title": "Configure IBM Db2 as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/29763482203028-Configure-IBM-Db2-as-a-Source",
    "content": "In this Article:\nOverview\nPrerequisites\nStep-by-Step Configuration\nStep 1: Add IBM Db2 as a Source\nStep 2: Configure the Connection\nStep 3: Save the Connection\nStep 4: Scan the Data Source\nBrowsing the Data Catalog\nTroubleshooting & FAQs\nAdditional Resources\nOverview\nThis article describes the steps needed to configure IBM Db2 as a source in DvSum Data Insights (DI). The same steps apply to configure a source in DvSum Data Qualtiy (DQ) with only a slight variation.\nPrerequisites\nIBM DB2 configuration\nUser\n: Create a user to be used in DvSum, or identify an existing user.\nTable permissions\n: Grant read-only access to the user for schemas and tables that you would like to catalog and profile.\nSystem table permissions:\nGrant read-only access to the user for the following system tables:\nSYSIBM.SYSDUMMY1\nSYSCAT.SCHEMATA\nSYSCAT.TABLES\nSYSCAT.COLUMNS\nSYSCAT.KEYCOLUSE\nSYSCAT.REFERENCES\nSYSCAT.TABCONST\nStep-by-Step Configuration\nStep 1: Add IBM Db2 as a Source\nNavigate to: Administration â†’ Data Sources â†’ Add Source (+).\nSelect: IBM Db2.\nEnter a Name for the source.\nClick Save.\nStep 2: Configure the Connection\nOnce the source is saved, you will be redirected to the connection settings detail page.\nMost common\n: Connect Using DvSum Web Service\nWhitelist the DvSum application by IP address as indicated. Then enter the required connection information:\nHostname\nPort\nDatabase\nDB Login & Password\nAlternative\n: You may\ninstall a DvSum Edge Gateway\nbehind your firewall. Then connect to your IBM Db2 instance using this web service installed on premises. Read more details in\nDvSum Web Service vs On-Premises Edge Gateway\n.\nAfter entering the credentials, Authenticate the source.\nOnce the source is Authenticated, the Database section will appear.\nSelect the schemas which you want to catalog.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\n3. Save the Connection\nScroll up and click the Done button.\nClick Save to finalize the configuration.\n4. Scan the data source\nGo to the Settings page or Scan History.\nClick Scan Now.\nWhen the scan completes, the status will change to \"Completed\".\nAfter the scan completion, click on Scan Name (SCN-001071 in the above example) and it will open the Scan Summary page of this scan.\nThe Scan Summary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this scan.\nYour IBM Db2 connection is now fully configured and functional.\nBrowsing the Data Catalog\nTo analyze discovered tables and columns:\nNavigate to the Data Dictionary from the left sidebar.\nClick on the Recently Refreshed tab.\nSelect a table name to see its details\nTroubleshooting & FAQs\nCommon Issues & Solutions\nIssue\nPossible Solution\nConnection test fails\nEnsure the hostname, port, and firewall rules allow DvSum access.\nInvalid credentials error\nVerify the username and password. Ensure the user has read permissions.\nFAQs\nQ:\nCan I connect multiple IBM Db2 instances to DvSum?\nA:\nYes, repeat the steps to add additional sources.\nQ:\nHow do I verify if the connection is successful?\nA:\nRun a test query in IBM Db2 before connecting: SELECT * FROM SYSIBM.SYSDUMMY1;\nQ:\nWhat should I do if my scan doesnâ€™t return any tables?\nA:\nEnsure the user has access to the required schemas and tables.\nAdditional Resources\nRead more about\nDvSum Web Service vs On-Premises Gateway\nRead more about\nGateway Installation",
    "scraped_at": "2026-02-02 15:27:26"
  },
  {
    "title": "Data Governance Roles and Responsibilities",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41098426049172-Data-Governance-Roles-and-Responsibilities",
    "content": "In this Article\nOverview\nGovernance Roles\nData Owner\nData Steward\nApprover\nSystem Roles & Permissions\nAdmin\nEditor\nUser / Viewer\nOverview\nThis article outlines clear roles and responsibilities to ensure proper governance, accountability, and data quality management. These roles define who is responsible for data, how it is managed, and the level of system access each user has.\nGovernance Roles\nData Owner\nResponsible for data definition, usage, and compliance.\nEnsures data follows company standards and policies.\nMonitors and manages data quality issues.\nSets data rules and access policies.\nUltimately accountable for the correct and proper use of data.\nData Steward\nMaintains day-to-day data quality within assigned domains.\nEnsures glossary terms and definitions are accurate.\nEdits and updates terms in the glossary.\nMonitors and fixes data quality issues.\nWorks with system owners to make technical updates or corrections.\nApprover\nReviews changes made by Data Stewards.\nCompares changes to validate modifications.\nApproves or rejects terms submitted by Stewards.\nApproves multiple terms at once using Mass Update.\nMonitors term activity history via audit trail.\nNote:\nFor more details, refer to\nApproving Business Glossary Terms\n.\nSystem Roles & Permissions\nAdmin\nFull access to all modules and settings, including access to the\nAdministration tab\n.\nCan create, edit, and delete user roles.\nConfigures fine-grained access, governance workflows, and module settings.\nManages data sources, tables, and glossary terms.\nEditor\nCan view and edit specific datasets, tables, or glossary terms assigned via Fine-Grained Control.\nCan submit changes for approval if workflow is enabled.\nCannot change user roles or global module settings.\nAccess to the\nAdministration tab\nis restricted.\nUser / Viewer\nCan only view assigned datasets, tables, or glossary terms.\nCannot edit or approve changes.\nAccess is restricted by Fine-Grained Control and module settings.\nNote:\nFor more details, refer to\nUser Role and Module Settings\n.",
    "scraped_at": "2026-02-02 15:27:30"
  },
  {
    "title": "Default Roles Access Matrix",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/32742836074516-Default-Roles-Access-Matrix",
    "content": "Understanding Default User Roles in DvSum Data Insights\nManaging user roles and permissions is essential for maintaining security, efficiency, and collaboration within\nDvSum Data Insights\n. Our\nUser Roles Access Matrix\nprovides a comprehensive overview of the permissions assigned to each default roleâ€”Owner, Admin, Editor, and Userâ€”across various modules, including Administration, Data Dictionary, Field Dictionary, and more.\nThis matrix is a valuable resource to help you understand how each role interacts with different features and actions, ensuring users have the appropriate access to perform their tasks effectively. You can explore the matrix in detail by downloading the Excel file provided.",
    "scraped_at": "2026-02-02 15:27:35"
  },
  {
    "title": "User Role and Module Settings",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/17327470244244-User-Role-and-Module-Settings",
    "content": "In this article:\nIntroduction\nHow to Add a New User Role\nFine-Grained Control (Admin)\nHow Section-Level Permissions Work\nFine-Grained Control (System Catalog)\nFine-Grained Control (Business Glossary)\nConfiguring Module Settings in DvSum\nBest Practices for Assigning Roles and Configuring Module Settings\nLink Between User Roles and Module Settings\nIntroduction\nWith\nDvSum Data Intelligence\n, users can access specific datasets and glossary terms. In environments with numerous tables and terms, granting individual access is crucial. Simply instructing users to avoid certain tables isnâ€™t a practical approach.\nWhen assigning specific access levelsâ€”such as \"View\" for some tables or terms and \"Edit\" for othersâ€”it can become complex. However, DvSumâ€™s\nUser Roles\nand\nFine Grain\nsettings enable administrators to precisely control access. They can define whether a user has viewing or editing privileges for each table or term.\nAnother key feature, alongside\nUser Roles\n, is\nModule Settings\n. Users can configure governance and workflow settings for different data domains or sub-domains. Additionally, at the table level, they can enable or disable features like\nChat with Data\nand\nData Quality\n. This article will provide further details on these capabilities.\nHow to Add a New User Role\nGo to Administration Tab and then Users Management > User Roles\nClick on User Roles.\nClick Add Role and enter a name for the new role.\nAssign a user group to the role (this group will define which users inherit the roleâ€™s permissions).\nConfigure access settings under Admin, System Catalog, and Business Glossary Tabs.\nClick Save Changes.\nFrom the Admin tab, users can select Admin permissions, which include options for No Admin, Admin, or custom permissions through fine-grained control.\nFine-Grained Control (Admin)\nDvSum allows administrators to assign\nNo Access\n,\nView\n, or\nEdit\npermissions to various sections of the Administration Panel. These permissions help tailor access for different user groups based on their responsibilities.\nHow Section-Level Permissions Work\nSome sections in the Administration Panel represent a group of related features. When you assign a permission level to such a section,\nthat level is automatically applied to all features within it\n.\nExample: Jobs Section\nThe\nJobs\nsection includes:\nJob Definition\nJob Execution\nIf you assign\nView\naccess to the\nJobs\nsection, users will be able to view both\nDefinition\nand\nExecution\n. Similarly, selecting\nEdit\nwill allow users to modify both.\nThis ensures consistent access control across related features without needing to configure each one individually.\nPermissions can be configured directly from the\nSystem Catalog\nand\nBusiness Glossary\ntabs.\nFine-Grained Control (System Catalog)\nFine Grain allows further control over Data Sources and Tables inside them for Catalog and the same goes for Terms in the Glossary. Imagine you're an Admin, and a colleague wants to edit a Snowflake catalog. With Fine Grain control, you can grant them edit permissions for that specific dataset.\nIn the above example, edit permissions will be granted to this user role only for the enabled data source. The admin can decide the level of access he wants to give, he can either give \"View & Chat\" or \"Edit\" Permissions. Now admin can further filter the data source by using the Table Exclusion criteria below.\nNote: There are two options for Table Exclusion criteria which are shown above.\nLet us say that in the data source to which you have given access, you do not want to give this user role access to the tables which have sensitive tags then you will need to mention the criteria like this:\nThis change would have reflected like this:\nOn the Admin account:\nThe both highlighted tables above won't be showing in my user account which will have the user role settings that are mentioned above because these tables have Sensitive tags. On the User account, it will show like this:\nHere not only did we restrict the access of this user to specific Tables but we further restricted the edit access of this user. Now this user can make edit changes on any of the tables.\nOne Important thing is that Fine Grain Control will only be available if the permission above selected is either Editor or Viewer, otherwise, Fine Grain Control won't show on the UI.\nFine-Grained Control (Business Glossary)\nImagine you're a Glossary admin with access to all domains and sub-domains. A glossary editor requests access to specific domains and terms within them, but you only want to display terms in this domain with the status \"Published.\" Fine Grain Control allows you to achieve this level of precision.\nThis can be achieved as follows:\nIn this instance, Edit Access is granted to the \"Securities\" domain and its sub-domains, along with the terms inside it. An additional filter is applied, ensuring that only terms with the status \"Published\" can be edited.\nFor Example, on the Admin Account, we have three terms that are not published:\nNow the User who will have the user role settings above won't be able to see the highlighted terms because they are fulfilling the exclusion criteria. Here is the view of User account:\nNote:\nFor complete details on user roles and responsibilities, refer to the\nData Governance Roles and Responsibilities\narticle\nConfiguring Module Settings in DvSum\nModule Settings control governance workflows, Chat with Data, and Data Quality features at a global level. These settings affect all users in the organization.\nHow to Access Module Settings\nGo to Administration > Account Settings > Module Settings.\nChoose from the three available module settings:\nGovernance\nChat with Data\nData Quality\nEnable or disable these settings as needed.\n1. Governance\nIn the\nGovernance\ntab, activating the\nData Governance\ncheckbox enables the workflow for domains and sub-domains. If this checkbox is disabled, the workflow option for a domain or sub-domain becomes inaccessible.\nSimilar to\nUser Roles\n,\nFine Grain Control\nin\nModule Settings\nallows for applying specific filters. When\nFine Grain\nis enabled, the\nDefault\nsettings appear with two options. If set to\nEnabled\n, any newly added domain will automatically have its workflow activated, removing the need for manual configuration.\nConsider this example: Governance is enabled for the domain\n\"Default\"\nand all terms within it. However, terms tagged as\n\"Restricted\"\nwill remain excluded from governance.\nNow once the governance is enabled user can enable workflow from the Data Domain or Sub domain:\nAfter the workflow is enabled every term in this domain will have governance enabled except for the term which has the tag \"Restricted\" as mentioned in the exclusion criteria:\nAnd the existing Term which has the exclusion criteria won't have governance enabled:\n2. Chat with Data\nWhen enabled, Chat with Data allows users to interact with datasets using AI-powered agents.\nHow to Configure Chat with Data\nNavigate to Module Settings > Chat with Data.\nEnable the Chat with Data Checkbox.\nSelect specific data sources for Chat with Data.\nUse Fine-Grained Control to exclude specific tables or columns (e.g., exclude columns tagged as \"Sensitive\").\nClick Save Changes.\nBelow the Default settings, the user can enable Chat with Data for specific tables. For example, if the user wants to enable agent for one specific source then that source can be moved to Enabled:\nThis can be further filtered by adding Table exclusion criteria or Column exclusion criteria. For Example if the user does not want to include some tables in agents that have the \"Restricted Tag\" and columns that have the \"Sensitive\" tag then these tables and columns will not be available on the agents of that particular source.\nSo the expected behavior would be that all the tables inside the Datasource \"Snowflake_4th_August\" will be included in agents except for the Tables that will have the tag \"Restricted\". Now here is an example of a table \"Dwh.Nyc City Bikes\" that has the exclusion criteria:\nAnd for columns that are excluded from the Agents will be excluded from the chat and shown like this:\n3. Data Quality\nWhen enabled, the\nData Quality\nmodule facilitates monitoring of data integrity and quality scores.\nHow to Enable Data Quality Monitoring\nNavigate to Module Settings > Data Quality.\nEnable the Data Quality Checkbox.\nSelect specific data sources for Data Quality monitoring.\nUse Fine-Grained Control to exclude specific tables or columns (e.g., exclude columns tagged as \"Sensitive\").\nClick Save Changes.\nBelow are the Default settings, the user can enable Data Quality for specific tables. For example, if the user wants to enable the Data quality tab for one specific source then that source can be moved to Enabled:\nThis can be further filtered by adding Table exclusion criteria or Column exclusion criteria. Like if the user wants to disable the DQ tab for tables that have the \"Restricted Tag\" and columns which have the \"Sensitive\" tag should be disabled in the chat.\nSo the expected behavior would be that all the tables inside the Datasource \"Snowflake_DWH\" will have the Data Quality tab enabled except for the Tables which will have the tag \"Restricted\":\nColumns that are not included in the Data Quality tab will simply not appear under the \"Details\" heading on the Data Quality tab:\nBest Practices for Assigning Roles and Configuring Module Settings\nAssign Admin roles only to necessary personnel to prevent unauthorized system modifications.\nUse Viewer roles for non-technical users who only need read-only access.\nEnable Fine-Grained Control to give selective permissions rather than broad access.\nRegularly review role assignments to ensure users have the correct level of access.\nUse module settings strategically to enable governance, Chat with Data, and Data Quality only where necessary.\nLink Between User Role and Module Settings\nUser role settings are specific to individual users, while module settings apply globally, affecting all associated users and the admin account. For instance, if the admin grants View and Chat access for certain sources in user roles, enabling Chat with Data in module settings is necessary to activate Chat topics. This establishes a kind of parent-child relationship between Module settings and User roles, but they can also function independently based on specific requirements.",
    "scraped_at": "2026-02-02 15:27:40"
  },
  {
    "title": "Manage Users",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/203082210-Manage-Users",
    "content": "In this Article\nOverview\nAdding New Users\nSteps to Add a New User\nEditing and Deactivating Users\nOverview\nDvSum provides administrators and owners with a robust set of features for adding and managing users. The\nUsers\npage displays a list of existing users along with key details, including\nEmail, User Group, Status, Modified By, Modified On, Expiration Date,\nand\nLast Login.\nThis guide is designed for administrators responsible for managing user access and permissions in DvSum.\nAdding New Users\nWhen adding a new user, administrators can define the\nStart Date\nand\nExpiration Date\nfor their account. These settings cannot be modified by the user:\nThe\nStart Date\ndetermines when the user gains access to the application.\nThe\nExpiration Date\nrestricts access after the specified date unless an administrator extends it.\nBy default, the\nStart Date\nis set to the current date, and the\nExpiration Date\nis set one year later. Both fields can be edited by an administrator.\nSteps to Add a New User\nNavigate to\nAdministration > User Management > Users\n.\nClick\nAdd User\n.\nEnter the user's\nEmail\nand assign a\nUser Group\n.\nSet the\nStart Date\nand\nExpiration Date\n.\nClick\nSave\nto complete the process.\nNote:\nBy default, the \"Start Date\" is set to the current date, and the \"Expiration Date\" is set to one year later. Both of these fields are editable.\nEditing and Deactivating Users\nUser access is restricted to the period between their designated\nStart Date\nand\nExpiration Date\n. Administrators can manage user access through the\nActions\nmenu, which offers the following options:\nDeactivate/Activate Users\nâ€“ Temporarily disable or restore a userâ€™s access.\nDelete Users\nâ€“ Permanently removes a user, updating their status to\nDeleted\n.\nReactivating a Deleted User\nâ€“ If a deleted user is reactivated, they are treated as a newly added user and must be reconfigured accordingly.",
    "scraped_at": "2026-02-02 15:27:45"
  },
  {
    "title": "SSL Certificate Types Supported for Gateway Installation",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/45315196278804-SSL-Certificate-Types-Supported-for-Gateway-Installation",
    "content": "Overview\nDuring Gateway installation, an SSL certificate is used to secure communication between the Gateway and other systems. DvSum supports two primary SSL certificate types: self-signed certificates and CA-signed certificates.\nThis article explains the difference between these certificate types, when each is typically used, and how they relate to the SSL configuration options available during Gateway installation.\nCertificate Types at a Glance\nGateway SSL configuration is based on how the certificate is trusted.\nThe two supported trust models are\nself-signed certificates\nand\nCA-signed certificates\n.\nThe SSL options available during installation define how the certificate and keystore are created or reused, rather than introducing different certificate types.\nSelf-Signed Certificates\nA self-signed certificate is generated by the Gateway itself during installation and is not issued by a trusted Certificate Authority.\nWhen this option is used\nSelf-signed certificates are commonly used when deploying non-production or test environments, running the Gateway\nwithin an internal or controlled network,\nor when external trust by browsers or third-party systems is not required.\nCharacteristics\nGenerated automatically during installation.\nNot trusted by browsers or external systems by default.\nMay require manual trust configuration if accessed externally.\nRelated Gateway installation option\nThis certificate type is generated when selecting\nGenerate a new certificate\noption during the\ngateway installation\n.\nCA-Signed Certificates\nA CA-signed certificate is issued by a trusted Certificate Authority, either internal or public. This option is typically used for production or externally accessible environments.\nWhen this option is used\nCA-signed certificates are commonly used when deploying the Gateway in production, when external systems or browsers must trust the Gateway, or when security or compliance requirements apply.\nCharacteristics\nTrusted by browsers and external systems.\nIssued by a Certificate Authority.\nRequires an existing certificate and corresponding private key.\nRelated Gateway installation options\nCA-signed certificates are used when selecting\nCreate a new keystore with your certificate\n,\nUse an existing keystore\n, or\nAdd certificates to an existing keystore\noption during\ngateway installation\n. These options allow a trusted certificate to be supplied or reused during installation.\nHow This Relates to Gateway Installation\nThe Gateway installer presents multiple SSL configuration options. These options control how certificates are generated, stored, or reused during setup.\nFrom a certificate perspective, the configuration always falls into one of two categories: using a self-signed certificate generated by the Gateway or using a CA-signed certificate provided during installation.\nRecommended Approach\nIn general, self-signed certificates are suitable for development or internal testing, while CA-signed certificates are recommended for production or externally accessible deployments.\nSelecting the appropriate certificate type helps ensure secure communication and aligns with deployment and security requirements.",
    "scraped_at": "2026-02-02 15:27:50"
  },
  {
    "title": "Gateway Upgrade Process",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/30864921622676-Gateway-Upgrade-Process",
    "content": "In this Article\nDetailed Steps for Gateway Upgrade\nPreparation\nConfiguration\nInstallation\nNotes on Certificate Handling\nDetailed Steps for Gateway Upgrade\n1. Preparation\nStep 1: Download the New Gateway Version\nDownload the latest gateway version.\nDvSum Gateway Installer Links\n.\nStep 2: Uninstall the Current Gateway Service\nBefore installing the new gateway, uninstall the existing version to prevent conflicts. Use the following command to uninstall the service:\n# Windows\n.\\dvsum_gateway_setup.bat uninstall\n# Linux\n./dvsum_gateway_setup.sh uninstall\nStep 3: Verify Uninstallation\nEnsure the service is no longer running:\n# Windows\n.\\dvsum_gateway_setup.bat status\n# Linux\n./dvsum_gateway_setup.bat status\nStep 4: Backup the Previous Version\nMove the current gateway folder to create a backup:\n# Windows\nRename the existing dvsum_gateway directory to <dvsum_gateway_1.2.3>\n# Linux\nmv\ndvsum_gateway/ dvsum_gateway_1.2.3/\n2. Configuration\nWindows\nStep 1: Set up a New Folder for the Latest Gateway\nCreate a new folder for the latest gateway files.\nStep 2: Extract the Gateway Files\nExtract the downloaded .zip file to the new folder. Avoid using the Downloads folder.\nStep 3: Migrate Previous Settings\nFor Gateway version\n1.3.8 and earlier\nYou can\ncopy the configuration and SSL files directly\nfrom the old gateway folder to the new one:\nconfiguration.properties\nssl_config.properties\nIf you are using a CA-signed certificate, also transfer the relevant certificate or keystore files:\ncp\ndvsum_gateway_1.2.3/my_domain.jks dvsum_gateway_1.2.4/my_domain.jks\nFor Gateway version\n2.0.0 and above\nDo\nnot\ncopy the configuration files from an older version (e.g.,\n1.x.x\n).\nInstead, manually create new\nconfiguration.properties\nand\nssl_config.properties\nfiles and populate them using the updated format.\nNote:\nConfiguration files can only be copied when upgrading\nwithin the same minor version range\n(e.g., from 2.0.x to another 2.0.x version).\nStep 4: Migrate existing scan and chat information\nCopy the data_analysis folder from the old installation to the new one.\nCopy the dq_analysis folder from the old installation to the new one.\nCopy the scan_output folder from the old installation to the new one.\nNot all installations have all of these folders. Copy the folders that you have, but it's fine if you do not have all of them.\nLinux\nStep 1: Set up a New Folder for the Latest Gateway\nCreate a new folder for the latest gateway installation:\nmkdir\ndvsum_gateway\nStep 2: Extract the Gateway Files\nExtract the .tar.gz file into the newly created folder:\ntar -xzf dvsum_gateway_latest.tar.gz -C dvsum_gateway/\nStep 3: Migrate Previous Settings\nFor Gateway version\n1.3.8 and earlier\nYou can\ncopy the configuration and SSL files directly\nfrom the old gateway folder to the new one:\nconfiguration.properties\nssl_config.properties\nIf you are using a CA-signed certificate, also transfer the relevant certificate or keystore files:\ncp\ndvsum_gateway_1.2.3/my_domain.jks dvsum_gateway_1.2.4/my_domain.jks\nFor Gateway version\n2.0.0 and above\nDo\nnot\ncopy the configuration files from an older version (e.g.,\n1.x.x\n).\nInstead, manually create new\nconfiguration.properties\nand\nssl_config.properties\nfiles and populate them using the updated format.\nNote:\nConfiguration files can only be copied when upgrading\nwithin the same minor version range\n(e.g., from 2.0.x to another 2.0.x version).\nStep 4: Migrate existing scan and chat information\nCopy the data_analysis folder from the old installation to the new one.\nCopy the dq_analysis folder from the old installation to the new one.\nCopy the scan_output folder from the old installation to the new one.\nNot all installations have all of these folders. Copy the folders that you have, but it's fine if you do not have all of them.\n3. Installation\nWindows\nAfter setting up the new gateway and transferring the necessary files, install the gateway:\n.\\dvsum_gateway_setup.bat install --ssl-option=4 --non-interactive\nNext, start the gateway:\nOpen PowerShell in Administrator mode.\nNavigate to the gateway folder:\ncd\nc:\\dvsum_gateway\nRun the install command:\n.\\dvsum_gateway_setup.bat install\nLinux\nTo complete the installation and start the gateway on Linux:\n./dvsum_gateway_setup.sh install --ssl-option=4 --non-interactive\nNotes on Certificate Handling\nFor\nself-signed certificates\n, if you donâ€™t need to retain previous settings, you can follow the general installation steps in the Gateway Installation article.\nFor\nCA-signed certificates\n, itâ€™s recommended to reuse the same keystore to avoid reconfiguring the private key and related settings. Simply copy the keystore from the previous installation to the new gateway folder to maintain secure communication.\nFinal Remarks\nEnsure that\nconfiguration.properties\nand\nssl_config.properties\nare correctly migrated/configure.\nKeeping a backup of the previous version is recommended to avoid potential issues with SSL configurations or other settings.\nFor CA-signed certificates, reusing the existing keystore simplifies the process, avoiding the need to track down the private key.",
    "scraped_at": "2026-02-02 15:27:54"
  },
  {
    "title": "DvSum Gateway Installer Links",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/29756078237460-DvSum-Gateway-Installer-Links",
    "content": "In this Article\nDI Platform Download Links\nDQM Platform Download Links\nDI Platform Download Links\nLatest Version (v. 2.2.0):\nDvSum Data Intelligence Gateway Windows Installer\nDvSum Data Intelligence Gateway Linux Installer\nOlder Versions:\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.8)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.8)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.7)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.7)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.6)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.6)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.5)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.5)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.4)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.4)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.3)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.3)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.2)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.2)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.1)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.1)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.1.0)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.1.0)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.7)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.0.7)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.6)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.0.6)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.5)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.0.5)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.4)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.0.4)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.3)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.0.3)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.2)\nDvSum Data Intelligence Gateway Windows Installer(v. 2.0.2)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.1)\nDvSum Data Intelligence Gateway Windows Installer (v. 2.0.1)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.0)\nDvSum Data Intelligence Gateway Windows Installer (v. 2.0.0)\nDvSum Data Intelligence Gateway Linux Installer(v. 2.0.0)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.8)\nDvSum Data Intelligence Gateway Linux Installer(v. 1.3.8)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.7)\nDvSum Data Intelligence Gateway Linux Installer(v. 1.3.7)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.6)\nDvSum Data Intelligence Gateway Linux Installer(v. 1.3.6)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.5)\nDvSum Data Intelligence Gateway Linux Installer(v. 1.3.5)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.4)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.3.4)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.3)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.3.3)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.2)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.3.2)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.1)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.3.1)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.3.0)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.3.0)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.2.9)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.2.9)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.2.8)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.2.8)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.2.7)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.2.7)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.2.6)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.2.6)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.2.5)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.2.5)\nDvSum Data Intelligence Gateway Windows Installer (v. 1.2.4)\nDvSum Data Intelligence Gateway Linux Installer (v. 1.2.4)\nDQM Platform Download Links\nLatest Version (v. 3.2.3):\nDvSum Data Quality Gateway Installer\nOlder Versions:\nDvSum Data Quality Gateway Installer (v. 3.2.2)\nDvSum Data Quality Gateway Installer (v. 3.2.1)",
    "scraped_at": "2026-02-02 15:27:59"
  },
  {
    "title": "Gateway Installation",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/29188280659988-Gateway-Installation",
    "content": "In this Article\nOverview\nDownload Links\nPrerequisites (all platforms)\nNetwork Configuration\nDefine a Gateway\nWindows Installation\nNon-interactive Installation\nInteractive Installation\nSSL Configuration Option\nUninstall the Gateway\nLinux Installation\nDownload the installer\nPython\nNon-interactive Installation\nInteractive Installation\nAppendix\nUsing Port 443\nOverview\nThis document outlines the steps for installing the DvSum Gateway for use with Data Intelligence Platform. It provides detailed instructions for both Windows and Linux systems, including configuration and installation procedures.\nNote: Gateway, Connector, SAWS (Stand-Alone Web Service) and webservice are used interchangeably referring to the DvSum Edge Gateway.\nDownload Links\nDvSum Data Catalog Gateway Windows Installer\nDvSum Data Catalog Gateway Linux Installer\nPrerequisites (all platforms)\nMinimum memory: 8GB (16GB recommended)\nMinimum disk space: 10GB (20GB recommended)\nCPU cores: 2 (4 recommended)\nPython 3.x must be installed for the gateway to function properly.\nLinux: Ensure that your Linux distribution has systemd support and x86_64 server architecture.\nWindows: Windows Server Edition 2012 or later\n# Confirm Architecture\nlscpu\n# Expected output includes:\n# Architecture: x86_64\nNetwork Configuration\nIf the DvSum Gateway is installed on a server with restricted outside access to specific URLs, you'll need to whitelist the following addresses on the network. This will enable the DvSum Gateway to communicate with the DvSum SaaS application and ensure the service functions properly and securely connects to the necessary resources on the network. Please configure the network settings accordingly to allow for this communication.\nURLs that must be whitelisted for outbound access on port 443\nhttps://apis.dvsum.ai\nwss://3k7tif2dyi.execute-api.us-west-2.amazonaws.com/prod\nhttps://dvsum-app-data-prod.s3.amazonaws.com\nPorts that must be open for inbound access from end user computers\nDefault: 8183\nCommon alternative: 443\nTesting network configuration\nThe installer includes an option to test the network configuration.\nLinux:\n./dvsum_gateway_setup.sh --check-network\nWindows:\n.\\dvsum_gateway_setup.bat --check-network\nDefine a Gateway\nYou must define a gateway in the DvSum GUI defining the details that you will use when you install the gateway. Navigate to Administration â†’ Account Settings â†’ Gateway.\nClick on \"Add\" Button as shown in the image below. Provide the Web Service name, Host Name and Port (default 8183) and save it.\nThis defines a Gateway with a unique communication key by which the gateway communicates with the DvSum web application.\nKeep track of the communication key; it will be required later during the installation.\nWindows Installation\nDownload the installer\nUse the link provided above, or the installer can also be downloaded directly through the Download button available in the GUI.\nNote:\nOnce the installer is downloaded, extract it and move it to the desired folder.\nDefine a Gateway\nYou must define a gateway in the DvSum GUI defining the details that you will use when you install the gateway. Navigate to Administration â†’ Account â†’ Gateway.\nClick on \"Add\" Button as shown in the image below. Provide the Web Service name, Host Name and Port (default 8183) and save it.\nThis defines a Gateway with a unique communication key by which the gateway communicates with the DvSum web application.\nKeep track of the communication key; it will be required later during the installation.\nLaunch PowerShell\nNavigate to Start, look for Windows PowerShell, then right-click and select \"Run as Administrator.\"\nNavigate in PowerShell to the gateway folder\n# In this example, the gateway was unzipped in c:\\dvsum_gateway\ncd c:\\dvsum_gateway\ndir\nNon-interactive Installation\nThe non-interactive mode proceeds automatically without the need for manual input during the process. Instead, you enter all relevant information into the two properties files before running the installation.\nEdit\nconfiguration.properties\nSet the value for\nAPI Key\nConfirm the value for\nPort\nConfirm the value for\nPython Port\nReview all other properties â€” in most cases, no updates are needed.\nversion 1.3.8 or previous\n- api.key (copy from app.dvsum.com -> Account -> Gateway -> copy communication key of your gateway)\n- port (default value: 8183)\n- python.service.port (default value: 8185)\nversion 2.0.0 or later\n- dvsum.apiKey (copy from app.dvsum.com -> Account -> Gateway -> copy communication key of your gateway)\n- dvsum.port (default value: 8183)\n- dvsum.pythonServicePort (default value: 8185)\nEdit\nssl_config.properties\nThe properties that you need to set in this file depend on the SSL option that you select when running the script.\nGenerate a cert (SSL Option 1): complete sections 1 and 2.\nAdd cert to existing keystore (SSL Option 2): complete sections 2 and 3.\nAdd cert to new keystore (SSL Option 3): complete sections 2 and 3.\nUse existing keystore (SSL Option 4): complete section 2.\nComments on section 2:\nKeystore Configuration\nThese settings define how the Java keystore is configured for SSL support.\nFor Gateway version\n1.3.8 and earlier\n# Settings for the Java keystore that will hold the SSL certificates\nkeystore_path= # Typically left blank. It will be named based on the domain_name.\nkeystore_pass= # Use a strong password.\nkey_store_type= # Typically left blank. Defaults to JKS if not specified.\nalias= # Any name can be used. dvsum-gateway-cert is a good descriptive value.\nFor Gateway version\n2.0.0 and above\n# 2. Keystore Configuration\n# ----------------------------------------\n# Settings for the Java keystore that will hold the SSL certificates\ndvsum.keystorePath= # Typically left blank. It will be named based on the domain_name.\ndvsum.keystorePass= # Use a strong password.\ndvsum.keyStoreType= # Typically left blank. Defaults to JKS if not specified.\ndvsum.alias= # Any name can be used. A descriptive alias like dvsum-gateway-cert is recommended.\nLaunch the installer\nUse the following command to execute the gateway setup script and proceed with the installation and configuration of the DvSum Gateway automatically, without requiring any user interaction.\n# In this example, Generate a cert (SSL Option 1) is selected.\n.\\dvsum_gateway_setup.bat --ssl-option=1 --non-interactive\nNote: On Windows, Python needs to be installed for the python service to be installed successfully.\nValidate the Gateway\nFollow these steps to verify if the installation succeeded.\n. Verify that the Gateway is accessible at: https://<your-domain>:<port>\n. Check the logs at ./logs/SAWS.log for any important messages\nInteractive Installation\nIn an interactive installation the installer will prompt you for all options and all properties that you need to provide.\nLaunch the installer\nUse the following command to access the gateway setup file and proceed with installing and configuring the DvSum Gateway.\n.\\dvsum_gateway_setup.bat\nThe help command displays all the necessary commands required to operate with the gateway.\n# Display detailed help\n.\\dvsum_gateway_setup.bat help\nThe API key and the Port Name are required, these must match the properties you used when defining the gateway above.\nSSL Configuration Option\nFour different options will be offered for SSL configuration. You will choose one of these options. All four options are explained below.\nGenerate a new certificate\nChoice: 1\nIf you do not have an SSL certificate already, choose this option. This is the most common option for new users.\nIt will prompt you for all details needed to generate a certificate. Then it will generate a self-signed certificate that you can use immediately as well as a certificate signing request (CSR) that you can use to obtain a CA-signed certificate later.\nAdd certificates to an existing keystore\nChoice: 2\nUse this option when you have a DvSum keystore from a previous installation and you have an updated certificate that you want to use.\nYou will be prompted for the keystore location and password as well as the certificate location.\nHow to Use Your CA-Signed Certificate:\n1. A CSR file is utilized to obtain a CA-signed certificate from a Certificate Authority, such as GoDaddy or Digicert.\n2. Once obtained, you will receive a folder containing the certificate, which includes four files as illustrated.\n3. Copy these files and place them in the DvSum folder.\n4. Once copied, open PowerShell and select option 2.\n5. Provide the path to the keystore (obtained in step 1).\nAfter successful execution, the results can be observed by browsing to the gateway.\nCreate a new keystore with your certificate\nChoice: 3\nUse this option if you have a certificate and private key. The script will create a new keystore and save your certificate in the keystore.\nUse an existing keystore with certificate and private key\nChoice: 4\nUse this option if you already have a keystore fully configured with the appropriate SSL certificate. The script will update the DvSum gateway to use this keystore.\nAll the information of the keystore is displayed and the user is asked if it's the correct keystore, configuration is completed accordingly.\nUninstall the Gateway\nUse the following command to uninstall the gateway:\n# Uninstall the gateway\n.\\dvsum_gateway_setup.bat uninstall\nNote:\nAfter uninstalling the previous gateway, proceed with the new dvsum_gateway_setup.bat file in order to install the latest gateway.\nLinux Installation\nDownload the installer\nUse the link provided above. You can either download directly to the Linux server, or download to a desktop first and then upload to the Linux server. Your corporate firewalls and network policies will determine which is easier.\nDownload directly\nThe provided link redirects to the latest version of the download. As a best practice, it's good to determine what version of the installer you will download, and use that name when you save the file.\n# Simple curl command to see the redirected URL\n# Note the filename \"DvSum_1.2.3.tar.gz\" in the response\ncurl \"\nhttps://apis.dvsum.ai/admin/account/saws/download-gateway?env=linux&app=dc\"\n# response:\nhttps://dvsum-app-data-prod.s3.amazonaws.com/connector/linux/DvSum_1.2.3.tar.gz?AWSAccessKeyId=ASI..\n.\nNow that we have the filename \"DvSum_1.2.3.tar.gz\" from the command above,\n# Using the filename \"DvSum_1.2.3.tar.gz\" from the previous output\n# Run curl using the same URL as above but with these additional parameters\n# -L to follow the redirect\n# -o to specify the downloaded filename\ncurl -L -o DvSum_1.2.3.tar.gz \"\nhttps://apis.dvsum.ai/admin/account/saws/download-gateway?env=linux&app=dc\"\nDownload to desktop first\nIf you are unable to download directly to the Linux server, then you can download first to your desktop computer. From there upload to the Linux server using SCP or any other file transfer mechanism.\nUnzip the installer\n# Create a directory for the gateway\nmkdir /home/my_user/dvsum_gateway\n\n# Unzip the installer\n# In this example these directories are used:\n# /home/ubuntu/dvsum_installers\n# /home/ubuntu/dvsum_gateway\ncd /home/my_user/dvsum_gateway\ntar -vxzf ../dvsum_installers/DvSum_2.0.0.tar.gz\nPython\nPython3 must be installed. The installer will install the required packages. Administrators may optionally install these packages before running the installer.\n# Confirm Python version\n$ python --version\nPython 3.12.3\n\n# Install required packages manually (optional)\ncd /home/my_user/dvsum_gateway\npython3 -m venv ./python_service/venv\n./python_service/venv/bin/pip install --upgrade pip\n./python_service/venv/bin/pip install -r ./python_service/requirements.txt\nNon-interactive Installation\nThe non-interactive mode reads all relevant information from two properties files during the installation. This is the most common installation method.\nEdit\nconfiguration.properties\nSet the value for\napi.Key\nConfirm the value for\nport\nConfirm the value for\npython.service.port\nReview all other properties â€” in most cases, no updates are needed.\nNote:\nFor\nGateway version 1.3.8 and below\n, use the property names as listed above (\napi.Key\n,\nport\n,\npython.service.port\n).\nFor\nGateway version 2.0.0 and above\n, use the updated property names:\ndvsum.apiKey\ndvsum.port\ndvsum.pythonServicePort\nEdit\nssl_config.properties\nThe properties that you need to set in this file depend on the SSL option that you select when running the script.\nGenerate a cert (SSL Option 1): complete sections 1 and 2.\nAdd cert to existing keystore (SSL Option 2): complete sections 2 and 3.\nAdd cert to new keystore (SSL Option 3): complete sections 2 and 3.\nUse existing keystore (SSL Option 4): complete section 2.\nComments on section 2:\nKeystore Configuration\nThese settings define how the Java keystore is configured for SSL support.\n# 2. Keystore Configuration\n# ----------------------------------------\n# Settings for the Java keystore that will hold the SSL certificates\ndvsum.keystorePath= # Typically left blank. It will be named based on the domain_name.\ndvsum.keystorePass= # Use a strong password.\ndvsum.keyStoreType= # Typically left blank. Defaults to JKS if not specified.\ndvsum.alias= # Any name can be used. A descriptive alias like dvsum-gateway-cert is recommended.\nLaunch the installer\nUse the following command to execute the gateway setup script and proceed with the installation and configuration of the DvSum Gateway automatically, without requiring any user interaction.\n# In this example, Generate a cert (SSL Option 1) is selected.\n./dvsum_gateway_setup.sh --ssl-option=1 --non-interactive\nValidate the Gateway\nFollow these steps to verify if the installation succeeded.\n. Verify that the Gateway is accessible at: https://<your-domain>:<port>\n. Check the logs at ./logs/SAWS.log for any important messages\nInteractive Installation\nIn an interactive installation the installer will prompt you for all options and all properties that you need to provide.\nLaunch the installer\nUse the following command to access the gateway setup file and proceed with installing and configuring the DvSum Gateway.\n./dvsum_gateway_setup.sh\nAppendix\nUsing Port 443\nMost customers prefer to open the standard port for https, 443, rather than opening port 8183. The simplest way to achieve this is to simply redirect traffic from port 443 to port 8183.\nUbuntu example\n# Add the iptables rule.\nsudo iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 8183\n\n# Install iptables-persistent if not already present.\nsudo apt-get update\nsudo apt-get install iptables-persistent\n\n# Save the current iptables rules\n# and ensure they start on reboot.\nsudo netfilter-persistent save\nsudo systemctl enable netfilter-persistent",
    "scraped_at": "2026-02-02 15:28:04"
  },
  {
    "title": "Gateway Downloads",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/24882110844692-Gateway-Downloads",
    "content": "To connect to a data source, you use either the DvSum web service or an on-premises gateway. Refer to the article\nDvSum Web Service vs On-Premises Gateway\nfor details.\nIf you're using an on-premises gateway, you'll need to download and install the gateway.\nAll Gateway Downloads\nData Catalog Linux\nData Catalog Windows\nData Quality Windows",
    "scraped_at": "2026-02-02 15:28:09"
  },
  {
    "title": "Sample Data and On-Premises Gateway",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/23896259032596-Sample-Data-and-On-Premises-Gateway",
    "content": "In this Article\nOverview\nDetails about locally-stored sample data\nData in scan_output folder\nSample data unavailability\nGateway not running\nCertificate validation required\nData Retention\nOverview\nWhen a local gateway is in use and a data source is scanned, a new file is generated and stored in a folder called \"scan_output\". Sample data for each table is stored in a corresponding file named *_sample_data.json.\nDetails about locally-stored sample data\nData in scan_output folder\nThe sample data file is available in an encrypted format. It's saved in the folder \"scan_output\".\nThe\nAES/CBC/PKCS5Padding algorithm is used for symmetric data encryption.\nSample data unavailability\nSample data is normally available to DvSum users to view. Sample data becomes unavailable in the DvSum application in the following two situations.\nGateway not running\nIf the gateway is not in the running state, then it cannot respond to requests to display the sample data.\nSolution: Restart the gateway\nCertificate validation required\nIf the gateway is running, but it's using a self-signed certificate, then then the browser cannot establish a connection to the gateway. Certificate Validation is required in order to connect to the gateway.\nSolution: Click \"here\" in the displayed message. Accept the self-signed certificate.\nAfterwards, a manual browser refresh is required in order to view the sample data again.\nData Retention\nThe scan_output folder contains separate folders corresponding to all of the scans that took place. The retention policy for these folders can be configured as follows:\nEdit the file configuration.properties.\nSet the property \"dataRetentionPolicyEnabled\" to \"true\" or \"false\"\nSet the property \"dataRetentionPeriodInDays\" as desired",
    "scraped_at": "2026-02-02 15:28:14"
  },
  {
    "title": "Configure Azure Databricks (Service Principal Access)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/22643550730132-Configure-Azure-Databricks-Service-Principal-Access",
    "content": "Step 1: Configure an app in Azure portal\nRegister an application with the Microsoft Entra ID endpoint in the Azure portal. Alternatively, you can use a Microsoft Entra ID app that is already registered.\nSign in to the\nAzure portal\n.\nIf you have access to multiple tenants, subscriptions, or directories, click the\nDirectories + subscriptions\n(directory with filter) icon in the top menu to switch to the directory in which you want to register the application.\nSearch for and select\nMicrosoft Entra ID.\nWithin\nManage, select\nApp registrations > New registration.\nFor\nName, enter a name for the application.\nIn the\nSupported account types\nsection, select\nAccounts in this organizational directory only (Single tenant).\nIn the\nRedirect URL (optional)\nleave empty\nClick\nRegister.\nOn the application pageâ€™s\nOverview\npage, in the\nEssentials\nsection, copy the following values:\nApplication (client) ID\nDirectory (tenant) ID\nAdd\nAzureDatabricks\nto the required permissions of the registered application. You must be an admin user to perform this step. If you encounter a permissions-related issue while you perform this action, contact your administrator for help.\nOn the application pageâ€™s\nOverview\npage, on the\nGet Started\ntab, click\nView API permissions.\nClick\nAdd a permission.\nIn the\nRequest API permissions\npane, click the\nAPIs my organization uses\ntab, search for\nAzureDatabricks, and then select it.\nEnable the\nuser_impersonation\ncheck box, and then click\nAdd permissions.\nClick\nGrant admin consent for ###\nand then\nYes. To perform this action, you must be an admin user or have the privilege to grant consent to the application.\nStep 2: Add the Microsoft Entra ID service principal to your Azure Databricks account\nThis steps works only if your target Azure Databricks workspace is enabled for\nidentity federation\n. If your workspace is not enabled for identity federation, skip ahead to Step 3.\nIn your Azure Databricks workspace, click your username in the top bar and click\nManage account.\nAlternatively, go directly to your Azure Databricks account console at\nhttps://accounts.azuredatabricks.net\n.\nSign in to your Azure Databricks account, if prompted.\nOn the sidebar, click\nUser management.\nClick the\nService principals\ntab.\nClick\nAdd service principal.\nEnter a\nName\nfor the Microsoft Entra ID service principal.\nFor\nUUID, enter the\nApplication (client) ID\nvalue from Step 1.\nClick\nAdd. Your Microsoft Entra ID service principal is added as an Azure Databricks service principal in your Azure Databricks account.\nStep 3: Add the Microsoft Entra ID service principal to your Azure Databricks workspace\nIf your workspace is enabled for\nidentity federation\n:\nIn your Azure Databricks workspace, click your username in the top bar and click\nAdmin Settings.\nClick on the\nIdentity and access\ntab.\nNext to\nService principals, click\nManage.\nClick\nAdd service principal.\nSelect your Microsoft Entra ID service principal from Step 2 and click\nAdd. Your Microsoft Entra ID service principal is added as an Azure Databricks service principal in your Azure Databricks workspace.\nSkip ahead to Step 4.\nIf your workspace is not enabled for identity federation:\nIn your Azure Databricks workspace, click your username in the top bar and click\nAdmin Settings.\nClick on the\nIdentity and access\ntab.\nNext to\nService principals, click\nManage.\nClick\nAdd service principal.\nClick\nAdd new.\nFor\nApplicationId, enter the\nApplication (client) ID\nfor your Azure service principal from Step 1.\nEnter some\nDisplay Name\nfor the new service principal and click\nAdd. Your Microsoft Entra ID service principal is added as an Azure Databricks service principal in your Azure Databricks workspace\nStep 4: Assign workspace-level permissions to the service principal\nIf the admin console for your workspace is not already opened, click your username in the top bar and click\nAdmin Settings.\nClick on the\nIdentity and access\ntab.\nNext to\nService principals, click\nManage.\nClick the name of your service principal to open its settings page.\nOn the\nConfigurations\ntab, check the box next to each entitlement that you want your service principal to have for this workspace, and then click\nUpdate. Check following check boxes\nActive\nDatabricks SQL Access\nWorkspace access\nOn the\nPermissions\ntab, grant access to any Azure Databricks users, service principals, and groups that you want to manage and use this service principal.\nStep 5: Enable Personal access token for Service principal\nLog into your Databricks workspace\nIf the admin console for your workspace is not already opened, click your username in the top bar and click\nAdmin Settings.\nClick on the\nAdvanced\ntab.\nIn Access control\nEnable personal access token\nClick on permission settings\nSearch for your service principal name\nSelect permission \"Can Use\"\nClick Add\nStep 6: Enable Hive metadata access for Service principal\nLog into your Databricks workspace\nClick on the Catalog\ntab in left menu.\nClick on catalog that was set earlier.\nOpen permissions tab\nClick on Grant\nSearch for your service principal name\nCheck the privileges you would like to grant\nSelect \"All\"\nClick Grant\nStep 7:\nEnable cluster access for\nService principal\nTo configure the\nCompute\ncluster permissions:\nNavigate to the\nPermissions\nsection.\nSelect the relevant\nService Principal (SP) name\n.\nSet the permission to\nCan Attach To\n.\nTo configure the\nSQL Warehouse\npermissions:\nNavigate to the\nPermissions\nAssign the permission level\nCan Use\nAdditional References:\nhttps://learn.microsoft.com/en-us/azure/databricks/dev-tools/app-aad-token\nhttps://learn.microsoft.com/en-us/azure/databricks/dev-tools/service-principals",
    "scraped_at": "2026-02-02 15:28:20"
  },
  {
    "title": "Scan Logs",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/21582531523348-Scan-Logs",
    "content": "In this Article:\nIntroduction\nAuthentication & Test Connection Logs\nParticular Scan Job Logs\nReading Logs on Scan Job Page\nCancelling the Scan\nAborting the Scan\nScheduled Scan Jobs\nIntroduction\nScanning is a very important part of the DvSum Application. It is bringing the Data from the Database to our application.\nDuring the scan, a lot of things are going on in the back-end like how DvSum Webservice is connected to the Database and how tables are profiled one by one in our application. Most of the part is Technical but there are some areas where normal users can find useful insights by checking the logs that are being generated when the data is coming in our application.\nWe will further explore how users can abort the ongoing scans.\n1- Authentication & Test Connection Logs\nWhen the credentials of any data source are authenticated, the database and schemas are selected, and \"Test Connection\" is passed then there are some basic logs that are generated like the connection is established with the database and the schemas in it. The generated logs can be seen in the \"Logs\" tab.\nAll the logs related to the Source can be found in this tab and they are generated in the below cases:\nCredentials are authenticated & Test Connection is passed\nTables are being scanned\nQuestions asked on Agent\nFor more information about adding a Data source here is the article\nAdding Snowflake as Data Source.\nThe Logs are being continuously generated at the back-end so in order to fetch the latest logs user will be required to click on the \"Refresh\" button icon. The search bar can also be used for searching particular words in the Logs.\n2- Particular Scan Job Logs\nWhen a scan is started, a Scan job is created for that particular scan in the \"Scan History\" tab.\nOn the Scan job page, only the logs that are related to the scanning of the tables are present. This can be more useful instead of looking for the whole source-related logs.\nLet us cover some flow how scans can be started, and cancelled, and how logs can be viewed. Since the Scans are of different types so for more information regarding different scans here is the article\nCataloging & Profiling Data Sources.\n2.1- Reading Logs on Scan Job Page\nAs soon as a scan is started, logs are generated on the scan job page. The Logs may not just appear as soon as the scan is started and they might take 3-5 minutes to start. As the logs are continuously being generated they need to be refreshed to bring the latest logs.\nSince the tables are being profiled during the scan so users can check the status that how many tables have been profiled. This can be done easily by looking at the logs or users can use the search bar to check that how many tables have been fetched into the application.\nOnce all the tables are profiled then the scan will be completed and the status will be changed from running to completed. When the scan is completed users can still see all the logs that were generated.\n2.2- Cancelling the Scan\nWhen a scan is started, before going to the running state, for a few seconds it goes scheduled. In this state, if any Scan Job page is opened and the user cancels the scan then that particular Scan job will be cancelled.\nOnce all the tables are profiled then the scan will be completed and the status will be changed from running to completed and the users can still see all the logs that were generated.\n2.3- Aborting the Scan\nWhen a scan is started it goes to the running state and it starts the catalog execution and tables are profiled one by one. During this state, if the running scan is cancelled then it will be aborted.\nOn the UI there will be an \"Abort Scan\" button visible and when it is clicked, the scan job will be aborted.\nPlease note that if a scan is running and some tables are refreshed/ profiled and the scan is now aborted, the tables which were already fetched into the application will not be affected.\n3- Scheduled Scan Jobs\nUsers have the option of running scheduled scans in which the scan will start running at a particular time mentioned by the user in the settings of the source. The logs generated in scheduled scans are the same as the on-demand scans.",
    "scraped_at": "2026-02-02 15:28:24"
  },
  {
    "title": "Catalog, Profile & Lineage Scan on Data Sources",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/20310539391252-Catalog-Profile-Lineage-Scan-on-Data-Sources",
    "content": "In this Article:\nIntroduction\nCatalog Scan\nProfiling\nCatalog & Profile Scans\nCatalog, Profile & Lineage Scans\nScheduling a Job\nSpecial Cases for Certain Sources\nIntroduction\nTo get the data in the DvSum Catalog, the Data source should be Scanned from the database. The scanning process is divided further into three steps which are catalog scan,Â  profile scan & lineage scan. This article will show how to perform these three types of scans. Before moving further make sure that you know how to add a data source and how a source is authenticated. Here is the\nlink\nto the article that shows how to add an Oracle Data source.\nCatalog Scan\nCataloging the data source means that during the authentication there are some databases and schemas added and schemas further contain tables that have the data. When the source is authenticated then \"Scan Now\" button can be seen. When the \"Scan Now\" button is clicked it further shows two options:\nProfile\nLineage\nWhen the user doesn't select any of the above options the \"Catalog\" scan will run automatically. Catalog scan will run in any case.\nWhen any source is scanned using the scan type \"Catalog\" then a job will be scheduled with the scan type \"Catalog\".\nThe scan type can be will be shown in the job detail page.\nAfter few minutes the catalog step will be completed and the status will be changed from \"Running\" to \"Completed\".\nWhen the Scan is completed then scan information can be viewed by clicking on the \"Job Execution ID\". The scan results page shows us the total tables and columns that are scanned. The scan type is also mentioned on the Scan Results page.\nGo to the\nDictionary\ndropdown o\nn the Database tables tab\n, the data sources that are newly scanned can be seen. Since the data source was \"cataloged\" so the tables will come but there will be no records in the tables and it can be verified by looking at the record count which will be empty.\nProfiling\nOnly the columns in the tables are not useful until the data is not there so in order to bring data of the tables in the catalog, the table must be profiled. When any table is selected then \"Run Profiling\" button can be seen above.\nOn clicking \"Run Profiling\" the tables will be profiled (all the data present in the tables will be fetched in the catalog) and a job will be scheduled.\nWhen the Job will be completed then all the tables will be profiled. On the Database Tables tab there will be record count for the tables that were profiled indicating that the data is now present in the tables:\nNote: Scan Results tab for the scan type \"Profile\" does not exist\nCatalog & Profile Scans\nWhen the source is authenticated then on the \"Scan Now\" button there is an option of \"Profile\" which is a combination of cataloging and profiling (full scan). It will bring the schemas and tables along with the data inside them in the catalog. Once the \"Scan\" button is clicked, a scan job will be created of the type \"Catalog & Profile\" will be created.\nOnce the scan is completed, the scan details information can be seen by clicking on the Job IDÂ  which will open up the job detail page which contains all the information related to the scan along with the scan type:\nGo to the\nDictionaries\ndropdown o\nn the Database Tables tab, all the tables along with the data will be present in the catalog. Here we do not require profiling separately as it was done during the scan.\nIt is to be noted that scan type \"Catalog & Profile\" basically brings all the schemas and tables present inside it along with the data. If the data for some tables is required then the source must be cataloged first and then profiling can be applied to specific tables.\nCatalog, Profile & Lineage Scans\nOn the Scan Now button the user has the option of running \"Lineage\" scans with \"Catalog\" or the user can also run \"Catalog\", \"Profile\" and \"Lineage\" scans by checking all the checkboxes. Now along with the \"Catalog\" and \"Profile\" scan, the Lineage will also be rendered into the application. Once the user clicks on the Scan button the job will be created for \"Catalog\", \"Profile\" & \"Lineage\".\nUsers can access the information related totheÂ  scan by clicking on the particular scan name.\nScheduling a Job\nThe above examples of scans are basically on-demand which means the moment the \"Scan\" button is clicked, a job will be created and start running but if there is a scenario in which scans are required to run daily or start from a specific date then scans can be scheduled. On the settings tab of the Data Source, the \"General\" tab contains scanning information, and on clicking \"Edit\", jobs can be scheduled and the user can mention the scan types:\nIt is to be noted that only scan types \"Catalog\", \"Catalog & Profile\", \"Catalog & Lineage\" & \"Catalog, Profile & Lineage\" can be scheduled. The scan type \"Profile\" can not be scheduled for a particular time. On selecting the right scan type, the scan frequency, start, and end time can be selected according to the requirement. Once the information is saved, a job will be scheduled with the scan type that was selected:\nSpecial Cases for Certain Sources\nIn our application, there are some sources that don't have the option of separate \"Catalog\" & \"Profile\" and \"Catalog\" & \"Lineage. For these sources on the \"Scan Now\" button on Data sources, there will be no drop-down appearing asking for selecting scan type. In these sources, the scan type will be \"Catalog & Profile\" or \"Catalog\" & \"Lineage\" by default. These 4 sources are:\nAzure Data Lake Storage (Catalog & Profile scan available)\nPower BI (Catalog & Lineage scan available)\nTableau (Catalog & Lineage scan available)\nFile Upload (Catalog & Profile scan available)",
    "scraped_at": "2026-02-02 15:28:30"
  },
  {
    "title": "Configure Salesforce as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/19345773418388-Configure-Salesforce-as-a-Source",
    "content": "In this Article:\nOverview\nStep-by-Step Configuration\nConfigure a Connected App in Salesforce\nConfigure Permission Sets in Salesforce\nAdding Salesforce as a Data Source in DvSum\nStep 1: Add a New Source\nStep 2: Configure Connection Settings\nStep 3: Save the Configuration\nRunning a Data Scan\nStep 1: Start a Scan\nStep 2: Review Scan Results\nOverview\nIntegrating Salesforce with DvSum allows users to analyze CRM data, generate reports, and ensure data governance. This guide outlines the steps to configure Salesforce as a source in DvSum.\nStep-by-Step Configuration\n1. Configure a Connected App in Salesforce\nOpen the browser, navigate to\nsalesforce.com\n, and log in.\nNavigate to Setup â†’ Apps â†’ AppManager to create an app.\nTip:\nUse the Quick Find box.\nIn the Connected Apps section, click \"New Connected App\".\nEnter a name to be displayed to users when they log in to grant permissions to your app, along with a contact email address.\nEnable\nOAuth Settings\n, and enter the relevant value in the\nCallback URL\nbox:\nFor Data Intelligence:\nhttps://apis.dvsum.ai/data-sources/sources/salesforce/saveVerifierCode\nSelect the scope of permissions that your app should request from the user. Save the changes and Continue to the next screen.\nClick your app name to open a page with information about your app. Note the OAuth client credentials. These properties are needed to add Salesforce as a source in DvSum DQ:\nConsumer Key\nConsumer Secret\n2. Configure Permission Sets in Salesforce\nTo connect to Salesforce, the user must have the necessary permissions to call Salesforce APIs. This is achieved by configuring the appropriate properties within the relevant Permission Set.\nThe Permission Set can be accessed through one of the following paths (the interface layout may vary depending on whether Lightning Experience or Classic view is used, but the navigation remains the same):\nADMINISTRATION â†’ Users â†’ Permission Sets â†’ <permission_set>\nADMINISTRATION â†’ Users â†’ Users â†’ <user> â†’ Permission Set Assignments â†’ <permission_set>\nOnce there, enable\nSystem Permissions â†’ API Enabled\nto grant access to any Salesforce.com API.\nOnce the Connected App and Permission Set are configured, you can proceed with setting up DvSum.\nAdding Salesforce as a Data Source in DvSum\nStep 1:\nAdd a New Source\nNavigate to the Data Sources tab.\nClick Add Source.\nSelect Salesforce as the source type.\nProvide a Source Name and click Save.\nStep 2: Configure Connection Settings\nOpen the newly created source.\nEnable On-Premise Web Service.\nSelect the appropriate SAWS (if applicable).\n(Optional) Enable Sandbox Mode and enter the sandbox URL.\nEnter the Client ID and Client Secret from the Salesforce Connected App.\nClick Authenticate.\nLog in to Salesforce when prompted.\nNote\n: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click\nhere\nStep 3: Save the Configuration\nOnce authenticated, return to the source settings page.\nClick Done (top-right corner).\nClick Save.\nNote\n: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click\nhere\nStep 3:\nOnce the Authenticate button is clicked, it will redirect to a new tab and open the Salesforce login page. Login to the salesforce account. Once the login step is complete, it will redirect back to the source detail page and there user will see that the source is authenticated successfully.\nStep 4:\nAfter the credentials are authenticated, we need to save the source. For that, scroll up to the top. From the top right corner click on the â€œDoneâ€ button.\nAfter that click the â€œSaveâ€ button. The source will get saved successfully.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nRunning a Data Scan\nStep 1: Start a Scan\nNavigate to Scan History.\nClick Scan Now.\nA job will be created to fetch data from Salesforce.\nAfter the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.\nStep 2: Review Scan Results\nOnce the scan completes, click on the Scan Name.\nThe Scan Summary page will display:\nNew tables and columns were fetched.\nData insights.\nAfter the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.\nTo explore table details, g\no to the\nDictionaries\ndropdown and select the\nData Dictionary\nâ†’ Recently Refreshed.\nClick on any table name to view details.",
    "scraped_at": "2026-02-02 15:28:36"
  },
  {
    "title": "Configure MongoDB as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/17494822453524-Configure-MongoDB-as-a-Source",
    "content": "In this Article:\nOverview\nStep-by-Step Configuration\nAdd MongoDB as a Data Source\nConfigure Connection\nSave the Connection\nRunning a Data Scan\nStart a Scan\nReview Scan Results\nBrowsing the Data Catalog\nAdditional Resources\nOverview\nIntegrating\nMongoDB\nwith\nDvSum\nallows users to query NoSQL data, perform analytics, and enforce data governance. This guide provides step-by-step instructions for configuring\nMongoDB\nas a data source in\nDvSum\n.\nStep-by-Step Configuration\n1. Add MongoDB as a Data Source\nNavigate to\nAdministration â†’ Data Sources â†’ âŠ• Add Source\n.\nSelect\nMongoDB\nfrom the list of available sources.\n2. Configure Connection\nOnce the source is saved, you will be redirected to the connection settings detail page.\nMost common\n: Connect Using DvSum Web Service\nWhitelist the DvSum application by IP address as indicated. Then enter the required connection information:\nHost\nPort\nInstance Name\nDB Login\nDB Password\nAlternative\n: You may\ninstall a DvSum Edge Gateway\nbehind your firewall. Then connect to your SQL Server instance from this web service installed on premises. Read more details in\nDvSum Web Service vs On-Premises Edge Gateway\n.\nAfter entering the credentials, Authenticate the source.\nOnce the source is Authenticated, the Database section will appear.\n3. Save the Connection\nAfter credentials are authenticated and the database is selected, then you must save the source. For that, scroll up to the top. From the top right corner click the â€œDoneâ€ button.\nAfter that click the â€œSaveâ€ button.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nRunning a Data Scan\n1. Start a Scan\nNavigate to\nScan History\n.\nClick\nScan Now\n.\nThe scan will initiate, creating a job to fetch data from\nMongoDB\n.\n2. Review Scan Results\nOnce the scan completes, the status will change to\nCompleted\n.\nClick on the\nScan Name\nto open the\nScan Summary\npage.\nThe\nScan Summary\ndisplays:\nNumber of new tables and columns fetched.\nData insights and schema updates.\nWhen the scan completes, the status will change to \"Completed\".\nAfter the scan completion, click on Scan Name and it will open the Scan Summary page for this scan.\nThe Scan Summary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this scan.\nYour MongoDB connection is now fully configured and functional.\nBrowsing the Data Catalog\nNavigate to the Data Dictionary on the left sidebar.\nClick on the Recently Refreshed tab to view tables discovered in the recent scan.\nClick on table names to see detailed metadata and structure.\nWatch this quick video tutorial of how to add and configure an Mongo DB source into DvSum app.\nAdditional Resources\nRead more about\nDvSum Web Service (Cloud SAWS)\nRead more about\nDvSum Edge Gateway (On-Premises SAWS)",
    "scraped_at": "2026-02-02 15:28:43"
  },
  {
    "title": "Configure PostgreSQL as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/12597290833172-Configure-PostgreSQL-as-a-Source",
    "content": "In this Article:\nOverview\nAdding PostgreSQL Source in DvSum\nPrerequisite: Enabling General Query Log for PostgreSQL\nStep-by-Step Configuration\nStep 1: Add a New Postgres DataSource\nStep 2: Configure Connection Settings\nStep 3: Select Database and Schema(s)\nStep 4: Save the Connection\nStep 5: Run a Data Scan\nReviewing Scan Results\nVideo Tutorial\nOverview:\nThis article outlines the process of configuring PostgreSQL as a data source in DvSum, enabling integration for data cataloging and profiling. The steps provided apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations based on the specific platform.\nAdding PostgreSQL source in DvSum:\nPrerequisite: Enabling General Query Log for PostgreSQL\nBefore configuring MySQL as a source, ensure that the general query log is enabled for your PostgreSQL server. This is essential for tracking data lineage and obtaining insights into usage patterns. For more information, refer to the\nEnabling General Query Log for Data Sources\narticle.\nYou can follow the steps mentioned below to configure and authenticate a PostgreSQL source:\nStep-by-Step Configuration\nStep 1: Add a New Postgres DataSource\nNavigate to\nData Sources\n.\nClick on\nAdd Source\n.\nSelect\nPostgreSQL\nas the data source.\nEnter a\nsource name\nand click\nSave\n.\nStep 2: Configure Connection Settings\nAfter saving, you will be redirected to the\nconnection settings\npage.\nEnable the\nOn-premise Web Service\ncheckbox if applicable.\nSelect the\nSAWS (Smart Adaptive Web Service)\nthat is set up and running.\nEnter the following details:\nHost\nPort\nDatabase Login\nPassword\nNote\n: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click\nhere\nStep 3: Select Database and Schema(s)\nOnce authentication is successful, the\nDatabase\nsection appears.\nSelect the\ndatabase\nyou wish to scan.\nIf scanning all schemas, leave the\nSchema checkbox\nunchecked.\nTo scan specific schemas:\nCheck the\nSchema checkbox\n.\nSelect the required schemas from the\nAvailable Schemas\nlist.\nMove them to the\nSelected Schemas\ntab.\nOnce the database is selected, we now have an option of limiting the scan to some specific schema(s) or we have to scan all of them. For the PostgreSQL source, we would have a Schema field as well which will contain a checkbox, If we want to scan all the schemas then we shouldn't check this checkbox and proceed with saving and scanning the source. But if we want to limit our scan to some specific schemas then check this checkbox.\nOnce it is checked then the list of available schemas will be displayed. Users can select single or multiple schemas from the Available Schemas list and move them to the Selected Schemas tab on the right.\nStep 4: Save the Connection\nScroll to the top of the page.\nClick Done.\nClick Save.\nThe PostgreSQL source is now configured successfully.\nAfter that click the â€œSaveâ€ button. The source will get saved successfully.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nStep 5: Run a Data Scan\nNavigate to the Scan History page.\nClick Scan Now.\nThe scan will run, and a job will be created.\nOnce completed, the status will change to Completed.\nClick on the Scan Name to view the Scan Summary.\nOnce the status of the job gets Completed, our new PostgreSQL source's scan will be completed successfully.\nAfter the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.\nReviewing Scan Results\nThe\nScan Summary\npage displays insights such as the number of tables and columns fetched.\nNavigate to\nData Dictionary\nto view the scanned tables.\\\nClick on the\nRecently Refreshed\ntab to see newly added tables.\nClick on individual table names for more details.\nVideo Tutorial:\nWatch this quick video tutorial on how to add and configure a PostgreSQL source into DvSum app.",
    "scraped_at": "2026-02-02 15:28:51"
  },
  {
    "title": "Configure Databricks as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/11918822204436-Configure-Databricks-as-a-Source",
    "content": "In this article:\nOverview\nPrerequisites\nStep-by-Step Configuration\nStep 1: Adding a Databricks Source in DvSum\nStep 2: Configure Connection\nStep 3: Select Database\nStep 4: Save & Test Connection\nStep 5: Scan the Data Source\nAuthentication Options\nScenario 1: Authentication via Access Token\nScenario 2: Authentication via Client Secret\nReviewing Scan Insights\nOverview:\nAzure Databricks is an optimized platform for Azure, offering tight integration with services like Azure Data Lake Storage, Azure Data Factory, Azure Synapse Analytics, and Power BI. It allows data storage in a unified, open lakehouse while consolidating analytics and AI workloads.\nThis article describes the process of configuring Databricks as a data source in DvSum, facilitating the integration of data cataloging and profiling. The steps outlined apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with minor platform-specific variations.\nAdding Databricks source in DvSum:\nPrerequisites:\nEnabling Query History for Databricks\nBefore configuring Databricks as a source, ensure that query history is enabled for your Databricks account. This is crucial for tracking data lineage and gaining insights into usage patterns. For more information, refer to the\nEnabling Query History for Data Sources\narticle.\nCluster and Account Setup:\nFor authentication of the Databricks Source, a user must have an account on the Azure Databricks portal on which a cluster is running attached to a database. On the Azure Databricks portal, go to the Compute tab and start your cluster if it is in the stop state.\nRequired Table Access for Catalog Scans\nTo support cataloging and schema discovery in Databricksâ€”especially for use cases involving the ability to pick and choose catalogsâ€”it is\nimportant to have read-only access\nto the following system tables:\nsystem.information_schema.catalogs\nsystem.information_schema.schemata\nsystem.information_schema.tables\nsystem.information_schema.columns\nWithout access to these tables,\ncataloging scans will not function as expected\n. Please ensure this access is in place before initiating scans.\nStep-by-Step Configuration\nStep 1: Adding a Databricks Source in DvSum\nNavigate to\nData Sources\n.\nClick on\nAdd Source\n.\nIn the modal, select\nDatabricks\n.\nProvide a\nsource name\nand click\nSave\n.\nStep 2: Configure Connection\nOnce the source is saved, you will be redirected to the connection settings detail page of the new Databricks source. First, enable the\nOn-premise Web Service\ncheckbox and select the appropriate\nSAWS (Secure Access Web Service)\nthat is currently set up and running.\nYou can authenticate using either:\nAccess Token\nClient Secret\nNote\n: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click\nhere\nAuthentication Options\nYou can authenticate using either:\nAccess Token\nClient Secret\nScenario 1:\nAuthentication via Access Token\nEnable the\nOn-Premise Web Service\ncheckbox.\nSelect the\nSAWS\n(Secure Access Web Service) that is set up and running.\nEnter the following details:\nServer Hostname\nHTTP Path\nPersonal Access Token\nClick\nAuthenticate\n.\nScenario 2: Authentication using Client Secret\n~Prerequisites for Configuring Azure Databricks (\nService Principal Service\n):\nPlease refer to the article to configure\nAzure Databricks (Service Principal Service).\nUnder the\nHost Information\nsection, select\nClient Secret\n.\nEnter the following details:\nServer Hostname\nHTTP Path\nAzure Client Id\nOAuth Secret\nClick\nAuthenticate\n.\nNote:\nTo optimize job performance and memory usage with\nOAuth Secret\n(a confidential key used to securely authenticate and authorize applications when integrating with external services), select the checkbox and enter the OAuth Secret value. This will automatically initiate and connect to the clusters prior to execution.\nThe OAuth Secret can be generated by the admin from the Service Principal's secret tab.\nStep 3: Select Database\nOnce authenticated, the\nDatabase\nsection will appear.\nSelect the appropriate\nCatalog Name\nfrom the dropdown.\n(Optional) To restrict scanning to specific schemas, enable the\nLimit to specific schemas\ncheckbox and choose the required schemas.\nStep 4: Save & Test Connection\nScroll to the top and click\nDone\n.\nClick\nSave\n.\nClick\nTest Connection\nto validate the setup.\nAfter that click the â€œSaveâ€ button. The source will get saved successfully and after that click on the â€œTest Connectionâ€ button.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nStep 5: Scan the Data Source\nNavigate to Scan History.\nClick Scan Now.\nWait for the job status to change to Completed.\nClick on the Scan Name to view the Scan Summary\nReviewing Scan Insights\nGo to the\nDictionaries\ndropdown and select the\nData Dictionary\ntab.\nClick on\nRecently Refreshed\nto view newly discovered tables.\nClick on table names to explore metadata details.",
    "scraped_at": "2026-02-02 15:28:56"
  },
  {
    "title": "Configure Tableau as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10713006510868-Configure-Tableau-as-a-Source",
    "content": "In this Article:\nOverview\nPrerequisites\nStep-by-Step Configuration\nStep 1: Prepare Your Tableau Account\nStep 2: Add Tableau as a Source in DvSum\nStep 3: Configure the Connection Settings\nStep 4: Save the Tableau Source\nStep 5: Scan the Tableau Source\nStep 6: Analyzing Tableau Reports and Datasets\nViewing Reports from Tableau Scan\nViewing Data Sets from the Tableau Scan\nVideo Tutorial\nOverview:\nTableau is a leading data visualization tool used for data analysis and business intelligence. We use Tableau as a data source in our DvSum Data Intelligence app. By adding and scanning the tableau source in our app, customerâ€™s reports and tableau data sources will be fetched into our app to make the data more insightful to analyze. Letâ€™s get started with Tableau. Now letâ€™s get started with adding Tableau as a data source.\nPrerequisites:\nBefore configuring Tableau as a data source, ensure you have:\nAccess to a\nTableau Server\nor\nTableau Online\naccount.\nRequired connection details:\nServer URL\nSite ID\nAuthentication method\n(Access Token)\nNecessary API access and permissions for retrieving Tableau data.\nUser role with adequate permissions to access and fetch Tableau workbooks and datasets.\nStep-by-Step Configuration\nStep 1: Prepare Your Tableau Account\nLog in to your\nTableau\naccount.\nEnsure that some\ndatasets and workbooks\nare available in your Tableau account.\nCreate new\nworkbooks\nconnected to the existing datasets.\nEnsure that sheets are created within these workbooks.\nThe screenshot below shows multiple datasets and workbooks in the Tableau account:\nStep 2: Add Tableau as a Source in DvSum\nIn the DvSum application, navigate to the Data Sources tab.\nClick on the Add Source button.\nSelect Tableau from the list of available sources.\nProvide a Source Name and click Save.\nStep 3: Configure the Connection Settings\nAfter creating the Tableau source, you will be redirected to the Connection Settings page.\nIn the Credentials section, enter the following details:\nServer URL\nSite Name\nAccess Token Name\nAccess Token Value\nClick on Authenticate to verify the credentials.\nStep 4: Save the Tableau Source\nOnce authentication is successful, scroll to the top of the page.\nClick on the Done button in the top-right corner.\nClick on Save to finalize the configuration.\nAfter that click the â€œSaveâ€ button. The source will get saved successfully.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nStep 5: Scan the Tableau Source\nNavigate to the\nScan History\npage.\nClick the\nScan Now\nbutton.\nA scan job will be created.\nOnce the job status is\nCompleted\n, the Tableau source scan will be successful.\nClick on the\nScan Name\nto open the\nScan Summary\npage.\nThe\nScan Summary\npage displays insights, including the number of\ndatasets and workbooks fetched\n.\nStep 6: Analyzing Tableau Reports and Datasets\nClick on the Analytics\nDictionary\nfrom the left navigation tab.\nIn the\nRecently Refreshed\ntab, view the fetched\nReports\nand\nData Sets\n.\nReports from Tableau scan:\nFrom the listing select some recently scanned Report and the user will be redirected to the detail page of the Report. The screenshot below shows how the Reports detail page looks like.\nOn the detail page, a description of the report is fetched from the Tableau report. This means this report has the same description in Tableau and the same description is fetched and shows up here. The author of the report is basically the person who created this report in Tableau so this value is also basically fetched. Further, we have a lineage of the Report which actually shows us that the current Report is derived from which data set and which tables are connected to that data set.\nViewing Data Sets from the Tableau Scan\nSelect a recently scanned dataset from the listing.\nYou will be redirected to the\nDataset Detail page\n.\nThe\nOverview\ntab displays the description and author fetched from Tableau.\nThe\nData\ntab includes\nProfiling Info\nand\nField Summary.\nThe\nField View\nsection displays\nData Source\nfields and\nData Connection\nfields.\nvideo tutorial:\nWatch this quick video tutorial of how to add and configure an Oracle source into DvSum app.",
    "scraped_at": "2026-02-02 15:29:04"
  },
  {
    "title": "Configure MySQL as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10636157522708-Configure-MySQL-as-a-Source",
    "content": "In this Article\nOverview\nPrerequisites\nAdding MySQL as a Source in DvSum\nStep 1: Install and Set Up MySQL Locally\nStep 2: Add MySQL as a Data Source in DvSum\nStep 3: Configure Connection Settings\nStep 4: Select a Database Schema\nStep 5: Save and Scan the Source\nVideo tutorial\nOverview\nThis article explains how to configure MySQL as a data source in DvSum for data cataloging and profiling. The steps apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations depending on the platform.\nPrerequisites\nBefore setting up MySQL as a source in DvSum, ensure the following:\nRequired Access & Configuration:\nA running MySQL database (on-premises, cloud, or managed service like AWS RDS, Azure Database for MySQL)\nGeneral Query Log enabled (for tracking data lineage and usage patterns). See\nEnabling General Query Log for Data Sources\nfor details.\nAdding MySQL as a Source in DvSum\nStep 1: Install and Set Up MySQL Locally\nInstall MySQL and MySQL Workbench on the machine where SAWS (Self-Hosted Agent Web Service) is running.\nEnsure MySQL Workbench is configured properly.\nInstallation Guide\nStep 2: Add MySQL as a Data Source in DvSum\nOpen\nDvSum DI\n.\nNavigate to\nData Sources\n> Click\nAdd Source\n.\nSelect\nMySQL\nas the data source.\n4. Enter a\nSource Name\nand click\nSave\n.\nStep 3: Configure Connection Settings\nYou will be redirected to the\nConnection Settings\npage.\nEnable\nOn-premise Web Service\nand select the SAWS instance.\nEnter the following details:\nHost\n: MySQL server address\nPort\n: Default is 3306\nDB Login\n: MySQL username\nPassword\n: MySQL password\nClick\nAuthenticate\n.\nNote:\nMySQL Workbench and SAWS must be on the same machine; otherwise, authentication will fail.\nNote:\nBy Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click\nhere\nStep 4: Select a Database Schema\nAfter successful authentication, a\nDatabase Selection\nfield will appear.\nChoose the schema to be scanned (single selection only).\nStep 5: Save and Scan the Source\nScroll up and click\nDone\n.\nClick\nSave\nto finalize the MySQL source setup.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nGo to\nScan History\nand click\nScan Now\n.\nOnce the scan completes, click on the\nScan Name\nto view the\nScan Summary\n.\nOn the Scan Summary page, it will show all the insights of the scan i.e how many new tables and columns are fetched in this scan from the database that we selected earlier.\nTo have more insights into the tables' details:\nClick on\n\"Data Dictionary\"\nfrom the sidebar.\nThe\nTable Listing\nview will appear.\nClick on the\n\"Recently Refreshed\"\ntab.\nThis tab displays all the tables retrieved in the most recent scan.\nClick on any\ntable name\nto view more details about that table on the\ndetail page\n.\nAuthentication Failed\nEnsure correct username and password.\nBy following these steps, you can successfully integrate MySQL as a data source in DvSum and leverage its data insights and quality features.\nvideo tutorial:\nWatch this quick\nvideo tutorial\nof how to add and configure an Oracle source into DvSum app.",
    "scraped_at": "2026-02-02 15:29:09"
  },
  {
    "title": "Configure Azure SQL & Azure Synapse Analytics",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10636132296852-Configure-Azure-SQL-Azure-Synapse-Analytics",
    "content": "In this Article:\nOverview\nPrerequisite: Enable Query History via Query Store\nStep-by-Step Configuration\nStep 1: Adding a Databricks Source in DvSum\nStep 2: Configure Source\nStep 3: Scan Data Source\nStep 4: Review Data Dictionary\nVideo Tutorial\nAdditional Resources\nOverview:\nThis article outlines the process of configuring Azure SQL or Azure Synapse Analytics as a data source in DvSum, enabling integration for data cataloging and profiling. The steps provided apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations based on the specific platform.\nPrerequisite: Enable Query History via Query Store\nBefore proceeding, ensure that\nQuery Store\nis enabled on your Azure SQL Database or Azure Synapse Analytics instance. Query Store captures query history, which is required for lineage tracking and usage insights in DvSum. For detailed steps, refer to the article\nEnabling Query History for Data Sources.\nFor step-by-step instructions on creating users, configuring Query Store, and setting up required permissions, refer to\nAzure SQL & Azure Synapse Analytics User Setup and Query Store Configuration for Metadata Extraction\nYou can follow the steps mentioned below to configure and authenticate a Azure SQL or Azure Synapse Analytics source:\nStep by Step Configuration\nStep 1:Â Adding a Databricks Source in DvSum\nNavigate to\nAdministration â†’ Data Sources\nin DvSum.\nClick\n\"âŠ• Add Source\"\nto open the wizard.\nSelect the appropriate\nData Source Type\n.\nEnter a\nname\nfor the data source.\nClick\nSave\n.\n2. Configure Source\nThe\nConnection\ntab opens automatically after saving.\nAlternatively, navigate to:\nAdministration â†’ Data Sources â†’ Settings â†’ Connection\n.\nClick\nEdit\nand configure the connection details:\nServer Name\nDatabase Name\nAuthentication\nClick\nAuthenticate\nto test the connection.\nAfter authentication, select a\ndatabase\nand optionally, specific\nschemas\n.\nClick\nDone\nand\nSave\nto complete the process.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\n3. Scan Data Source\nNavigate to\nAdministration â†’ Data Sources â†’ Scan History\n.\nClick\n\"Scan Now\"\nto perform an initial scan.\nThe scan will progress from\nScheduled â†’ Running â†’ Completed\n.\nAfter completion, click the\nScan Name\nto open the\nScan Summary\n.\nReview the scan insights\n4. Review Data Dictionary\nNavigate to\nData Dictionary\nin the DvSum menu.\nClick on the\n\"Recently Refreshed\"\ntab to find recently scanned tables.\nClick on the table name for detailed information.\nVideo tutorial:\nWatch this quick video tutorial on how to add and configure an Azure Data Source into DvSum app.\nAdditional Resources\nRead more about\nDvSum Web Service (Cloud SAWS)\nRead more about\nDvSum Edge Gateway (On-Premises SAWS)",
    "scraped_at": "2026-02-02 15:29:16"
  },
  {
    "title": "Configure Snowflake as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10627532986004-Configure-Snowflake-as-a-Source",
    "content": "In this Article:\nOverview\nSetting Up Snowflake for DvSum Integration\nUser and Role Configuration for Metadata Access\nLineage Access Configuration\nSteps to Configure Snowflake as a Source\nStep 1: Add Snowflake as a Source\nStep 2: Configure Connection Settings\nStep 3: Select Database & Schema\nStep 4: Save the Source Configuration\nStep 5: Run a Scan\nReview Data Dictionary\nVideo Tutorial\nOverview\nThis article outlines the process of configuring Snowflake as a data source in DvSum, enabling integration for data cataloging and profiling. The steps provided apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations based on the specific platform.\nSetting Up Snowflake for DvSum Integration\nBefore configuring Snowflake in DvSum, a dedicated user and role must be created with the appropriate privileges for metadata access. Additionally, specific account-level grants are required to enable access to\nquery history\nand\ndata lineage\ninformation.\nBefore proceeding with the source configuration in DvSum, follow the steps below to set up Snowflake with the required user, role, and privileges.\nUser and Role Configuration for Metadata Access\n1. Metadata Access Configuration\nTo enable cataloging and profiling in DvSum, a Snowflake user must have access to metadata such as tables and columns. How this access is granted may vary depending on internal Snowflake policies.\nThe script below is a\nsample only\nand can be adapted as needed. Itâ€™s not required to run this script as-is â€” the key requirement is that the user should be able to execute the following test queries successfully.\nTest Queries â€“ Required Metadata Access\nThese queries can be used to confirm that the user has sufficient access to metadata:\n-- * Tables metadata (replace <DATABASE_NAME>)\nSELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.TABLES ORDER BY created DESC;\n\n-- * Columns metadata (replace <DATABASE_NAME>)\nSELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.COLUMNS ORDER BY table_name, ordinal_position;\nSample Script â€“ User and Role Creation\nIf a suitable user and role are not already available, the following script can be used as a reference to create them with minimal, controlled access. This is one possible approach â€” feel free to adapt or implement based on your organizationâ€™s standards.\nBefore executing the script, update the placeholders (\n<USERNAME>\n,\n<PASSWORD>\n,\n<ROLE_NAME>\n,\n<WAREHOUSE>\n, etc.) with appropriate values for your Snowflake environment.\n-- =====================================================\n-- Snowflake Script: Create User\n-- Purpose: Create user with minimal privileges for metadata access\n-- =====================================================\n\n-- Configuration placeholders - Update these values\n-- SET username = '<USERNAME>';\n-- SET password = '<PASSWORD>'; \n-- SET role_name = '<ROLE_NAME>';\n-- SET warehouse = '<WAREHOUSE>';\n\n-- =====================================================\n-- SECTION 1: USER CREATION\n-- =====================================================\n-- * Create user with secure defaults\nCREATE USER <USERNAME>\nPASSWORD = '<PASSWORD>'\nDEFAULT_ROLE = '<ROLE_NAME>'\nDEFAULT_WAREHOUSE = '<WAREHOUSE>'\nCOMMENT = 'User for reading metadata and query history';\n\n-- =====================================================\n-- SECTION 2: ROLE CREATION AND ASSIGNMENT\n-- =====================================================\n-- * Create custom role with minimal privileges\nCREATE ROLE <ROLE_NAME>\nCOMMENT = 'Role for accessing metadata and query history with minimal privileges';\n\n-- * Assign role to user\nGRANT ROLE <ROLE_NAME> TO USER <USERNAME>;\n\n-- =====================================================\n-- SECTION 3: WAREHOUSE ACCESS\n-- =====================================================\n-- * Grant warehouse usage for metadata queries\nGRANT USAGE ON WAREHOUSE <WAREHOUSE> TO ROLE <ROLE_NAME>;\n\n-- =====================================================\n-- SECTION 4: DATABASE AND SCHEMA ACCESS\n-- =====================================================\n-- * Grant USAGE on all databases for INFORMATION_SCHEMA access\nGRANT USAGE ON ALL DATABASES IN ACCOUNT TO ROLE <ROLE_NAME>;\nGRANT USAGE ON ALL SCHEMAS IN ACCOUNT TO ROLE <ROLE_NAME>;\n\n-- * Grant USAGE on future databases and schemas\nGRANT USAGE ON FUTURE DATABASES IN ACCOUNT TO ROLE <ROLE_NAME>;\nGRANT USAGE ON FUTURE SCHEMAS IN ACCOUNT TO ROLE <ROLE_NAME>;\n\n-- * Tables metadata (replace <DATABASE_NAME>)\n/*\nSELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.TABLES ORDER BY created DESC;\n*/\n\n-- * Columns metadata (replace <DATABASE_NAME>)\n/*\nSELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.COLUMNS ORDER BY table_name, ordinal_position;\n*/\n2. Lineage Access Configuration\nTo enable query history and lineage extraction in DvSum, the user must have access to Snowflakeâ€™s\nACCOUNT_USAGE\nviews and the appropriate privileges.\nAs with the previous section, the\ngoal\nis for the user to be able to successfully run the test queries below. If that works, lineage-related access is correctly configured. The grants script provided afterward is a\nreference\n, not a requirement to follow exactly.\nTest Queries â€“ Required for Lineage Extraction\n-- * Query history (all users, last 30 days)\nSELECT *\nFROM snowflake.account_usage.query_history\nWHERE start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP())\nORDER BY start_time DESC;\n\n-- * Views metadata\nSELECT * FROM snowflake.account_usage.views WHERE deleted IS NULL;\n\n-- * Functions metadata\nSELECT * FROM snowflake.account_usage.functions WHERE deleted IS NULL;\n\n-- * Stored procedures metadata\nSELECT * FROM snowflake.account_usage.procedures WHERE deleted IS NULL;\nSample Grants Script â€“ Lineage Access\nThe following script is intended as a reference to help grant the minimum required privileges for enabling query history and lineage features in DvSum.\nIf your existing setup already supports the test queries mentioned earlier, you may skip this step.\nOtherwise, use this script as a baseline and adapt it to align with your internal roles, naming conventions, and security policies.\n-- * Switch to ACCOUNTADMIN for account-level privileges\nUSE ROLE ACCOUNTADMIN;\n\n-- ===========================================================\n-- SECTION 5: ACCOUNT-LEVEL PRIVILEGES FOR LINEAGE METADATA\n-- ===========================================================\n-- * Grant MONITOR privilege for query history access across all users\nGRANT MONITOR ON ACCOUNT TO ROLE <ROLE_NAME>;\n\n-- * Grant access to SNOWFLAKE.ACCOUNT_USAGE views\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE <ROLE_NAME>;\n\n-- =====================================================\n-- SECTION 6: VERIFICATION QUERIES\n-- =====================================================\n-- * Query history (all users, last 30 days)\nSELECT *\nFROM snowflake.account_usage.query_history \nWHERE start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP())\nORDER BY start_time DESC;\n\n-- * Views metadata \nSELECT * FROM snowflake.account_usage.views WHERE deleted IS NULL;\n\n-- * Functions metadata\nSELECT * FROM snowflake.account_usage.functions WHERE deleted IS NULL;\n\n-- * Stored procedures metadata\nSELECT * FROM snowflake.account_usage.procedures WHERE deleted IS NULL;\nSteps to Configure Snowflake as a Source\nStep 1: Add Snowflake as a Source\nGo to the Administration dropdown and select the\nData Sources\ntab.\nClick the\nAdd Source\nbutton.\nIn the modal that appears:\nSelect\nSnowflake\nas the data source.\nProvide a meaningful\nSource Name\n.\nClick\nSave\n.\nStep 2: Configure Connection Settings\nAfter saving, you will be redirected to the\nConnection Settings\npage.\nEnable the checkbox for\nOn-premise Web Service\n(if applicable).\nSelect the\nSAWS\n(Secure Agent Web Service) that is set up and currently running.\nEnter the required connection details:\nURL\nWarehouse\nDatabase Login\nPassword\nClick\nAuthenticate\nto validate the connection.\nNote\n: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click\nhere\nStep 3: Select Database & Schema\nOnce authenticated, the\nDatabase Selection\nsection appears.\nChoose the database you need to scan (only one database can be selected).\nChoose one of the following options:\nScan\nall schemas\n(leave the schema checkbox unchecked).\nScan\nspecific schemas\n(check the schema checkbox and select schemas from the list).\nOnce it is checked then the list of available schemas will be displayed. User can select single or multiple schemas from the Available Schemas list and move them to the Selected Schemas tab on the right.\nStep 4: Save the Source Configuration\nScroll to the top of the page.\nClick the Done button in the top-right corner.\nClick Save to complete the setup.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\nStep 5: Run a Scan\nNavigate to the\nScan History\npage.\nClick the\nScan Now\nbutton.\nA job will be created, and once the status shows\nCompleted\n, the scan will be successful.\nAfter the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.\nOn the Scan Summary page, it will show all the insights of the scan i.e how many new tables and columns are fetched in this scan from the schemas we selected earlier.\nReview Database Tables\nNavigate to the Dictionary tab from the sidebar.\nClick on the Recently Refreshed tab.\nThis tab displays all tables fetched in the most recent scan.\nClick on the table name to access more detailed metadata and structure information.\nBy following these steps, you can successfully integrate Snowflake as a source in DvSum, enabling advanced data profiling and cataloging capabilities.\nVideo tutorial:\nWatch this quick video tutorial of how to add and configure an Snowflake source into DvSum app.",
    "scraped_at": "2026-02-02 15:29:24"
  },
  {
    "title": "Configure Microsoft SQL Server as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10618954602004-Configure-Microsoft-SQL-Server-as-a-Source",
    "content": "In this Article:\nOverview\nStep-by-Step Configuration Guide\nAdd a New Data Source\nConfigure Connection\nOption 1: Connect Using DvSum Web Service (Recommended)\nOption 2: Connect Using DvSum Edge Gateway (For On-Premises SQL Server)\nSelect Database & Schemas\nScan the Data Source\nBrowsing the Data Catalog\nVideo Tutorial\nAdditional Resources\nOverview\nThis article describes the steps needed to configure Microsoft SQL Server as a source in DvSum Data Catalog (DC). The same steps apply to configure a source in DvSum Data Quality (DQ) with only a slight variation.\nStep-by-Step Configuration Guide\n1. Add a New Data Source\nNavigate to\nAdministration\nâ†’\nData Sources\n.\nClick\nâŠ• Add Source\n.\nSelect\nMicrosoft SQL Server\n.\nProvide a name for the source and click\nSave\n.\n2. Configure Connection\nOnce the source is saved, the connection settings detail page appears.\nOption 1: Connect Using DvSum Web Service (Recommended)\nWhitelist the DvSum application\nby IP address as indicated in the settings.\nEnter the required connection details:\nHost\n: SQL Server hostname or IP.\nPort\n: Default is 1433 unless changed.\nInstance Name\n: (if applicable).\nDB Login & Password\n: Use SQL Authentication credentials.\nClick\nAuthenticate\nto verify credentials.\nOption 2: Connect Using DvSum Edge Gateway (For On-Premises SQL Server)\nInstall\ninstall a DvSum Edge Gateway\nbehind the firewall.\nConnect to SQL Server using the installed web service.\nAuthenticate the source.\n3. Select Database & Schemas\nOnce authenticated:\nChoose the\nDatabase\nfrom the available list.\n(Optional) Limit the scan to specific schemas by selecting them.\nClick\nDone\n(top-right corner) and then\nSave\n.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\n4. Scan the Data Source\nNavigate to\nScan History Page\n.\nClick\nScan Now\n.\nOnce the scan completes, the status will change to\nCompleted\n.\nClick on the\nScan Name\nto view the Scan Summary.\nThe Scan Summary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this scan.\nYour SQL Server connection is now fully configured and functional.\nBrowsing the Data Catalog\nNavigate to\nData Dictionary\n.\nClick the\nRecently Refreshed\ntab.\nView newly discovered tables and their details.\nVideo Tutorial\nWatch this quick video tutorial of how to add and configure an Microsoft SQL Server source into DvSum app.\nAdditional Resources\nRead more about\nDvSum Web Service (Cloud SAWS)\nRead more about\nDvSum Edge Gateway (On-Premises SAWS)",
    "scraped_at": "2026-02-02 15:29:31"
  },
  {
    "title": "Configure Oracle as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10583264674836-Configure-Oracle-as-a-Source",
    "content": "In this Article:\nOverview\nPrerequisites\nStep by Step configuration\nAdd Source\nConfigure Connection\nMethod 1: Connect Using DvSum Web Service\nMethod 2: Connect Using DvSum Edge Gateway\nSave the Connection\nScan the data source\nBrowsing the Database Columns\nLineage Support for Oracle\nQuery History in Oracle\nAdditional Resources\nOverview\nThis article describes the steps needed to configure Oracle as a source in DvSum Data Intelligence (DI). The same steps apply to configure a source in DvSum Data Quality (DQ Legacy) with only a slight variation.\nPrerequisites\nOracle configuration\nUser\n: Create a user to be used in DvSum, or identify an existing user.\nTable permissions\n: Grant read-only access to the user for schemas and tables that you would like to catalog and profile.\nSystem table permissions:\nGrant read-only access to the user for the following system tables:\nV$SQL\nDBA_USERS\nALL_USERS\nALL_OBJECTS\nALL_TABLES\nALL_SYNONYMS\nALL_TAB_COMMENTS\nALL_EXTERNAL_TABLES\nALL_TAB_COLS\nALL_COL_COMMENTS\nALL_CONSTRAINTS\nALL_CONS_COLUMNS\nIn order to grant access the below statement can be used:\nGRANT SELECT ON [TABLE_NAME] TO [USER]\nOracle GRANT documentation\nStep by Step configuration\n1. Add Source\nTo create a data source, navigate to Administration â†’ Data Sources â†’ âŠ•Add Source.\nSelect Oracle.\nGive the source a name, and save it.\n2. Configure Connection\nOnce the source is saved, you will be redirected to the connection settings page.\nMethod 1: Connect Using DvSum Web Service\nWhitelist the DvSum application by IP address as indicated.\nEnter the required connection details:\nHost\nPort\nSID or Service Name\nDatabase Login\nDatabase Password\nClick\nAuthenticate\nto verify the connection.\nMethod 2: Connect Using DvSum Edge Gateway\nIf your Oracle database is behind a firewall,\ninstall a DvSum Edge Gateway\nto enable a secure connection.\nLearn more about the differences in\nDvSum Web Service vs On-Premises Edge Gateway\n.\nOnce authenticated, the\nDatabase section\nwill appear.\nSelect the schemas to catalog.\n3. Save the Connection\nAfter entering credentials and selecting schemas, scroll to the top of the page.\nClick\nDone\nin the top-right corner.\nClick\nSave\nto finalize the connection setup.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\n4. Scan the data source\nNavigate to the\nScan History\npage.\nClick\nScan Now\nto initiate a scan.\nOnce the scan is complete, the status will update to\nCompleted\n.\nClick on the\nScan Name (e.g., SCN-000286)\nto view the scan summary.\nThe\nScan Summary Page\nprovides insights into the scan, including:\nThe number of new tables and columns fetched.\nYour Oracle connection is now fully configured and functional.\nBrowsing the Database Columns\nAfter a successful scan, explore the discovered tables and columns:\nGo to the Dictionaries dropdown and select the\nDatabase Columns\ntab.\nClick on the\nRecently Refreshed\ntab to see the latest scanned tables.\nClick on a table name to view detailed metadata.\nLineage Support for Oracle\nDvSum supports lineage extraction for Oracle sources using query history and stored procedure or package code analysis.\nTo enable this:\nEnsure the Oracle user has\nread-only access\nto the\nALL_SOURCE\nview to allow access to stored procedure or package definitions.\nAll relevant schemas containing the procedure or package code must be included in the source configuration.\nFor detailed instructions on how to set this up, refer to the article:\nLineage in DvSum\nQuery History in Oracle\nQuery history is enabled by default for Oracle, so no additional configuration is required. DvSum automatically scans query history for lineage analysis.\nTo verify that query history is being captured, you can run the following SQL query:\nSELECT v.*\nFROM v$sql v\nLEFT JOIN all_users du ON v.PARSING_USER_ID = du.user_id\nWHERE command_type IN (1, 2, 6, 7, 189)\nAND ROWNUM < 10;\nNote\n: This query returns recent DML operations using Oracle's\nv$sql\nview and can be used to confirm that query history is available to DvSum.\nAdditional Resources\nRead more about\nDvSum Web Service vs On-Premises Gateway\nRead more about\nGateway Installation",
    "scraped_at": "2026-02-02 15:29:37"
  },
  {
    "title": "Configure Microsoft Power BI as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/8960509886868-Configure-Microsoft-Power-BI-as-a-Source",
    "content": "In this Article:\nOverview\nPrerequisites\nStep-by-Step Configuration\nConfirm Availability of Datasets and Reports\nAdd Power BI as a Source in DvSum\nConfigure Connection Settings\nSave the Connection\nScan the Data Source\nBrowsing the Data Catalog\nTroubleshooting Tips\nVideo Tutorial\nNext Steps\nAdditional Resources\nOverview\nThis guide walks you through setting up Microsoft Power BI as a data source in DvSum, enabling seamless data integration, governance, and analysis. By adding Power BI as a source, you can scan reports and datasets, making them available for documentation, curation, and tracking lineage from their originating data sources.\nPrerequisites\nBefore proceeding, ensure you have the following:\nA valid Power BI account with appropriate permissions.\nThe Client ID, Client Secret, and Tenant ID for your Power BI application.\nAccess to the Power BI API for seamless integration.\nA DvSum account with access to Data Catalog (DC) or Data Quality (DQ).\nStep-by-Step Configuration\n1. Confirm Availability of Datasets and Reports\nLog in to your Power BI account.\nEnsure that reports and datasets are available and accessible based on your user permissions.\n2. Add Power BI as a Source in DvSum\nLog into DvSum Data Catalog (DC).\nNavigate to\nAdministration\nâ†’\nData Sources\nâ†’\nAdd Source\n.\nIn the pop-up window:\nSelect\nPower BI\nas the source.\nProvide a\nSource Name\n.\nClick\nSave\n.\n3. Configure Connection Settings\nOnce a new Power BI source is created, you will be redirected to the connection settings detail page of the new source. In the Credentials section in Connection settings, enter these three properties:\nClient ID\nClient Secret\nTenant ID\nAfter entering the credentials, authenticate the source.\nOnce the source is authenticated, a\n\"Workspaces\"\nsection will appear below the Authenticate button. By default, all workspaces will be scanned.\nTo limit the scan to specific workspaces, check the\n\"Limit to specific workspaces\"\noption. This will enable two tabs:\n\"Available Workspaces\"\nand\n\"Selected Workspaces\"\n, allowing you to choose which workspaces to include in the scan. You can also choose whether to\ninclude personal workspaces\nby selecting the corresponding checkbox.\nIf \"Limit to specific workspaces\" is checked, the workspaces will be listed in the â€œAvailable Workspacesâ€ tab. Select workspaces that are to be scanned, and move them to the â€œSelected Schemasâ€ tab.\n4. Save the Connection\nTo save, scroll up to the top. From the top right corner click on the â€œDoneâ€ button.\nAfter that click the â€œSaveâ€ button. The source will get saved successfully.\nNote:\nWhen adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.\nThe connection is saved, but it cannot be used until valid authentication details are updated.\n5. Scan the Data Source\nNavigate to the\nScan History Page\n.\nClick\nScan Now\nto initiate the scan.\nOnce completed, the status will change to\nCompleted\n.\nClick on the\nScan Name\nto open the Scan Summary page.\nThe summary will show the number of reports and datasets fetched.\nNavigate to Scan History Page and click the \"Scan Now\" button. When the scan is complete, the status will change to \"Completed\".\nAfter the scan completes, click on Scan Name and it will open the Scan Summary page of this scan.\nThe Scan Summary page displays all the insights of the scan. It indicates how many new workbooks and datasets were found in the scan. In the screenshot below it shows that 9 Reports and 5 Datasets were found.\nYour Power BI connection is now fully configured and functional.\nBrowsing the Data Catalog\nSemantic Models\nNow, letâ€™s analyze the semantic models that we got from this scan. Navigate to the Assets Dictionary on the left sidebar and apply the asset type filter \"BI Semantic Model\". You will be able to see all the semantic models. From there, go to the Â \"Recently Refreshed\" tab and select the semantic models that we have got in the recent scan.x\nThe Semantic Model can further have BI Tables if they were created in it, and the list of BI tables can be seen on the detail page of the Semantic Model:\nThe BI table detail page has an \"Overview\" tab, which contains all the properties of the BI tables, and a \"Data\" tab, which will contain the BI fields of the Table. Users can also see the lineage on BI Table if it exists:\nReports\nFrom the BI Reports listing, select a recently scanned Report, and the user will be redirected to the detail page of the Report. The screenshot below shows what the Reports detail page looks like.\nPlease note that only BI Reports have a separate listing page; for other BI assets, users will have to go to the Assets dictionary and apply the asset type filter\nOn the detail page, a description of the report is fetched from the Power BI report. This means this report has the same description in Power BI, and the same description is fetched and shows up here. The author of the report is basically the person who created this report in Power BI, so this value is also basically fetched. Further, we have a lineage of the Report, which actually shows us that the current Report is derived from which data set and which tables are connected to that data set.\nDashboards\nFrom the Assets dictionary listing, users can apply the asset type filter \"BI Dashboard,\" and all the dashboards (if there were any) can be seen on the listing page. The screenshot below shows what the dashboard detail page looks like:\nTroubleshooting Tips\nIf the test connection fails, confirm that prerequisites are complete and API validation has been performed (see\n[Configure Microsoft Power BI Permissions]\n).\nEnsure the Client Secret is valid and not expired.\nVerify that metadata scanning is enabled in Power BI.\nVideo Tutorial\nWatch this quick video tutorial on how to add and configure a Power BI source into DvSum app.\nNext Steps\nFor setting up permissions in Power BI:\n[\nConfigure Microsoft Power BI Permissions\n]\nAdditional Resources\nRead more about\nDvSum Web Service (Cloud SAWS)\nRead more about\nDvSum Edge Gateway (On-Premises SAWS)",
    "scraped_at": "2026-02-02 15:29:44"
  },
  {
    "title": "Supported Connectors/Data Sources",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/6067542488724-Supported-Connectors-Data-Sources",
    "content": "Introduction\nDvSum supports a wide range of connectors to help users integrate their data seamlessly. This article provides a comprehensive list of supported sources and key details for each. Whether you are working with databases, cloud data warehouses, or enterprise applications, DvSum ensures smooth connectivity to enhance data governance, quality, and analytics.\nWhat is a DvSum Connector?\nA DvSum Connector is a unified service that encapsulates native data connection drivers (e.g., JDBC, ODBC, SAP JCo, etc.), a scheduler, connection routing, security, and a server to handle requests. A single connector can service multiple data sources and users, providing a flexible and scalable solution for data integration.\nSupported Data Sources\nDvSum can connect to\n150+ data sources\n, including:\nEnterprise Applications\n(SAP, NetSuite, Salesforce)\nSaaS Applications\n(Google Analytics, HubSpot, Zendesk)\nOn-Premises Databases\n(Oracle, SQL Server, PostgreSQL, MySQL)\nCloud Databases\n(Snowflake, BigQuery, Amazon Redshift, Azure SQL)\nData Lakes\n(AWS S3, Azure Data Lake, Google Cloud Storage)\nFile-Based Sources\n(CSV, Excel, Google Sheets, Shared Drives)\nIf your application is not listed here, DvSum can build a connector to your application as long as it supports\nexternal application access through JDBC or ODBC connectivity\n.",
    "scraped_at": "2026-02-02 15:29:49"
  },
  {
    "title": "DvSum Web Service vs On-Premises Gateway",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/16448743421716-DvSum-Web-Service-vs-On-Premises-Gateway",
    "content": "In this Article\nWhat is DvSum Web Service?\nIP Whitelisting\nBefore IP whitelisting\nAfter IP whitelisting\nAdvantages Over Local On-Premises SAWS\nWhat is DvSum Web Service?\nDvSum Web Service (sometimes referred to as Cloud SAWS) is an online connector designed to facilitate seamless connectivity to sources available online. It provides analogous functionality to the on-premises gateway (sometimes referred to as SAWS), but it eliminates the need for users to install the connector locally on their machines. The DvSum Web Service enables users to establish connections with various online sources, such as APIs, databases, or services, without the burden of setting up and managing local installations.\nBy leveraging the DvSum Web Service, users can access and interact with online sources directly, streamlining the process of retrieving data or integrating with external systems. The connector is hosted in the cloud, offering convenience, scalability, and ease of use for users. They can utilize Cloud SAWS to securely connect to authorized online sources, leveraging its capabilities to fetch data, make requests, or perform desired actions without the need for local installation.\nDvSum connects to data sources using one of these:\nDvSum Web Service\nOn-Premises Gateway\nIn this article, we will be covering the process through which a source can be connected using the DvSum Web Service and how this compares to using an on-premises gateway.\nBy default when any source is added, it will be connected using the DvSum Web Service, so there is no extra step required for connection. But if the user chooses to authenticate using the on-premises gateway then they will need to enable the checkbox of On-premises Web Service.\nFor Configuring of Data Source (Snowflake) using On-Premises Local Saws, click\nhere\nIP Whitelisting:\nFor the Sources in which any Inbound Connection is not allowed or requires VPN for the connection to be made will be required to whitelist the given IP address in order to authenticate the credentials for that source. Currently, there are two sources for which IP whitelisting will be required:\nMicrosoft SQL Server\nOracle (Host selected as \"qasaws.dvsum.com\")\nOnce the IP address is whitelisted the network firewall will allow the particular connector to be able to connect with that particular source.\nBefore IP whitelisting\nAfter IP whitelisting\nNote\n: For whitelisting, users will be required to whitelist the DvSum IP where the organization's data source has been hosted. Admin of that source will be authorized to do that.\nAfter Successful Authentication, the user can choose any database and save the changes and scan that particular source to bring in the tables:\nAdvantages Over Local On-Premises SAWS\nNo Prior Installations Required:\nIn the case of Local On-premises SAWS, it should be locally installed and running but in the case of Cloud SAWS, no installation is required.\nMuch Faster than Local On-Premises SAWS:\nScans that run on Cloud SAWS take less time as compared to the scan when it is run using the local on-premises SAWS.",
    "scraped_at": "2026-02-02 15:29:55"
  },
  {
    "title": "DvSum Edge Gateway Installation (SAWS) for Data Catalog and CADDI on Linux",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/14609957062164-DvSum-Edge-Gateway-Installation-SAWS-for-Data-Catalog-and-CADDI-on-Linux",
    "content": "In this Article\nOverview\nDetailed Steps\nNetwork Configuration\nLinux Server Prerequisites\nAdditional Information\nInstallation Video\nOverview\nThis document describes the installation process for the DvSum Edge Gateway for DvSum Data Catalog and CADDI (DC). It covers all aspects of the installation including middleware needed and how to deploy the Gateway on a Linux machine.\nFor Windows, refer to\nDvSum Edge Gateway Installation (SAWS) for Data Catalog and CADDI\n.\nNote:\nGateway, Connector, SAWS (Stand-Alone Web Service) and webservice are used interchangeably referring to the DvSum Edge Gateway.\nDetailed Steps\nNetwork Configuration\nIf the DvSum Gateway is installed on a server with restricted outside access to specific URLs, you'll need to whitelist the following addresses on the network. This will enable the DvSum Gateway to communicate with the DvSum SaaS application and ensure the service functions properly and securely connects to the necessary resources on the network. Please configure the network settings accordingly to allow for this communication.\nURLs that must be whitelisted for outbound access on port 443:\nhttps://apis.dvsum.ai\nwss://3k7tif2dyi.execute-api.us-west-2.amazonaws.com/prod\nhttps://dvsum-app-data-prod.s3.amazonaws.com\nPorts that must be open for inbound access from end user computers:\nDefault: 8183\nCommon alternative: 443\nLinux Server Prerequisites\nEnsure that your Linux distribution has systemd support and x86_64 server architecture.\n# Confirm Architecture\nlscpu\n# Expected output includes:\n# Architecture: x86_64\nThe minimum memory requirement is 2GB (with 4GB recommended).\nThe minimum disk space requirement is 1GB (with 5GB recommended).\nDownload Link for\ntar.gz\nfile\nStep 1:\nDownload the DvSum service\ntar.gz\nfile from the provided link by DvSum Support.\nStep 2:\nCopy the downloaded tar.gz file to your Linux server.\nCreate a new directory, e.g., DvSum_Service, and copy the downloaded tar.gz into that directory.\nStep 3:\nNavigate to the directory where you have copied the tar.gz file and unzip it.\nUse the following command: \"tar -xzf DvSum_1.1.11.tar.gz\"\nStep 4:\nIn the DvSum application, create the DvSum gateway and copy the communication key.\nStep 5\n: Update the configuration.properties file with the copied communication key.\nThe default port will be set as 8183.\nStep 6:\nTo install the DvSum service, execute the shell script \"install_dvsum_service.sh\".\nAdditional Information\nInstallation Video\nVideo tutorial link\nVideo tutorial on how to install the DvSum Gateway on Linux.\nLinux Configuration\nIt can be useful/desirable to connect to the gateway on the standard SSL port 443. There are multiple ways to achieve this. One common configuration technique is to redirect incoming traffic on port 443 to the DvSum Gateway listening on port 8183 using iptables.\nsudo iptables -t nat -I PREROUTING -p tcp --dport 443 -j REDIRECT --to-ports 8183\nTroubleshooting\nConfirm that your gateway has started successfully and is now listening on port 8183.\nnetstat -at\n# If the machine has many services, it can be useful to filter the output of netstat\nnetstat -at | grep 8183\nConfirm that the machine is able to reach the DvSum SaaS application (confirm that it's not blocked by a firewall).\n# Sample commands to confirm connectivity\ncurl -v --tlsv1.2 https://apis.dvsum.ai/catalog/scan/sockets-url\ncurl -v --tlsv1.3 https://dvsum-app-data-prod.s3.amazonaws.com",
    "scraped_at": "2026-02-02 15:30:02"
  },
  {
    "title": "DvSum API Access and Authentication Management",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41740390031764-DvSum-API-Access-and-Authentication-Management",
    "content": "In This Article:\nOverview\nAuthentication Methods\nGenerate and Store Credentials\nRotate API Credentials\nDownload New Credentials\nDelete API Clients\nView Assigned APIs\nUser Roleâ€“Based API Access\nUsing API Documentation & Authentication Flow\nOverview\nDvSum provides a robust set of APIs that allow external systems to interact with the platform. These APIs can be used to:\nManage data elements (tables, fields, terms, etc.)\nTrigger approval workflows\nAutomate repetitive tasks\nAuthentication is handled through\nAWS Cognito client credentials\n, ensuring secure machine-to-machine communication. This allows seamless integration with enterprise processes while maintaining high security standards.\nRefer to\nDvSum API Documentation\nfor comprehensive API documentation.\nAuthentication Methods\nDvSum supports\ntwo OAuth2-based authentication methods\nfor accessing its APIs.\nBoth methods use\nAWS Cognitoâ€™s\nclient_credentials\nflow\nto obtain an\naccess_token\n, and both provide access to the\nsame set of APIs.\nThe key difference lies in\nwho owns the credentials\nand\nhow permissions (RBAC) are assigned.\nMethod 1: Service Authentication (Machine-to-Machine)\nUses\nOAuth2 Client Credentials flow\nvia AWS Cognito.\nCredentials (\nclient_id\nand\nclient_secret\n) are created under\nAdministration â†’ Account â†’ Application Security â†’ Manage OAuth2 Clients\n.\nRepresents a\nservice principal\nwith its own identity.\nPermissions are\nassigned directly to the auth client. No roles involved.\nIdeal for\nautomated workflows, background jobs, or system integrations\nwithout user involvement.\nMethod 2: User-Based Authentication\nAlso uses\nOAuth2 Client Credentials flow\nvia AWS Cognito.\nRequires updating the\nrole\nto allow\nAPI Access\nand associating\nscopes\nwith the role.\nCredentials (\nclient_id\nand\nclient_secret\n) are created under\nMy Settings â†’ API Credentials\nfor individual users.\nTokens inherit\nuser-specific roles and permissions\nvia RBAC.\nSuitable for scenarios requiring\nuser context, auditing, or personalized access control.\nAdd a Client\nNavigate to\nAdministration â†’ Account â†’ Application Security â†’ Manage OAuth2 Clients\nand click\nAdd\n.\nDefinition\nProvide a\nName\nfor the OAuth client and an optional\nDescription\n.\nSet the\nSecret Rotation Interval\nto define the duration after which the credentials expire. The interval can be set from\n7 days to 1 year\n.\nAlternatively, select\nNever Expires\n. A warning message highlights that indefinite credentials increase security risks, and regular rotation is recommended.\nThese are machine-to-machine credentials, that are not tied to any user account. They act as independent identities for scripts or background processes accessing DvSum APIs.\nScope\nSelect which APIs the client should have access to:\nAll Scopes\nâ€“ grants access to all APIs.\nSpecific Scopes\nâ€“ grant access only to selected APIs.\nAccess Level\n:\nRead\naccess is enabled by default.\nWrite\naccess can be enabled where applicable (e.g., Asset Listing).\nConfigure Alerts\nIn the\nAlert\ntab, you can configure notifications to be sent when credentials are about to expire.\nInternal Recipients\n:\nSelect from available\nGroups\nor\nUsers\nand move them to the\nSelected\nlist.\nThe OAuth client creator is automatically added by default.\nExternal Recipients\n:\nEnter comma-separated emails (e.g.,\nexample@email.com\n).\nOnly emails from\nwhitelisted domains\nare allowed.\nExpiry Alerts\n: Expiration alerts are sent\n3 days before credential expiry\n, then repeated daily until credentials are refreshed or rotated.\nGenerate and Store Credentials\nReview all settings.\nClick\nSave\nto generate the\nClient ID\nand\nClient Secret\nfor authentication.\nImportant:\nCredentials can only be viewed once. After closing the modal, they cannot be retrieved again.\nDownload and store them securely (e.g., password manager or encrypted vault).\nThe downloaded CSV file contains both the Client ID and Client Secret, as shown below.\nRotate API Credentials\nIf credentials are compromised or nearing expiry, you can\nrotate\nthem.\nClick the\nMore Options (â‹®)\nmenu for the client.\nSelect\nRotate\n.\nConfirm Rotation\nA message will confirm that new credentials will be generated.\nOld credentials remain valid for\n24 hours\n, giving time to update integrations.\nAfter 24 hours, old keys are automatically disabled.\nDownload New Credentials\nOnce rotated, new credentials are issued.\nCopy or download them immediately â€” they cannot be viewed again after closing.\nNote:\nRemember to update all applications and services with the\nnew Client ID/Secret\nbefore the old ones expire.\nDelete API Clients\nYou can delete one or multiple OAuth2 clients if they are no longer required. Deletion immediately revokes access for the associated Client ID and Secret.\nDelete an Individual Client\nClick the\nMore Options (â‹®)\nmenu next to the client.\nSelect\nDelete\n.\nConfirm the deletion.\nBulk Delete Multiple Clients\nUse the\nmain checkbox\nat the top of the OAuth2 client list to select all displayed clients, or select individual checkboxes for multiple clients.\nOnce selected, the\nDelete\noption will appear at the top of the list.\nClick\nDelete\nand confirm to remove all selected clients.\nNote:\nDeleting an OAuth2 client is irreversible. Any applications or integrations using the deleted credentials will immediately lose access. If access is still required, create a new client and update your systems with the new credentials.\nView Assigned APIs\nOn the\nManage OAuth2 Clients\npage, the\nResources\ncolumn shows how many APIs are assigned to each client.\nClicking the API count (e.g.,\nâ€œ6 APIsâ€\n) opens a detailed view of the assigned scopes.\nThe detailed view lists all APIs granted to the client, along with their\naccess levels\n(Read, Write, Delete).\nThis helps administrators quickly verify which endpoints each client can access and adjust scope if needed.\nSummary of Service-Based Authentication\nUse the generated credentials to authenticate via\nOAuth2 Client Credentials flow\n.\nRefer to the\nAPI documentation\nfor details on endpoints, request formats, and sample payloads.\nUser Roleâ€“Based API Access\nIn addition to service-based authentication clients, DvSum also supports\nuser roleâ€“based credentials\n. This allows administrators to enable API access tied directly to a userâ€™s role.\nEnable API Access for a Role\nNavigate to\nAdministration â†’ User Management â†’ User Roles\n.\nEither add a new role or edit an existing one.\nGo to the\nAPI Access\ntab.\nSelect\nEnable API Access\n.\nConfigure Role-Based API Access\nSet a\nSecret Rotation Interval\n(e.g., 180 days) or choose\nNever Expires\n(not recommended).\nSelect the\nAPIs\nthis role should have access to.\nBy default, any APIs already available to the role in the UI are pre-selected.\nAdditional APIs can be enabled as needed.\nSave the configuration.\nGenerate and Manage User Credentials\nOnce API access is enabled for a role:\nUsers assigned to that role will see a new\nAPI Credentials\ntab under\nMy Settings\n.\nFrom there, each user can:\nGenerate\ntheir own Client ID and Secret.\nRotate\ntheir credentials when needed.\nDelete\ntheir credentials if they are no longer required.\nNotes:\nThe\nexpiry interval\nis set at the role level by administrators. All users in that role inherit the same expiry.\nUsers\ncannot change expiry\n, but they can generate, rotate, and delete their own credentials.\nThe API access scope always reflects the permissions granted to the user role. If role permissions change, the user access changes automatically.\nOnce generated, users authenticate using the same flow as service-based clients: generate an\naccess token\nvia the Authentication API and use it in subsequent API calls.\nAPI Credentials Tab Availability\nBy default, the\nAPI Credentials\noption under\nMy Settings\nwill appear\ngreyed out\nfor users.\nTo activate this tab, the user must be part of a\nUser Group\nthat is assigned to a\nUser Role with API Access enabled\n.\nAPI Credentials disabled before Role Assignment:\nAPI Credentials Enabled after Role Assignment:\nManage Generated Credentials\nOnce the\nAPI Credentials\ntab is active and the user generates their credentials:\nDownload\n: Just like with service-based clients, credentials must be\ndownloaded immediately\n. They cannot be viewed again once the modal is closed.\nRotate\n: Users can rotate their own credentials to issue a new secret. The old secret remains valid for 24 hours, after which it is automatically disabled.\nDelete\n: Users can delete their credentials if they are no longer required. A new set can be generated at any time as long as their role has API access enabled.\nNote:\nRotation and deletion are managed at the\nuser level\n, but\nexpiry\nis controlled by the\nrole configuration\nset by administrators.\nUsing API Documentation & Authentication Flow\nAll available DvSum APIs are listed in the official API documentation:\nDvSum API Documentation\nYou can download the collection from this site and import it into\nPostman\nor another API client.\nBefore You Begin (Pre-requisites)\nBefore running the authentication request, make sure you have the following set up in\nPostman\n:\nCreate an Environment (if not already set):\nGo to the\nEnvironments\ntab in Postman and create a new environment.\nAdd a variable named\nauthUrl\nwith the value:\nauth.dvsum.ai\nClick\nSet active\nto activate the environment.\nSet up Headers for Authentication:\nIn the\nHeaders\ntab of your authentication request (\n/oauth2/token\n), ensure the following keys and values are added:\nContent-Type: application/x-www-form-urlencoded \nAccept: application/json \nAuthorization: Basic <base64_encoded_clientid_secret>\nOnce these pre-requisites are complete, proceed with\nStep 1: Authenticate with Client Credentials\nas described below.\nStep 1: Authenticate with Client Credentials\nUse the\nClient ID\nand\nClient Secret\nyou generated earlier to obtain an\naccess token\n.\nOpen the\nAuthentication API\n(first in the documentation).\nCombine your Client ID and Secret in the format:\nclient_id:client_secret\nConvert this string into\nBase64 encoding\nOnce generated,\ncopy the encoded value\nand\npaste it into the Authorization header\nin Postman as shown below:\nAuthorization: Basic <base64_encoded_clientid_secret>\nWhen you send the request, the API will return an\naccess token\n.\nStep 2: Use the Access Token\nThe access token is a\nBearer token\nvalid for\n1 hour\n.\nCopy this token and include it in the header for all subsequent API requests:\nAuthorization: Bearer <your_access_token>\nStep 3: Run an API Example\nOnce you have an access token, you can use it to call APIs that your client/role has access to.\nExample request:\nGET {{baseUrl}}/node-listing/gov-view?node_type=TBL&node_id=40837560\nHeaders:\nAccept\n:\napplication/json\nAuthorization\n:\nBearer <your_access_token>\nRunning\nGet Governance Views\nAPI in Postman:\nA successful call returns\n200 OK\nwith the response payload.\nImportant Notes\nAccess is restricted to the APIs that were explicitly granted in the\nScope\ntab when creating the client.\nTokens must be refreshed every hour using the same authentication API.\nTip:\nImport the full Postman collection from the\nDvSum API Documentation\nfor comprehensive examples of all available APIs.",
    "scraped_at": "2026-02-02 15:30:09"
  },
  {
    "title": "Logical View",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/44352096354964-Logical-View",
    "content": "In This Article\nOverview\nLogical View as an Asset\nCreating a Logical View\nRole of SQL Definition\nViewing a Logical View\nWhen to Use a Logical View\nSummary\nOverview\nA\nLogical View\nis an asset in DvSum that represents a\nlogical dataset defined using a SQL query\n, independent of physical database tables.\nWhile tables in DvSum are created directly from database metadata, Logical Views allow users to define datasets at a\nlogical or schema level\n. This makes them useful for modeling filtered, derived, or analytical datasets that do not exist as standalone physical tables.\nLogical Views behave like datasets within DvSum and can be viewed, managed, and used similarly to other assets.\nLogical View as an Asset\nLogical View is available as a\nsystem asset type\nand can be created from\nAsset Management\n.\nThese are visible in the\nAsset Dictionary\nand can be browsed and managed like other assets.\nCreating a Logical View\nTo create a Logical View:\nNavigate to\nAsset Management\nClick\nAdd Asset\nSelect\nLogical View\nas the Asset Type\nSelect the\nSchema\nProvide a\nName\nand optional\nDescription\nSave the asset\nAfter saving, the Logical View opens in its dataset view.\nRole of SQL Definition\nThe\nSQL Definition\ndefines the query used to construct the Logical View.\nUsers can enter a SQL query that selects and filters data from one or more underlying tables or views. The result of this query represents the dataset exposed by the Logical View.\nAfter entering the SQL query, click\nValidate\nto verify that the query is valid and can be executed successfully.\nIf validation is successful, a confirmation message is displayed.\nA SQL definition can be used to:\nSelect specific columns\nFilter records based on conditions\nApply business logic to shape the dataset\nOnce validated, the SQL definition can be saved and used as the Logical Viewâ€™s data source.\nViewing a Logical View\nAfter saving and validating the SQL definition, the Logical View behaves like a dataset within DvSum.\nThe\nOverview\ntab displays asset-level details\nThe\nData\ntab shows the dataset defined by the SQL\nThe\nData Quality\ntab allows data quality rules to be applied\nWhen to Use a Logical View\nLogical Views are useful when:\nData needs to be represented logically rather than physically\nA dataset is derived using filters or transformations\nPhysical tables do not align with analytical or validation needs\nYou want to apply data quality rules on a SQL-defined dataset\nThey provide flexibility without requiring changes to the underlying database.\nSummary\nLogical Views extend DvSumâ€™s asset model by allowing users to define and manage\nSQL-based logical datasets\nindependently of physical database tables.\nThey enable flexible data modeling, support analytical use cases, and integrate seamlessly with existing dataset and data quality workflows.",
    "scraped_at": "2026-02-02 15:30:15"
  },
  {
    "title": "Getting Started with Custom Assets and Attributes in DvSum DI",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/38888864440340-Getting-Started-with-Custom-Assets-and-Attributes-in-DvSum-DI",
    "content": "This article provides quick access to documentation related to custom asset and attribute configuration in the DvSum DI Application. Use the links below to navigate to the full articles.\nCreating and Managing Custom Asset Types\nLearn how to define custom asset types, configure relationships, and enrich assets within the application.\nDefining and Configuring Custom Attributes\nUnderstand how to create, configure, and control visibility of custom attributes across asset types.\nThese resources will help you tailor the metadata experience to your organizationâ€™s governance and data management needs.",
    "scraped_at": "2026-02-02 15:30:19"
  },
  {
    "title": "Managing Custom Assets in DvSum",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/38663390098964-Managing-Custom-Assets-in-DvSum",
    "content": "In This Article:\nOverview\nDefining a Custom Asset Type\nCustomizing the Asset Type\nConfiguring Relationships for an Asset Type\nAdding a Custom Asset\nEditing and Enriching the Asset\nOverview\nThe DvSum DI Application supports\nCustom Assets\n, allowing organizations to define and manage asset types that align with their unique data landscape. This capability enables teams to create asset categories tailored to specific governance frameworks, business processes, and domain requirements.\nCustom Assets are fully integrated into the DvSum platformâ€”they appear in the asset feed, support Custom Attributes, and participate in workflows, lineage, and search. This ensures a flexible and consistent metadata experience across the catalog.\nDefining a Custom Asset Type\nBefore creating a Custom Asset, the asset type must be defined. To do this:\nNavigate to the\nAsset Management\nsection from the left-side menu.\nClick on\nAsset Types\n.\nClick the\nAdd\nbutton at the top right.\nOn the listing page, a predefined set of default asset types is displayed. These system-provided assets are visible even if no custom assets have been added. Any newly created custom assets will appear alongside these default entries.\nThis opens the\nAdd Asset Type\npop-up.\nEnter the required details for the new asset type.\nClick\nAdd\nto save.\nCustomizing the Asset Type\nOnce the asset type is created, the\nAsset Type Detail\npage is displayed. This page acts as a\ntemplate\nfor all assets created using this type. Any configurations made hereâ€”such as custom attributes, tags, or relationshipsâ€”will automatically apply to all assets of this type. Changes made to the asset type template will reflect across all associated assets.\nAdd or edit\nCustom Attributes\nDefine\nTags\nSet up\nRelationships\nwith other asset types\nThese configurations allow you to enrich your custom asset with relevant metadata, improving its discoverability, traceability, and integration within workflows.\nConfiguring Relationships for an Asset Type\nAs part of configuring a custom asset type, you can define how it relates to other assets in the system using the\nRelationships\nsection.\nNavigate to the\nRelationships\ntab on the asset type detail page.\nYou can\nadd multiple relationship types\n, each with a:\nRelationship Type Name\n(e.g.,Â Contains,Â Governed By)\nTarget Asset Type\n(e.g.,Â BI Model and Report,Â Data Domain)\nEach relationship type includes a checkbox that allows you to\nenable or disable multi-asset linking\n.\nEnable this option to define relationships that support linking multiple assets of the selected type. For example, allowing an asset to be related to several database tables.\nYou can also remove a relationship type using the\ntrash can icon\nunder the Actions column.\nAdding a Custom Asset\nOnce your custom asset type is configured, you can begin adding assets to the system.\nSteps to Add a Custom Asset:\nNavigate to the Assets Dictionary\nFrom the\nleft-side navigation menu\n, click on\nAssets Dictionary\n.\nClick on \"Add Asset\"\nIn the\nAll Assets\nsection, click the green\nAdd Asset\nbutton at the top right.\nThis will open a form where you can enter details for the new asset, including selecting the appropriate\nAsset Type\n, filling in standard and custom attributes, and establishing relationships.\nEditing and Enriching the Asset\nAfter adding a new asset, youâ€™ll be directed to the\nAsset Detail Page\n, where you can view and manage all the metadata associated with the asset.\nThe configurations made on the\nAsset Type Detail Page\nâ€”such as\nCustom Attributes\n,\nTags\n, and\nRelationships\nâ€”are automatically applied to the asset. From here, you can:\nEdit or update custom attributes\n(e.g., URLs, rich text fields)\nAdd additional relationships\nto link the asset with other entities\nDelete existing relationships\nif they are no longer relevant\nAssign or update ownership\nvia the\nManaged By\nsection (e.g., Data Owner, Data Steward)\nAdd or modify tags\nlike Data Classification, Certification, or Dataset Quality\nInclude additional information\nsuch as descriptions, data domains, and other contextual metadata",
    "scraped_at": "2026-02-02 15:30:29"
  },
  {
    "title": "Creating and Managing Custom Attributes",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/38568362581908-Creating-and-Managing-Custom-Attributes",
    "content": "In This Article\nOverview\nAccessing Custom Attributes\nAdding Custom Attributes\nAttribute Detail Page\nUpdating Custom Attribute Values via import\nAttribute Configuration Options\nInput Patterns for Validation\nAttribute Visibility Control\nOverview\nThe DvSum DI Application now supports\nCustom Attributes\n, allowing users to tailor the asset metadata to their organizationâ€™s specific needs. Previously, asset attributes were limited to a predefined set of fields. With this enhancement, users can define and manage their own attributes, improving flexibility, governance, and contextual relevance across all asset types.\nCustom Attributes can be created for various asset categories such as databases, schemas, reports, models, and more. These attributes appear alongside standard fields in the asset detail pages and can be used to enrich asset metadata, support workflows, and enhance discoverability.\nAccessing Custom Attributes\nTo manage Custom Attributes in the DvSum DI Application:\nNavigate to the\nAdministration\nfrom the left-side menu\nClick on\nAsset Management -> Custom Attributes\n.\nThis page displays a list of all defined attributes with details such as:\nAttribute Label\nType (e.g., Date, Checkbox, URL, Pick List)\nApply To Asset Types\nUsage Count\nModified By\nModified On\nEnabled (On/Off)\nAdding Custom Attributes\nTo add a new Custom Attribute:\nOn the\nCustom Attributes\npage, click the green\nAdd\nbutton at the top right.\nA pop-up window titled\nAdd Attribute\nwill appear.\nFill in the following fields:\nType\n(Required)\nâ€“ Choose from the dropdown list:\nCheckbox\nDate\nDate Time\nNumber\nPick List\nRich Text\nMark Down\nSingle Line Text\nUpload\nURL\nLabel\n(Required)\nâ€“ Enter a name for the attribute.\nDescription\n(Optional)\nâ€“ Add any notes or context for the attribute.\nClick\nAdd\nto save the attribute or\nCancel\nto discard.\nCustom Attributes are\nreusable\nand can be applied across\nmultiple asset types\n, enabling consistent metadata enrichment and governance.\nAttribute Detail Page\nAfter the attribute is added, a detail page is displayed to complete the setup. This page is divided into two sections:\nBasic Configuration\nThis section displays the core details of the attribute, including type, label, and description. An option is available to enable a tooltip, which will appear on asset detail pages to provide additional context.\nAttribute Configuration\nThis section allows further customization of the attribute. Configuration options include validation rules, default values, mandatory status, and multi-value support. A checklist is provided to specify which asset types the attribute should apply to, ensuring relevance and consistency across the catalog.\nThe\nShow Tooltip\ncheckbox enables a tooltip to be displayed on asset detail pages. When hovered, the attributeâ€™s description will appear as a tooltip, providing users with quick contextual information without needing to open the full attribute details.\nAttribute Configuration Options\nThe Attribute Configuration screen includes several checkboxes that define how the custom attribute behaves across assets:\nMandatory\nWhen enabled, this setting ensures that a value must be provided for the attribute. It enforces data completeness by preventing asset creation or updates without a valid entry.\nAllow Multiple Values\nThis option allows the attribute to accept more than one value. It is useful for cases where an asset may be associated with multiple tags, categories, or other multi-select metadata.\nFine-Grained Control (Apply To Asset Types)\nEnabling this checkbox allows attribute behaviorâ€”such as mandatory status and default valuesâ€”to be customized per asset type. This overrides global settings and provides more granular governance across different asset categories.\nUpdating Custom Attribute Values via import\nCustom attribute values can be added or updated for the assets to which the attributes are applied. To update custom attribute values in bulk, first add the custom attribute column to a custom view for the relevant asset type and then download that view. The downloaded Excel template will automatically include all custom attributes applied to the assets.\nNote:\nData Import feature can be used to add or update values for custom attributes, the custom attributes themselves cannot be imported.\nFor more information on using the Data Import feature, refer\nData Import\narticle.\nInput Patterns for Validation\nFor attribute types such as\nURL\n,\nNumber\n, and\nDate\n, input patterns can be defined to enforce structured data entry. The\nAttribute Validation\nsection provides a dropdown with predefined patterns, and also includes an option to create a custom pattern.\nClicking on\nCreate a Custom Pattern\nopens a configuration form where a new pattern can be defined using regular expressions. The form includes fields for pattern name, regex, description, and sample values to illustrate expected input formats.\nNote\n:\nThe regular expressions used in custom attribute validation follow the\nECMAScript (JavaScript) regex standard\n, which is natively supported by modern browsers. This allows you to use familiar JavaScript-style regex patterns for creating flexible and intuitive validations.\nThis flexibility allows organizations to tailor validation rules to match specific formatting requirements and ensure consistent data quality across assets.\nAttribute Visibility Control\nThe\nCustom Attributes\npage includes options to manage attribute visibility and lifecycle. Attributes can be toggled between\nActive\nand\nInactive\nusing the\nAttribute Visibility\ndropdown. This allows administrators to control which attributes are available for use without permanently deleting them.\nVisibility can be managed in multiple ways:\nMass Update\nâ€“ Multiple attributes can be selected and updated in bulk using the visibility dropdown.\nSingle Attribute Toggle\nâ€“ Individual attributes can be activated or deactivated directly from the listing.\nWithin Attribute Detail Page\nâ€“ Visibility settings can also be adjusted from within the attributeâ€™s configuration screen.",
    "scraped_at": "2026-02-02 15:30:35"
  },
  {
    "title": "Assets Dictionary",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35793716144532-Assets-Dictionary",
    "content": "In This Article\nOverview\nAccessing the Assets Dictionary\nAdding an Asset\nDownloading Assets from the Asset Dictionary\nMass Update\nSteps to Perform Mass Update\nManaging Tags in Assets Dictionary\nAdding Tags\nRemoving Tags\nDeleting Assets in Assets Dictionary\nImporting Assets in the Assets Dictionary\nExpanded Asset Coverage\nViewing and Navigating Assets\nAsset Detail Pages\nRelationships Section in the Overview Tab\nOverview\nThe Assets Dictionary in the DvSum DI Application provides a centralized and structured view of all assets within the system, enhancing discoverability, governance, and consistency. By standardizing asset management across various categories, including databases, schemas, reports, and models. It ensures seamless integration with workflows while giving users greater control over their data.\nAccessing the Assets Dictionary\nOn the left-side menu of the DvSum application, click on\nAssets Dictionary\nto navigate to its detailed page.\nThis page displays the consolidated list of all the assets in the DvSum application.\nAdding an Asset\nThe Add Asset feature in the Assets Dictionary allow users to add the custom Assets to the system.\nSteps to Add a Custom Asset\nNavigate to the Assets Dictionary\nFrom the left-side navigation menu, click on\nAssets Dictionary\n.\nClick on â€œAdd Assetâ€\nIn the\nAll-Assets\nsection, click the green\nAdd Asset\nbutton at the top right.\nA form will open where you can enter details for the new asset. Only custom or user-defined assets can be added manually using the Add Asset option.\nClick\nAdd\nto save.\nAfter adding a new asset, youâ€™ll be directed to the\nAsset Detail Page\n, where you can view and manage all the metadata associated with the asset.\nThe configurations made on the\nAsset Type Detail Page\nâ€”such as\nCustom Attributes\n,\nTags\n, and\nRelationships\nâ€”are automatically applied to the asset. From here, you can:\nEdit or update custom attributes\n(e.g., URLs, rich text fields)\nAdd additional relationships\nto link the asset with other entities\nDelete existing relationships\nif they are no longer relevant\nAssign or update ownership\nvia the\nManaged By\nsection (e.g., Data Owner, Data Steward)\nAdd or modify tags\nlike Data Classification, Certification, or Dataset Quality\nInclude additional information\nsuch as descriptions, data domains, and other contextual metadata\nSearch Asset from Asset Dictionary\nYou can also search governance assets directly using the\nsearch icon\nin the\nGovernance View\nand across listing pages.\nThis provides seamless access to governed terms, policies, and related metadata without switching contexts, improving discoverability and navigation.\nDownloading Assets from the Asset Dictionary\nDvSum enables users to export data from the Asset Dictionary into an Excel file. This allows you to store, analyze, or share data offline as needed.\nClick\nDownload\n.\nAn Excel file containing the asset data will be generated and automatically downloaded.\nMass Update:\nThe\nMass Update\nfeature in the\nAssets Dictionary\nallows users to update multiple asset records simultaneously.\nSteps to Perform Mass Update:\nSelect the assets you want to update.\nClick on\nMass Update\nfrom the top menu.\nIn the\nMass Update\npop-up, choose a field to update from the\nField\ndropdown. Available options include:\nStatus\nData Domain\nSelect the appropriate value for the chosen field.\nClick\nApply\nto confirm the changes or\ncancel\nto discard them.\nManaging Tags in Assets Dictionary\nAdding Tags\nSelect the assets to which you want to add tags.\nClick on the\nTags\ndropdown and select\nAdd Tags\n.\nIn the\nAdd Tags\ndialog, select the desired tag(s) from the dropdown.\n(Optional) Check the\nOverwrite Tags\noption if you want to replace existing tags.\nClick\nAdd\nto apply the tags.\nRemoving Tags\nSelect the assets from which you want to remove tags.\nClick on the\nTags\ndropdown and select\nRemove Tags\n.\nIn the\nRemove Tags\ndialog, select the tag(s) to be removed.\nClick\nRemove\nto finalize the changes.\nDeleting Assets in Assets Dictionary\nSelect the assets you want to delete.\nClick on\nDelete Asset(s)\n.\nA confirmation dialog will appear stating:\n\"This action will remove the asset(s). This action cannot be undone. Are you sure?\"\nClick\nDelete\nto proceed or\nCancel\nto abort the action.\nNote:\nAsset deletion is irreversible. Verify your selection carefully before proceeding. Only Deprecated assets can be deleted.\nImporting Assets in the Assets Dictionary\nWhen selecting a specific asset in the Assets Dictionary, users will also see an\nImport\noption. This allows users to bring in updated or additional asset information directly from supported sources.\nNavigate to the asset you want to update.\nClick on the\nImport\noption from the top menu.\nFollow the prompts to upload or sync data.\nNote:\nThe Import feature functions the same way as described in the Data Import article.\nFor more information on using the Data Import feature, refer\nData Import\narticle.\nExpanded Asset Coverage\nThe Assets Dictionary now includes detail pages for additional asset types beyond tables, columns, reports, and models. The newly supported assets include:\nDatabases\nSchemas\nBusiness Intelligence (BI) sources\nSource fields\nETL programs\nEach of these asset types now has a dedicated detail page, providing structured information for better governance and traceability.\nCreating and Managing Views\nThe\nAsset Dictionary\nalso supports the creation of\ncustom views\n, allowing users to filter and organize datasets based on their preferences. These views can be accessed from the dropdown menu at the top of the asset list, which includes both system-generated and user-created views.\nSelecting a view updates the display to show only the relevant assets and opens a\ngovernance-focused view\nfor the selected asset type. From here, users can perform key actions such as:\nDownloading or importing datasets\nSaving and sharing views\nFiltering by criteria like \"Recently Refreshed\" or \"Assigned to Me\"\nThis functionality enhances user productivity by enabling focused asset management and streamlined governance workflows.\nViewing and Navigating Assets\nThe Assets Dictionary consolidates all assets into a single view, allowing users to:\nSearch for specific assets easily\nClick on any of the Names to display the detail page\nAccess detail pages for a comprehensive view of asset properties\nAssets Detail Pages\nEach asset type features a standardized detail page, which includes an\nOverview\ntab displaying key properties such as:\nDescription\nâ€“ A brief summary of the asset\nData Domains\nâ€“ Classification of the asset based on its data context\nGovernance Metadata\nâ€“ Information related to data ownership, compliance, and usage\nAdditional Info\nâ€“ Supplementary details related to the asset\nSimilar Assets\nâ€“ Suggestions of assets with shared characteristics\nRelationships\nâ€“ Links to related assets within the system\nRelationships Section in the Overview Tab\nThe\nRelationships\nsection provides a structured view of how an asset is connected to other assets within the system. It displays all associated assets, helping users understand dependencies and data flow.\nNote:\nThe domain assigned to an asset is automatically inherited by all its dependent assets.\nMore information on\n[How to Link Terms to Columns]\nand\n[How to Manage Relationships between Assets]",
    "scraped_at": "2026-02-02 15:30:41"
  },
  {
    "title": "Executing Jobs via API",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/30417646570132-Executing-Jobs-via-API",
    "content": "In this article:\nOverview\nPrerequisites & API Authentication\nAPI Endpoints for Job Execution\nRequest Parameters\nJob Setup\nExamples of Job Execution via API\nManual Execution\nUsing Postman\nScript Automation\nOverview\nDvSum allows users to execute jobs programmatically using APIs. This guide provides step-by-step instructions to trigger jobs, check execution status, and handle API responses efficiently.\nEach job can run in one of two distinct modes: it can either be scheduled to run at predetermined times or triggered via an API for on-demand execution. However, only one method can be active at a time; jobs cannot operate as both scheduled and API-triggered simultaneously. The mode of execution should be chosen based on specific requirements to ensure consistent operation under a single, exclusive mode.\nFor more details on how to create a job, refer to the\nCreating\na\nNew\nJob\narticle.\nPrerequisites\nBefore executing a job via API, ensure you have the following:\nA Job:\nThe job must be configured for API execution (it cannot be scheduled).\nOAuth Client Credentials:\nYou can create an OAuth2 Client from the Application Security tab and use the Client ID and Secret or enable API Access at the User Role level.\nAccess Token:\nUsing the above OAuth credentials, generate an access token through the Authentication API mentioned in the\nAPI Documentation.\nRefer to the article\nâ€˜\nDvSum API Access and Authentication Management\nâ€™\nfor details on creating an authentication client and generating access tokens.\nAPI Endpoints for Job Execution\n1. Execute Job (POST Request)\nThis endpoint triggers the execution of a job.\nEndpoint:\nPOST https://apis.dvsum.ai/integration/jobs/[job_id]/execute\n2. Get Job Execution Details (GET Request)\nThis endpoint retrieves the status and details of the job execution.\nEndpoint:\nGET https://apis.dvsum.ai/integration/jobs/[execution_id]/details\nRequest Parameters\nInclude the following headers for both POST and GET requests:\nAuthorization:\nBearer <access_token>\nContent-Type:\napplication/json\nNote:\nThe access token is valid for 1 hour. Generate a new token using your Client ID and Secret when it expires.\nStep 1: Configure the Job for API Execution\nNavigate to the\nSchedule\ntab in the DvSum platform.\nSelect the schedule type as\nExecute via API\n.\nFor detailed instructions, refer to the\nCreating\na\nJob\narticle.\nStep 2: Generate API URLs\nClick on\nGenerate API URL\n. This will generate both the POST and GET request URLs needed for API execution.\nExamples of Job Execution via API\nTo facilitate job execution via API in DvSum, three main approaches are available: manual execution, using Postman, and script automation. Each method provides options for managing job execution and monitoring through the API, with step-by-step instructions provided for each option below. The approach can be selected based on workflow and technical requirements.\nManual Execution\nThis section covers the process for manually executing jobs via API. If you prefer direct control over job execution, follow this step-by-step guide.\nStep-by-Step Guide to Executing Jobs via API\nStep 1: Execute the Job\ncurl -X POST\nhttps://apis.dvsum.ai/integration/jobs/[job_id/execute\n-H \"Authorization: Bearer <access_token>\" \\\n-H \"Content-Type: application/json\" \\\nCopy the POST request URL and paste it into your command prompt or API client (e.g., Postman).\nRun the command to execute the job:\nPOST\n\"https://apis.dvsum.ai/integration/jobs/[job_id]/execute\"\nStep 2: Retrieve Execution Details\ncurl -X GET\nhttps://apis.dvsum.ai/integration/jobs/[execution_id]/details\n-H \"Authorization: Bearer <access_token>\"\n-H \"Content-Type: application/json\"\\\nAfter executing the job, copy the GET request URL and paste it into your command prompt or API client.\nReplace the following placeholders:\n[execution_id]\nwith the execution ID from the POST request.\n[<access_token>]\nwith your valid access token obtained using OAuth Client credentials or User Role based credentials\nRun the GET request to retrieve job execution details:\nGET\n\"https://apis.dvsum.ai/integration/jobs/[execution_id]/details\"\nStep 3: View Execution Details\nThe GET request will display the status and other details of the job execution once completed.\nStep 4: Fetch Exception Records\nCase1:\nOn-prem Gateway\nFor an on-prem gateway, exception records can be downloaded, and the requested URL can be accessed using the provided\nx-api-key\nin the response.\nCase 2:\nCloud Gateway\nFor a cloud gateway, exception records can be downloaded using the\npre-signed URL\nprovided in the response.\nNote:\nException records can only be downloaded up to a limit of\n10,000 records\n.\nUsing Postman\nEnhance the process by importing the API requests into Postman for more structured testing. This method allows you to interact with the API and automate requests efficiently visually.\nCalling API Endpoints for Job Execution\n1. Execute Job (POST Request)\nThis endpoint triggers the execution of a job.\nEndpoint:\nPOST\nhttps://apis.dvsum.ai/integration/jobs/[job_id]/execute\n2. Retrieve Execution Details (GET Request)\nThis endpoint retrieves the status and details of the job execution.\nEndpoint:\nGET\nhttps://apis.dvsum.ai/integration/jobs/[execution_id]/details\n3. View Execution Details\nThe provided download request URL in the execution details can be used to view the execution details.\nScript Automation\nAutomate job execution by using a script. This method is ideal for users who want to integrate the API into their workflow. Sample scripts can be provided for reference to help automate repetitive tasks.",
    "scraped_at": "2026-02-02 15:30:47"
  },
  {
    "title": "Creating a new Job",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/27001229007380-Creating-a-new-Job",
    "content": "In this article:\nOverview\nStep-by-Step Guide\nStep 1: Navigate to the Job Creation Page\nStep 2: Choose a Job\nStep 3: Configure Job Parameters\nStep 4: Configure Notification Settings\nStep 5: Set Job Scheduling Options\nScheduling Information\nOne-Time Execution\nRecurring Execution\nEnd Conditions\nAdd a Job from Job Detail Page\nOverview:\nA job in DvSum automates data validation, transformation, and integration tasks by executing a specified number of Data Quality (DQ) rules and profile tables. Jobs can be scheduled or triggered via API, but not both simultaneously. This guide walks you through creating a new job, configuring its parameters, and scheduling it for execution.\nStep-by-Step Guide\nStep 1: Navigate to the Job Creation Page\nLog in to DvSum and navigate to the\nDQ Rules\ntab.\nSelect any rule, click on\nMore Actions\n, and then\nAdd to Jobs\n.\nStep 2: Choose a Job\nFrom the dropdown, select an already created job or click on\nCreate a New Job\n.\nIf creating a new job, a dialog box will appear. Click\nOK\n.\nStep 3: Configure Job Parameters\nIn the\nDefinition\ntab, add a job description.\nChoose the selection type:\nFixed\nDynamic\nApplying Filters When Using Dynamic Selection\nWhen configuring the\nSelection Type\n, if you choose\nDynamic\n, filters must be applied to define which\nrules or tables\nwill be included in the job at runtime.\nTo apply filters:\nSelect the\nDynamic\noption under\nSelection Type\n.\nUnder the\nAdd Criteria\nsection, click\nAdd Condition Group\nto define one or more filter conditions.\nOnce the conditions are set, click\nApply Filter\n.\nAfter applying the filter:\nThe\nSelected Rules\nor\nSelected Tables\nsection will automatically populate based on the applied criteria.\nThese selections are\nnot editable\nâ€” you cannot manually add or remove individual items.\nNote: If no filter conditions are applied, no rules or tables will be included dynamically in the job.\nStep 4: Configure Notification Settings\nIn the\nNotification\ntab, you can define how and when email notifications are sent, and specify who should receive them.\nEmail Notification Options\nWhen you first open the Notification tab, you'll see two options:\nSend schedule completion email\nSend email on job delay\nOnce\nSend schedule completion email\nis selected, an additional option appears:\nSend email on alerts\nAll three options can be selected together. Here's what each one does:\nNotification Option\nPurpose\nSend schedule completion email\nNotifies recipients when the job finishes its scheduled execution.\nSend email on alerts\nSends notifications when alert conditions are triggered during a job.\nSend email on job delay\nAlerts recipients if the job exceeds its expected duration.\nJob Delay Alert Configuration\nIf\nSend email on job delay\nis selected, a configuration panel appears with the following settings:\nDuration Threshold\nAutomatically Determined Duration\n(default): Uses Interquartile Range (IQR) outlier detection on job run times to identify delays. Manual edits are allowed until four executions, after which thresholds auto-adjust.\nSet Custom Duration\n: Manually specify the maximum expected duration. An alert is triggered if the job exceeds this time.\nInput format:\nHH:MM:SS\nAlert Frequency\nDefine how often reminder emails should be sent after a delay is detected.\nInput format:\nHH:MM:SS\nEmail Recipients\nYou can specify both internal and external recipients:\nInternal Recipients\nSelect users or groups from the\nAvailable\nlist.\nMove selections to the\nSelected\nlist using the arrow buttons.\nAt least one internal recipient is required.\nExternal Recipients\nEnter comma-separated email addresses.\nOnly emails from whitelisted domains are allowed.\nExample:\nexample@email.com, another@email.com\nStep 5: Set Job Scheduling Options\nNavigate to the\nSchedule\ntab.\nChoose a schedule type:\nScheduled\n(For scheduled execution)\nExecute via API\n(Refer to the\nExecuting Job via API article\n)\nScheduling Information\nOne-Time Execution\nChoose a\nstart date\nand\ntime\n.\nSet an\nend date\n, if applicable.\nRecurring Execution\nDaily\nChoose the number of days between executions.\nWeekly\nSelect the number of weeks and the days for execution.\nMonthly\nChoose the number of months.\nSelect\nrepeat on days of the month\nor\nweek of the month\n.\nEnd Conditions\nNever Ends\n: The job runs indefinitely.\nEnds After\n: Set the number of occurrences.\nEnds On\n: Set a specific end date.\nAdditional Way to Add a Job: From the Job Detail Page\nApart from adding jobs via the DQ Dictionary tab, you can also add a job directly from the\nJob Detail page\n. This provides a convenient way to manage jobs and quickly add related jobs without navigating back to the dictionary.\nHow to add a job from the Job Detail page:\n1-From administration Tab navigate to the\nJobs\ndrop down and select\nDefinitions\nin the\nDI tool\n.\n2- Click the\nAdd Job\nbutton located at the top.\n3- Follow the same steps to select rules, configure parameters, notifications, and scheduling as described earlier.\nCloning Jobs:\nFrom the Job Detail page, you can also use the\nMore Actions\nmenu to quickly clone an existing job. This allows you to duplicate the job configuration and make adjustments without creating it from scratch.\nJob can also be cloned from the\nJob Detail page\nby navigating to\nEdit > Clone Job\n.\nProfile Tables\nA\nProfile Table\njob is used to analyze the structure and quality of your data by generating profiling statistics on selected tables. Profiling helps you understand the characteristics of your data before applying rules or transformations.\nPurpose of Profile Jobs\nIdentify data distribution and patterns.\nDetect data quality issues such as missing values, duplicates, or anomalies.\nGain insights into column-level statistics (null counts, distinct values, min/max, etc.).\nHow to Create a Profile Job\n1. Job Category\nSelect\nProfile Tables\nas the job category. This specifies that the job will profile data instead of executing rules.\n2. Follow the same steps to select tables, configure parameters, notifications, and scheduling as described earlier.\nScan Source\nA\nScan Source\njob is used to discover metadata from a data source, helping build a catalog of data assets. Optionally, the scan can include profiling and lineage for deeper insights.\nPurpose of Scan Source Jobs\nDiscover and catalog datasets, tables, columns, and schemas\nGenerate metadata inventory for governance and data discovery\nOptionally include profiling statistics and data lineage mapping\nHow to Create a Scan Source Job\nJob Category\nSelect\nScan Source\nas the job category. This indicates that the job will scan a data source for metadata.\nData Source and Scan Catalog Options\nChoose the appropriate\nData Source\nfrom the dropdown.\nUnder\nScan Catalog\n, you can optionally select:\nProfile\nâ€“ to include profiling statistics in the scan\nLineage\nâ€“ to analyze data relationships and flow\nIf neither option is selected, the job will schedule a\ncatalog-only scan\n, capturing only metadata like table names, columns, and schema details.\nFollow the same steps to configure parameters, notifications, and scheduling as described earlier.",
    "scraped_at": "2026-02-02 15:30:51"
  },
  {
    "title": "Data Exclusion Policy",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/39824964543764-Data-Exclusion-Policy",
    "content": "In this Article:\nOverview\nAccessing and Applying the Data Exclusion Policy\nManaging Exclusion Policies\nVisual Examples: How Exclusion Affects Views\nData Dictionary â€“ Column View\nData Quality (DQ) Dictionary â€“ Profiling Tab View\nRestricting the Data View by Data Domain\nRestricting the Data View by Source\nRestricting the Data View by Databases\nOverview\nThe Data Exclusion Policy feature empowers administrators to control data visibility based on user roles. This policy ensures that sensitive table data is hidden from users who should not have access to it. Once access to a table is revoked, associated column data and rule-level exception data are also automatically restricted.\nThis functionality helps safeguard sensitive data and streamlines the user experience by ensuring that each user role only interacts with data they are permitted to view.\nAccessing and Applying the Data Exclusion Policy\nTo configure a new Data Exclusion Policy:\nNavigate to:\nAdministration â†’ Account â†’ User Security\nScroll to the\nSample Data Exclusion Policy\nsection.\nClick\nAdd Exclusion Policy\n.\nFill in the Required Information:\nPolicy Name:\nSpecify a unique and identifiable name for the policy.\nRole:\nSelect the role to which this policy will apply.\nUsers can then configure criteria by selecting specific columns. Multiple condition types are available, allowing flexible and targeted policy setup based on the selected column values.\nNote:\nThe data will be hidden only for users assigned to the role(s) selected while creating the policy.\nUsers with all other roles will continue to view the data without any restriction.\nManaging Exclusion Policies\nAdministrators can manage existing exclusion policies through the following actions:\nBulk Actions\nSelect multiple policies to update visibility settings or delete them in one go.\nEdit/Delete\nClick the\nellipsis (â‹¯)\nnext to a policy to make changes or remove it.\nActivate/Deactivate\nUse the\ntoggle switch\nto enable or disable a policy without permanently deleting it.\nVisual Examples: How Exclusion Affects Views\n1. Data Dictionary â€“ Column View\nBefore Adding the Role in the Exclusion Policy:\nAll users can view every column and its metadata in the Grid View. Showing all the metadata inside the Column.\nAfter Adding the Role in the Exclusion Policy:\nColumn data will be completely hidden from users assigned to restricted roles.\nIf you select the Dictionary view and click on any column name to view the\nColumn Reference\npage, the visual distribution will no longer be visible. Instead, a message will be displayed stating that the information is restricted due to the applied policy.\n2. Data Quality (DQ) Dictionary â€“ Profiling Tab View\nBefore Adding the Role in the Exclusion Policy:\nOn the\nRules Detail\npage, Exception data from all rules are visible to the users.\nAfter Adding the Role in the Exclusion Policy:\nAll the exception data of the rules will be hidden from users with restricted roles.\nImportant:\nEven if a user is part of a role covered by the exclusion policy, they can still access exception data for rules they own.\nExample:\nIn this case, the user\nAmulya\nbelongs to a role restricted by the exclusion policy. However, because she is the owner of the rule, she retains access to its exception data.\nThe rule that Amulya owns:\nThe rule that Amulya does not own:\n3. Restricting the data view by Data Domain.\nWhen a criteria condition is applied to a Data Domain to limit view access, all datasets within that domain will have their column-level details hidden in the Data tab, restricting users from viewing individual columns.\n4. Restricting the data viewÂ by Source\nSource-level restrictions limit access to column details in the Data tab. This control applies uniformly across all datasets linked to that source, supporting consistent data governance and preventing unauthorized access to sensitive information.\n5. Restricting the data view by Databases:\nWhen a criteria condition is applied to a database, profiling and exception details are restricted across all related databases as per the exclusion policy. This cascading restriction plays a key role in maintaining data governance and security.",
    "scraped_at": "2026-02-02 15:30:59"
  },
  {
    "title": "Data Classification",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/39292360682900-Data-Classification",
    "content": "In This Article\nOverview\nTagset\nApplying Classification Tags\nRoles and Permissions: Data Security Admin vs General Users\nLinking Columns to Tags\nTag Behavior\nCustomization\nData Masking in Rule Execution & Export\nOverview\nData Classification is a foundational capability in DvSum, enabling organizations to manage data sensitivity and enforce privacy policies. Classification tags can trigger privacy actions such as masking and column hiding, all of which are configurable by Data Security Admins.\nThis article explains how data classification tags are created, applied, and managed within DvSum. It covers who can assign or modify tags, how roles affect permissions, and how classification levels drive masking and other privacy-related actions.\nTagset\nClassification tags are centrally managed in DvSum by administrative users. These tags are customizable and typically include:\nPublic\nInternal\nSensitive\nRestricted\nAccessing the Tagsets:\nNavigate to\nAdministration.\nGo to\nAsset Management.\nClick on\nTags.\nSelect the\nData Classification\ntagset.\nWithin the Data Classification tagset, users have the ability to create new tags. By default, these tags are assigned a low severity level and cannot be linked to columns or terms that carry a higher sensitivity classification (e.g., Sensitive or Restricted).\nThese classification tags can be applied to both\ncolumns\nand\nterms\n.\nApplying Classification Tags\nFor Columns (Field Dictionary)\nNavigate to the\nDictionaries >\nDatabase Columns.\nSelect the desired column(s).\nOpen the\nMore Actions\ndropdown.\nClick on\nData Classification\n.\nFor Terms (Business Glossary)\nGo to the\nDictionaries >\nGlossary Terms.\nSelect the desired term(s).\nOpen the\nMore Actions\ndropdown.\nClick on\nData Classification\n.\nRoles and Permissions: Data Security Admin vs General Users\nData Security Admin\nFull access to assign, elevate, or downgrade the sensitivity of classification tags.\nCan configure masking, hiding, and other privacy actions for each tag.\nGeneral Users\nCan increase the sensitivity level of a tag (e.g., from\nInternal\nto\nSensitive\n).\nCannot downgrade the classification level.\nIf they attempt to lower the sensitivity, a warning message is displayed, as shown in the screenshot below.\nUpgrade and Downgrade Rules\nData Security Admin\nCan\nupgrade\nclassification tags in all scenarios.\nCan\ndowngrade a column\nonly if\nall linked terms have equal or lower severity\n.\nFor example: If a column is tagged as Sensitive, and all its linked terms are Internal, the column can be downgraded to Internal.\nCan\ndowngrade a term\nonly if\nall linked columns have equal or lower severity\n.\nFor example: If a term is tagged as Restricted, and all its linked columns are Internal, the term can be downgraded to Internal.\nGeneral Users\nCan\nupgrade\nclassification levels freely.\nCannot\ndowngrade\nclassification tags under any condition.\nIf the severity remains the same (e.g., Sensitive to Sensitive), they are\nnot allowed to reassign\nthe tag\nLinking Columns to Tags\nUsers can associate columns with terms that have the same or\nlower\nseverity level.\nExample:\nIf a column is classified as\nSensitive\nor\nPublic\n, and a term is marked as\nSensitive\n, the column can be linked to that term.\nNote:\nIf a user tries to link a column to a term with\nhigher\nseverity, DvSum will display a warning message.\nSeverity-Based Linking Rules\nThe ability to link columns and terms depends on their classification severity:\nFrom a Term\n:\nA user can link a column only if the\ncolumnâ€™s severity is equal to or less than\nthe termâ€™s severity.\nFrom a Column\n:\nA user can link a term only if the\ntermâ€™s severity is equal to or greater than\nthe columnâ€™s severity.\nThis keeps the logic aligned with the data model and avoids inappropriate tag associations.\nTag Behavior:\nEach classification tag in DvSum can be configured to trigger specific privacy-related actions. These behaviors are determined by the\nseverity level\nof the tag and are enforced consistently throughout the platform.\nDefault Tag Behaviors:\nInternal / Public\n: Columns are marked as default; no masking is applied.\nSensitive\n: Data is masked as \"XXX\" throughout the application.\nRestricted\n: Columns are both hidden and masked as \"XXX\" throughout the application. This behavior can be modified by a Data Security Admin.\nEach classification tag can be configured to perform specific privacy actions, such as:\nMasking\n: Replacing sensitive data with masked values like \"XXX\".\nHiding\n: Making columns invisible to users without appropriate access.\nDefault Behavior When Tag or Access Is Not Specified\nIn cases where classification or access settings are not explicitly provided:\nIf\nno classification tag is assigned\n, the column or term\ndefaults to Public\n.\nIf\nâ€œMaskedâ€\naccess is set, the behavior will follow that of the\nSensitive\ntag.\nIf\nâ€œNo Accessâ€\nis set, the behavior will follow that of the\nRestricted\ntag.\nDefault Configuration Examples:\nRestricted\n: Columns are masked and hidden across the application.\nSensitive\n: Columns are masked, and a sensitivity icon appears next to the column name.\nCustomization:\nData Classification Tagset\nThe â€œClassificationâ€ tagset is a system-defined tagset created by default in DvSum to enforce data privacy and masking policies.\nDefault Configuration:\nExists by default in the system.\nApplies to assets such as columns, tables, and glossary terms.\nSupports one or multiple classifications per asset, depending on the\nâ€œOnly Single Value Allowedâ€\nsetting.\nDefault tags: Public, Internal, Sensitive, Restricted.\nNote:\nThis customization can only be performed by users with Data Security Admin privileges.\nWhat Can Be Modified (by Data Security Admin):\nUsers can tailor the tagset to match their organizationâ€™s sensitivity framework. They can:\nRename tags (e.g., change â€œSensitiveâ€ to â€œConfidentialâ€)\nChange the associated asset types (e.g., apply to Columns, Tables, Terms)\nAdd new tags (new tags are created with low severity by default)\nAdjust the\nâ€œOnly Single Value Allowedâ€\nsetting:\nCheck\nit if only one classification tag should be assigned per asset.\nUncheck\nit to allow multiple classification tags per asset.\nWhat Cannot Be Modified:\nTagset Type is Fixed:\nThis tagsetâ€™s type is always set to \"Classification\" and cannot be changed to another type.\nData Masking in Rule Execution & Export\nWhen a column is marked as\nSensitive\n, it is masked as \"XXX\" throughout the UI.\nThe same masking applies during\nrule execution\n.\nIf the rule runs on a table containing masked columns, the resulting\nexported Excel file\nwill also display masked values for those columns.",
    "scraped_at": "2026-02-02 15:31:09"
  },
  {
    "title": "Data Security Admin",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/22680186312468-Data-Security-Admin",
    "content": "In this Article\nIntroduction\nData Security Admins\nHiding Tables and Columns\nDeleting Assets\nAdding \"Data Classification\" Tags\nIntroduction\nThis article explains how a user can be added as a Data Security Admin and describes the exact authorizations for a Data Security Admin.\n1. Data Security Admins\nThe Account Admins can add Data Security admins from the User Security tab which is present in the Account Settings page.\nIn the dropdown, all the active users in the application will be shown. The users who have \"Admin\" access to the catalog can only be added as Data Security Admins. The users who do not have \"Admin\" access to the catalog can be added to the list of Data Security Admins but there will be a warning sign shown to update its permissions.\nIn this case, the user \"Hassan Ghafoor\" is in the list of Data Security Admins but actually not a Data Security admin until its permissions are not set to Catalog Admin. On clicking \"Update User Role\", the user settings will open from where permissions can be changed:\nNow when the user is logged in from the Data Security Admin account there will be some extra options that will be available on the Data Dictionary and Field Dictionary:\n2. Hiding Tables and Columns\nData Security Admins see additional options on the Database Tables page that other users cannot see which allow them to hide and show objects. When any tables are selected from the Database Tables, the \"More Actions\" button will be visible. The \"More Actions\" button is a drop-down with the following 3 options:\nHide Table(s)\nUnhide Table(s)\nDelete Table(s)\nWhen any tables are selected and the user selects the option \"Hide Table(s)\" then all the selected tables will be hidden. The hidden tables can be seen when the \"Show hidden tables\" checkbox is checked.\nUsers can mark any table as hidden. The hidden tables won't show at:\nTable Dictionary\nEnterprise Search\nExported Excel File\nHiding columns:\nOn the Database Columns when any column or columns are selected from the Database Columns, the \"More Actions\" button will be visible. The \"More Actions\" button is a drop-down with the following 4 options:\nHide Columns\nUnhide Columns\nData Classification\nDelete Columns\nWhen any columns are selected and the user selects the option \"Hide Column(s)\" then all the selected unhidden tables will be hidden. The hidden columns can be seen when the \"Show hidden columns\" checkbox is checked. The hidden columns will be greyed out.\nWhen a column is marked as hidden, The hidden column won't show at:\nField Dictionary\nOn the Rules Creation form\nRule Detail Page - Data Tab\nRule Detail Page - Definition Tab\nExported Excel File\n3. Deleting Assets(Tables, Columns and Rules):\nDeleting Table and Columns:\nThe selected tables and columns can be deleted and they will be removed from everywhere on the application:\nNote:\nTo delete tables or columns, users must either be the owners of those assets with Catalog Admin permissions and have the Data Security Admin role.\nDeleting rules:\nThe selected rules can be deleted by clicking the\nMore Actions\ndropdown. Once deleted, they will be removed from everywhere on the application.\nNote:\nTo delete rules, users must either be the ruleâ€™s creator with\nCatalog Admin\npermissions or have the\nData Security Admin\nrole.\n4. Adding \"Data Classification\" Tags\nUsers can add the \"Data Classification\" tags which can be added only by Data Catalog Admins:\nThe following 4 tags can be seen on the \"Data Classification\":\nInternal\nPublic\nRestricted\nSensitive\nThe description of every tag can be seen on hover. For more information about the tags separately there is the link of the \"\nData Masking\n\" article.\nOnce any tag is assigned to column(s), they can be seen on the listing view:\nThe selected columns can also be deleted and they will be removed from everywhere on the application:\nNote: For the deletion of columns, users should have Catalog Admin permissions but they don't have to be necessarily Data Security Admin\nOne More thing to note here is that only the tables or columns that will have the status \"Deleted\" can be deleted from the Catalog.\nManaging Data Classification Tags\nAs a\nData Security Admin\n, you are responsible for applying classification tags (such as\nConfidential\n,\nPII\n, etc.) to data assets like columns, tables, and glossary terms.\nTagging behavior depends on how the\nCustom Tagset\nis configured in the system.\nSingle vs Multiple Tags\nEach tagset has a setting:\nâ€œOnly Single Value Allowedâ€\n. This determines whether you can assign\nonly one\ntag or\nmultiple\ntags from that tagset to a single asset.\nIf\nchecked\n, you can apply\nonly one\nclassification tag per asset.\nIf\nunchecked\n, you can apply\nmultiple classification tags\nto the same asset.\nThis allows more flexibility in labeling data that falls under more than one classification (e.g., both\nInternal\nand\nConfidential\n).",
    "scraped_at": "2026-02-02 15:31:16"
  },
  {
    "title": "Differentiating Empty & Nulls Strings",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/36505019344532-Differentiating-Empty-Nulls-Strings",
    "content": "In this Article\nOverview\nUnderstanding the concept of Empty Strings\nUnderstanding the concept of Nulls\nAdditional Resources\nOverview\nIn databases,\nempty\nand\nnull\nvalues may look similar but have different meanings. An\nempty\nvalue means an empty string (\n''\n) is stored in a column, especially in string-type fields. Understanding this difference is crucial, as the concept is used in\nBlanks\nand\nValue Range Rule\nchecks. A\nnull\nmeans no value has been storedâ€”you'll see it\nNULL\nexplicitly written in the database.\n1. Understanding the concept of Empty Strings\nBefore jumping into the application, let's try to understand the concept directly from the database. Here, we have a column named\n\"capital\"\n, and we can see that empty strings are stored in itâ€”these are empty values:\nNow, if we check the same column in the application, we can see that the\n\"capital\"\ncolumn has some empty values.\nIf we apply the\nBlanks Rule\nwith the\n\"Empty\"\ncheckbox selected, 4 exceptions are expected. Navigate to the\nBlanks Rule\nand apply this check.\nNote:\nSelecting at least one of the checkboxes is necessary to save the Blanks Rule.\nAfter saving and running the rule, the number of exceptions will be\n4\n, which is the expected result.\nThis same check for\nempty strings\ncan also be used in the\nValue Range Rule\n, and the concept remains the same as explained above.\n2. Understanding the concept of Nulls\nNow, letâ€™s consider the same\n\"capital\"\ncolumn. In the database, we can see entries where it\n[NULL]\nis mentionedâ€”these are the\nnull\nvalues.\nFrom the profiling information in the application, we can see that the\n\"capital\"\ncolumn also has some null values.\nIf we apply the\nBlanks Rule\nwith the\n\"Null\"\ncheckbox selected, the expected number of exceptions will be\n284\n.\nAs shown earlier, the same check is available in the\nValue Range Rule\n, and the concept is identical. In the\nRuleset Rule\n, both\nBlanks\nand\nValue Range\nchecks are available, and the logic remains the same.\nAdditional Resources\nTo better understand the context of this article, itâ€™s essential to be familiar with the following rules, as they are used when applying blank and null checks:\nFoundational DQ - Blanks Rule\nFoundational DQ - Value Range Rule\nFoundational DQ - Ruleset Rule",
    "scraped_at": "2026-02-02 15:31:22"
  },
  {
    "title": "Data Quality Overview",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/23520582795924-Data-Quality-Overview",
    "content": "In this article:\nEdit Table\nColumn Sequence\nRules\nIntroduction\nData quality refers to the accuracy, completeness, consistency, and reliability of data within an organization. High-quality data ensures better decision-making, compliance, and operational efficiency. The Data Quality tab in DvSum contains all the settings related to rules. Users can access the Recommended & Available Rules created for the table opened and create new rules. This article provides an overview of all the major options available in the Data Quality tab.\nEdit Table\nEdit the table details by clicking on the\nGear Icon\nbutton\nThe following fields can be edited:\nLoad Profile\nAll Data\n: Select this load profile if data is fully refreshed regularly.\nIncremental Data\n: Select this if data is normally loaded incrementally (appended).\nMetric Time\n: If the load profile is set to incremental, then select from the Data and Timestamp fields in the table. This field will be used in relevant DQ rules, such as calculating daily load volumes.\nException Record Limit\nUsers can define a threshold for how many exceptions a rule can generate. For example, if the user does not want more than 500 exceptions for any rule created on the table, they can set this limit accordingly.\nFilter Condition\nThe\nRich Filter Condition\nallows users to specify specific conditions to apply to columns before executing rules. Users can add a\nFilter Condition\nor a\nCustom Expression\n. Multiple conditions and condition groups can be set as needed.\nOn the Rich Filters, users can add multiple conditions and condition groups according to the requirement\nNote: Condition group joins various conditions (the ones in the silver box)\nColumn Sequence\nThe sequence of the columns can be opened from the\nGear icon\nof settings and the user can change the sequence of the columns. This change of sequence will be observed once the rules are executed.\nRules\nAvailable Rules\nCreate new rules and review existing rules.\nSelect an existing rule to run it online or offline. For details, refer to the article\nRun Online/Offline.\nMass Update:\nThe Mass update option allows the users to set the:\nWorkflow Status (Acknowledge or Resolve)\nPriority (High, Medium, Normal)\nTags:\nUsers can add or remove the tags from the Rules. Once any rule or rules are selected the option is visible above:\nMore Actions:\nAdd to Job -\nupdate the existing schedules or add a new schedule.\nRemove from Job -\nThis option allows the user to remove any existing schedule of the rule(s)\nClone rule\n- This option allows the user to clone the current rule\nDelete rule\n- This option allows the user to Remove the rule permanently\nRules - Recommended rules\nRecommended Rules\nThese are the rules that are recommended based on the profiling of the source's tables\nIf the User wants to add a recommended rule they can simply click on the\nADD\nbutton\nThe following rule types are\nRecommended\n:\nBlank\nValue Range\nCount\nMetric",
    "scraped_at": "2026-02-02 15:31:27"
  },
  {
    "title": "Pivot View Configuration in Data Quality Rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/44369988407572-Pivot-View-Configuration-in-Data-Quality-Rules",
    "content": "In This Article\nOverview\nAccessing Pivot View\nConfiguring Pivot View\nOverview\nThe Pivot View in Data Quality Rules provides an interactive way to analyze exception data by grouping and aggregating records directly within the application. It allows users to summarize large volumes of exception data, identify patterns, and gain insights.\nThis article is part of the\nRule Detail Page\ndocumentation. For information on the\nData tab\n, including how exception data is generated and displayed, see\nRule Detail Page â€“ Data Tab\n.\nAccessing Pivot View\nThe Pivot View can be accessed from within a Data Quality rule.\nOpen the\nDQ Rule\n.\nGo to\nData Quality â†’ Data\n.\nClick on the\nPivot View\nicon.\nSelect\nConfigure Pivot View\n.\nDefine the required grouping and summarization parameters.\nSave the configuration to render the Pivot View results.\nNote:\nIf the Pivot View has not been configured previously, the system displays a message indicating that the Pivot View is not yet configured.\nConfiguring Pivot View\nTo configure the Pivot View, users must select\nConfigure Pivot View\nfrom the Pivot View panel within the Data Quality rule. All available columns from the\nData\ntab are displayed for configuration.\nUsers can configure it by:\nDefining Row Groups\nSelect aggregation options such as Maximum and Minimum\nSelect Column\nSave the configuration settings.\nThese configurations determine how the exception data is grouped and summarized in the Pivot View.\nNote:\nAfter saving the Pivot View configuration, only a preview of up to 200 records is displayed. To populate the Pivot View with the full dataset, the rule must be re-run. users can configure the maximum number of exception records displayed in the data grid, up to 75,000 records.\nPivot View data can be exported to Excel, similar to the Data tab. When exported in Grid format, grouped columns are downloaded as individual columns in the Excel file.\nPivot View is intended for interactive, UI-based analysis. Pivoted data can be manually exported from the UI but is not included in automated exception outputs.",
    "scraped_at": "2026-02-02 15:31:33"
  },
  {
    "title": "Cross-System DQ - Reconciliation Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/44350372876308-Cross-System-DQ-Reconciliation-Rule",
    "content": "In This Article\nOverview\nCreating a Reconciliation Rule\nRule Configuration\nReconciliation Type\nAggregate-Level Reconciliation Types\nData Source and Table Selection\nKey Field Mapping\nMetric Fields\nAdditional Fields\nSort Order\nData Comparison\nWhat You Can Achieve with the Reconciliation Rule\nOverview\nThe\nReconciliation rule\nis a Cross-System Data Quality (DQ) rule used to compare data between two tables, either at the\nrecord level\nor the\naggregate level\n, within a single source or across different sources.\nThis rule allows users to:\nMatch records between tables using one or more key fields\nCompare metric values for matched records\nValidate record counts between datasets\nCompare aggregated metric values\nCreating a Reconciliation Rule\nTo create a Reconciliation rule:\nNavigate to a table\nClick\nAdd Rule\nSelect\nCross System DQ\nChoose\nReconciliation\nRule Configuration\nAfter selecting the Reconciliation rule, configure the rule using the sections described below.\nReconciliation Type\nThe\nReconciliation Type\ndefines how the comparison between the source and reference tables is performed.\nDvSum provides\nfour reconciliation types\n, grouped into\nrecord-level\nand\naggregate-level\nchecks.\nRecord-Level Reconciliation Types\nEach reconciliation type exposes a specific set of configuration sections based on the comparison being performed.\nRecord Level Metric Check\nThe\nRecord Level Metric Check\ncompares\nnumeric metric values for matched records\nbetween the source and reference tables.\nRecords are first matched using key fields, and metric values are then compared for each matched record.\nAvailable configuration sections:\nData Source & Table Selection\nChoose the source and reference datasets for comparison.\nKey Fields Mapping\nSelect one or more fields that uniquely identify records. The same fields must be selected on both source and reference.\nMetric Fields\nSelect numeric fields whose values will be compared between source and reference.\nAdditional Fields\nSelect additional context or identification fields to include in the comparison results.\nRecord Level Check\nThe\nRecord Level Check\ncompares records between the source and reference tables based on defined key fields, without comparing metric values.\nThis check focuses on\nrecord presence and alignment\nbetween datasets.\nAvailable configuration sections:\nData Source & Table Selection\nChoose the source and reference datasets for comparison.\nKey Fields Mapping\nSelect one or more fields that uniquely identify records.\nAdditional Fields\nSelect additional context or identification fields to include in the results.\nAggregate-Level Reconciliation Types\nAggregate Count Check\nThe\nAggregate Count Check\ncompares the\ntotal number of records\nbetween the source and reference tables.\nThis check evaluates whether record counts are aligned across datasets.\nAvailable configuration sections:\nData Source & Table Selection\nChoose the source and reference datasets for comparison.\nKey Fields Mapping\nSelect one or more fields to group records for comparison (if applicable).\nAbsolute Delta\nWhen enabled, the system displays the absolute difference between source and reference counts without considering the sign.\nAggregate Metric Check\nThe\nAggregate Metric Check\ncompares\naggregated metric values\nbetween the source and reference tables.\nMetric values are aggregated before comparison using a selected aggregation function.\nAvailable configuration sections:\nData Source & Table Selection\nChoose the source and reference datasets for comparison.\nKey Fields Mapping\nSelect one or more fields to group records for aggregation.\nMetric Fields\nSelect numeric fields to aggregate and compare. Aggregation functions include\nSUM, MIN, MAX, and AVG\n.\nData Source and Table Selection\nFor all reconciliation types, users must define:\nA\nSource\nconnection and table\nA\nReference\nconnection and table\nFilters can be applied independently to both tables to control which records participate in the reconciliation.\nKey Field Mapping\nKey fields define how records are matched between the source and reference tables.\nBy default, records are matched using the same field or set of fields selected on both the source and reference tables.\nWhen record uniqueness depends on more than one field, multiple fields can be selected to form a composite key.\nIf\nAsymmetric Keys\nis enabled, the selected fields are concatenated to form a single key value that is used for comparison.\nThis allows reconciliation of datasets where no single field uniquely identifies a record.\nMetric Fields\nMetric fields define the\nnumeric values\nthat are compared between the source and reference tables.\nFor each selected metric field:\nThe system automatically calculates the\ndifference (Delta)\nbetween the source and reference values.\nWhen grouping is enabled, an aggregation function (\nSUM, MIN, MAX, AVG\n) must be selected for each metric.\nAbsolute Delta\nWhen\nAbsolute Delta\nis enabled, the system displays the\nabsolute value of the calculated difference\n, without considering whether the source value is higher or lower than the reference value.\nThis is useful when the focus is on the\nsize of the difference\n, rather than the direction of the difference.\nUse\nAdd Field\nto compare additional metric fields.\nAdditional Fields\nAdditional fields are used to include\nextra identifying or contextual columns\nin the reconciliation results.\nThese fields do not participate in key matching or metric comparison. Instead, they help identify\nwhich specific records\nare being compared when reviewing reconciliation results, especially for\nrecord-level checks\n.\nThese fields help users interpret reconciliation results more effectively.\nSort Order\nThe\nSort Order\nsection controls how reconciliation results are sorted when the results view is loaded.\nUsers can define one or more columns to sort the results and specify the sort direction for each column.\nConfiguration\nSort By\nSelect a column by which the reconciliation results should be sorted.\nAscending\nSorts results from lowest to highest (Aâ€“Z, smallest to largest).\nDescending\nSorts results from highest to lowest (Zâ€“A, largest to smallest).\nAdd Sort Column\nAllows multiple sort columns to be added to define a hierarchical sort order.\nIf no sort criteria are defined, the results are displayed using the default system order.\nData Comparison\nThe\nData Comparison\nsection defines how differences between the source and reference data are evaluated.\nVariance Percent\nVariance Percent\ndefines the maximum allowed percentage difference between the\nsource metric value\nand the\nreference metric value\nbefore the comparison is marked as a mismatch.\nA value of\n0\nmeans the source and reference values must match exactly.\nA non-zero value allows small differences within the specified percentage range.\nThis is useful when minor variations are expected due to rounding, timing differences, or calculation behavior between systems.\nComparison Type\nThe\nComparison Type\ndetermines how records from the source and reference tables are matched during reconciliation.\nFull\nIncludes all records from both source and reference tables, whether matched or unmatched.\nInner\nIncludes only records that exist in both source and reference tables.\nLeft\nIncludes all records from the source table and matching records from the reference table.\nRight\nIncludes all records from the reference table and matching records from the source table.\nThe selected comparison type controls which records appear in the reconciliation results and how unmatched records are handled.\nWhat You Can Achieve with the Reconciliation Rule\nUsing the Reconciliation rule, users can:\nCompare records and metrics across systems\nValidate data completeness and accuracy\nIdentify mismatches at both record and aggregate levels\nReconcile transactional or metric-based datasets using flexible matching logic",
    "scraped_at": "2026-02-02 15:31:43"
  },
  {
    "title": "Jira Integration for DQ Rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41467238935700-Jira-Integration-for-DQ-Rules",
    "content": "Overview\nIn this article:\nOverview\nJira Integration Setup\nConfiguring User Settings in JIRA\nSteps to Configure Jira Integration\nHow to Create a Jira API Token\nConfiguring Jira in DQ Rules\nEnable Integration\nTicket Creation Modes\nManual Ticket Creation\nAutomatic Ticket Creation\nTicket Workflow & Status Sync\nTicket ID Column\nNon-Exception Rule Ticket Creation\nMass Update\nSummary\nThis article explains how to create Jira tickets directly from Data Quality (DQ) rules in DvSum.\nBy integrating Jira workflows into the platform, teams can:\nManage rule exceptions more efficiently.\nEliminate manual exception tracking.\nEnsure systematic assignment, monitoring, and resolution of issues.\nJira Integration Setup:\nBefore creating tickets, users must configure Jira integration within the application. The integration ensures that Jira tickets can be created and assigned properly.\nPrerequisite:\nConfiguring User Settings in JIRA\nThe Admin of the JIRA account first needs to add users to the account, who will create API tokens and create JIRA integration in DvSum.\nOnce the user is added, the role must be assigned to the user, and the minimal role that must be assigned is \"User.\"\nAfter the user is added to the organization, the user must be added to the project on which the Jira integration will be created. On Jira, select your project and go to its settings, where users can be added:\nWhile adding the user, the user must have a \"Member\" level role at a minimum to be able to create tickets in Jira.\nOnce the user is added to the project, they will receive an invite, and this user can now create an API token from this account, and the user will be able to create Jira integration in DvSum.\nSteps to Configure Jira Integration:\nNavigate to the\nAdministration\ntab >\nAccount\n.\nClick on the\nIntegrations\ntab\n.\nClick\nAdd\nIntegration and choose\nJira\n.\n4. Fill in the required fields:\nName:\nProvide a recognizable name for the integration.\nService URL:\nEnter the URL of your Jira instance.\nProject Key:\nEnter the project key (The key you provided while creating the project in Jira for ticket creation).\nUsername/Email:\nEnter the Jira account username or email.\nAPI Token:\nAdd the Jira API token if you already have one.\nIf you want to generate a new one, you can click on the link button, which will navigate you to the Atlassian/Jira account.\nHow to create a Jira API token:\nLog in to your Atlassian/Jira account in a browser.\nOpen Account Settings â†’ Security â†’ API tokens (or navigate to https://id.atlassian.com/manage-profile/security/api-tokens).\nClick Create API token and provide a label (for example: Token Name, like (Jira Integration), and token expiry date).\nGiven the token name\nSet the expiry date and click on create.\nOnce done, copy the generated token.\nPaste this token into the integrationâ€™s API Token field within the DvSum app.\nJira Cloud API tokens inherit the permissions of the user who created them. Ensure that the user has appropriate access to the project.\nNote:\nAt present, the Create API token with scopes option is not supported. Only the standard Create API token functionality is available.\nOnce the Jira account details are added and tested successfully, at the end of the connection details, you will see the project details, e.g., Issue Type, Priority, and Assignee.\nNote:\nWhen adding or editing an Integration, if incorrect details or invalid credentials are entered, the system will still allow the test connection and to save it. However, the Integration will not be usable until the credentials are verified.\nConfiguring Jira in DQ Rules\nEnable Integration\nOnce Jira integration is complete, rules can be configured to create tickets when exceptions are detected.\nNavigate to DQ Rules.\nSelect a rule where you want Jira integration to be enabled.\nGo to Edit Settings and navigate to External Ticketing.\nTurn on the option Enable Jira Ticket Creation.\nSelect the integration value from the dropdown.\nDefine how exceptions should trigger ticket creation:\nManual creation.\nAutomatic creation (based on exception detection only)\nClick on save.\nTicket Creation Modes\nManual Ticket Creation\nNavigate to a DQ rule with exceptions.\nClick the Create Ticket button.\nFill in ticket details\nRecords to include\n(You can select any one option based on what exceptions you want to see as an attachment in the ticket).\nSelect Issue Type (the dropdown will show the values defined in your Jira project).\nTitle (By default, it will show the rule description, but you can add your own defined title as well).\nThe\ndescription\nfield has the pre-defined format, but you can also add a description or the details as per what you want to see in the ticket.\nSelect the\nassignee\nof the ticket from the dropdown (these are the users who are added in your Jira project).\nClick on the create button to finish.\nAs soon as you click the Create button, a Jira ticket will be created that is linked to the DQ rule.\nNote:\nClicking on the ticket ID will redirect you to the Jira account for a detailed view of the ticket. You can see the ticket was created.\nRecords To Include Options:\nAll Exception:\nIt will record all the exceptions in the ticket.\nException records since last ticket creation:\nThis includes only the exceptions that have appeared since the last time a ticket was created.\nException records since last ticket creation from latest run only:\nIt captures exceptions from the most recent run of the rule execution that also havenâ€™t been ticketed yet.\nNote: Since both options are related to Exception Tracing, they will only be enabled when Exception Tracing is turned on.\nFor more details on how to configure exception tracing on DQ rules, you can go through this article on\nConfiguring Exception Settings for Rules in DvSum\n.\nNo Exception ticket:\nThese are tickets that are created manually or for reasons other than automatic exceptions.\nIf the ticket is newly created, it will capture only the exceptions that have occurred since the last ticket creation. This ensures that previously logged exceptions are not duplicated, and the ticket reflects only the latest set of issues that need attention.\nAutomatic Ticket Creation\nNavigate to a DQ rule with exceptions.\nGo to Edit Settings and navigate to External Ticketing.\nChange creation mode to Automatic\nChoose the exception option under the Records to Include.\nSelect the Issue type and assignee\nNote:\nFor automatically generated tickets, fields such as the title and description cannot be manually controlled. These are automatically populated and updated based on the rule configuration.\nAfter saving the settings, the Create Ticket button will be disabled, and a ticket will be created automatically during the next execution of the rule.\nTicket Workflow & Status Sync\nTo Do:\nWhen a new ticket is created, the ticket action workflow is enabled, and the initial status is set to To Do.\nIn Progress:\nWhen a ticketâ€™s status is updated from To Do to In Progress in Jira, the change automatically syncs with the DvSum rule detail page. This triggers the workflow, and the rule status is updated to In Progress.\nJira Ticket\nDvSum Rule\nResolved/Done:\nWhen the assignee updates the ticket status to Resolved or Done, the workflow syncs with the rule, and the rule status is updated to Resolved.\nNote:\nIf the DQ workflow is enabled for this rule, its execution will be managed through the associated Jira ticket. The workflow status will mirror the Jira ticket status, meaning the rule is governed via Jira instead of the action workflow.\nTicket ID Column\nA new column called Ticket ID has been added in the DQ Dictionary governance view available column and in the data tab of the rule detail page as well, which displays only the most recently created ticket for the rule.\nNon-Exception Rule Ticket Creation\nFor non-exception rule types such as Unique Values, Count, Metric (Sum), and Freshness, Jira tickets can be created either manually or through automatic ticket creation.\nThe overall process for creating a Jira ticket is a little simpler for these rules, as they don't have any exception dat,a so a simple ticket with title and description is created\nBehavior Based on Rule Status\nIf the rule status is Healthy, then a manual ticket can bee created but an automatic ticket can not be created.\nIf the rule status is Alerting, then both manual and automatic tickets can be created.\nMass Update\nUsers can now update DQ rules to integrate with Jira using the mass update functionality from the DQ Dictionary listing view.\nNavigate to the DQ Dictionary listing view.\nSelect one or more DQ rules, then click Mass Update.\nFrom the drop-down, choose Ticketing Integration.\nSelect the desired action (Update or Make Empty) and then pick the appropriate Jira integration value.\nClick Apply to save the changes.\nSummary\nThe Jira integration with DvSum streamlines exception management for DQ rules by:\nAllowing\ndirect Jira ticket creation\n(manual or automatic) from within the platform.\nEnsuring tickets are linked to specific rules, making it easy to track health and resolution.\nProviding a\nsingle view in DvSum\nto monitor rule exceptions and related tickets.\nLeveraging Jiraâ€™s workflows, notifications, and project tracking features for efficient issue resolution.",
    "scraped_at": "2026-02-02 15:31:50"
  },
  {
    "title": "Configuring Exception Settings for Rules in DvSum",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/39028423072916-Configuring-Exception-Settings-for-Rules-in-DvSum",
    "content": "In this article:\nAccessing the Exception Configuration Panel\nException Configuration Options\nAlerting Exception Files\nExceptions to Include\nEmail Recipients\nEnable Exception Tracing\nNew Metadata Columns in the Data Tab\nThe\nException Configuration\npanel provides greater control over how exceptions are managed in DvSum. By customizing these settings, users can streamline exception handling, reduce alert fatigue, and improve traceability.\nThis article explains the purpose and functionality of the\nException Configuration\npanel and how to use it effectively.\nAccessing the Exception Configuration Panel\nTo access the\nException Configuration\npanel:\nTo configure exception settings for a rule:\nNavigate to the\nDictionaries\n>\nDQ Rules\n.\nChoose a rule from the\nlisting page\n.\nGo to the\nData Quality\ntab.\nClick on\nEdit\nto open the rule settings.\nSelect the\nNotifications\nsection from the left panel.\nScroll down to locate the\nException Configuration\npanel.\nException Configuration Options\nThe\nException Configuration\npanel includes the following options:\nAlerting Exception Files\nWhen enabled, this option generates exception files during rule execution.\nThese files can be used for auditing or further analysis.\nExceptions to Include\nAll Exceptions\n: Includes all exceptions identified during the rule run.\nNew Exceptions Only\n: Includes only exceptions that are new since the last execution.\nEmail Recipients\nAllows users to define who should receive exception alerts.\nYou can select from default email groups or specify custom recipients.\nThese recipients will receive the exception notification email when the rule is run.\nEnable Exception Tracing\nEnabling\nException Tracking\nis required to identify and differentiate between\nnew\nand\nexisting\nexceptions during rule execution.\nThis feature uses a\nkey field\n(e.g.,\ncustomer_id\n) to trace recurring exceptions across multiple runs. Once enabled, users can easily spot\nnew exceptions\n, which are typically the primary focus during data quality reviews, along with the\nexisting exceptions\n.\nNote:\nAfter enabling Exception Tracking and executing the rule, additional metadata columns will appear in the\nData\ntab to support exception analysis.\nNew Metadata Columns in the Data Tab\nWhen Exception Tracing is enabled, the following columns are added to the\nException Deep Dive\nview in the\nData\ntab:\nDV First Observed\nâ€“ Timestamp of when the exception was first detected.\nDV Status\nâ€“ Indicates whether the exception is\nNew\nor\nExisting\n.\nDV Age\nâ€“ Shows how long the exception has existed since it was first observed.\nThese columns help users monitor exception trends and prioritize remediation efforts more effectively.\nStatus\nfilter has been added, allowing users to quickly switch between viewing\nAll\n,\nNew\n, or\nExisting\nexceptions.",
    "scraped_at": "2026-02-02 15:31:56"
  },
  {
    "title": "Foundational Data Quality - Uniqueness Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35889122920468-Foundational-Data-Quality-Uniqueness-Rule",
    "content": "In this Article:\nOverview\nSteps to Configure the Uniqueness Rule\nStep 1: Access Data Dictionary\nStep 2: Select Data Quality Rules\nStep 3: Add a New Rule\nStep 4: Configure Basic Inputs\nStep 5: Validate and Execute the Rule\nOverview\nThe \"UNIQUENESS\" rule ensures that values within a specific data element are unique across the dataset. This rule is crucial for maintaining data integrity by detecting and preventing duplicate entries.\nFor instance, if a customer table contains duplicate records for the same customer, the Uniqueness rule will identify these duplicates and display the duplicate keys along with their count. This helps users pinpoint and resolve data duplication issues efficiently.\nFor more details about the ruleâ€™s scope, threshold, notifications, and other configurations, refer to the\nRule Detail page.\nSteps to Configure the Uniqueness Rule\nStep 1: Access Data Dictionary\nLog in to\nDvSum\n.\nGo to the\nDictionary\ndropdown and select the\nDatabase Tables\ntab.\nChoose the relevant\nData Source\nand\nTable Name\n.\nStep 2: Select Data Quality Rules\nClick on the\nData Quality\ntab.\nChoose\nAvailable Rules\n.\nStep 3: Add a New Rule\nClick on the\nâŠ• Add Rule\nbutton.\nSelect the\nFoundational DQ\ncategory.\nFrom the available options, choose\nUNIQUENESS\n.\nStep 4: Configure Basic Inputs\nIn the\nRule Wizardâ€™s Basic Input\nsection:\nEnter the\nRule Description\n.\nSelect the\nColumn Name\nfor which uniqueness needs to be validated.\nStep 5: Validate and Execute the Rule\nSave the rule configuration.\nClick\nRun\nto execute the rule.\nReview the results to identify and address any duplicate records.",
    "scraped_at": "2026-02-02 15:32:01"
  },
  {
    "title": "Access DQ Rule information via REST API",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/33018039015828-Access-DQ-Rule-information-via-REST-API",
    "content": "In this Article\nOverview\nDetailed Steps\nAPI Key\nMaking an API Call to Retrieve DQ Rules\nKey Information\nExample API Call\nDefining and Viewing DQ Rules in DvSum\nCustom Views and API Extraction\nTesting with Postman\nOverview\nAll information about Data Quality (DQ) rules is accessible through the DvSum web user interface and is also available via API. Using the API provides users with greater flexibility and control over their DQ results. For example, you can integrate the data into a corporate BI tool or dashboard, or archive the results for future review.\nThe data retrieved via API corresponds to what is displayed on the\nDQ Dictionary\npage and is filtered based on the specific views of your rules that you define on this page.\nDetailed Steps\nAuthentication Update\nDvSum APIs now use\nOAuth2 Client Credentials\nfor secure authentication instead of direct API keys.\nTo access APIs:\nNavigate to Administration â†’ Account â†’ Application Security â†’ Manage OAuth2 Clients.\nCreate an\nAuthentication Client\nand download the generated\nClient ID\nand\nClient Secret\n.\nor\nAlternatively, enable API access at the user role level, and then generate a user roleâ€“based credential from\nMy Settings â†’ API Credentials\ntab.\nUse these credentials to generate an\nAccess Token\nusing the Authentication API mentioned in the\nAPI Documentation.\nRefer to the article\nâ€˜API Access and Authentication Managementâ€™\nfor details on creating an authentication client and generating access tokens.\nDvSum API Access and Authentication Management\nMaking an API Call to Retrieve DQ Rules\nTo fetch a list of Data Quality (DQ) Rules, follow the steps below:\nKey Information\nBase URL\n:\nhttps://apis.dvsum.ai/node-listing/listing/rules?view-name=\nParameter\n:\nName\n:\nview-name\nValue\n: Specify the name of the DQ Rules view you have defined in DvSum.\nExample: Use\n\"DQ Rules\"\nfor the pre-defined view available to all users.\nHeaders\n:\nValue\n: Use the API key obtained in the previous step.\nAuthorization\n: Bearer\n<access_token>\n.\nExample API Call\nhttps://apis.dvsum.ai/node-listing/listing/rules?view-name=DQ Rules\nEnsure to replace\nview-name\nwith your desired DQ Rules view name as required.\nDefining and Viewing DQ Rules in DvSum\nTo access and define views in DvSum, follow these steps:\nNavigate to the DQ Dictionary\n:\nGo to the\nDictionary\ndropdown and select the\nDQ Rules\nin the DvSum platform.\nSelect a View\n:\nChoose any available view to retrieve the rules and fields relevant to your needs.\nDefault View\n:\nThe default view is\n\"DQ Rules\"\n, which is accessible to all users.\nYou can also define a custom view to specify the rules and fields you want to retrieve.\nThese views can be used as input parameters for API calls to fetch specific Data Quality rules and related fields.\nCustom Views and API Extraction\nWhen using\ncustom views\nfor API extraction, keep the following in mind:\nOwner vs. Non-Owner Accounts:\nIf the view is created by the\nowner account\n, it is directly available for API calls.\nIf the view is created by a\nnon-owner account\n, it must first be\nshared with the owner account\nbefore it can be used in the API.\nSharing a Custom View:\nNavigate to the DQ Dictionary and locate the custom view you created.\nUse the\nshare option\nto share the view with the owner account.\nOnce shared, the owner can access the view, and it becomes available for API extraction.\nNote:\nIf a custom view from a non-owner account is not shared with the owner, attempting to use it in an API call will result in no data being retrieved.\nTesting with Postman\nBelow is an example where the selected view is\nâ€˜Analyze Rulesâ€™\n.\nThe headers and the successful JSON response to the GET request are displayed.",
    "scraped_at": "2026-02-02 15:32:08"
  },
  {
    "title": "Reference Dictionary with Value Range/Ruleset Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/28703505711636-Reference-Dictionary-with-Value-Range-Ruleset-Rule",
    "content": "In this Article\nOverview\nDetailed Steps\nRuleset Rule\nOverview\nReference dictionaries can be added to value range and ruleset rules. This allows you to map the columns defined in the reference dictionary to the columns in your dataset, ensuring that data is validated according to the data defined in the reference dictionary. For more information, please see this detailed article on\nreference dictionary\n.\nDetailed Steps:\nStep 1:\nLog in to DvSum, navigate to the Dictionary dropdown, select the Database Tables tab, and then choose the relevant data source and table name.\nStep 2:\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3:\nSelect the \"âŠ• Add Rule\" button, then choose the \"Foundational DQ\" category. From the list of options, click on \"Value Range\".\nStep 4:\nBasic Input\nIn the Rule Wizard's Basic Input section, Select \"Ref Dictionary\" from dropdown in Comparison type.\nStep 5:\nSelect the Dictionary and add relevant source fields.\nAlso, we can cancel the fields and map the fields only we want to.\nStep 6:\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.\nRuleset Rule\nStep 1:\nSelect the \"âŠ• Add Rule\" button, then choose the \"Foundational DQ\" category. From the list of options, click on \"Ruleset\".\nStep 2:\nIn the Rule Wizard's Basic Input section, Select \"Ref Dictionary\" from dropdown in Value range check.\nNOTE:\nReference dictionaries with any defined hierarchical levels will not be shown here.\nStep 3:\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:32:13"
  },
  {
    "title": "Reference Dictionary",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/28688102882196-Reference-Dictionary",
    "content": "In this Article\nOverview\nDetailed Steps\nStep 1: Access Reference Dictionary\nStep 2: Add a New Dictionary\nStep 3: Add Code/Value Pairs\nStep 4: Import File\nStep 5: Verify Data\nOverview\nReference Dictionary is like a pre-organized reference book where users can create their own dictionaries, associating useful information with sets of values. This is helpful for DvSum validation audits, allowing users to choose a reference dictionary with a set of data values instead of listing individual ones.\nDetailed Steps\nStep 1: Access Reference Dictionary\nLog in to DvSum, proceed to Administration tab > Organizations > Reference Dictionary.\nStep 2: Add a New Dictionary\nSelect\nAdd Dictionary\nand add Name, Description and select one of the below:\nKey/value pair:\nA key/value pair is a way to store and organize information where each piece of data (value) is labeled with a unique identifier (key).\nis Hierarchical:\nA dictionary that arranges attributes in a parent-child hierarchy.\nStep 3: Add Code/Value Pairs\nChoose how to add code value pairs either by entering values manually or import file.\nStep 4: Import File\nClick on Import File and choose the file to be uploaded, click OK and then click Done.\nAlso there is an option to Enter data manually\nStep 5: Verify Data\nFields are populated from upload file in values section.",
    "scraped_at": "2026-02-02 15:32:19"
  },
  {
    "title": "Dashboard and Widgets",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/30481164045588-Dashboard-and-Widgets",
    "content": "In this Article\nOverview\nDefault Dashboards\nDashboard Creation\nPick from Gallery\nCreate your Own\nSharing the Dashboard\nDelete a Dashboard\nClone a Dashboard\nDrill-down Functionality\nOverview\nA dashboard is a centralized interface that visually organizes and presents key data points, enabling users to monitor, analyze, and make informed decisions. In the DvSum application, dashboards provide an efficient way to manage and track data across various modules, offering customizable views and functionality. This article covers the types of dashboards available in DvSum, how to create and customize your own dashboards, and the options provided for sharing, downloading, and managing dashboard data. Whether using default dashboards or building your own, DvSum's\ndashboard features help streamline data presentation and enhance decision-making.\nDefault Dashboards\nFour default dashboards are provided:\nCADDI Dashboard\nCatalog Dashboard\nDQ Dashboard\nPlatform Dashboard\nNote:\nThe default dashboards can not be shared with any other user.\nCreated by Me: Displays all dashboards created by the user.\nShared with Me: Lists all the dashboard shared with the user by someone else.\nDashboard Creation\nClick on Create Dashboard\nSpecify a name and description. Click on \"Create\" button.\nAn empty Dashboard is displayed.\nClick on Add Widget, choose either \"Pick from Gallery\" or \"Create your Own\".\nPick from Gallery\nIncludes all available widgets from every module within the DvSum application.\nAfter selecting a widget, the detail page is displayed These template widgets automatically populate the title, module, preferred\nchart type\nfor\nslicers\n(Y-axis) and\nmetrics\n(X-axis). The data can be\nsorted\nfor display on the chart, and\nData Labels\n(legends) can be enabled on the charts.\nCreate your Own\nA new widget can be created and the module can be targeted, own filter criteria and chart type can be selected.\nModule Filter Criteria\nModule filter criteria are predefined conditions that allow users to narrow down data within a module based on specific parameters, helping to focus on relevant information. These filters streamline user interaction by providing a customized view that aligns with their needs and objectives.\nNew filter criteria can also be added.\nThis newly created criteria will then be populated in Selected Filter:\nAfter saving the widget, it\nwill start appearing on Dashboard.\nEnabling Data Labels\nTo enable data labels on widgets, users can do so from the create/edit widget form, as shown below.\nWhen data labels are enabled for the widget, it will be displayed as shown in the image below.\nDisabling Data Labels\nWhen data labels are disabled for the widget, it will be displayed as shown in the image below.\nYou can add more widget by clicking on \"\nAdd Widget\n\" Button.\nThe widget can be expanded by clicking the Expand Icon:\nUpon clicking the three dots, below options are displayed:\nAdd to Home:\nThe selected widget will start appearing on the Home page as shown below:\nDownload Widget\nThe widget can be downloaded as a CSV file.\nSharing the Dashboard\nIt is possible to either keep a Dashboard private or share it with others. When sharing, two options are available: granting Editor or Viewer rights. If shared as a Viewer, the recipient will have three options: expanding and viewing widgets in full screen, downloading, and adding the dashboard to their home screen.\nViewer Access\nEditor Access\nDelete a Dashboard\nWhen deleting a dashboard, a prompt appears indicating the number of people the dashboard is shared with and asking if the user would like to proceed.\nClone a Dashboard\nDashboards that are set as default can be cloned but cannot be deleted. However, dashboards created by the user can be both deleted and cloned.\nDrill-down Functionality\nDrill-down functionality is available in Dvsum's dashboard widgets, allowing users to click on charts, pies, or bars to access more detailed data. This feature enables exploration of granular insights directly from the visual elements of the dashboard.",
    "scraped_at": "2026-02-02 15:32:26"
  },
  {
    "title": "Table Detail Page",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/42041709770772-Table-Detail-Page",
    "content": "In this Article\nOverview\nAccessing the Table Detail Page\nOverview Tab\nDefinition Section\nAsset-Specific Details\nManaged By\nTags\nCustom Attributes\nData Quality Tabs\nUsability Score\nLineage\nRelationships\nAdditional Info\nActions Available\nEdit\nDelete Asset\n4. Data Tab\n1. Profiling Info\n2. Column Summary\n3. Column View\n5. Data Model\n6. Data Quality\nOverview\nThe Table Detail Page provides a comprehensive view of a data table and its associated metadata, relationships, and quality metrics. It helps users understand the tableâ€™s context, ownership, and attributes.\n1. Accessing the Table Detail Page\nNavigate to\nDictionaries\n>\nDatabase Tables.\nClick on the\nTable Name\nto open the Table Detail Page.\n2.Overview Tab\nThe\nOverview\ntab summarizes key information about the selected table, including general metadata, relationships, and custom attributes.\n1.Definition Section\nThis section provides a high-level description of the table, explaining its purpose and contents.\nDescription:\nA brief summary of what the table contains or represents.\nData Domain:\nThe subject area the table belongs to (e.g., Finance, HR, Education).\nRelationships:\nHow the table connects to other entities, such as:\nBelongs To Database Schema\nâ€“ identifies the schema or database.\nContains Database Columns\nâ€“ shows the number of columns in the table.\nValidated By Data Quality Rules\nâ€“ lists any rules linked to the table.\n2. Asset-Specific Details\nDisplays system-generated metadata about the table.\nRecord Count:\nNumber of records in the table.\nLast Scanned On:\nThe last time the table was profiled or scanned.\nLast Updated On:\nWhen metadata was last updated.\n3. Managed By\nShows ownership and stewardship information.\nData Owner:\nIndividual or team accountable for the dataset.\nSystem Owner:\nPerson or application responsible for technical maintenance.\nData Steward:\nResponsible for governance, quality, and documentation.\nAudit Trail:\nRecords of when and by whom the table was created or updated. By clicking on the three dots on audit trail shows the activities done on that table.\n4. Tags\nTags categorize and enhance discoverability of the table. Tags can be used for search, filtering, and reporting.\nData Classification:\ne.g., Confidential, Public.\nCertification Tag:\nIndicates whether the dataset has been validated.\nCustom Tags:\nUser-defined labels for grouping or filtering.\nData Quality Tags:\nHighlight data quality aspects or issues.\nNote: For more information on Tags, refer the article\nHow to create Tag\n.\n5. Custom Attributes\nDisplays organization-specific metadata fields that enrich the table definition.\nExamples include:\nAuthentication Type:\nSpecifies the access mechanism.\nValidation Check:\nBoolean or validation field.\nNumeric or Text Attributes:\nCustom fields like credit card numbers or notes.\nCustom attributes vary based on organizational configuration.\nFor more information on the custom attribute, refer the article\nCreating and Managing Custom Attribute\n.\n6. Data Quality Tabs\nClicking the arrow mark will redirect to the Data Quality page and provides insights into profiling results, rule validations, and quality metrics.\nFor more details on the DQ rules, refer the article\nRule Detail Page\n.\n7. Usability Score\nClicking on the eye icon in the overview page, user can see the details of the usability score of the table. The\nUsability Score\nreflects the completeness and quality of metadata. A higher score indicates that more detailsâ€”like definitions, tags, and ownershipâ€”are filled in.\nFor more information on understanding and setting up the usability score refer to the article\nUsability Score\n.\n8. Lineage\nLineage\nrefers to the traceability of data through various stages in its lifecycle. It describes how data moves, transforms, and is used throughout an organization or system. Essentially, data lineage provides a \"map\" of the data's journey, showing where it originates, how it is processed, and where it is ultimately consumed.\nFor more information on lineage, refer to the article\nLineage in DvSum\n.\n9. Relationships\nRelationships describe how different assets are connected to each other. In a well-structured data asset management system, relationships help link tables, datasets, and data elements to ensure data is organized logically and can be traced back to its origin.\nFor more information on adding relationship to asset, refer to the article\nHow to Manage Relationships Between Assets\n.\n10. Additional Info\nThe Additional Info tab allows users to add, edit, or remove supplementary information related to a table. It provides a text editor with formatting options (such as font style, size, color, and numbering) to include detailed notes or context as needed. Once the information is entered, users can save it as an item, modify or delete it, and choose to publish or discard the changes. Published additional information becomes part of the tableâ€™s details.\nFor more information on Additional info tab, refer to the article \"\nRich Text Additional Information\n\"\n3.Actions Available\nEdit:\nUpdate table metadata, description, or tags.\nWhen we click the\nEdit\nbutton on the Overview page, it switches to editable mode, allowing the user to modify all the metadata information of the table mentioned above.\nClicking on the pencil icon allows the user to edit the required metadata.\nThe user then needs to click the\ncheckmark\n, select\nDone\n, and finally click the\nPublish\nbutton to save and apply the changes.\nDelete Asset:\nTables marked as\nDeleted\nwill have the option to be permanently removed from the system.\n4. Data Tab\nThe\nData Tab\nprovides a detailed view of the dataset's content, allowing users to examine individual records and column-level statistics.\nNote: For more information on the Run Profiling, refer to the article\nRun Online & Run Offline\n.3.\n1. Profiling Info\nThe\nProfiling Info\nsection provides essential metadata about the dataset, helping users understand the scope and last interactions with the data:\nRecord Count\n: Displays the total number of records in the dataset. This helps assess the size of the data and understand its scope.\nLast Scanned On\n: Shows the last time the dataset was scanned or updated. This is crucial for determining the freshness of the data.\nLast Updated On\n: Indicates when the data was last modified or updated, helping track data changes over time.\n2. Column Summary\nThe\nColumn Summary\nsection provides an overview of the tableâ€™s structure, including key details about each column:\nTotal Columns\n: The total number of columns in the dataset.\nGranularity\n: Specifies the level of detail at which the data is recorded. For instance, granularity could be at the level of individual transactions, customer records, or events.\nAttributes\n: Lists the attributes in the dataset that are key to understanding the dataset's structure. For example, a numerical attribute like 'Weight' or a categorical one like 'Action'.\nMeasures\n: This would include any calculated or aggregated values in the dataset, such as totals or averages.\nTime Series\n: If the dataset includes time-based data (such as timestamps or dates), this section would highlight that, enabling users to track trends over time.\n3. Column View\nThe\nColumn View\nprovides detailed insights into each column in the dataset, allowing users to visualize the distribution and patterns within the data.\nIn column view there are two types of view,\nGrid view\nDictionary view\nGrid View:\nThe\nGrid View\nprovides a visual representation of the record counts across columns, as shown below.\nDictionary view:\nThe Dictionary View provides the list of columns in the table.\nUsers can edit the columns in the\nDictionary View\n. After making the necessary changes, click\nSave\nto apply and update the data.\nUsers can use the\nFilter\noption in the\nDictionary View\nto easily access the required columns.\n5. Data Model\nAn\nEntity Relationship (ER) Diagram\nvisually represents how datasets and their columns are connected within the system. It helps users understand data relationships and gain a clearer view of the overall data structure.\nFor detailed information the Entity Relationship (ER) Diagram, refer to the article\nEntity Relationship Diagram - ERD\n.\n6. Data Quality\nThe\nData Quality\ntab in DvSum manages all rule-related settings, allowing users to view recommended and available rules or create new ones to ensure accurate, complete, and reliable data.\nThe\nData Quality\ntab in the Database Tables contains the following sections:\nStatistics\n: Displays key metrics such as DQ Score, total rules, rules with alerts, total exceptions, total records scanned, and last rule execution status.\nRules\n: Lists all data quality rules applied to the column, including details such as rule ID, description, run status, alert status, and exception count.\nNote: To learn more about the Rules section in Data Quality tab, refer the article\nData Quality Overview\n.",
    "scraped_at": "2026-02-02 15:32:31"
  },
  {
    "title": "Enabling Query History for Data Sources",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/33861396141076-Enabling-Query-History-for-Data-Sources",
    "content": "In This Article\n1. Enable Query History on Databricks\n2. Enable Query History in SQL Server, Azure SQL, Azure Synapse\n3. Enable Query History in PostgreSQL\n4. Enable Query History in Snowflake\n5. Enable Query History in MySQL\n6. Enable Query History in Netezza\n7. Query History in Oracle\nIntroduction\nQuery history plays a vital role in data lineage formation within applications. By capturing the details of query execution it allows for precise tracking of how data flows and evolves. This article provides a detailed guide to enabling query history across major data platforms such as Databricks, SQL Server, Azure SQL, Azure Synapse, PostgreSQL, and Snowflake. It also touches on additional considerations for databases where query history is enabled by default, such as Oracle and Netezza.\nWhether working with on-premises databases or cloud-based solutions, this guide will help you configure query history to ensure seamless lineage formation and support effective data analysis and governance.\n1. Enable Query History on Databricks\nTo activate\nQuery History\nin Databricks for lineage tracking, follow these essential steps:\nAuthenticate\nusing one of the supported methods:\nPersonal Access Token (PAT)\nOAuth Client Credentials (Client ID & Secret)\nAssign necessary permissions\nat multiple levels:\nCatalog\nCluster\nSQL Warehouse\n(Optional)\nVerify access\nby executing a validation query or making an API call to confirm that historical query data is being captured successfully.\nChoose an Authentication Method\nYou can authenticate using either a\nPersonal Access Token\nor\nClient Secret credentials\nfor the service principal.\nPersonal Access Token\nOpen your workspaceâ€™s\nAdmin Settings\n(click your username in the top bar > Admin Settings).\nGo to the\nAdvanced\ntab.\nUnder\nAccess Control\n:\nEnable\nPersonal Access Token\n.\nClick on\nPermission Settings\n.\nSearch for your user.\nAssign the\n\"Can Use\"\npermission.\nClick\nAdd\n.\nOnce enabled, run the Lineage scans on the Databricks source to begin bringing in Lineage data into the application.\nClient Secret Credentials (Alternative to Personal Access Token)\nIf using Client Secret credentials, ensure the following permissions are in place:\nNavigate to\nAdmin Settings > Access Control\n.\nAssign the following minimum required permissions:\nWorkspace permissions\n: Can Use\nCatalog access\n: Required permissions at the catalog level (see next section).\nCompute access\n: Can Attach To for the cluster or SQL Warehouse permissions.\n2. Enable Catalog access for Users\nLog into your Databricks workspace.\nClick on the\nCatalog\ntab in the left menu.\nSelect the\ncatalog\nconfigured for Lineage.\nOpen the\nPermissions\ntab.\nClick\nGrant\n.\nSearch for your\nuser or service principal\n.\nGrant the necessary privileges; at a minimum, SELECT and READ_METADATA are required.\nClick\nGrant\n.\nEnable cluster access for the User\nTo configure the\nCompute\ncluster permissions:\nNavigate to the\nPermissions\nsection.\nSelect the relevant\nuser\nor\nservice principal.\nSet the permission to\nCan Attach To\n.\nTo configure theÂ SQL WarehouseÂ permissions:\nNavigate to theÂ Permissions\nAssign the permission levelÂ Can Use\nGrant Access to\nsystem.access\nSchema for Lineage Queries\nDatabricks now provides access to fine-grained query history and user actions via the\nsystem.access\nschema.\nThis schema is required to access detailed historical logs used for lineage tracking.\nSteps:\nNavigate to\nthe Catalog Explorer in your Databricks workspace.\nExpand the\nsystem\ncatalog and select the\naccess\nschema.\nClick the\nPermissions\ntab.\nGrant your user or service principal the following\nminimum permissions\n:\nSELECT\nUse_Schema\nExample SQL (if using Unity Catalog SQL):\nGRANT\nSELECT\n, Use_Schema\nON\nSCHEMA system.access\nTO\n`your\n-\nuser\n-\nor\n-\nservice\n-\nprincipal`;\nThis enables your application or service principal to query audit tables like\nquery_history\n,\nquery_text\n, and\nquery_execution\n.\nDatabricks Validation Check: Did it work?\nYou can test access by calling the query history endpoint using\ncurl\n:\ncurl --location\n'https://<DATABRICKS_HOST>/api/2.0/sql/history/queries'\n\\\n--header\n'Authorization: Bearer <PAT/ OAUTH token>'\nReplace\n<DATABRICKS_HOST>\nwith your workspace URL and\n<PAT/ OAUTH token>\nwith your personal access or OAuth token.\n2. Enable Query History in SQL Server, A\nzure SQL, Azure Synapse\nUse the\nQuery Store\n.\nThe Query Store is enabled by default for new Azure SQL Database and Azure SQL Managed Instance databases.\nQuery Store is not enabled by default for SQL Server 2016 (13.x), SQL Server 2017 (14.x), SQL Server 2019 (15.x). It is enabled by default in the\nREAD_WRITE\nmode for new databases starting with SQL Server 2022 (16.x). To enable features to better track performance history, troubleshoot query plan related issues, and enable new capabilities in SQL Server 2022 (16.x), we recommend enabling Query Store on all databases.\nQuery Store is not enabled by default for new Azure Synapse Analytics databases.\nEnabling the Query Store can be done in multiple ways.\nEnable Query Store using the Query Store page in SQL Server Management Studio\nIn Object Explorer, right-click a database, and then select\nProperties\n.\nNote: Requires version 16 or later of Management Studio\nIn the\nDatabase Properties\ndialog box, select the\nQuery Store\npage.\nIn the\nOperation Mode (Requested)\nbox, select\nRead Write\n.\nEnable Query Store using Transact-SQL statements\nUse the\nALTER DATABASE\nstatement to enable the query store for a given database. For example:\nALTER DATABASE <Your Database name>\nSET QUERY_STORE (QUERY_CAPTURE_MODE = ALL);\nIn Azure Synapse Analytics, enable the Query Store without additional options, for example:\nALTER DATABASE <database_name>\nSET QUERY_STORE = ON;\nSQL Server, Azure SQL, Azure Synapse Validation Check: Did it work?\nAfter completing the above steps, verify that query logging is properly enabled by running the following SQL query:\nSELECT TOP 100 qsr.*\nFROM sys.query_store_query qsr\nLEFT JOIN sys.query_store_query_text qsrt\nON qsr.query_text_id = qsrt.query_text_id\nLEFT JOIN sys.query_context_settings qcs\nON qcs.context_settings_id = qsr.context_settings_id\nLEFT JOIN sys.schemas s\nON s.schema_id = qcs.default_schema_id;\n3. Enable Query History in PostgreSQL\nTo enable query history in PostgreSQL, you can modify the configuration file (postgresql.conf) and use SQL statements for more fine-grained control.\nModify postgresql.conf\nYou need superuser privileges to edit this file.\nYou can locate\npostgresql.conf\nin your PostgreSQL data directory.\nCommon locations include:\n/etc/postgresql/<version>/main\n(Linux)\nC:\\Program Files\\PostgreSQL\\<version>\\data\n(Windows)\nEdit\npostgresql.conf\nSet\nlogging_collector\nto '\non\n'\nSet\nlog_statement\nto 'all' to track SQL queries. For reference, these are the valid values:\nnone\n(default): Log no statements.\nddl\n: Log data definition language (DDL) statements like CREATE, ALTER, and DROP.\nmod\n: Log moderate-level statements, including DDL and most of the data manipulation language (DML) statements.\nall\n: Log all statements, including SELECT, INSERT, UPDATE, DELETE, and more.\nUse SQL Statements for Fine-Grained Control\nIf you want to enable or disable query history for specific databases or sessions, you can use SQL statements. This can be especially useful for debugging or auditing purposes. You can change logging settings on a per-session or per-database basis using the following SQL commands:\nTo enable query history for the current session only:\nSET log_statement = 'all';\nTo enable query logging for a specific database (replace your_database with the actual database name):\nALTER DATABASE your_database SET log_statement = 'all';\nTo disable query logging for a specific database:\nALTER DATABASE your_database SET log_statement = 'none';\nThese SQL statements will take effect immediately for the current session or database.\nRemember to be cautious with query history in production environments, as it can generate large log files and potentially impact performance. Always monitor your log files and adjust the log level as needed to balance the need for more information and the need for better performance.\nPostgreSQL Validation Check: Did it work?\nAfter completing the above steps, verify that query logging is properly enabled by running the following SQL query:\nS\nELECT\npst.\n*\nFROM\npg_stat_statements pst\nLEFT\nJOIN\npg_database pd\nON\npst.dbid\n=\npd.\"oid\"\nLEFT\nJOIN\npg_catalog.pg_user pu\nON\npu.usesysid\n=\npst.userid\nLIMIT\n10\n;\nThis query will return the recent queries logged in PostgreSQL. If it returns results, query logging is successfully enabled.\n4. Enable Query History in Snowflake\nQuery history is always enabled in Snowflake. So the only task required is to make sure that the relevant user has access to the history.\nThe configured user should have access to SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nBy default, the SNOWFLAKE database is visible to all users; however, access to schemas in this database can be granted by a user with the ACCOUNTADMIN role using either of the following approaches:\nGrant IMPORTED PRIVILEGES on the SNOWFLAKE database.\nUSE ROLE ACCOUNTADMIN;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE SYSADMIN;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE customrole1;\nFor more information regarding snowflake, this article\nAccount Usage in Snowflake\ncan be referred\nSnowflake Validation Check: Did it work?\nAfter completing the above steps, verify that query logging is properly enabled by running the following SQL query:\nSELECT\nQUERY_ID,\nQUERY_TEXT,\nDATABASE_NAME,\nSCHEMA_NAME,\nQUERY_TYPE,\nUSER_NAME,\nEXECUTION_STATUS,\nSTART_TIME,\nEND_TIME,\nTOTAL_ELAPSED_TIME,\nSESSION_ID\nFROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE START_TIME > (CURRENT_TIMESTAMP - INTERVAL '30 DAY')\nLIMIT 10;\n5. Enable Query History in MySQL\nMySQL does not provide a built-in query history like some other databases, but query history can be tracked using the following methods:\nUsing the\nMySQL General Query Log\nThe General Query Log records all queries executed on the server.\nSteps to Enable:\nEnable the general log:\nSET\nGLOBAL\ngeneral_log\n=\n'ON'\n;\nSET\nGLOBAL\nlog_output\n=\n'TABLE'\n;\n-- or 'FILE'\nIf using the\nTABLE\noption, view the logs:\nSELECT\n*\nFROM\nmysql.general_log;\nIf using the\nFILE\noption, check the log file location:\nSHOW\nVARIABLES\nLIKE\n'general_log_file'\n;\nConsiderations:\nEnabling the General Query Log may impact performance.\nLogs should be cleared regularly to avoid storage issues.\nUsing the\nMySQL Performance Schema\nThe Performance Schema tracks recent queries with performance-related details.\nSteps:\nEnable the\nperformance_schema\nin the MySQL configuration file (\nmy.cnf\nor\nmy.ini\n):\n[mysqld]\nperformance_schema\n=\nON\nRestart the MySQL server.\nQuery the\nevents_statements_history\ntable for recent queries:\nSELECT\n*\nFROM\nperformance_schema.events_statements_history;\nFor a summary of all queries:\nSELECT\nDIGEST_TEXT, COUNT_STAR\nFROM\nperformance_schema.events_statements_summary_by_digest\nORDER\nBY\nCOUNT_STAR\nDESC\n;\nAdditional Notes on Performance Schema Query History:\nDefault Behavior:\nThe\nevents_statements_history\ntable stores only a limited number of entries, depending on the\nperformance_schema\nconfiguration.\nCheck the current size limit:\nSHOW\nVARIABLES\nLIKE\n'performance_schema_events_statements_history_size'\n;\nIncreasing History Size:\nTo retain more query history, adjust the\nevents_statements_history_size\nparameter:\nEdit the\nmy.cnf\nor\nmy.ini\nfile:\n[mysqld]\nperformance_schema_events_statements_history_size\n=\n10000\nRestart MySQL to apply changes.\nAlternatively, change the size dynamically for the current session:\nSET\nGLOBAL\nperformance_schema_events_statements_history_size\n=\n10000\n;\nAfter increasing the history size, more queries will be stored in\nevents_statements_history\n.\nMySQL Validation Check: Did it work?\nAfter completing the above steps, verify that query logging is properly enabled by running the following SQL query:\nSELECT ess.*\nFROM performance_schema.events_statements_summary_by_digest ess\nLEFT JOIN performance_schema.events_statements_history esh\nON ess.DIGEST = esh.DIGEST\nLIMIT 10;\n6. Enable Query History in Netezza\nIn Netezza, query history can be accessed using the\nHISTDB.HISTDBOWNER.NZ_QUERY_HISTORY\ntable, which is part of the History Database (HISTDB). This advanced feature provides detailed historical information about queries, sessions, and system activity.\n7. Query History in Oracle\nFor Oracle-specific lineage and query history setup, refer to\nConfigure Oracle as a Source\n.",
    "scraped_at": "2026-02-02 15:32:36"
  },
  {
    "title": "Lineage in DvSum",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/33694180953108-Lineage-in-DvSum",
    "content": "Overview\nData lineage in the DvSum Application provides a comprehensive view of the data's journey, helping users understand how data moves, transforms, and interacts across systems. It offers a visual map of the data flow, from the source to the destination, showing all stages of transformation and any dependencies involved. By utilizing data lineage, it is possible to trace the movement of data in real time, detect issues quickly, and assess the impact of changes to processes. This feature is essential for improving data transparency, ensuring data quality, and maintaining compliance with governance standards. This article covers the following topics:\nPrerequisites for Data Lineage\nPerforming a Lineage Scan\nStep 1: Navigate to Data Sources\nStep 2: Initiate a Scan\nStep 3: Select Scan Options\nStep 4: View Scan Details\nUnderstanding Lineage Visualization\nLineage View\nFooter Options\nHeader Options\nPlaceholder Tables\nData Quality (DQ) Observability and Data Classification\nDQ Observability\nData Classification\nExploring Functional and Technical Lineage\nFunctional Lineage\nTechnical Lineage\nETL Nodes\nManaging Lineage Permissions in User Roles\nPermission Levels\nEntity Level vs. Entity-Attribute Level\nCurrent Functionality\nDemonstration\nAdding an Upstream Node\nSupported Data Sources\nPrerequisites for Data Lineage\nTo enable effective data lineage in the DvSum application, the following prerequisites must be met:\nComplete Database and Schema Scan:\nEnsure that all relevant databases, schemas, and their associated tables/views are scanned in the DvSum application.\nQuery History Configuration:\nQuery history must be enabled for your data sources to derive both functional and technical lineage. For detailed instructions, refer to the\nEnable\nQuery\nHistory\nfor\nData\nSources\narticle.\nComprehensive Query Tracking:\nEnsure all query types relevant to lineage (e.g.,\nINSERT\n,\nMERGE\n,\nUPDATE\n, and\nSELECT\n) are captured and made available for lineage analysis.\nLineage Extraction from Procedures, Functions, and Notebooks:\nTo successfully capture lineage from Stored Procedures, Functions, and Databricks Notebooks, users must have the necessary access permissions based on the source system:\nDatabricks:\nThe user must authenticate using a\nPersonal Access Token\n.\nThe user must have access to\nDatabricks Notebooks\n.\nSnowflake:\nThe\nuser cataloging the Snowflake source must be the owner of the stored procedure\nbeing used. Without this ownership, DvSum will not be able to extract lineage information.\nFor instructions on how to grant ownership, refer to the\nConfigure Snowflake as a Source\narticle.\nAll Other Sources:\nThe user must have access to the relevant stored procedures to retrieve lineage information.\nPerforming a Lineage Scan\nStep 1: Navigate to Data Sources\nAccess the\nData Sources\nsection in the application.\nSelect the desired data source.\nStep 2: Initiate a Scan\nNavigate to the\nScan History\ntab.\nClick the\nScan Now\nbutton, which provides the following options:\nProfile\n: Profiles data assets.\nLineage\n: Tracks and visualizes the data flow.\nThe catalog scan runs by default. Users can use checkboxes to select additional scan types.\nStep 3: Select Scan Options\nSelecting the\nLineage\ncheckbox triggers a lineage scan.\nSelecting both\nProfile\nand\nLineage\noptions triggers all three scans (Catalog, Profile, and Lineage) simultaneously.\nThis setup allows for the customization of scans based on specific requirements.\nStep 4: View Scan Details\nLocate the scan name and click on it to access detailed insights.\nThe detailed Insights are displayed as such:\nNavigate to the\nTable Detail\npage for further exploration.\nUnderstanding Lineage Visualization\nLineage View\nThe lineage visualization opens in a new tab, displaying:\nReference Node\n(center): The selected table or view.\nUpstream Nodes\n: Sources influencing the reference node.\nDownstream Nodes\n: Entities impacted by the reference node.\nFooter Options\nExpand All Upstream Nodes\n: Displays all upstream relationships.\nReference View\n: Tables reference view ID displayed\nSet Context\n: Focuses the lineage on a specific node or column.\nIn the example, lineage is already set for a node, but any other node can be selected as Context:\nResetting to the original node is still an option if another node is set:\nExpand Columns: All the columns can be expanded and have the same footer options as tables:\nPlaceholder Tables in Lineage\nPlaceholder tables are tables that exist in the database but are not recognized within the application. These tables are visually indicated by a dotted line around their boundary and placeholder text will be written on the node.\nKey behaviors of placeholder tables:\nThey will\nnot be accessible elsewhere\nin the application except in the lineage view.\nThe\nReference View\nfor placeholder tables\nwill not open\n.\nThe\nReference View for their columns will also not open\n.\nOther than the above two options, the rest of the options on the placeholder node will be the same as that of any other table node.\nThis ensures that users can now identify and track placeholder tables within the lineage flow while maintaining standard lineage functionalities.\nHeader Options\nSearch:\nSearch for specific tables, nodes, or columns within the lineage view. Searches highlight all relevant elements, providing a quick way to locate specific data flows.\nReset View\n: Resets the visualization to the default reference node view.\nShare Lineage:\nShares a link to this lineage view with people or groups\nLayers:\nThis displays two new options when clicked\nData Quality (DQ) Observability and Data Classification\nDQ Observability\n: Highlights upstream nodes in the lineage view with active alerting rules applied.\nData Classification\n: Sensitive data tags and classifications (e.g., PII, PCI) are prominently displayed within the lineage visualization.\nExploring Functional and Technical Lineage\nFunctional Lineage\nFunctional lineage illustrates the data flow at an abstract level, showing tables, columns, and relationships (Everything discussed above was Functional Lineage)\nTechnical Lineage\nTechnical lineage includes:\nSource and target code blocks derived from query history.\nAttribute-level insights are not available for precise analysis.\nIf the angle brackets are clicked the code is displayed:\nETL Nodes in Technical Lineage\nTechnical Lineage also includes\nETL Nodes\n, representing\nStored Procedures (SPs)\nin the database. Stored Procedures are pre-written SQL scripts that execute complex data transformations and manipulations within the database. In our application, we refer to them as\nETL Nodes\n.\nKey Features of ETL Nodes:\nTransformations on Columns\n: ETL Nodes display the transformations applied to tables and columns within the lineage.\nStored Procedure Code Visibility\n: Users can now\nview the full SQL code of the stored procedure\nfrom the ETL node.\nColumn-Level Transformations\n: The lineage details now include transformations applied at the column level, providing deeper insights into data movement.\nJust like any other node, the ETL node can also be set as context for viewing lineage from its perspective. The same thing can be done on transformations on ETL nodes.\nManaging Lineage Permissions in User Roles\nThe\nLineage\nsection in user roles configuration allows managing access to lineage data. Below are the available permissions:\nPermission Levels\nUser access to lineage features depends on assigned roles:\nNo Access\n: Restricts lineage visibility.\nFunctional Lineage\n: Allows entity and attribute-level exploration.\nFunctional and Technical Lineage\n: Grants full access, including technical insights.\nEntity Level\n: Displays relationships between entities like tables or datasets.\nEntity-Attribute Level\n: Shows relationships at both entity and attribute levels.\nCurrent Functionality\nAdmin Role\n: Only admins can upload lineage data through the import process.\nUpstream Nodes\n: Users can add upstream nodes at the\ntable level\nbut not at the\ncolumn level\n.\nDemonstration\nTo add an upstream node:\nGo to the\nTable Detail Page\n.\nClick\nEdit\nto add upstream nodes at the table level.\nSupported Data Sources\nLineage functionality is currently supported for:\nRelational Databases\n: Oracle, SQL Server, MySQL, PostgreSQL, IBM Db2, Azure SQL\nData Lakes and Warehouses\n: Snowflake, Databricks, Azure Synapse, Netezza, Amazon S3\nBI Tools\n: Tableau, Power BI\nNot Supported\n: MongoDB, Salesforce, Azure Data Lake, and file uploads.\nEnabling Lineage in Oracle\nTo enable lineage tracking from Oracle in DvSum, especially when stored procedures or packages are involved, the following setup is required.\nAccessing Stored Procedure/Package Code\nTo extract lineage from stored procedures or packages, DvSum needs access to their source code. This source code is stored in Oracleâ€™s\nALL_SOURCE\nsystem view. The Oracle user should have\nread-only access\nto this view to allow DvSum to parse the code and identify lineage relationships.\nYou can use the following SQL query to retrieve the procedure or package code:\nSELECT\nOWNER,\nNAME,\nTYPE,\nTEXT,\nLINE\nFROM\nALL_SOURCE\nWHERE\nOWNER IN ('{schema_names}')\nNote\n: Replace\n{schema_names}\nwith the actual schema names where the procedures or packages are defined.\nSource Configuration Requirement\nMake sure that all relevant schemas (where the procedure or package code resides) are included in the source configuration in DvSum. If any required schemas are missing, lineage extraction may not work as expected.",
    "scraped_at": "2026-02-02 15:32:42"
  },
  {
    "title": "Run Online & Run Offline",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/27004996852500-Run-Online-Run-Offline",
    "content": "In this Article\nOverview\nDetails\nTable Profiling\nRule Execution\nOverview\nUsers can execute many different tasks in DvSum Data Catalog (DC). Testing a rule might take just a few seconds. But some rules could take several minutes or even longer. Therefore DC offers the ability to \"\nRun Online\n\" and \"\nRun Offline\n\" to give users the flexibility to run jobs as required.\nExecuting a rule synchronously and waiting to see the results is often the most efficient way to test a new rule. Executing a long-running job in the background and reviewing the results later is sometimes preferred.\nDetails\nThe option to \"\nExecute Now\n\" or \"\nRun Offline\"\napplies in several places.\nDatabase Tables\nDQ Rules\nTable detail page\nRule detail page\nThe options are complementary and give developers the flexibility to work as they prefer.\nRun\nOnline\n- Will immediately execute the action. Note that if the user navigates to another page while the task is executing, then the task will be aborted.\nRun Offline\n- Will schedule the action to run soon. Typically the task will begin execution within a few seconds. The job will be visible in the \"\nJobs\"\ntab.\nTable Profiling\nTable profiling can be executed from multiple places.\nGo to the\nDictionary\ndropdown and select the\nDatabase Tables\ntab.\nRule Execution\nDQ Rules:\nRule Detail Page:\nNote: Only the first 300 exceptions are displayed in the Grid on the Data tab. if needs to view more exceptions, must enable the â€œInclude all Exceptionsâ€ checkbox in the Run Online dropdown before running the rule.",
    "scraped_at": "2026-02-02 15:32:47"
  },
  {
    "title": "Entity Relationship Diagram - ERD",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/19960718695700-Entity-Relationship-Diagram-ERD",
    "content": "In this Article\nIntroduction\nData Model Tab\nAdd/Edit Relationships\nSetting Tables as Context\nGrid View\nLevels in ERD\nIntroduction\nAn Entity Relationship (ER) Diagram is a visual map that acts like a flowchart, revealing the connections between various elements, such as people, objects, or ideas, within a system. In our application, the ER Diagram serves as a helpful tool to show how different datasets and their respective columns are related. It provides a clear, organized view, allowing users to grasp the connections between datasets and gain a deeper understanding of the overall data structure. This article explores the role of an ER diagram in improving data comprehension.\nData Model Tab\nNavigate to Dictionaries >Database Tables from the side bar\nClick on a table name\nThe ER diagram can be found on the detail page of any table within the \"Data Model\" tab.\nIn the ER diagram, you'll notice a main table that stands out, and the other tables connected to it are linked with arrows to show their relationships.\nBy default, you can see the name of each table.\nFor more detailed information, you have the option to expand each table to see the specific columns it contains.\nTo the left of the ER diagram there are some interactions such as:\nZoom In\nZoom Out\nFit View\nToggle Interactivity\nUsers can change the zoom level of the diagram based on their preferences. This can be done by either using the mouse scroll along with the CTRL button or through zoom controls. To move around the diagram, users can right-click and drag their mouse to pan the view in different directions.\nThe \"Fit View\" option instantly resets the ER diagram to its default view on the screen.\nThis \"Toggle Interactivity\" function serves to enable or disable various interactions within the ER diagram. For instance, when the toggle interactivity is in the locked state, users cannot add relations or access reference views of columns in the ER diagram.\nAdd/Edit Relationships\nBefore we proceed to add or edit relationships, let's first define the three fundamental relations within the application:\nRelationship added from Database\nRelationship recommended by DvSum\nRelationship added by User\nEvery relationship has a reference view and it can be opened by simply clicking on the line that connects the two tables\nThe Relationship that is added to the database can neither be edited nor deleted.\nThe second type of relationship is the one that is recommended by DvSum. These relationships can be edited or deleted. There is another option of \"Confirm\" along with the \"Edit\". Once the user confirms the relationship this relationship is changed to the \"User added\" relationship type.\nThe \"User added\" relationship is the third type of relationship. One way of making the relationship \"User added\" is just confirming the relationship which is recommended by DvSum:\nAnother method to create a \"User-added\" relationship is by manually adding and defining the relationship.\nTo add a relationship, click on the \"+\" icon located in the top-right corner of the source table. This icon allows users to initiate the process of adding a new relationship.\nOnce the \"+\" icon is clicked the \"Add Relationship\" reference view is opened. Here the Source table is mentioned, Users can add the Reference table from the drop-down menu and define any type of relationship\nAfter adding the Reference table and selecting the Relationship type the user will be required to add source fields and reference fields for which the relationship is being added. Users can add single or multiple columns for the relationship.\nNote\n: It is to be noted that once any column is added to a relationship then that column can not be added again, the column will be greyed out when a user tries to add it to the relationship:\nOnce the relevant fields are added user can click on \"Save Changes\" and the relationship will be created. In this way, the user can add multiple relationships to the source table. The relationship that is added by the user can be edited again by clicking on the line connecting the tables. If the relationship is not required then it can be removed also:\nSetting Tables as Context\nOn the ER diagram other than the source table there is an arrow icon on the other tables on the top right.\nWhen this button is clicked the detail page of the respective table is opened and the Data Model Tab is displayed:\nGrid View\nBy default, the view which is opened is the Model view of the \"Data Model\" tab. Along with the model view, there is another view \"Grid View\" which basically shows all the relationships defined in the ER diagram and some other information. The relationships that are recommended by DvSum or user-added can also be edited/deleted from here.\nLevels in ERD\nThere are levels defined in the ER diagram.\nOn\nLevel 1\nthe tables that are directly related to the source table are shown:\nMoving to\nLevel 2\n, you'll discover the tables associated with tables on Level 1.\nIn the same way, the tables that are further related to the highlighted tables will be shown in the 3rd+ level thus showing the ER diagram in detail.",
    "scraped_at": "2026-02-02 15:32:52"
  },
  {
    "title": "Rich Text Additional Information",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/19188186862996-Rich-Text-Additional-Information",
    "content": "In this Article\nIntroduction\nAccessing Additional Info tab for Tables\nAccessing Additional Info tab for Terms\nIntroduction\nDvSum allows its users to add additional information to the Table Detail page and Glossary Detail page. Although this feature already existed, in order to increase the capability of the feature, enhancements have been made, which are discussed in this article.\nAccessing Additional Info tab for Tables\nThe Table Detail Page can be accessible through the Database Tables from the left menu, and it can be opened by clicking on the Table name.\nOnce the Table Detail Page is opened, the user can perform different tasks like \"\nEdit\n\" the changes in the Table.\nOnce the \"\nEdit\"\nbutton is clicked, the Table detail page changes to edit mode. Additional Information can be found by scrolling downwards.\nOnce the user clicks the Edit icon on Additional Info, the text editor will open up, and just like any text editor, it has options like selecting different font families and font sizes, changing font colors, adding numbering to the text, etc.\nAny necessary additional information regarding the table can be added here according to the requirement of the user.\nOnce the information is added, the user can click on the blue arrow icon on the bottom right to add the additional information.\nAfter incorporating the information, it will be included as an item. At this point, the user can choose to edit the newly added information or remove it. Additionally, the user has the option to append another additional info item if desired\n. If the user has added the additional info item that is/are required, then the user can click on the \"tick\" button on the top right:\nIf the user chooses to publish the changes, the added information becomes part of the table. Conversely, if the user discards the changes, the additional information will not be included in the table.\nNote:\nFor more information on Table detalied page, refer to the article \"\nTable Detail Page\"\nAccessing Additional Info tab for Terms\nJust like the Table detail page, the term detail page can be opened from the Business Glossary tab from the left menu:\nOnce the detail page is opened, the user can enter the edit mode by clicking the \"\nEdit\n\" button, and additional info can be found by scrolling downwards. Users can add the required additional info:\nOnce the user has added the information, they can publish the changes:\nIf workflow is enabled for a term, instead of \"Publish,\" the user will see \"Submit for Approval.\" So, to finalize the changes, approval from the designated approver is necessary before publishing the term.\nFor more information about the workflow of terms, go through the article\nhere.",
    "scraped_at": "2026-02-02 15:32:58"
  },
  {
    "title": "Entity Field Format in the Field Dictionary Template",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/36407279883156-Entity-Field-Format-in-the-Field-Dictionary-Template",
    "content": "In this Article\nOverview\nEntity Field Format\nField Dictionary Download\nField Dictionary Import\nOverview\nWhen working with the Field Dictionary in DvSumâ€”either downloading or importingâ€”you may notice that the\nEntity\nfield contains dot-separated text. This format provides the full path of the entity, including its associated domain and subdomain.\nNote:\nThe Entity field represents a glossary term in DvSum. While it also displays the full hierarchy including domain and subdomain (<DataDomain>.<SubDomain>.<EntityName>), its primary purpose is to map the field or column to the corresponding glossary term.\nEntity Field Format\nThe\nEntity\nfield uses the following structure:\n<DataDomain>.<SubDomain>.<EntityName>\nExample\nDefault.Default.US_PASSPORT\nThis structure ensures clarity when identifying entities that may exist under different domains or subdomains.\nField Dictionary Download\nWhen downloading the Field Dictionary:\nThe\nEntity\ncolumn will display the full hierarchy as described above.\nThe\nData Domain\ncolumn reflects the domain assigned to the column itself, which may be different from the domain shown in the Entity field.\nField Dictionary Import\nWhen importing the Field Dictionary:\nThe\nEntity\nfield must follow the format:\n<DataDomain>.<SubDomain>.<EntityName>\nThis format is also reflected in the\nsample file\nthat can be downloaded from the import screen. Use it as a reference when preparing your import.\nEnsure any edits made to the Entity field maintain the correct format to avoid errors during import.",
    "scraped_at": "2026-02-02 15:33:02"
  },
  {
    "title": "Database Columns",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/34934067464084-Database-Columns",
    "content": "In this Article\nOverview\nDatabase Columns\nAccessing the Field Dictionary\nDatabase Columns Details\nColumn Detail Page\nOverview Tab\nProfiling Tab\nData Quality Tab\nOverview\nThe Database ColumnsÂ in DvSum provides users with a comprehensive view of all data columns across different sources and datasets. It consolidates metadata, profiling details, and data quality insights into a structured interface, enabling users to explore and manage their data efficiently.\nAccessing the Database Columns\nOn the left-side menu of the DvSum application, Go to the\nDictionary\ndropdown and select the\nDatabase Columns\nto navigate to its detailed page.\nThis page displays all column-specific details, provided that the sources are\nconnected and cataloged\n.\nDatabase Columns Details\nThe Field Dictionary provides an organized view of column-related metadata, including:\nSource\nTable\nColumn Name\nBusiness Name\nColumn Description\nData Type\nProfiling Info\nRange Value\nDQ Score\nColumn Sub-type\nColumn Detail Page\nClicking on a\nColumn Name\nopens the\nColumn Detail Page\n, which contains three tabs:\nOverview\nâ€“ Displays general metadata and descriptions.\nProfiling\nâ€“ Shows profiling statistics and insights.\nData Quality\nâ€“ Highlights rule violations and data quality metrics.\nOverview Tab\nThe\nOverview\ntab in the\nField Dictionary\ncomprises several key sections that provide a structured summary of column attributes and configurations. These sections typically include:\nColumn Name & Description\n: Displays the name of the column along with a brief description of its purpose.\nData Type\n: Specifies the type of data the column holds (e.g., String, Integer, Boolean).\nSource & Lineage\n: Identifies the origin of the column and traces its transformation across datasets.\nUsage & Dependencies\n: Indicates where and how the column is utilized within the system.\nConstraints & Validation\n: Lists any applied rules, such as mandatory columns, unique constraints, or default values.\nTags & Classification\n: Allows categorization of columns for better organization and governance.\nAdditional Info\n: Displays any extra details relevant to the column.\nSimilar Columns\n: Suggests columns with similar characteristics to aid in analysis and comparison.\nEditing and Deleting a Column\nOn the\nOverview\npage, users have the option to either\nEdit\nor\nDelete\na column.\nEdit\n: Clicking the\nEdit\nbutton (represented by a blue button with a pencil icon) allows users to modify the columnâ€™s properties, such as its name, tags, or other metadata.\nDelete\n: The\nDelete\nbutton (represented by a red trash bin icon) enables users to remove the column permanently. Users should exercise caution while deleting, as this action may not be reversible.\nProfiling Tab\nThe\nProfiling\ntab in the Field Dictionary contains the following sections:\nStatistics\n: Displays key metrics such as record count, number of empty/null values, unique values, and completeness percentage.\nInformation\n: Provides details including the last profiled date, min-max values, whether the column is a primary key or nullable, technical data type, column position, size, and the number of fractional digits.\nNote:\nThe Min-Max values depend on the column's data type. For string data types, the minimum and maximum values are determined based on lexicographical order.\nVisualization\n: Includes graphical representations of data distributions, with options to view distribution and pattern insights.\nHistogram time intervals automatically determined based on the span of date values:\n0â€“1 day â†’ Hourly\n2â€“30 days â†’ Daily\n31â€“60 days â†’ Weekly\n61â€“730 days â†’ Monthly\n731â€“1460 days â†’ Quarterly\nOver 1460 days â†’ Yearly\nRun Profiling\n: An option is available to initiate online profiling for updating column statistics and insights.\nData Quality Tab\nThe\nData Quality\ntab in the Field Dictionary contains the following sections:\nStatistics\n: Displays key metrics such as DQ Score, total rules, rules with alerts, total exceptions, total records scanned, and last rule execution status.\nRules\n: Lists all data quality rules applied to the column, including details such as rule ID, description, run status, alert status, and exception count.\nAdd Rule\n: Provides an option to add new data quality rules for monitoring and validation. Only the following three rules can be applied:\nBlanks\n: Checks for missing or empty values in the column.\nValue Range\n: Ensures that data falls within a specified range.\nData Format\n: Validates the format of the data based on predefined patterns.\nDQ Score History\nClicking on the arrow icon history is displayed.\nLearn more about the Rules section in Data Quality tab\nhere\nField Reference Page\nThe Field Reference page appears when a user clicks on a column name from the homepage.\nIt slides out from the right side of the screen, displaying relevant information about the selected column.",
    "scraped_at": "2026-02-02 15:33:07"
  },
  {
    "title": "Glossary Term",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35943193733396-Glossary-Term",
    "content": "In this Article:\nIntroduction\nAccessing the Glossary Term\nAdding Terms in Glossary Term\nKey Feature: Auto-Update of Linked Columns\nIntroduction:\nGlossary Term is a structured repository of business terms and definitions used to ensure consistent terminology across data assets. It supports data governance by enabling categorization, improved data searchability. This guide outlines the step-by-step process to add and manage terms efficiently.\nAccessing the Glossary Term\nLog in to DvSum.\nNavigate to the Glossary Term tab.\nAdding terms in Glossary Term\nAdding single term\nRefer to the article\nAdding New Term\nfor detailed steps for adding new term in the Glossary term.\nAdding multiple terms\nSteps to Bulk Upload Terms:\nClick on the\n\"Download\"\noption.\nAn\nExcel sheet\ncontaining all existing terms and their details will be downloaded.\nOpen the Excel sheet and\nadd multiple new terms\nalong with their details.\nSave the updated Excel sheet.\nClick on the\n\"Import\"\noption.\nFor more information on importing, refer\nHow to import files\n.\nKey Feature of Glossary Terms\nOne of the powerful features of the\nGlossary terms\nin\nDvSum\nis its ability to automatically update linked columns when a change is made to a term.\nHow It Works:\nIf a\nterm\nin the Business Glossary is linked to multiple\ntables and columns\nin different databases, any changes made to that term are\nautomatically reflected\nacross all linked columns.\nFor Example:\nEdit the term\n\"First Name\"\nin the\nGlossary Terms\n.\nModify relevant details such as definition.\nIf a tag is\nadded\n,\nremoved\n, or\nmodified\non the term, all linked columns will immediately reflect the change.\nClick\n\"Save\"\nto apply the changes.\nResult:\nThe update is\nautomatically reflected\nin all linked columns.\nNo manual updates are required in individual columns.\nBefore:\nThe term\n\"First Name\"\nhas no tags assigned except \"Sensitive tag\".\nA tag\n\"Test\"\nis added to the term\n\"First Name\"\nin the Glossary Terms.\nAfter:\nAll columns linked to the term\n\"First Name\"\nnow show the\n\"Test\"\ntag automatically along with the existing tag.\nThis eliminates the need to manually update individual columns, ensuring\nconsistency and accuracy\n.\nAuto Linking: Based on Name + Data Type\nDvSum supports\nauto-linking of glossary terms to dataset columns\nbased on:\nTerm Name\nmatching the\ncolumn name\nMatching data type\nThis means that if a column name and data type match an existing term in the Glossary terms,\nlinking happens automatically\n, reducing the need for manual mapping.\nFor example, a glossary term \"Customer ID\" of type\nString\nwill auto-link to columns named \"Customer ID\" with type\nString\nin datasets.\nBefore and After Example\nBefore:\nGlossary term \"First Name\" has only the \"Sensitive tag\"\nSeveral columns are linked to this term but do not have the \"Test\" tag\nAction:\nThe \"Test\" tag is added to the glossary term \"First Name\"\nAfter:\nAll columns linked to \"First Name\" automatically inherit the \"Test\" tag\nNo changes were needed at the column level\nThis automated propagation maintains alignment across metadata and ensures ongoing data quality.\nLinking Terms to Columns\nTo manually or programmatically link glossary terms to columns, refer to the article:\nHow to link glossary Terms",
    "scraped_at": "2026-02-02 15:33:12"
  },
  {
    "title": "How to Sign Up for DvSum Data Catalog",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/15389497274260-How-to-Sign-Up-for-DvSum-Data-Catalog",
    "content": "Overview\nJoin the thousands of satisfied users who have already unlocked the full potential of our Data Catalog. Sign up now and experience the difference for yourself! Follow these simple signup steps below and get started using DvSum Data Catalog in less than 3 minutes.\nStep 1:\nBrowse to DvSum's\nAgile Data Catalog\npage and click on \"\nStart for Free\n\".\nStep 2:\nOn the next screen enter an email you want to register for the trial account. A verification link will be sent to the provided email.\nNote:\nThis user will be the\nAccount Owner\n.\nStep 3:\nIn the received email click on the \"\nVerify Email\n\" button. You will be redirected to the Signup form. Fill this out to create your account.\nStep 4:\nOn the\nDvSum Data Catalog Sign-In page\n, enter your login credentials to sign in and access all the amazing features and functionalities our Data Catalog has to offer.\nNext steps:\nFollow the product tour wizard to get started.\nHappy Cataloging!",
    "scraped_at": "2026-02-02 15:33:18"
  },
  {
    "title": "An Overview of Usage Analytics",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/8596413680020-An-Overview-of-Usage-Analytics",
    "content": "In this Article\nOverview\nUsage Dashboard Sections\nCatalog Inventory\nData Catalog\nAnalytics Catalog\nBusiness Glossary\nTags\nUsers\nOverview\nUsage analytics refers to the use of interactive visualizations and metrics displayed on a dashboard to monitor and analyze data\nUsage Analytics can be accessed from\nAdministration Tab -> Usage\nAnalytics.\nUsage Dashboard Sections\nUsage Dashboard consists of multiple sections:\nThe catalog inventory section shows the data count of items.\nOther sections are the Data Catalog, Analytics Catalog, Business Glossary, Tags, and Users which show data in the form of tables with summary pages.\nCatalog Inventory\nThe catalog inventory section shows the data count of the following items:\nSources\nDatasets\nFields\nDomains\nCategories\nEntities\nTagset\nTags\nUser Groups\nUsers\nThe dashboard shows this information on runtime if the count is updated for any items. It will be reflected in the Catalog inventory as well.\nBelow the Catalog inventory section, there is a clickable navigation button that shows data of a clicked button.\nFor example: Click on the Data Catalog button, section will show data from the Data Dictionary in the form of tables with summary pages.\n1. Data Catalog\nThis section shows information about sources, Datasets, and fields.\n1.1\nData Sources\nUsability\nIt shows the average usability score of sources, which includes Datasets and columns.\nUsability score of source out of 10.\nQuality\nIt shows the average quality of sources, including Datasets and columns.\nBut the Quality score will be calculated for Fields only. The quality score shows as a percentage.\n1.2\nDatasets\nCount\nIt shows the total count of all Datasets in the source.\nUsability\nIt shows the average usability score of Datasets in the source.\nUsability score of Dataset out of 10.\n1.3\nFields\nCount\nIt shows total count of all Fields in the source.\nUsability\nIt shows the average Meta score of Fields in the source.\nUsability score of Fields out of 10.\nQuality\nIt shows the average Data Quality score of Fields in the source.\nThe quality score shows as a percentage.\nCuration Tracking\nThe curation Tracking section shows the results of all sources from the Data Dictionary Table.\nEvery Source row in the below Data dictionary section is clickable, when a user clicks on any source row, detailed information will be shown in the curation tracking section for the selected source.\nThe curation tracking section shows information as Count & percentage for Datasets, fields & All items ( Combined count of Datasets and fields).\nDescription\nIt shows how many Datasets & fields have descriptions available.\nOwnership\nIt shows Data owners/System owners are assigned to how many Datasets.\nNote\n: Ownership is not applicable to Fields.\nLinked To Entity\nIt shows Functional area is linked to how many Datasets & fields.\nProfiling\nIt shows the Profiling count for Fields.\nNote\n: Profiling is not applicable to Datasets.\n2. Analytics Catalog\nThis section shows information about sources, Datasets, and fields from Analytics Dictionary.\n1.1\nData Sources\nUsability\nIt shows the average usability score of sources, which includes Datasets and columns.\nUsability score of source out of 10.\n1.2\nDatasets\nCount\nIt shows the total count of all Datasets in the source.\nUsability\nIt shows the average usability score of Datasets in the source.\nUsability score of Dataset out of 10.\n1.3Â  Â Reports\nCount\nIt shows the total count of all Reports in the source.\nUsability\nIt shows the average usability score of Reports in the source.\nUsability score of Dataset out of 10.\n1.4\nMetrics\nCount\nIt shows total count of all Metrics in the source.\nUsability\nIt shows the average Meta score of Metrics in the source.\nUsability score of Fields out of 10.\nCuration Tracking\nThe curation Tracking section shows the results of all sources from the Analytics Dictionary.\nEvery Source row in the below Analytics Catalog section is clickable, when a user clicks on any source row, detailed information will be shown in the curation tracking section for the selected source.\nThe curation tracking section shows information as Description, Lineage and Duplicates.\n3. Business Glossary\nThis section shows information about Entities:\nDomain\nThe domain Column shows a list of all domains.\nCategories\nIt shows the count of categories linked to the Domain\nEntities\nIt shows the count of entities linked to the category and out of total entities how many entities are linked.\nCount of Linked Data Assets\nIt shows the total count of linked Data assets for Entities\nTop Entities\nThe top entities section shows the top 5 entities of all domains that are linked to Data dictionary items. Every Domain row in the above Glossary section is clickable, when the user clicks on any domain, detailed information of the top 5 entities will be shown against the selected domain.\nThis table shows:\nEntity\nThe Entity column shows names of entities.\nDescription\nThis column shows a description of entities.\nLinked Data dictionary Items\nIt shows the count of Entities linked to Data dictionary items.\nSample\nThis column showsÂ  1 or 2Â  columns linked-to the entity.\n4.Tags\nThis section shows information about Tagset:\nTagset\nThis column shows the list of all tags\nTags\nThis column shows the count of tags available in the respective tagset and the Percentage shows how many tags are used in Dataset / Fields/ Terms\nTagged Assets\nIt shows the total count of used tags in Datasets/fields/terms\nTop Tags\nThe top Tags section shows the top 5 most linked tags of all the tags. Every Tagset row in the above Tags section is clickable, when the user clicks on any tagset, detailed information of the top 5 entities will be shown against the selected domain.\nThis table shows:\nTags\nThis column shows the top tags\nLinked Count\nThis column shows the count of tags that are linked to Dataset / Fields/ Terms\nSample\nThis column showsÂ  1 or 2Â  Datasets/ Terms/ Entity linked to Tags.\n5. Users\nCatalog Usage:\nThis section shows information about the user and their activities:\nUsers\nThe first column shows the list of users with badges assigned on the basis of activities:\ni)Â  Master Badge\nii) Star Badge\niii) Explorer Badge\nActivity\nThis column shows the total count of activity for the user with streak count.\nNotes\nThis column shows the number of searches the user has performed, Contribution made by users, and streak count.\nSearch activity includes performing the search, create a governance view, opening a ref page & open a governance view.\nContribution activity includes Making a suggestion for Datasets/ terms, Endorsing and publishing a Dataset/term.\nStreak contains the number of consecutive days the user is active on the application.\nSteward Activity\nThis section shows information about steward user and their activities:\nUsers\nThe first column shows the list of steward users with their roles.\nAssigned Assets\nThis column shows the count of assigned assets to the respective steward user\nUsability Score\nThis column shows the usability score of assetsÂ  assigned to steward user\nSeward Activity\nIt shows a count of activities (searches & contributions) performed by the steward with a streak.",
    "scraped_at": "2026-02-02 15:33:22"
  },
  {
    "title": "Asset and Glossary Status Lifecycle",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/43341691915668-Asset-and-Glossary-Status-Lifecycle",
    "content": "In this Article\nOverview\nAsset Statuses\nGlossary Statuses\nOverview\nThis article explains the purpose, definition, and lifecycle flow of each status used in the\nAsset Dictionary\n,\nData Dictionary\n,\nField Dictionary\n,\nRules\n, and the\nBusiness Glossary\n.\nAsset Statuses\nApplicable to: Assets, Tables, Columns, and Rules.\nNew\nThe\nNew\nstatus indicates that an asset has been scanned and detected for the very first time. It is assigned automatically during the initial scan when the asset enters the system. No review or action is required from data stewards or owners at this stage, and the asset is visible to all users.\nPurpose:\nTo clearly identify assets that are newly introduced into the system and have not yet been compared against any previous scan.\nDefault\nThe\nDefault\nstatus indicates that an asset has remained unchanged since the previous scan.\nFrom the second scan onward, if the scanner finds the same asset with no metadata differences, the status automatically becomes\nDefault\nwithout steward or owner intervention, and visibility remains open to all users.\nPurpose:\nTo show that the asset has been validated by at least one re-scan and is in a consistent, unchanged state compared to its previously scanned version.\nNote:\nIf an asset disappears or its schema changes during the second scan, it moves directly from\nNew â†’ Deleted\n, not Default.\nModified\nThe\nModified\nstatus indicates that the assetâ€™s metadata has been updated by the user compared to the previous version, such as when descriptions, domains, or business metadata are updated. The status will change from\nNew/Default\nto\nModified\nwhen the user manually updates any metadata and saves the asset without publishing it.\nThese assets remain visible to all users but indicate that internal updates have been made and are pending review or publication.\nPublished\nThe\nPublished\nstatus means that the asset has been fully reviewed, validated, and approved. This status is set manually by a user after completing validation.\nWorkflow behavior:\nIf the asset belongs to a\nworkflow-enabled domain\n, the transition from\nModified â†’ Published\nrequires approval from the assigned data steward or data owner.\nIf the asset does\nnot\nbelong to a workflow-enabled domain, it can be published directly without approval.\nDeleted\nThe\nDeleted\nstatus Â is assigned automatically when the asset is no longer detected in the source system, such as when a table is dropped or a column is removed or when schema is changed in the source.\nNo steward or owner involvement is needed as this status will be automatically updated after the scan, and deleted items remain visible to all users.\nNote:\nUsers cannot manually change an active assetâ€™s status to Deleted. Deleted assets remain in the system for audit and historical reference.\nPlease refer to this article for more details on how to mark an asset as deleted:\nHow to delete asse\nts\nGlossary Statuses\nThese are the other statuses applicable specifically to Glossary Terms.\nNote:\nWhen creating Views for Glossary Terms, the\nDefault\noption in filters refers to the\nDraft\nstatus.\nDraft\nThe term is created but not yet reviewed or approved; it may still be undergoing edits.\nModified\nIndicates that the termâ€™s metadata has been updated from its previously published state and requires review before re-publishing.\nPublished\nThe term has been reviewed, approved, and validated.\nFor more details on approving and publishing the Glossary Trems, refer to the article:\nApproving Glossary Terms\n.\nNote:\nA status will appear in the filter only if it has been applied to at least one asset. For example, if the\nDraft\nstatus is not applied to any asset, it will not be shown in the filter when the user clicks on the funnel.",
    "scraped_at": "2026-02-02 15:33:28"
  },
  {
    "title": "Visualizing Agent Questions on Dashboard",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/43031984347924-Visualizing-Agent-Questions-on-Dashboard",
    "content": "In this Article:\nOverview\nCapturing API-based Properties & Parameters\nAutomatic Reflection on the Dashboard\nWidget Creation & Customization\nOverview\nThis feature introduces a dynamic integration between\nagent questions\nand the\ndashboard\n. When a user interacts with an AI agent through an API and provides\ncustom properties and parameters\n, those parameters are automatically exposed on the dashboard for further analysis. This enables users to visualize and monitor agent interactions using customizable widgets, filters, and charts all based on the dynamically generated fields.\nCapturing API-based Properties & Parameters\nWhen a user asks a question through the API (for example):\nâ€œAnalyze customer IDs 12345658 for FTTHâ€\nThe request may include additional\nproperties\nand\nparameters\nsuch as location, market code etc.\nAutomatic Reflection on the Dashboard\nAutomatic Reflection on the Dashboard\nThe captured parameters are dynamically displayed in the\nAgent Question Analytics\nof the dashboard.\nWhenever a user adds new properties or parameters through an API query, these are instantly reflected as additional columns in the dashboard dataset.\nFor example, after the query above, the following columns might appear automatically:\nCustomer ID, Location, Market Code, etc.\nNote:\nAnything that is sent as a\nkeyâ€“value pair under the â€œpropertiesâ€ attribute\nin the API request will automatically be captured and shown on the dashboard as a separate field.\nThis means every new key introduced in the API payload appears as its own column in the dataset â€” no manual configuration or schema update is needed.\nAll parameters or properties captured from the API automatically appear under the\nAdditional Attributes\nsection when creating or editing a widget.\nThis ensures that all newly introduced fields are readily available for selection and analysis.\nExample view:\nThe below image shows how these dynamically generated fields appear within the\nAgent Question Analytics\nwidget.\nExample 2:\nThis chart shows how many records exist for each market code.\nWidget Creation & Customization\nUsers can then navigate to the Dashboard and create a Widget of type\nâ€œAgent Question Analytics.â€\nIf youâ€™re not familiar with creating a widget, see the detailed guide here:\nDashboard and Widgets\nWithin this widget:\nUsers can apply slicers (filters) on any of the dynamic parameters such as location, market code, or service type.\nUsers can select chart types (e.g., bar, pie, line, table) to visualize data according to their needs.\nAll the dynamically created columns from the API (e.g., location, market code, tool ID) will be available for data slicing and analysis.\nThis allows flexible visualization of agent query data with no need for predefined schema updates.",
    "scraped_at": "2026-02-02 15:33:34"
  },
  {
    "title": "Enterprise Search",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/42822153397908-Enterprise-Search",
    "content": "In this Article:\nOverview\nSearching for Assets\nSearch Suggestions\nSearch by Asset Name\nSearch by Table and Column Name\nSearch by Unique Asset ID\nSearch by Custom Attribute Values\nUsing Filters\nFiltering Options\nFilters Available for All Assets\nAvailable Filters for Tables\nAvailable Filters for Columns\nAvailable Filters for Terms\nFilters for Rules\nSearch Results\nConsistent Result Display\nExample Search Scenarios\nSummary\nOverview\nThe\nEnterprise Search\nfeature provides a unified way to explore and discover assets across your organization including datasets, reports, tables, rules, schemas, and more. It enables users to locate information quickly without needing to navigate through multiple modules.\nSearching for Assets\nEnterprise Search supports flexible search capabilities to help you find exactly what you are looking for.\nSearch Suggestions\nAs you type a query, Enterprise Search provides real-time suggestions to help refine your search.\nIf the system detects alternate or closely related terms, it displays a\nâ€œDid you meanâ€¦â€\nline above the suggestion list.\nUsers can click one of the suggested phrases to update the search term or press\nGo\nto execute the current query.\nFuzzy Search\nTypos trigger â€œDid you meanâ€¦?â€ suggestions, while results are based on exact or prefix matches.\nExpanded Search\nEnable\nExpanded Search\nto include similar and partial matches for broader, more relevant results.\nFor example, if you search for\nâ€œClaim Bâ€\nwith this option checked, results will also include assets containing the word\nâ€œClaimâ€\nâ€”not just exact matches.\nThis helps surface related or similar assets that might otherwise be missed.\nExample Behavior\nThe â€œDid you meanâ€¦â€ line shows suggested alternative keywords.\nThe suggestions list shows recent or recommended searches and matching assets with basic metadata.\nClicking a suggestion or the â€œDid you meanâ€¦â€ phrase runs a new search with that term.\nSearch by Asset Name\nEnter the full or partial name of any dataset, table, report, or rule to view all related results.\nFor example, typing\nSales\nwill bring up all assets whose names contain that keyword.\nNote:\nExact matches to your entered keyword appear at the\ntop of the results\n, allowing quick access to the most relevant tables, columns, or reports.\nSearch by Table and Column Name\nYou can search directly using a table name or column name.\nIf you add a period (\n.\n) and the column name after the table name (for example,\nOrders.OrderDate\n), the search will return all matching assets containing that column.\nThis also supports\nfull-path search\nsuch as\nDB.Schema.Table.Column\n, allowing more precise targeting across complex data models.\nSearch by Unique Asset ID\nEnterprise Search also allows searching through a specific\nunique ID\nof an asset.\nThis ID can be found in the URL of the asset (for example, a Power BI report or dataset).\nEntering this ID in the search bar will return that particular asset directly.\nYou can also paste a\nPower BI report URL\nto locate that exact report in Enterprise Search.\nSearch by Custom Attribute Values\nAssets with custom attributes such as picklist values can be located by searching those values.\nFor example, if a picklist attribute contains values like\nSingle\nor\nDouble\n, typing those terms in the search bar will display all assets assigned with that particular value.\nIn addition to standard assets, Enterprise Search supports a wider range of technical and custom objects including\ndatabases, schemas, stored procedures, and custom assets\n, ensuring broader visibility across your organizationâ€™s data catalog.\nUsing Filters\nIn addition to free-text search, users can apply filters to refine search results based on different asset characteristics.\nFiltering Options\nFilters in the left panel enable refinement of search results across asset types such as tables, columns, rules, terms, and agents. Each asset type provides its own set of filter categories, allowing more targeted exploration of data assets.\nFilters available for All Assets\nData Owners -\nFilter by the assigned data owner responsible for the Table.\nData Steward\n- Filter by the assigned data steward responsible for the Table.\nData Domain\nâ€“ Filter by assigned domain.\nLast Updated On\n- Filter based on the last updated date.\nStatus\nâ€“ Filter based on Deleted, Default, New, Modified, and Â Published.\nTags\nâ€“ Filter by applied metadata tags.\nAvailable Filters for Tables\nYou can refine your search using the filters on the left panel:\nFormat\nâ€“ Filter by format of the data i.e. Files or Tables.\nLast Refreshed On\nâ€“ Filter based on the last scan or refresh date.\nRecord Count\nâ€“ Filter by data size or volume.\nSource\nâ€“ Filter by data source name.\nThe available filter list changes dynamically based on the selected asset type.\nNote:\nSelecting the Plus (+) icon beside a filter reveals available options for more precise asset selection, such as tables, columns, rules, or agents.\nSelecting Clear at the top of the filter section resets all applied filters.\nAvailable Filters (for Columns)\nYou can refine your search using the filters on the left panel:\nData Type\nâ€“ Filters by the columnâ€™s data type (e.g., String, Integer, Date).\nEmpty or Null\n- Filters based on the number of empty or null values. Users can set the minimum and maximum limits for empty or null values to retrieve the desired columns.\nEntity\nâ€“ Filters by the related entity or object.\nTable -\nFilters based on the table names to which the columns belong.\nUnique Count\nâ€“ Filters by the distinct value count of a column.\nAvailable Filters for Terms\nYou can refine your search using the filters on the left panel:\nSub-domain\nâ€“ Filters by the sub-domain under which the term is classified.\nType\nâ€“ Filters by term type (e.g., Business Term, Technical Term).\nFilters for Rules\nYou can refine your search using the filters on the left panel:\nAlert Status\nâ€“ Filter by the alert level or current notification state.\nLast Executed On\n- Filters by the latest execution date.\nRule Type\nâ€“ Filter by rule type such as Orphan Records, Missing Values, or Data Consistency.\nRun Status\nâ€“ Filter by the latest execution status (e.g., Valid, Invalid, Failed).\nSearch Results\nSelecting a column from the search results opens a detailed panel on the right side, displaying key metadata and attributes\nSelecting the arrow icon in the detail panel redirects to the Dictionary tabâ€”specifically to the Database Column pageâ€”where the column can be viewed in full context along with related governance information.\nConsistent Result Display\nThe presentation of search results has been standardized to ensure consistent metadata visibility and layout across all asset types.\nThis makes it easier to interpret and compare results within the Enterprise Search interface.\nExample Search Scenarios\nSearching\nOrders\nreturns all tables and reports containing that table name.\nSearching\nOrders.OrderDate\nlists all assets containing that column.\nEntering a unique\nPower BI asset ID\n(from its URL) displays that specific report.\nSearching\nLahore\nreturns all assets with that picklist value assigned in their custom attributes.\nPasting a\nPower BI report URL\nshows the matching report directly.\nApplying filters for\nAsset Type = â€œData Quality Ruleâ€\nand\nStatus = â€œPublishedâ€\ndisplays only published data quality rules.\nSummary\nEnterprise Search centralizes discovery across all connected data assets.\nIt helps you find, filter, and access the information you need faster and more efficiently.\nWhether you are searching by name, column, ID, or URL, or refining results through filters and tags, Enterprise Search ensures that all relevant assets are easily within reach.\nNote:\nAI Agents are not included as part of Enterprise Search.",
    "scraped_at": "2026-02-02 15:33:40"
  },
  {
    "title": "Data Domains & Data Sub-Domains",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41940755917204-Data-Domains-Data-Sub-Domains",
    "content": "In this Article\nOverview\nAccessing Data Domains\nAdding a New Data Domain\nMass Update Data Domain for Tables and Columns\nMass Update Data Domain for DQ Rules\nData Sub-Domain\nAccessing Data Sub-Domains\nAdding a New Data Sub-Domain\nMass Update Data Sub-Domain\nOverview\nData Domains help organize and manage data assets by grouping them into logical categories based on business or functional areas. Defining Domains and Sub-Domains makes it easier to manage ownership, improve governance, and maintain better visibility across all data.\nAccessing Data Domains\nNavigate to\nAdministration > Organization\n.\nClick on\nData Domains\n.\nThe\nData Domains\npage displays a list of all existing domains configured in the system.\nAdding a New Data Domain\nTo create a new domain:\nClick the\nAdd\nbutton.\n2. In the\nAdd Domain\nform, provide:\nDomain Name\nDescription\nData Steward\n3. Select\nEnable Metadata Workflow\nto activate metadata management options.\nMetadata Workflow: Enabled\nThe 'Workflow' dropdown under the 'Enable Metadata Workflow' section provides two available options:\n1. Anyone can approve â€“ Any assigned approver can approve the metadata update.\n2. All need to approve â€“ Every assigned approver must approve before publishing updates.\nNote:\nFor complete details on governance roles and responsibilities, refer to the\nData Governance Roles and Responsibilities\narticle.\nSub-Domain Selection\nThe\nâ€œChoose Sub-Domainsâ€\nsection allows the user to assign sub-domains to the new Data Domain.\nClick the\nSave\nbutton to save the domain. Then the newly added domain will appear at the top of the listing. An icon will indicate that the domain has the workflow enabled, as shown below.\nSteward that is selected while creating the Domain will be the one who will be editing the term and making the changes. Then changes will be sent for approval to the approver(s). Approver's job will be to review the changes made by the Steward and approve or reject them.\nFor more information on the approving glossary terms, refer to the article\nApproving Business Glossary Terms\n.\nMass Update Data Domain for Tables and Columns\nThe\nMass Update\nfeature allows updating the Data Domain for multiple assets simultaneously across different modules, including:\nAsset Dictionary\nDatabase Tables\nDatabase Columns\nStep 1: Navigate to the Module\nGo to the module where the update needs to be applied:\nAsset Dictionary\nDatabase Tables\nDatabase Columns\nStep 2: Select the Records\nIn the asset list, select the checkboxes next to the items to be updated.\nFrom the toolbar at the top, click\nMass Update\n. A\nMass Update\ndialog box will appear.\nStep 3: Configure the Update\nMake the necessary changes in the dialog box.\nClick\nApply\nto confirm the update. A confirmation message will appear once the update is successful.\nThe updated Data Domain will now be visible under the\nData Domain\ncolumn for all selected records.\nNote:\nData Domains can also be assigned to multiple assets through\nrelationships\n, similar to how it is done for rules, as described in the next section.\nMass Update Data Domain for DQ Rules\nTo update the data domains for multiple rules at once,\nNavigate to\nDQ Rules\n, select the rules wanted to update.\nSelect the Relationship to add the data domains.\n3. Select the data domain that needs to added/updated.\n4. Click\nApply\n.\nNote:\nData Domains and Data Sub-Domains can be updated for multiple assets simultaneously using the\nImport\nfeature. For more information, refer to the article\nData Import\n.\nData Sub-Domain\nThe Data Sub-Domains section allows users to view, manage, and organize sub-domains associated with various data domains.\nSub-Domains are logical subdivisions within Data Domain, used to organize datasets at a more granular level. They enable structured classification, enhance metadata traceability, and support precise governance across the data catalog\nAccessing Data Sub-Domains:\nTo access the\nData Sub-Domains\npage:\nGo to\nAdministration\n>\nOrganization.\nClick on\nData Sub-Domains\n.\nAdding a New Data Sub-Domain\nTo create a new Data Sub-Domain:\nClick on the\nAdd\nbutton\nIn the form, define:\nClassification\nOwnership\nGovernance\nChoose the\nPriority\nlevel of importance from the dropdown options:\nâ€¢\nNormal\nâ€“ Default priority level.\nâ€¢\nHigh\nâ€“ Marks sub-domains requiring critical attention.\nâ€¢\nLow\nâ€“ Used for less significant domains.\nOnce a domain is selected, the sub-domain will inherit the workflow settings from the domain. This means the sub-domain will have the same Workflow Type, Stewards, and Approvers as the parent domain. Click Save to save the sub-domain.\nClick\nSave\n.\nNote:\nIf the workflow is enabled in Domain then its associated sub-domain will inherit all the domain's workflow settings. And if the workflow is disabled in Domain, then workflow can be enabled separately for the associated sub-domain.\nApproval & Access Behavior\nDomains can be assigned to user groups. When a domain is linked to a group, approval is considered complete as soon as\nany one\nmember of that group approves it, even though the approval request is issued to all members.\nNote:\nGroup-based approval is\nnot supported\nwhen working with Import Data sources. For file imports, only\nindividual users\ncan be assigned and must approve.\nMass Update Data Sub-Domain\nThe\nMass Update\nfeature allows users to update\nData Sub-Domain\nfor Glossary Terms assets modules:\nNote:\nSub-Domains can only be assigned to\nGlossary Terms\n. For\nTables\nand\nColumns\n, Sub-Domains cannot be added separately. In the case of Columns, Sub-Domains are automatically inherited when a Glossary Term is associated with the column.\nStep 1: Go to Glossary Term and select the records for which Data Sub-domain needs update.\nStep 2: A\nMass Update\ndialog box will appear.\nStep 3: Configure the Update.\nStep 4: Click\nApply\nto confirm the update. A confirmation message will appear once the update is successful.\nStep 5: The updated\nData Domain\nwill now appear under the\nData Domain\ncolumn for all selected records.\nFor more information on the approving glossary terms, refer to the article\nApproving Glossary Terms\n.\nNote: Users can edit the default Data Domain and Sub-Domain; however, they cannot delete them as these are system defaults.",
    "scraped_at": "2026-02-02 15:33:45"
  },
  {
    "title": "How to Apply Filters in Governance Views",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/40953420948884-How-to-Apply-Filters-in-Governance-Views",
    "content": "In this Article:\nEquals\nDoes Not Equal\nIncludes\nExcludes\nIs Empty\nNot Empty\nWildcard Match\nNote\nDvSum enables users to apply different filter conditions while creating or customizing views in the Data Dictionary, Field Dictionary, Analytics Dictionary, and Business Glossary.\nFor detailed steps on creating Governance Views,Â refer to the\nHow to Create Governance Views article\n. Once a view is created, filters can be applied using the specify criteria as described below.\nEquals\nWhat it does:\nReturns rows where the column value exactly matches the specified value.\nExample:\nSource Name = Databricks â†’ only rows where Source Name is \"Adventure Works.\"\nDoes Not Equal\nWhat it does:\nReturns rows where the column value is not equal to the specified value.\nExample:\nSource Name â‰  Databricks â†’ excludes rows where Source Name is \"Adventure Works.\"\nIncludes\nWhen using the\nIncludes\nfilter under\nSpecify Criteria\n, the behavior varies depending on the selected field:\nFor Table Name:\nWhen â€œTable Nameâ€ is selected, users need to\ntype a keyword\nto search for available values from the database.\nA message appears below the field saying:\nâ€œType to search for values from database. You can select up to 100 values only.â€\nThis means users must start typing to fetch matching table names, and they can\nselect a maximum of 100 values\n.\nFor Other Fields:\nFor other filter fields (e.g., Columns, Tags, Source, etc.), the list of available values will automatically appear\nwhen you click on the text box\n, without the need to type a keyword.\nExcludes\nWhat it does:\nReturns rows where the column does not contain the specified value.\nExample:\nStatus excludes \"Published\" â†’ omits rows where the Status field contains \"Published.\"\nIs Empty\nWhat it does:\nReturns rows where the column has no value (blank/null).\nExample:\nGlossary Term is empty â†’ fetches rows with no glossary term assigned.\nNot Empty\nWhat it does:\nReturns rows where the column contains any value (not blank/null).\nExample:\nGlossary Term not empty â†’ fetches rows that have a glossary term assigned.\nWildcard Match\nWhat it does:\nAllows partial or pattern-based matching using wildcards (* or % depending on syntax).\nExample:\nTable Name wildcard match \"order\" â†’ matches names like \"order,\" \"order history,\" or \"vw order.\"\nIf * or % is used in the filter with *\n\"Add\n\"**, it will fetch all rows that start with\n\"Add.\"\nNote:\nUsers can add multiple criteria within a single view.",
    "scraped_at": "2026-02-02 15:33:51"
  },
  {
    "title": "Exception Storage Integration for Rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/39857375831956-Exception-Storage-Integration-for-Rules",
    "content": "In this Article\nOverview\n1. Creating an Integration\n1.1 Access Integration Settings\n1.2 Configure the Integration\n2. DDL Script Generation & Execution\n3. Test Connection and Validation Steps\n3.1 Schema Match Failures\n3.2 Write-Back Access Errors\n4. Log All Rule Executions (Optional)\n4.1 Configuration Option: \"Log All Rule Executions\"\n4.2 What Happens When Enabled?\n5. Enabling Exception Storage on Rules\n5.1 Navigate to Rule Configuration\n5.2 Enable Exception Storage in the Workflow Section\n6. Running Rules with Exception Storage\n6.1 Run Rule â†’ Online Execution\n6.2 Run Rule â†’ Offline Execution\n6.3 Gateway Compatibility Scenarios\n7. Unsupported Integration Types\n7.1 Unsupported Databases\n7.2 Unsupported Rule Types\nOverview\nAfter a DQ rule is run, the exception records are stored in encrypted files local to the DvSum gateway when an on-premises gateway is used. The records are stored in an encrypted S3 bucket by default when the cloud gateway is used. In both cases the records are visible to users within the DvSum GUI and via API.\nException Storage Integration allows exception records to be stored in external database tables. This enables easier traceability, auditability, and integration with reporting systems or downstream analytics.\n1. Creating an Integration\n1.1 Access Integration Settings\nNavigate to:\nAdministration â†’ Account â†’ Integrations (Tab)\n1.2 Configure the Integration\nSelect the Data Source Type\nChoose a supported connection (currently limited to databases).\nNote:\nThe Integrations tab is designed to support various types of external connections. As of now, it only supports\nDatabases\n.\nSpecify the Target Table Name\nThis is where rule execution results will be stored.\n2. DDL Script Generation & Execution\nOnce the source is configured, the system auto-generates a\nDDL script\ncontaining predefined columns (e.g., execution ID, rule ID, rule name, rule type, rule description, etc.). These fields are required for writing execution results to the target table.\nDynamic Table Name:\nIf you change the target table name, the table name in the DDL script will auto-update.\nTest Connection:\nIf the user clicks on\nTest Connection\nbefore executing the script in the database, the table will not exist or may not be accessible. As a result, schema validation will be skipped, and the write-back operation will not be performed.\nCopy and Execute the DDL Script\nAfter the DDL is generated,\ncopy it\nusing the\nâ€œCopy Codeâ€\nbutton or from the dialog window.\nExecute the script in the\nsame target database\nselected during integration setup. This creates the required table structure with predefined columns (e.g., execution ID, rule ID, rule name, rule type, etc.)\n3. Test Connection and Validation Steps\nAfter executing the DDL script in the selected database, return to the Integration setup and click\nTest Connection\n. The system performs the following three validation checks:\nTable Existence:\nThe table must exist in the selected database.\nSchema Match:\nThe table structure must match the expected schema (all required columns must be present).\nWrite-Back Access:\nThe system must be able to insert and delete a test record to verify permissions.\n3.1 Schema Match Failures\nIf the table already exists but wasnâ€™t created using the system-generated DDL script, it may lack one or more required columns.\nThe\nTest Connection\nmay fail during the schema validation step.\nClick\nâ€œShow Error Detailsâ€\nto view the list of missing or mismatched columns.\n3.2 Write-Back Access Errors\nScenario 1: Missing Insert Permission\nEven if the table structure is correct, the system will show:\nâ€œWrite-back access is not allowed â€“ insert permission test failedâ€\nif the database user does not have insert rights.\nDuring validation, the platform attempts to insert dummy JSON data into the external table and immediately deletes it to confirm access.\nNote:\nThis happens because, during Test Connection, the system tries to insert dummy JSON data into the external table.\nAfter the insertion, it deletes the dummy data. This helps validate whether write-back access is allowed.\nIn this case, the insert test fails due to missing permissions.\nScenario 2: Missing Delete Permission\nIf insert permissions are present but delete permissions are missing, you can see:\nâ€œWrite-back access is not allowed â€“ delete permission test failedâ€\n4. Log All Rule Executions (Optional)\nThis option determines whether you want to store execution results\nonly when exceptions are found\n, or\nfor every run,\neven when no exceptions are triggered.\n4.1 Configuration Option: \"Log All Rule Executions\"\nDuring integration setup, you can see a checkbox labelled\nâ€œLog all rule executionsâ€\n.\nEnabled:\nEvery rule execution is logged in the storage table, regardless of whether exceptions were found.\nDisabled (default):\nOnly executions that generate exceptions are logged.\n4.2 What Happens When Enabled?\nOn enabling this setting and clicking\nSave\n, all future executions of rules linked to this integration will be logged.\nEach run will insert a record in the target table, even when the result is\nhealthy\n(i.e., no exceptions).\nNow store exceptions externally in the specified table.\nBest practice: Enable this option if organization requires\nfull execution history\nfor reporting or compliance purposes.\n5. Enabling Exception Storage on Rules\nTo store exception data from rule executions into the configured external storage, Exception Storage must be explicitly enabled at the\nrule level\n.\n5.1 Navigate to Rule Configuration\nGo to the\nDictionaries\ndropdown\n,\nClick on\nDatabase Tables\n.\nSelect the desired table.\nOpen the\nData Quality\ntab and choose the rule you want to configure.\nClick\nEdit\non the rule.\n5.2 Enable Exception Storage in the Workflow Section\nIn the rule edit screen, scroll to the\nWorkflow\nsection.\nLocate the\nException Configuration\ntab.\nEnable the\nException Storage\ntoggle.\nFrom the dropdown, select one or more configured integrations where exceptions should be stored.\nClick\nSave\n.\nNote:\nUser can configure\nmultiple integrations\nfor a single rule if needed.\nClicking on an\nintegration\nname will redirect to the full\nIntegration configuration page\n, allowing to review or modify its setup.\n6. Running Rules with Exception Storage\nOnce Exception Storage is configured and enabled on a rule, exceptions can be logged to the external table during both\nRun Online\nand\nRun Offline\nexecutions. The mechanism of storing differs slightly based on how the rule is run.\n6.1 Run Rule â†’ Online Execution\nWhen a rule is run manually via\nRun Online\n, a\nConfiguration Preferences\ndialog appears with the following options:\nInclude All Exceptions\nâ€“ Check this option to include all exceptions during the rule execution.\nAdd Exceptions to Storage\nâ€“ Check this box if you want the exceptions to be stored in the external tables when the rule is run.\nAfter execution:\nA\nsuccess message\nappears confirming that exceptions were loaded into the configured table.\nThe same message also appears in the ruleâ€™s\nActivity section\nas a system comment.\nThe user can confirm this by reviewing the corresponding table in their external database.\nNote:\nTo convert the raw exception data into a valid JSON format, users can run the following SQL query on Microsoft SQL server.\nUSE [your_database_name];\n\nSELECT\n execution_id,\n exception_data AS RawExceptionData,\n TRY_CAST(exception_data AS NVARCHAR(MAX)) AS ExceptionData_Text,\n JSON_QUERY(TRY_CAST(exception_data AS NVARCHAR(MAX))) AS FormattedJson\nFROM\n [your_schema_name].[your_table_name]\nWHERE\n execution_id = [your_execution_id];\n6.2 Run Rule â†’ Offline Execution\nWhen a rule runs via\nRun Offline\n(through workflows or schedules), exception storage happens automatically based on the rule's configuration; no manual input is required.\nNotification Email Outcome:\nAfter execution, a notification email is sent to the rule owner and relevant users. The\nWorkflow Action\nsection shows one of the following:\nIf exceptions were stored successfully:\nâ€œExceptions have been loaded to storage location.â€\nIf no integration is configured on the rule:\nâ€œNot Configured.â€\n6.3 Gateway Compatibility Scenarios\nWhen the rule and the integration are hosted on different gateway types (On-Prem vs. Cloud), the system behavior varies:\nRule Location\nIntegration Location\nSystem Behavior\nOn-Prem\nCloud\nWarning:\n\"Selected integration uses a different gateway. This integration requires exception data to exit your network temporarily before being stored.\"\nCloud\nOn-Prem\nInfo:\n\"Selected integration uses a different gateway type.\" (Not blocking)\nNote: These messages help clarify when exception data might leave your internal network, allowing teams to address any compliance or security implication\n7. Unsupported Integration Types\n7.1 Unsupported Databases\nThe following integration targets are\nnot supported\n:\nAzure Data Lake Storage (ADLS)\nPower BI\nMongoDB\nAmazon S3\n(Rules can read from S3, but cannot write exceptions to S3 as external storage)\n7.2 Unsupported Rule Types\nAs of now, the following rule types are\nnot supported\nfor Exception Storage:\nFreshness Rules\nMetric Rules\nCount Rules\nUnique Value Rules\nThese rules are not yet compatible with Exception Storage because they do not produce row-level exceptions that can be stored in external systems.\nAll other rule types that generate exception samples are supported.",
    "scraped_at": "2026-02-02 15:33:57"
  },
  {
    "title": "Rule Detail page",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/26973748320532-Rule-Detail-page",
    "content": "In this Article:\nOverview\nInitial State of a Newly Created Rule\nKey Functionalities Overview\nScheduling Status\nAlert Status\nHealthy\nAlerting\nRun Result\nPassed\nFailed\nRun Options (Online & Offline)\nEdit Functionality\nOverview\nScope\nThreshold Configuration\nWorkflow\nWorkflow Actions\nTabs on Rule Detail Page\nData Tab\nHistory Tab\nChart View\nGrid View\nInstructions Tab\nColumn Sequence Tab\nOverview:\nMany features are added on the Rule detail page like offline and online execution capabilities, providing users with greater flexibility. The alert status of each rule is displayed as either \"Healthy\" or \"Alerting\" based on specified thresholds. Users can now easily edit rule definitions, descriptions, priorities, and scope, including window types and lookback days. Data aggregation into buckets enables detailed data quality checks. Various threshold types allow tracking and alerting based on exceptions. Rule notifications, action statuses, and scheduling options have been enhanced for improved rule management. The history tab provides execution charts and grids for comprehensive performance analysis. Overall, all these features enhance rule customization, monitoring, and analysis which will be explained in this article.\nInitial State of a Newly Created Rule:\nBefore moving to functionalities let's go through when any rule is created. The \"Data\" tab and \"History\"\ntab is disabled initially when the rule is created.\nHere on the \"Run Rule\", the user can either run the rule online or offline. For more information regarding\nRun Online & Run Offline, please visit the article \"\nRun Online/Run Offline\n\". Once the Rule is run, the \"Data\"\ntab (if there are exceptions) & History tab gets enabled:\nNow we will go through the major functionalities that have been implemented in the User Interface are\nhighlighted in the screenshot below for a quick review.\nKey Functionalities:\nThe Rule Detail page has a significant number of features Let's explore these enhancements in detail:\nNot Scheduled\nA notification at the top of the page is shown which indicates whether the rule is scheduled. If the rule is not scheduled it will not run automatically.\nAlert status\nAlert status for the rule will be shown as\nHealthy\n- if the metric value is within specified threshold limits, then the alert status will be Healthy\nAlerting\n- If the metric value is not within specified threshold limits, then the status will be Alerting\nRun Result\nPassed\n-\nIf the metric value has zero exceptions then the run result will be passed and the alert status will be healthy.\nFailed\n-\nIf the metric value is not within the defined threshold then the run result will fail and the alert status will be alerting applicable to Unique values, Freshness, Metric, and Count rule.\nRun:\nOnline & Offline rules can be run directly from the Rule detail page\nFor Details please check the article\nRun Online/Offline\nEdit Functionality:\nThis button will allow the user to update the following:\nOverview\nRule Description\n: Users can update the description for the rule\nPriority\n: Users can update the priority for the rule\nOpen Rule Definition:\nUsers can directly open the rule definition page from here\nAttributes\nUsers can add any tags to the rule\nScope\nMetric Time\nIf the metric time field is selected on the table level. Selected field names will be inherited from the rule as well\nOtherwise, the user can also select/update the metric time field at the rule level as well\nUsers can set the window type: - Used to define what is the scope of data to be selected.\nAll Data - By Default all the rules will window type as All data, which will consider all the data in the table during theÂ  execution of the rule\nData Max Time - In the case of incremental data, we can choose this option to run validation only on newly added data based on the timestamp available on the table\nClock Time -Â  In the case of incremental data, we can choose this option to run validation only on newly added data based on the current timestamp available on the table\nExample:\nThere is a rule scheduled to run on 2024-01-06 at 11:00:00. This rule is applied to the \"UPDATE_DT\" column. Please review the sample data below, which produces different outputs based on the selected window type:\nUsers can set the\nLookback days\nfor Data max time & Clock time\nUsers can optionally aggregate data into\nBuckets\nand DQ checks will be performed on each bucket.\nNo bucket\n1 Day\n1 Hour\nUsers can select available\nSlicer\noptions\nThreshold\nFor the Metric type, we can choose whether we want to track and alert based on the number of exceptions or percentage of exceptions.\nNo Threshold\n- Metric will not alert.\nConstant\n- Metric will be compared against constant thresholds.\nUsers can set the Upper bound and Lower Bound\nRelative\n- Percentage change (increase or decrease) in metric compared to the previous bucket or execution. For DQ exception checks, the decrease will not alert.\nUsers can set the percentage for the relative threshold type\nAdaptive\n- Thresholds auto-adjust based on observations using outlier detection techniques. It uses the Interquartile range technique to detect if the metric is an outlier.\nUsers can choose the threshold bounds from the 3 available options:\nUpper and Lower\nUpper\nLower\nWorkflow\nAssign a Rule to a User\nAssign the rule to a specific user for ownership and accountability.\nAdd/Update the Schedule for a Rule\nSelect or create a scheduled job to automate rule execution.\nException Configuration\n(New)\nAttach Exception File\n: Includes exception details in the notification email.\nEnable Exception Tracking\n: Allows tracking of new and existing exceptions across runs.\nFor a detailed explanation of Exception Configuration and how it impacts the Data tab, refer to the article:\nConfiguring Exception Settings for Rules in DvSum.\nWork Flow Actions:\nBefore moving to Workflow Actions, we need to add a Data Quality workflow enabledÂ  Data domain to our table. On the \"Data Domain\" tab create a new domain or use an already existing one:\nThe \"Data Quality Workflow\" checkbox should be enabled on creating the new Data Domain:\nNow on the \"Overview\" tab on the Table detail page, the above Data domain needs to be added:\nOnce we have added the Data Quality workflow Data Domain to our table we can then add any scheduled job from the drop-down\nWorkflow actions are going to appear at the top right\nActions dropdown will contain In Progress and Resolve\nWhen the user marks the rule as In Progress, the status will be changed to \"In Progress\".\nWhen the user resolves the rule, a Pop-up will be displayed with the Reason Code and Description\nWorkflow status is changed to resolved\nOne thing to note here is that the \"Actions\" button and its options on the top right show if the Rule has met the following conditions:\nThe rule must have exceptions\nThe rule must be executed\nThe Table on which the Rule is created must have the Data domain added for which the Data Quality workflow is enabled\nThe scheduler must be attached to the Rule\nJust so you know, workflow actions will also show when ticketing is enabled on the rule. For more details on Ticketing, please take a look at the article\nJIRA Integration on DQ Rules.\nThe workflow status is Activity bar. Clicking on the comments icon opens the detailed activities.\nOn the Activity bar, clicking on the comments icon will open the detailed activities.\nTabs on Rule Detail Page:\nOn the Lower Section of the Rule detail page, the user will be able to see only the 3 tabs by default:\nUsers can click on\nShow More\nto view the other tabs available:\nAfter clicking on the\nShow More\ntab, the user will see the following tabs and if the user wants to hide them they simply need to click on\nHide Others\nData Tab:\nData View\nThe \"Data\" tab is available after the rule is executed, regardless of whether exceptions are generated. By default, exceptions are highlighted in\nred\nfor the column(s) where the rule is applied. All exceptions, along with the corresponding data in the table, are displayed.\nUsers can filter the exceptions using the drop-down menu with the following options:\nCurrent:\nDisplays exceptions from the most recent execution.\n30 Days:\nShows exceptions from the past 30 days.\n90 Days:\nShows exceptions from the past 90 days.\nAll:\nDisplays the metric value for all available data, up to a maximum of the last 120 days (selectable via the drop-down).\nNote: Only the first 300 exceptions are displayed in the Grid on the Data tab. if needs to view more exceptions, must enable the â€œInclude all Exceptionsâ€ checkbox in the Run Online dropdown before running the rule.\nOne thing is to be noted here that the \"Data\" tab will be shown for the following rules only:\nData Format\nBlanks\nValue Range\nUniqueness\nRuleset\nCustom Query\nOrphan Keys\nOrphan Records\nCompare Schema\nCompare Metric\nNote:\nThe Data tab supports column-level filtering and allows copying specific rows or column values directly from the exception grid for easier analysis.\nPivot View\n.\nThe\nData\ntab provides a detailed, interactive view of rule execution results, with a focus on exception data through the\nPivot View\n.\nThe Pivot View in Data Quality Rules enables users to analyze exceptions by grouping and aggregating records directly within the application, helping identify patterns and derive insights.\nFor more information, refer to the sub-article\nâ€œ\nPivot View Configuration in Data Quality Rules.\nâ€\nHistory:\nThe history tab will show the execution history for the rule in the form of a chart\nThe History tab has 2 different views which are:\nChart View\n-\nIn this view, the data is shown to the user in the form of a chart\nWhen the slicer is selected, the data in the History Chart will get distributed based on the Slicer's Field\nGrid View -\nIn this view, the same data is shown to the user in a tabular form\nUsers can view execution history for:\nCurrent\n: It will show the metric value for the current execution\n30 Days\n: It will show metric value for 30 days\n90 Days:\nIt will show the metric value for 90 days\nAll:\nDisplays the metric value for all available data, up to a maximum of the last 120 days.\nInstructions:\nUsers can add any instructions.\nColumn Sequence:\nOn the \"Column Sequence\" tab, the users can set 3 different sequences that can be applied to the Rule:\nSuggested Sequence\nTable Specific Sequence\nRule Specific Sequence\nFor more details on Column Sequence, there is a separate article written for \"\nColumn Sequence\n\".",
    "scraped_at": "2026-02-02 15:34:02"
  },
  {
    "title": "Adaptive Thresholds",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18139200580116-Adaptive-Thresholds",
    "content": "In this Article:\nWhen to use Adaptive Thresholds\nHow Adaptive Thresholds are Calculated\nExample\nWhy IQR * 1.5?\nHow to set Adaptive Thresholds\nWhen to use Adaptive Thresholds\nClassic data quality (DQ) rules define a test that must be met for data to be considered valid. This could be a simple not-null test or a cross-reference integrity check or a more complex calculation. Typically the acceptable maximum threshold for this test would be zero. Invalid values are not acceptable.\nBut it turns out there are many real-world reasons to allow more flexible thresholds. One reason is that data is known to have a baseline number of invalid records. This is not\ndesired\n, but it's expected. And in practice the team monitoring data quality doesn't want to receive a notification every day that there are invalid records--since that might well be every single day. But they would certainly be interested to get a notification if the number of invalid records spikes one day.\nAn even more common use case for wanting adaptive thresholds is for\ndata observability\ntests. In many situations the number of records arriving each day should be roughly the same. A small variation is expected, but a large variation could indicate a problem. But how can we quantify \"small variation\" and \"large variation\"? This is where adaptive thresholds can help.\nHow Adaptive Thresholds are Calculated\nAdaptive thresholds in DvSum DQ are set using the common statistical technique called Interquartile Range (IQR). In this method, we first calculate the 25th percentile and 75th percentile for past values. These values are referred to as the 1st quartile (Q1) and 3rd quartile (Q3). The IQR is simply the difference between Q3 and Q1. We define outliers as values which are more than 1.5 IQRs below Q1 or more than 1.5 IQRs above Q3.\nExample\nAn example of the adaptive thresholds in action makes them clear. Consider the following values for the number of records loaded daily by a data pipeline.\nYou can also toggle to display the values in a grid. Here are those values:\n1009, 990, 840, 825, 1115, 586, 899, 1029, 773, 1151, 1394, 1327, 870, 1128, 1089, 1097, 982, 741, 1105, 992, 737, 1240, 1014.\nThe first 4 data points have no thresholds displayed. This is because the IQR threshold calculation requires a minimum of 4 previous data points to be meaningful. The thresholds for the 5th data point are calculated as follows.\nThe 4 previous data points: 1009, 990, 840, 825\nQ1: the median of 825 and 840 = 832.5\nQ3: the median of 990 and 1009 = 999.5\nIQR: Q3 - Q1 = 167\nIQR * 1.5 = 250.5\nUpper Threshold: 999.5 + 250.5 = 1250\nLower Threshold: 832.5 - 250.5 = 582\nThe same logic applies in identifying the data on Aug 17 as an outlier. The previous data points (1009, 990, 840, 825, 1115, 586, 899, 1029, 773, 1151) yield a Q1 value of 825 and a Q3 value of 1029. Those correspond to threshold values of 519 and 1,335.\nWhy IQR * 1.5?\nThe intuitive answer is that if the data is truly following a normal distribution and we're only seeing random noise in the results, then setting thresholds like this are expected to generate alerts only very rarely. The data quality team won't get too many notifications. But how many?\nThe complete details of the mathematical calculation are beyond the scope of this article. But setting the bounds at Q1 - 1.5*IQR and Q3 + 1.5*IQR will correspond to setting the bounds at\nÎ¼ Â± 2.7Ïƒ. With these bounds slightly fewer than 1% of measurements from a normal distribution will generate alerts.\nStated in a less-precise but more business-oriented manner, if you get an alert then it's\nquite likely\nthat something went wrong. Maybe a file got loaded twice, or the underlying data has truly changed. You should investigate. But of course it's\npossible\n(about a 0.7% chance) that this is just a random fluctuation in the data.\nHow to set Adaptive Thresholds\nPrerequisite: you have already created a DQ rule. Pipeline checks like COUNT and METRIC are the most common types to use adaptive thresholds. For more details about how to define DQ rules, refer to the\nDQ Rule Details\npage.\nEdit the rule, then navigate to the tab \"Thresholds\".\nThresholds for DQ rules are set in this dialog in DvSum Data Quality:",
    "scraped_at": "2026-02-02 15:34:07"
  },
  {
    "title": "Clone Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/27010646948884-Clone-Rule",
    "content": "In this Article\nCloning From DQ Dictionary Page\nMultiple Rules Cloning From Different Sources\nMultiple Rules With the Same Source\nCloning From Rule Detail Page\nUsers can now easily duplicate existing rules with minor adjustments and assign them to other users. This eliminates the need for manual rule creation, saving time and allowing users to create rules quickly using the new clone functionality. For more information on this feature, follow the steps outlined below.\nCloning From DQ Dictionary Page\nSingle Rule Cloning\nCloning From DQ Dictionary Page\nSteps 1:\nNavigate to the \"DQ Dictionary\" page and choose a specific rule. Click on \"More Actions\" at the top, then select \"Clone Rule\" from the drop-down menu, as illustrated below:\nStep 2:\nUpon selecting \"Clone Rule,\" the user will be taken to the \"Clone Rule\" modal. At the top, there will be a prefix. The user will be required to mention the target data source on which the rule is to be cloned:\nStep 3:\nAfter cloning the rule, the user will be taken to the Clone Summary detail page. In the grid below, you will find the Original Rule ID, the cloned Rule ID (which remains blank in case of failed cloning), the default source of the rule, the source on which the rule has been cloned, the status indicating whether the cloning was successful or failed, any remarks in case of unsuccessful cloning, and finally, a description. In the top right corner, there is a \"Download Summary\" button to download a report of this screen, as illustrated below:\nMultiple Rules Cloning From Different Sources\nMultiple Rules Cloning From Different Sources\nStep 4:\nIf the user selects multiple rules belonging to different sources, the same step 1 will be executed. Upon redirecting to the \"Clone Rule\" modal, you will observe the prefix, and below that, a target sources section. In the case of multiple rule sources, there will be both \"From Source\" and \"To Source\" options.\nStep 5:\nOn the Clone Summary detail page, you will be able to see the number of rules that encountered errors and those that were successfully cloned, as illustrated below:\nMultiple Rules With the Same Source\nMultiple Rules With the Same Source\nStep 6:\nChoose rules that belong to the same source. Opt for the \"Clone Rule\" option, as demonstrated below:\nYou'll encounter a familiar screen as before, but this time, as the rules belong to the same source, there's only one target source displayed. Users can modify the target source if necessary. Simply click the \"Clone\" button, as indicated below:\nUpon successful cloning, you will observe that the two rules have been duplicated, and their details will be displayed as shown below:\nNote:\nA rule that has been cloned can be treated independently from the original (parent) rule on which the cloning was performed. This implies that any configurations made in the cloned rule will not affect the parent rule.\nCloning From Rule Detail Page\nCloning From Rule Detail Page\nStep 7:\nChoose any rule and navigate to its detail page and click on \"3 dots\" where you will find a \"Clone Rule\" button on the top right side, as illustrated below:\nStep 8:\nAfter clicking the \"Clone\" button, you will be directed to the \"Clone Rule\" modal. Here Prefix of the Cloned Rule is mentioned and the user will have to mention the target data source on which the rule will be cloned.\nS\ntep 9:\nAfter cloning the rule, the user will be redirected to the Clone Summary detail page where the user can see whether the rule is successfully cloned or not. Under the heading \"Clone Rule\" there will be the link of the Cloned rule\nRules can also be cloned from the \"Data Quality\" tab on the Rule detail page:",
    "scraped_at": "2026-02-02 15:34:12"
  },
  {
    "title": "Column Sequencing",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/26980095608596-Column-Sequencing",
    "content": "In this Article\nSetting Column Sequence\nSuggested Column Sequence\nTable Specific Column Sequence\nRule Specific Column Sequence\nRules Where Column Sequence is Available\nNote:\nThe Column Sequence tab will be shown for the following rules:\nUsers have 3 different options for Column Sequencing on the Rule detail page. In this article, we will go through how users can set Column sequences and what are the 3 different column sequences defined on the Rule detail page.\nSetting Column Sequence\nOn the Data Quality tab of the Table Detail page, the column sequence can be opened by clicking on the Gear settings icon\nOnce Column Sequence is opened then the user can change the sequence for any column of the table:\nNow on the Rule detail page, on the \"Column Sequence\" tab user can select one of the three options that are:\nSuggested\nTable Specific\nRule Specific\nSuggested Column Sequence\nBy default, there is a sequence of the table that is defined and that is the \"Suggested\" column sequence.\nAfter running the rule the suggested column sequence that was defined can be seen on the \"Data\" tab.\nTable Specific Column Sequence\nThe \"Table Specific\" option is disabled if the user does not change the Column sequence from the \"Data Quality\" tab on the Table detail page. Once the user changes the Column sequence from there then this option is enabled. The Column sequence which was defined the the Table level can be seen here.\nOnce the user saves this option and runs the rule then the sequence on the \"Data\" tab will be the same as what was defined on the \"Data Quality\" tab.\nRule Specific Column Sequence\nIf the user changes the \"Suggested\" column sequence or the \"Table specific\" column sequence then the new sequence will be called \"Rule Specific\" specific. Users can make any changes in the column sequencing and save the option.\nAfter running the rule, the user can see the same sequence on the \"Data\" tab that was defined in the \"Column Sequence\" tab:\nNote:\nThe Column Sequence tab will be shown for the following rules on the Rule detail page:\nData Format\nBlanks\nValue Range\nOrphan Records\nRuleset",
    "scraped_at": "2026-02-02 15:34:19"
  },
  {
    "title": "Cross-System DQ - Compare Schema Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25404616533524-Cross-System-DQ-Compare-Schema-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIntroduction\nThe \"Compare Schema\" rule involves a comprehensive examination of data from two distinct schemas to verify that they are consistent with each other.\nConsider there is a scenario in which consistency of two schemas is to be checked. The Compare Schema Rule will compare the schema structure between two sources, highlight differences in tables and columns, include disparities in data types where applicable.\nIn essence, the Compare Schema rule ensures that data across different sources aligns accurately, providing insights into potential discrepancies or variations.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Configure the Rule:\nStep 1: Access Data Dictionary\nLog in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.\nStep 2: Select Table & Available Rules\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3: Add a New Rule\nSelect the \"âŠ• Add Rule\" button, then choose the \"Cross-System DQ\" category. From the list of options, click on \"COMPARE Schema\"\nStep 4: Basic Input Setup\nIn the Rule Wizard's\nBasic Input\nsection, provide the\nSource Schema\nand\nTarget Schema\nfor the comparison. You can also add filters to refine the scope of comparison. Additionally, use the\nIgnore Prefixes and Suffixes\noption to remove common naming patterns (e.g., 'vw_' or '_temp') from table or column names, improving matching accuracy between the source and target schemas.\nStep 5: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:24"
  },
  {
    "title": "Cross-System DQ - Compare Metric Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25404439140372-Cross-System-DQ-Compare-Metric-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule (Cross-System DQ â†’ Compare Metric)\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIntroduction\nThe \"Compare Metric\" rule involves assessing aggregated quantities or metrics from two distinct data sources to verify their consistency and detect any differences or discrepancies.\nIt denotes the process of examining aggregated quantities or metrics from two separate data sources to ensure alignment and identify any discrepancies or differences.\nFor instance, one might compare the total shipment volume for the last three months recorded in the Enterprise Resource Planning (ERP) system with the aggregated shipment volume data stored in the data warehouse. This comparison aims to validate that both sources provide consistent information regarding shipment volumes and to pinpoint any discrepancies between them.\nIn summary, the Compare Metric rule helps ensure data accuracy and reliability by examining combined metrics from various sources.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nDetailed Steps:\nStep 1: Access Data Dictionary\nLog in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.\nStep 2: Select Table & Available Rules\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3: Add a New Rule (Cross-System DQ â†’ Compare Metric)\nSelect the \"âŠ• Add Rule\" button, then choose the \"Cross-System DQ\" category. From the list of options, click on \"COMPARE METRIC\"\nStep 4: Basic Input Setup\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Metric Type, Column Name and Reference source, reference table and column name corresponding to the table in which you want to compare for the metric.\nStep 5: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:29"
  },
  {
    "title": "Cross-System DQ - Orphan Keys Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25403824473364-Cross-System-DQ-Orphan-Keys-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule (Cross-System DQ â†’ Orphan Keys)\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIn this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule (Cross-System DQ â†’ Orphan Keys)\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIntroduction\nThe \"Orphan Keys\" rule identifies a situation where a group of records of a particular type exists in one system but is absent in another system where it should also be present, categorized by their shared attributes or types.\nFor instance, shipments designated for Customer X might be recorded in one system, but not included in another system's list of shipments organized by customer. Also, Product Category B could be listed among shipments, yet it doesn't appear in the list of products. This discrepancy points to a mismatch in data synchronization or categorization between the systems.\nIn summary, the Orphan Keys rule reveals differences between datasets in various systems, underlining the importance of organizing data consistently across different platforms.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nDetailed Steps:\nStep 1: Access Data Dictionary\nLog in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.\nStep 2: Select Table & Available Rules\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3: Add a New Rule (Cross-System DQ â†’ Orphan Keys)\nSelect the \"âŠ• Add Rule\" button, then choose the \"Cross-System DQ\" category. From the list of options, click on \"Orphan Keys\"\nStep 4: Basic Input Setup\n<></>\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Column name, Reference source, Reference table and reference column name corresponding to the table in which you want to search for the orphan records.\nStep 5: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:35"
  },
  {
    "title": "Cross-System DQ - Orphan Records Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25403704015636-Cross-System-DQ-Orphan-Records-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule (Cross-System DQ â†’ Orphan Records)\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIntroduction\nThe \"Orphan Record\" rule identifies instances where a data entry exists in the records of one system but is absent from another system where it should be present based on expected data consistency between the two systems.\nFor example, if \"Order A\" is recorded in the list of orders within the Enterprise Resource Planning (ERP) system but is missing from the Transportation System's list of planned orders, it indicates an orphan record situation. This discrepancy suggests that there may be inconsistencies between the two systems regarding the management or synchronization of order data.\nAddressing orphan records typically involves aligning data between systems, ensuring data integrity, and establishing mechanisms to prevent such discrepancies in the future.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nDetailed Steps:\nStep 1: Access Data Dictionary\nLog in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.\nStep 2: Select Table & Available Rules\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3: Add a New Rule (Cross-System DQ â†’ Orphan Records)\nSelect the \"âŠ• Add Rule\" button, then choose the \"Cross-System DQ\" category. From the list of options, click on \"ORPHAN RECORDS\"\nStep 4: Basic Input Setup\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description, column name, reference source , reference table and reference column name corresponding to the table in which you want to search for the orphan records.\nStep 5: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:39"
  },
  {
    "title": "Pipeline Checks - Metric Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25247261939604-Pipeline-Checks-Metric-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule (Pipeline Checks â†’ Metric)\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIntroduction\nThe \"Metric\" rule is a kind of Pipeline Check used to ensure data pipelines are working correctly. It focuses on verifying data quantities, checking data quality and performance, and keeping track of data timeliness in processing workflows. By doing so, it improves the flow, quality, and efficiency of data.\nIn simple terms, the Metric rule checks if the total amount of data falls within an expected range. For example, it might ensure that the sum of open order quantities is between 75 million and 100 million units. This rule is essential for maintaining accurate and efficient data processing.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Configure the Rule:\nStep 1: Access Data Dictionary\nLog in to DvSum and proceed to the Data Dictionary tab under the Dictionaries and select the relevant Data source and Table Name.\nStep 2: Select Table & Available Rules\nStep 3: Add a New Rule (Pipeline Checks â†’ Metric)\nSelect the \"âŠ• Add Rule\" button, then choose the \"Pipeline Checks\" category. From the list of options, click on \"Metric\"\nStep 4: Basic Input Setup\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Metric Column, and Metric Type corresponding to the table in which you want to search if the aggregated quantity is within the expected range.\nStep 5: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:45"
  },
  {
    "title": "Pipeline Checks - Freshness rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25246892783380-Pipeline-Checks-Freshness-rule",
    "content": "In this Article:\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Available Rules\nStep 3: Configure Settings Reference\nStep 4: Add a New Rule (Pipeline Checks â†’ Freshness)\nStep 5: Basic Input Setup\nStep 6: Validate & Run the Rule\nIntroduction\nThe \"Freshness\" rule is a type of Pipeline Check that ensures the timeliness and reliability of data within data processing workflows. It verifies data quantities, assesses data quality and performance, and monitors if data is present or updated within a specified time frame.\nIn simple terms, the Freshness Check monitors whether data is available or updated within a certain timeframe. For instance, in a weather forecasting app, if data was initially fetched at \"2023-09-30 08:00:00\" and the next execution timestamp is \"2023-09-30 14:30:00,\" resulting in a time difference of 6 hours, but the defined threshold is 3 hours, then the Freshness Check would alert because the difference exceeds the defined threshold.\nThis rule enhances the flow, quality, and efficiency of data processing by ensuring that data remains current and relevant, thus improving the reliability of the overall system.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Configure the Rule:\nStep 1: Access Data Dictionary\nLog in to DvSum, proceed to the Data Dictionary tab under the Dictionaries, and select the relevant Data source and Table Name.\nStep 2: Select Table & Available Rules\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3: Configure Settings Reference (Load Profile & Metric Time)\nOpen the Settings Reference page and enter the Load Profile & Metric Time\nNote: The Load Profile should be set to Incremental Data\nStep 4: Add a New Rule (Pipeline Checks â†’ Freshness)\nSelect the \"âŠ• Add Rule\" button, then choose the \"Pipeline Checks\" category. From the list of options, click on \"Freshness\"\nStep 5: Basic Input Setup\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description and Threshold Hours corresponding to the table in which you want to search that data is not present or updated by a certain time.\nStep 6: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:50"
  },
  {
    "title": "Pipeline Checks - Count Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25246624213396-Pipeline-Checks-Count-Rule",
    "content": "In this article:\nIntroduction\nHow to Configure the COUNT Rule\nStep 1: Log in to DvSum\nStep 2: Access the Data Quality Tab\nStep 3: Add a New Rule\nStep 4: Basic Input Configuration\nStep 5: Validate & Execute\nIntroduction\nThe\nCOUNT\nrule is a type of Pipeline Check designed to ensure the accuracy and efficiency of data pipelines. It verifies data quantities, assesses data quality and performance, and monitors data timeliness within data processing workflows.\nIn essence, the\nCOUNT\nrule checks whether the number of records falls within the expected range. For example, in an open order extract, the COUNT rule would verify if the count of sales orders is between\n800,000 and 1 million records\n.\nThis rule enhances data flow, quality, and efficiency by ensuring that the expected number of records is processed, maintaining consistency and reliability in data pipelines.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Configure the COUNT Rule\nStep 1: Log in to DvSum\nNavigate to the\nDictionaries\n>\nData Dictionary\ntab.\nSelect the relevant\nData Source\nand\nTable Name\n.\nStep 2: Access the Data Quality Tab\nSelect the\nTable Name\n.\nClick on the\nData Quality\ntab.\nChoose\nAvailable Rules\n.\nStep 3: Add a New Rule\nClick the\nâŠ• Add Rule\nbutton.\nSelect the\nPipeline Checks\ncategory.\nFrom the list of options, choose\nCOUNT\n.\nStep 4: Basic Input Configuration\nIn the Rule Wizardâ€™s\nBasic Input\nsection, fill in the following details:\nRule Description\n: Provide a meaningful name for the rule.\nThreshold Count\n: Define the expected range for the number of records.\nStep 5: Validate & Execute\nAfter saving the rule, review its definition.\nClick\nRun\nto execute and test the rule.",
    "scraped_at": "2026-02-02 15:34:55"
  },
  {
    "title": "Foundational DQ - Ruleset",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25246448647828-Foundational-DQ-Ruleset",
    "content": "In this Article:\nIntroduction\nWhy is it Important?\nHow to Configure the Ruleset Rule\nStep 1: Log in to DvSum\nStep 2: Access the Data Quality Tab\nStep 3: Add a New Rule\nStep 4: Basic Input Configuration\nStep 5: Validate & Execute\nIntroduction\nThe\nRuleset Rule\nis a foundational Data Quality (DQ) check that combines three key validation rules:\nBlanks Check\nâ€“ Ensures that critical fields are not left empty.\nValue Range Check\nâ€“ Restricts values within a defined range.\nData Format Check\nâ€“ Verifies that data follows a specified format (e.g., numerical, string, date).\nBy applying these checks, the\nRuleset Rule\nhelps maintain data accuracy, consistency, and completeness.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nWhy is it Important?\nThe\nRuleset Rule\nplays a crucial role in preventing data quality issues, such as:\nMissing Data\nâ€“ Ensuring key fields like Customer_ID are always populated.\nIncorrect Values\nâ€“ Restricting inputs to valid predefined ranges.\nInconsistent Formatting\nâ€“ Standardizing data representation for consistency across datasets.\nFor example:\nCustomer_ID cannot be blank\n, ensuring no missing records.\nThe â€œCountryâ€ column must contain only two-character country codes (e.g., US, UK).\nDate fields should follow the â€œYYYY-MM-DDâ€ format.\nBy enforcing these rules, organizations can avoid incomplete or inaccurate records that may impact reporting and decision-making.\nHow to Configure the Ruleset Rule\nStep 1: Log in to DvSum\nNavigate to the\nDictionaries\n>\nDatabase Tables\ntab.\nSelect the relevant\nData Source\nand\nTable Name\n.\nStep 2: Access the Data Quality Tab\nSelect the\nTable Name\n.\nClick on the\nData Quality\ntab.\nChoose\nAvailable Rules\n.\nStep 3: Add a New Rule\nClick the\nâŠ• Add Rule\nbutton.\nSelect the\nFoundational DQ\ncategory.\nFrom the list of options, choose\nRULESET\n.\nStep 4: Basic Input Configuration\nIn the Rule Wizardâ€™s\nBasic Input\nsection, fill in the following details:\nRule Description\n: Provide a meaningful name for the rule.\nRuleset Checks\n: Choose the specific checks you want to apply:\nBlanks Check\nâ€“ Ensures no empty values in selected columns.\nValue Range Check\nâ€“ Restricts values to a specified range.\nData Format Check\nâ€“ Verifies consistency in data format.\nS\ntep 5: Validate & Execute\nAfter saving the rule, review its definition.\nClick\nRun\nto execute and test the rule.",
    "scraped_at": "2026-02-02 15:35:00"
  },
  {
    "title": "Foundational DQ - Custom Query",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25211951751060-Foundational-DQ-Custom-Query",
    "content": "In this Article:\nOverview\nWhy is it Important?\nHow to Configure the Custom Query Rule\nStep 1: Log in to DvSum\nStep 2: Access the Data Quality Tab\nStep 3: Add a New Rule\nStep 4: Configure the Custom Query\nStep 5: Validate & Execute\nOverview\nThe\nCustom Query Rule\nallows users to create and execute\ncustom SQL queries\nfor advanced data validation, analysis, and reporting. It provides flexibility to define unique data quality checks beyond predefined rules, enabling users to tailor queries to their specific needs.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nWhy is it Important?\nThe\nCustom Query Rule\nis essential for:\nComplex Data Validations\nâ€“ Ensuring data integrity using custom conditions.\nAdvanced Data Analytics\nâ€“ Running complex aggregations and transformations.\nTailored Reporting\nâ€“ Extracting and monitoring specific insights from data.\nFor example, users can create a query to:\nIdentify duplicate customer records by checking for repeated Customer_IDs.\nVerify if order totals match invoice amounts across multiple tables.\nDetect missing transactions in a financial dataset by checking sequential order numbers.\nHow to Configure the Custom Query Rule\nStep 1: Log in to DvSum\nNavigate to the\nDictionaries\n>\nDatabase Tables\ntab.\nSelect the relevant\nData Source\nand\nTable Name\n.\nStep 2: Access the Data Quality Tab\nSelect the\nTable Name\n.\nClick on the\nData Quality\ntab.\nChoose\nAvailable Rules\n.\nStep 3: Add a New Rule\nClick the\nâŠ• Add Rule\nbutton.\nSelect the\nFoundational DQ\ncategory.\nFrom the list of options, choose\nCustom Query\n.\nStep 4: Configure the Custom Query\nIn the Rule Wizardâ€™s\nBasic Input\nsection, provide:\nRule Description\nâ€“ A clear description of the ruleâ€™s purpose.\nCustom Query\nâ€“ The SQL query you want to execute.\nExample SQL Query:\nSELECT Customer_ID, COUNT(*)\nFROM Orders\nGROUP BY Customer_ID\nHAVING COUNT(*) > 1;\nThis query checks for duplicate\nCustomer_IDs\nin the\nOrders\ntable.\nUsing Stored Procedures (for Oracle Data Sources Only)\nWhen creating a\nCustom Query rule\nfor an\nOracle data source\n, you can optionally enable the\nStored Procedure\ncheckbox.\nThis allows the rule to execute a database stored procedure instead of a regular SQL\nSELECT\nquery.\nStored Procedure Overview\nWhen the\nStored Procedure\noption is enabled, the Data Quality rule will execute a stored procedure that returns its result as a\nREFCURSOR\n.\nThis means the procedure must output a result set (similar to a query output) that DvSum can read and use for Data Quality checks.\nHow it Works\nSelect the\nStored Procedure\ncheckbox under\nBasic Input\n.\nEnter the procedure name and its parameters in the\nCustom Query\nfield.\nExample:\nPROC_NAME(param1, param2, ..., resultparam\nOUT\nSYS_REFCURSOR);\nThe platform executes this procedure on the connected Oracle database.\nThe result returned via the\nSYS_REFCURSOR\nwill be used as the dataset for the rule.\nNote:\nThe Stored Procedure option is available\nonly for Oracle data sources\n. For other data sources, use a standard SQL query instead.\nStep 5: Validate & Execute\nAfter saving the rule, review its definition.\nClick\nRun\nto execute and test the rule.\nView the results and refine the query if needed.",
    "scraped_at": "2026-02-02 15:35:06"
  },
  {
    "title": "Foundational DQ - Unique Values Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25211808095892-Foundational-DQ-Unique-Values-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Select Table & Data Quality Rules\nStep 3: Add a New Rule (Foundational DQ â†’ Unique Values)\nStep 4: Basic Input Setup\nStep 5: Validate & Run the Rule\nIntroduction\nThe \"UNIQUE VALUES\" rule evaluates whether the count of unique values within a particular dataset falls within an acceptable range or matches a specified number.\nFor instance, if there are 100 ship-from locations expected, the rule anticipates the count of unique locations to range between 95 and 100. An occurrence exceeding 100 suggests an error, typically due to duplicated or erroneous entries. Conversely, if the count falls below 95, it signifies a potential error, often attributable to missing or incomplete data.\nBy applying this rule, data analysts can ensure that the number of unique values conforms to expectations, thereby upholding data quality and reliability.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Configure the Rule:\nStep 1: Access Database Tables\nLog in to DvSum and proceed to the Data Dictionary tab under the Dictionaries and select the relevant Data source and Table Name.\nStep 2: Select Table & Data Quality Rules\nSelect the table name then select the Data Quality tab and choose Available Rules.\nStep 3: Add a New Rule (Foundational DQ â†’ Unique Values)\nSelect the \"âŠ• Add Rule\" button, then choose the \"Foundational DQ\" category. From the list of options, click on \"Value Range\".\nStep 4: Basic Input Setup\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Column Name, corresponding to the table in which you want to search for Unique records.\nStep 5: Validate & Run the Rule\nAfter saving the rule, you'll see its definition. Click \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:35:10"
  },
  {
    "title": "Foundational DQ - Value Range Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25211047471124-Foundational-DQ-Value-Range-Rule",
    "content": "In this article:\nIntroduction\nHow to Set Up the Value Range Rule\nStep 1: Navigate to Data Quality Rules\nStep 2: Add a New Rule\nStep 3: Configure Rule Parameters\nSingle Value\nRange Value\nReference Dictionary\nStep 4: Validate the Rule\nIntroduction\nThe Foundational DQ Value Range Rule ensures that data values fall within an acceptable range, preventing incorrect or outlier values. This rule is crucial for maintaining data accuracy in fields such as transaction amounts, dates, and numerical identifiers.\nFor example, in manufacturing, the rule might enforce that the yield percentage should always be between 0.01 and 1. Similarly, in sales orders, the order type should be limited to specific values like ZOST, ZOCO, or ZOFR. In another scenario, for shipments, the item_category should only include values listed in the item_category reference.\nBy applying this rule, data integrity is maintained, ensuring that values are within acceptable boundaries or options, thus enhancing the reliability of analyses and processes relying on this data.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Set Up the Value Range Rule\nStep 1: Navigate to Data Quality Rules\nLog in to DvSum.\nGo to Dictionaries > Database Tables.\nSelect the relevant Data Source and Table Name.\nStep 2: Add a New Rule\nSelect the table name and go to the Data Quality tab.\nChoose Available Rules.\nClick on the âŠ• Add Rule button.\nSelect Foundational DQ and choose Value Range.\nStep 3: Configure Rule Parameters\nIn the Rule Wizard's Basic Input section:\nRule Description: Enter a meaningful description.\nColumn Name: Select the column to apply the rule.\nComparison Type: Choose one of the three options:\nSingle Value\nRange Value\nReference Dictionary\nSingle Value\nAs the name suggests, for a single value there is a single threshold. From the list of operators, you can define the threshold on a column name. When comparison type is mentioned then the user can further select 3 more options from Value which are:\nAbsolute Value\nExpression\nReference Column\nThe above different Value types are explained below with examples for better understanding.\nConsider an example, where we want to create a VR rule on \"Population\" which should be greater than equal to the \"Relative Budget\". Specifying this as a \"Valid\" range, this rule will show the records where \"Population\" is less than \"Relative Budget\" as exceptions.\nAnother example is that we can select the option \"Absolute Rule Value\" and in this case, only the value mentioned will be considered as valid value\nAnother example is that here we have defined an expression \" > 100 \" which means that the data will be valid where the column \"Population\" has values greater than 100:\nOne more example is that here we have defined an expression \">DATEADD(day, 7, CURRENT_DATE()) which means that the data will be valid where the column \"Population\" has values greater than 7 days from the current date.\nRange Value\nFor the Range values, you need to provide the minimum and maximum threshold. Values which will be not in range will be marked as exceptions.\nReference Dictionary\nIn the Rule Wizard's Basic Input section, Select \"Ref Dictionary\" from dropdown in Comparison type.\nSelect the Dictionary and add relevant source fields.\nIf user selects,\nTreat Empty Strings as Valid Values\n: Empty strings (\"\") will be considered\nvalid\nand allowed by the rule.\nTreat Null as Valid Values:\nNULL values will be considered\nvalid\n. Theyâ€™ll be treated as acceptable data\nand allowed by the rule.\nIgnore Case:\nThe rule will\nnot differentiate between uppercase and lowercase\ncharacters.\nFor example, \"Shipping\" and \"shipping\" would be treated as\nequal\n.\nNote:\nTo know more about Empty and Null values for Value rage rule, refer the article\nNull and Empty Values\n.\nStep 4: Validate the Rule\nSave the rule and check its definition.\nClick\nRun\nto execute and test the rule.",
    "scraped_at": "2026-02-02 15:35:16"
  },
  {
    "title": "Foundational DQ - Data Format Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25210951110420-Foundational-DQ-Data-Format-Rule",
    "content": "In this Article\nIntroduction\nUse Cases & When to Apply It\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nStep 2: Create a New Rule\nStep 3: Basic Input\nStep 4: Configure Validation Conditions\nStep 5: Save & Run the Rule\nIntroduction\nThe Foundational DQ Data Format Rule ensures that data adheres to predefined formats, such as date formats, phone numbers, or email structures. This rule helps maintain consistency and prevents data entry errors that can lead to integration issues and failed validations.\nFor example, if Social Security Numbers (SSN) should follow the pattern XXXX-XXX-XXX, any deviation would be flagged as an error.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nUse Cases & When to Apply It\nThe rule is particularly useful when:\nValidating email addresses, phone numbers, dates, and other structured data.\nEnsuring standardization across datasets before data integration.\nAvoiding processing errors due to incorrect formatting.\nMaintaining compliance with regulatory or business standards\nHow to Configure the Rule\nStep 1: Access Data Dictionary\nLog in to DvSum\nNavigate to\nDictionaries\nâ†’\nDatabase Tables.\nSelect the relevant\nData Source\nand\nTable Name\nData Quality\nâ†’ Choose\nAvailable Rules\n.\nStep 2: Create a New Rule\nClick\n\"\nâŠ•\nAdd Rule\"\n.\nSelect the\n\"Foundational DQ\"\ncategory.\nChoose\n\"DATA FORMAT\"\n.\nStep 3: Basic Input\nIn the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Column Name, Pattern, and Filter condition corresponding to the table in which you want to search for Data Format records.\nStep 4: Configure Validation Conditions\nDefine the expected format using a valid regular expression. The actual regex syntax supported depends on the underlying database technology connected to DvSum.\nHere are some commonly used patterns:\nEmail\nâ†’\n^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$\nMatches\nexample@domain.com\nPhone Number\nâ†’\n^\\(\\d{3}\\)\\s\\d{3}-\\d{4}$\nMatches\n(123) 456-7890\nDate (YYYY-MM-DD)\nâ†’\n^\\d{4}-\\d{2}-\\d{2}$\nMatches\n2025-07-02\nPostal Code (ZIP)\nâ†’\n^\\d{5}(-\\d{4})?$\nMatches\n12345\nor\n12345-6789\nNote: Since regex is evaluated at the database level, pattern support may vary by database (e.g., Snowflake, PostgreSQL). Always validate patterns against your specific data source.\nStep 5: Save & Run the Rule\nClick\n\"Save Rule\"\n.\nClick \"\nRun\n\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:35:21"
  },
  {
    "title": "Foundational DQ - Blanks Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25210423692564-Foundational-DQ-Blanks-Rule",
    "content": "In this Article\nIntroduction\nHow to Configure the Rule\nStep 1: Access Data Quality Rules\nStep 2: Create a New Rule\nStep 3: Define the Rule Parameters\nStep 4: Save & Run the Rule\nIntroduction\nThe \"BLANKS\" rule addresses the availability and completeness of data within a dataset. It focuses on identifying whether there are any missing values present. This rule is particularly important for ensuring data integrity and accuracy.\nFor example, let's consider a dataset containing customer information, including ship-to addresses. The BLANKS rule might be applied to check if there are any records where the address field is empty or missing in the customer ship-to information. By enforcing this rule, one can ensure that all necessary data fields are populated, thus maintaining the quality and reliability of the dataset.\nFor further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article\nRule Detail page.\nHow to Configure the Rule\nStep 1: Access Data Quality Ruless\nLog in to DvSum and\nNavigate to\nDictionaries\nâ†’\nDatabase Tables.\nSelect the relevant\nData Source\nand\nTable Name\n.\nData Quality\nâ†’ Choose\nAvailable Rules\n.\nStep 2: Create a New Rule\nClick\n\"\nâŠ•\nAdd Rule\"\n.\nSelect the\n\"Foundational DQ\"\ncategory.\nChoose\n\"BLANKS\"\n.\nStep 3: Define the Rule Parameters\nSelect the\nTable and Column\nwhere blank values should be restricted.\nFill in the Rule Description, and Column name.\nIf user selects:\nTreat Empty Strings as Blanks:\nAny field that contains an empty string (\"\") will be considered as a blank value.\nTreat Null Values as Blanks\n: Database-level NULL values will be treated the same as blank/missing values.\nNote:\nTo know more about the Null and Empty values for Blank Rule, refer the article\nNull and Empty Values.\nStep 4: Save & Run the Rule\nClick\n\"Save Rule\"\n.\nClick \"Run\" to execute and test the rule.",
    "scraped_at": "2026-02-02 15:35:26"
  },
  {
    "title": "How is DQ Score calculated for rules?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/27005589675796-How-is-DQ-Score-calculated-for-rules",
    "content": "Introduction\nThe DQ Score helps assess how well a data quality rule is configured and ready for execution. A higher score indicates that the rule is properly set up, while a lower score suggests areas that may need improvement. It provides a precise overview of data quality and assists users in making relevant decisions.\nDQ Score Calculation\nThe DQ Score is calculated slightly differently for different types of rules. The table below describes how the score is determined for each type of DvSum rule.\nRule Type\nDQ Score Calculation\nCount\nMetric\nIf the threshold minimum is 0 and the result is less than or equal to 0, then the DQ score is 100%.\nIf the threshold minimum is not 0 and the result is less than or equal to the threshold minimum, then the DQ score is:\nresult/threshold minimum\nIf the result is greater than or equal to 2 times the threshold maximum, then the DQ score is zero.\nIf the result is greater than or equal to the threshold maximum but less than 2 times the threshold maximum, then the DQ score is:\n(result - threshold maximum) / threshold maximum\nBlank Check\nValue Range\nRuleset\nOrphan Records\nCross Dataset\nOrphan Keys\nData Format\nIf there are no exceptions, then the DQ Score is 100%.\nIf there are exceptions, then the DQ Score is calculated as:\n(total records - exception records) / total records\nCUSTOM QUERY\nIf there are no exceptions, then the DQ score is 100%.\nIf there are exceptions, then the DQ score is:\n(total records - exception records) / total records\nIf the total record count is less than the custom query count, then the DQ score is 0%.\nCompare Schema\nCompare Metric\nIf the calculated result is less than or equal to the reference result, then the DQ score is:\ncalculated result/reference result\nIf the calculated result is greater than or equal to 2 times the reference result, then the DQ score is 0%.\nIf the calculated result is greater than or equal to the reference result, then the DQ score is:\n(calculated result - reference result) / reference result",
    "scraped_at": "2026-02-02 15:35:31"
  },
  {
    "title": "Post-Import Steps for Workflow-Enabled Domains",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/39826074221332-Post-Import-Steps-for-Workflow-Enabled-Domains",
    "content": "In this Article:\nWorkflow Behavior After Import\nSteps to Manually Submit Imported Records for Approval\nNavigate to the Asset Type\nFind Recently Imported Records\nSelect Records to Submit\nClick Mass Update\nSubmit for Approval\nConfirm Submission\nApprove the Records (For Approver)\nImport File Behavior for the Status Field\nKnown Limitation\nWorkflow Behavior After Import:\nAfter completing the import,\nrecords are not automatically submitted for approval\n.\nIn workflow-enabled domains:\nNewly imported records\nwill have\nDraft\nstatus\nUpdated records\nwill have\nModified\nstatus â€” regardless of their previous status or the value in the import file\nTo move records through the approval workflow, a\nmanual submission\nis required after import.\nSteps:\n1. Navigate to the Asset Type\nOpen the domain where the records were imported (e.g., Business Glossary, Data Dictionary, Field Dictionary).\n2. Find Recently Imported Records\nApply a filter to a column\nLast Updated On,\nlike:\nLast Updated On â†’ Greater than â†’ Yesterday\n3. Select Records to Submit\nSelect the relevant records from the grid.\nBy default, clicking the checkbox on the top-left corner selects all records currently visible on the page.\n2. To select all records in the current view (not just the visible page), click the\n\"Select all records in this view\"\noption that appears after selecting the first checkbox (as shown above).\nTip: There is\nno hard limit on records\n. The system allows bulk selection beyond 1000 records, provided you've filtered or configured the view accordingly.\n4. Click\nMass Update\n.\n5.\nSubmit for Approval\nIn the Mass Edit modal:\nField\n: Status\nValue\n:\nSubmit for Approval\nClick\nSubmit\n.\n6.\nConfirm\nRecords will now enter the\nworkflow process\nand await approval.\n7. Approve the Records (For Approver)\n1. Go to the\nAssigned to Me\nsection in the listing page of the relevant asset type.\n2. Select the records â†’ Click\nMass Update\nâ†’ Set\nStatus\nto\nApproved\nâ†’ Click\nSubmit\n.\nImport File Behavior for the Status Field\nThe system ignores the Status value during import. All new records are set to\nDraft\n, and all updated records are set to\nModified\n, regardless of whether the column is included or omitted.\nRegardless of the import type,\nmanual submission is always required\nto trigger the approval workflow.\nKnown Limitation\nAutomatic publishing or submission via import is not supported.\nRecords\ncannot be auto-submitted\ninto the workflow during import, even if the\nStatus\nis set to\nPublished\nin the import file.\nManual action is required post-import\nto submit both\nnewly added\nand\nupdated\nrecords for approval through the UI (e.g., via Mass Update).",
    "scraped_at": "2026-02-02 15:35:36"
  },
  {
    "title": "Post-Import Steps for Non-Workflow Domains",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/39824926810644-Post-Import-Steps-for-Non-Workflow-Domains",
    "content": "In this Article:\nWorkflow Behavior After Import\nSteps to Manually Publish Imported Records\nNavigate to the Asset Type\nFind Recently Imported Records\nSelect Records to Update\nClick Mass Update\nUpdate Status\nImport File Behavior for the Status Field\nKnown Limitation\nWorkflow Behavior After Import:\nAfter completing the import,\nrecords are not automatically published\n.\nIn non-workflow domains:\nNewly imported records will have\nDraft\nstatus\nUpdated records will have\nModified\nstatus, this happens regardless of their previous status or the value provided in the import file\nSteps:\n1. Navigate to the Asset Type\nOpen the domain where the records were imported (e.g., Business Glossary, Data Dictionary, Field Dictionary).\n2. Find Recently Imported Records\nApply a filter to a column\nLast Updated On,\nlike:\nLast Updated On â†’ Greater than â†’ Yesterday\n3. Select Records to Update\nSelect the relevant records from the grid.\nBy default, clicking the checkbox on the top-left corner selects all records currently visible on the page.\nTo select all records in the current view (not just the visible page), click the\n\"Select all records in this view\"\noption that appears after selecting the first checkbox (as shown above).\nTip: There is\nno hard limit on records\n. The system allows bulk selection beyond 1000 records, provided you've filtered or configured the view accordingly.\n4. Click Mass update.\n5. Update Status\nIn the Mass Update modal:\nField\n: Status\nValue\n: Published\nClick\nSubmit\n.\nConfirm the status is updated to\nPublished\n. Once the status is published, then you can verify the changes in the respective section where you modified.\nTip: Save a custom view with filters to repeat bulk updates easily.\nImport File Behavior for the Status Field\nThe system ignores the Status value during import. All new records are set to\nDraft\n, and all updated records are set to\nModified\n, regardless of whether the column is included or omitted.\nKnown Limitation\nAutomatic publishing or submission via import is not supported.\nManual action is required post-import\nto submit both\nnewly added\nand\nupdated\nrecords for approval through the UI (e.g., via Mass Update).",
    "scraped_at": "2026-02-02 15:35:42"
  },
  {
    "title": "Usability Score",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/37654166387860-Usability-Score",
    "content": "In this Article:\nIntroduction\nImproving Usability Score\nAccessing the Overview Page\nUnderstanding the Usability Score\nImproving the Usability Score\nCompleting the Required Fields\nFinalizing the Updates\nResult\nRequired Metadata Fields for Full Score\nIntroduction\nIn DvSum, the\nUsability Score\nmeasures the completeness of a tableâ€™s metadata. A full score of\n10\nindicates that all essential metadata fields have been properly filled in, promoting better governance, discoverability, and trust in the data asset.\nNote:\nFull Score = 10\n: Achieved when\nall required metadata fields\nare populated.\nPartial Score\n: If some fields are missing, the score is\ncalculated as the average\nof the available metadata inputs.\nIf the Usability score is not 10/10 then the asset will considered as\nIncomplete.\nTo mark an asset as Complete, the usability score must be updated to 10/10.\nImproving Usability Score\n1. Accessing the Overview Page:\nClick on any\ntable name\n,\nfield name\n, or\nterm name\n.\nYou will be redirected to the\nOverview Page\n.\nThe\nUsability Score\nis prominently displayed at the top of this page.\n2. Understanding the Usability Score:\nA\npartial usability score\nindicates that\nnot all required metadata fields\nhave been populated.\n3. Improving the Usability Score:\nClick on the\nEdit\nbutton.\nA list of\nrequired fields\nthat need to be filled will be displayed.\n4. Completing the Required Fields:\nNavigate to the\nrespective tab(s)\nwhere the required fields are located.\nEnter the\nnecessary information\nin each required field.\n5. Finalizing the Updates:\nAfter filling in all required fields, click\nSave\n.\nThen click\nDone\nand\nPublish\n.\n6. Result:\nOnce all steps are completed, the\nUsability Score\nwill be updated to\n10/10\n.\nOnce an asset achieves a 10/10 Usability Score, its status automatically transitions from\nIncomplete\nto\nComplete\n. For assets with workflow enabled, the asset must be approved and published after these updates. If any required fields are not filled in, the asset will continue to remain in\nIncomplete\nuntil all required information is provided.\nRequired Metadata Fields for Full Score\nTo reach a usability score of\n10/10\n, the following fields must be filled in:\nDescription\nOwnership\nTags\nDomains\nAdditional Info\nLineage\nAdditional info.\nNote:\nMissing even one of these fields will reduce the score.",
    "scraped_at": "2026-02-02 15:35:47"
  },
  {
    "title": "Configure Amazon S3 permissions: Use Existing Resources (Limited Permissions)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/37053578369684-Configure-Amazon-S3-permissions-Use-Existing-Resources-Limited-Permissions",
    "content": "In This Article:\nOverview\nPrerequisites\nCreate an AWS IAM User\nAssign Custom Inline Policy\nConnect Your S3 Data Source in DvSum\nOverview:\nThis guide walks you through configuring Amazon S3 as a data source in DvSum. It is intended for users who already have their S3 buckets created and populated and prefer to manage their AWS resources directly with limited IAM permissions.\nIf you'd rather have DvSum manage the setup using AWS Glue (including crawlers and databases), please refer to\nConfigure S3 Data Source in DvSum (Elevated Permissions Required)\n.\n1. Prerequisites\nEnsure the following before proceeding:\nYou have access to an AWS account with appropriate permissions.\nYour Amazon S3 buckets are already created and populated with data.\nYou are familiar with AWS IAM policies and can assign roles or managed policies.\n2. Create an AWS IAM User\nCreate a new IAM user in your AWS account that DvSum will use to connect to S3.\n3. Assign Custom Inline Policy\nTo fine-tune access, you can use a JSON policy like the following example. This ensures only the required buckets and services are accessible:\nNote:\nPlease Ensure all comments are removed from the final policy JSON before deployment\n{\n\"\nVersion\n\"\n:\n\"\n2012-10-17\n\"\n,\n\"\nStatement\n\"\n: [\n{\n\"\nSid\n\"\n:\n\"\nAthenaQueryExecution\n\"\n,\n\"\nEffect\n\"\n:\n\"\nAllow\n\"\n,\n\"\nAction\n\"\n: [\n\"\nathena:StartQueryExecution\n\"\n,\n\"\nathena:GetQueryExecution\n\"\n,\n\"\nathena:GetQueryResults\n\"\n,\n\"\nathena:StopQueryExecution\n\"\n,\n\"\nathena:GetWorkGroup\n\"\n,\n\"\nathena:ListDataCatalogs\n\"\n],\n\"\nResource\n\"\n:\n\"\n*\n\"\n/*\n-- Â Please Ensure all comments are removed from the final policy JSONÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â before deployment ----\nNote:\n- To restrict Athena to a specific WorkGroup:\nUse Resource ARN like:\n\"Resource\": \"arn:aws:athena:<REGION>:<YOUR-AWS-ACCOUNT-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ID>:workgroup/<YOUR-WORKGROUP-NAME>\"\n- Example:\n\"Resource\": \"arn:aws:athena:us-west-2:123123123123:workgroup/dvsum-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  workgroup\"\n*/\n},\n{\n\"\nSid\n\"\n:\n\"\nGlueCatalogAndCrawlerAccess\n\"\n,\n\"\nEffect\n\"\n:\n\"\nAllow\n\"\n,\n\"\nAction\n\"\n: [\n\"\nglue:GetDatabase\n\"\n,\n\"\nglue:GetDatabases\n\"\n,\n\"\nglue:GetTable\n\"\n,\n\"\nglue:GetTables\n\"\n,\n\"\nglue:UpdateTable\n\"\n,\n\"\nglue:GetPartition\n\"\n,\n\"\nglue:GetPartitions\n\"\n,\n\"\nglue:GetCatalogImportStatus\n\"\n],\n\"\nResource\n\"\n:\n\"\n*\n\"\n/*\n---- Please Ensure all comments are removed from the final policy JSONÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  before deployment ---\nNote: (remove these comments in actual policy JSON)\n- To restrict Glue access to specific Catalog/Databases/Tables:\nGlue Catalog ARN:\n\"Resource\":\n\"arn:aws:glue:<region>:<account-id>:catalog\"\nGlue Database ARN:\n\"Resource\":\n\"arn:aws:glue:<region>:<account-id>:database/<database-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name>\"\nGlue Table ARN:\n\"Resource\":\n\"arn:aws:glue:<region>:<account-id>:table/<database-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â name>\"\n*/\n},\n{\n\"\nSid\n\"\n:\n\"\nS3DataSourceBucketsAccess\n\"\n,\n\"\nEffect\n\"\n:\n\"\nAllow\n\"\n,\n\"\nAction\n\"\n: [\n\"\ns3:GetObject\n\"\n,\n\"\ns3:ListBucket\n\"\n,\n\"\ns3:GetBucketLocation\n\"\n],\n\"\nResource\n\"\n: [\n/*\nImportant:\nThis policy limits access to specific S3 buckets only.\nAs a result, only AWS Glue databases that catalog data stored inÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  these allowed S3 buckets can be queried successfully. Make sure thatÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â any Glue Data Catalog resources (databases and tables) you useÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  correspond to these permitted S3 buckets.\n*/\n\"\narn:aws:s3:::<YOUR-DATA-SOURCE-BUCKET-1>\n\"\n,\n\"\nARN:AWS:S3:::<YOUR-DATA-SOURCE-BUCKET-1>/*\n\"\n,\n\"\narn:aws:s3:::<YOUR-DATA-SOURCE-BUCKET-2>\n\"\n,\n\"\narn:aws:s3:::<YOUR-DATA-SOURCE-BUCKET-2>/*\n\"\n// Add more data buckets as needed\n]\n},\n{\n\"\nSid\n\"\n:\n\"\nAllowGetUserSelf\n\"\n,\n\"\nEffect\n\"\n:\n\"\nAllow\n\"\n,\n\"\nAction\n\"\n:\n\"\niam:ListAttachedUserPolicies\n\"\n,\n\"\nResource\n\"\n:\n\"\narn:aws:iam::<YOUR-AWS-ACCOUNT-ID>:user/<YOUR-IAM-USER-NAME>\n\"\n},\n{\n\"\nSid\n\"\n:\n\"\nS3AthenaQueryResultBucketAccess\n\"\n,\n\"\nEffect\n\"\n:\n\"\nAllow\n\"\n,\n\"\nAction\n\"\n: [\n\"\ns3:GetObject\n\"\n,\n\"\ns3:PutObject\n\"\n,\n\"\ns3:ListBucket\n\"\n,\n\"\ns3:GetBucketLocation\n\"\n],\n\"\nResource\n\"\n: [\n\"\narn:aws:s3:::<YOUR-ATHENA-STAGING-BUCKET>\n\"\n,\n\"\narn:aws:s3:::<YOUR-ATHENA-STAGING-BUCKET>/*\n\"\n]\n}\n]\n}\nImportant:\nPlease make sure to update the following placeholders in the policy JSON before deploying it:\n<REGION>\n: The AWS region where your resources are located (e.g., us-west-2).\n<YOUR-AWS-ACCOUNT-ID>\n: Your AWS account ID.\n<YOUR-WORKGROUP-NAME>\n: The name of your Athena workgroup.\n<YOUR-GLUEDB-NAME>\n: The name of your Glue database. It will be used for scanning S3 data sources.\n<YOUR-DATA-SOURCE-BUCKET-1>\n,\n<YOUR-DATA-SOURCE-BUCKET-2>\n: These are the names of your S3 data source buckets.\nItâ€™s important to include the buckets associated with GlueDB in orderÂ to grant access to them. You may add more buckets as needed.\n<YOUR-IAM-USER-NAME>\n: The name of the IAM user for whom you want to allow self-service access.\n<YOUR-ATHENA-STAGING-BUCKET>\n: The name of your S3 bucket for Athena query results.\nNote: Ensure that all comments are removed from the final policy JSON before deployment.\n4. Connect Your S3 Data Source in DvSum\nAfter successfully creating the IAM user with the necessary permissions, follow these steps within DvSum:\nNavigate to Add Source:\nGo to the \"Sources\" section in the DvSum application and click on \"Add Source.\"\nSelect Amazon S3:\nChoose \"Amazon S3\" as the source type.\nEnter Credentials:\nProvide the \"Access Key\" and \"Secret Key\" associated with the IAM user you created.\nVerify Authentication:\nClick the \"Check Authentication\" button to ensure DvSum can successfully connect to your AWS environment using the provided credentials.\nSpecify Glue Database and S3 Bucket(s):\nIn the designated field for \"Glue Database\" enter the name of the AWS Glue database you wish to connect to.\nIn the designated field for \"S3 Staging Bucket\" enter the name of the S3 bucket(s) containing used for athena metadata storage.\nSave Configuration:\nClick the \"Save\" button to finalize the Amazon S3 data source connection within DvSum.\nFor step-by-step instructions and screenshots of this process, refer to\nthis guide\n.",
    "scraped_at": "2026-02-02 15:35:52"
  },
  {
    "title": "How to Configure External Recipients & Whitelist Domains",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/37016421057428-How-to-Configure-External-Recipients-Whitelist-Domains",
    "content": "In this article:\nOverview\nAdd External Email Addresses in the Job Notification Tab\nHandle Domain Whitelisting\nVerify the Email Address\nAdding Multiple Recipients\nOverview:\nIn DvSum's DI (Data Intelligence) tool, you can configure job notifications to be sent to users who are not part of your DvSum organization (external users). This guide walks you through the steps required to successfully add external email addresses.\nFor more details onÂ  jobs, refer to the article\nCreating new Job\n.\nStep 1: Add External Email Addresses in the Job Notification Tab\nFrom the\nAdministration\nTab navigate to\nJobs\nand select\nDefinition\n.\nClick on the Job ID for which you want to add an external recipient.\nClick\nEdit\non the selected job.\nGo to the\nNotification\ntab.\nEnter the external email address in the external recipients field.\nPress Enter\nafter typing the email address.\nThis step is important:\nOnly after pressing Enter\nwill the email address be properly saved.\nOnce saved, the email will appear enclosed in a box.\nStep 2: Handle Domain Whitelisting\nIf you encounter an warning saying that the domain (e.g.,\ngmail.com\n,\nyahoo.com\n) is\nnot whitelisted\n, perform the following:\nGo to\nAdministration\n>\nAccount\n.\nNavigate to\nUser Security\n>\nExternal Domains\n.\nClick on\nAdd Domain\n.\nEnter the domain name (e.g.,\ngmail.com\n) and\nsave\nit.\nStep 3: Verify the Email Address\nAfter whitelisting the domain, return to the Job Notification tab.\nRe-enter the external email address.\nPress Enter\nto save it properly.\nThe email should now be accepted without any errors and displayed in a box format.\nAdding Multiple Recipients:\nUsers can enter multiple external email addresses, provided each address belongs to a whitelisted domain.",
    "scraped_at": "2026-02-02 15:35:57"
  },
  {
    "title": "How to set the exception record limit for rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/37016023734036-How-to-set-the-exception-record-limit-for-rules",
    "content": "In this article:\nNavigate to the Data Dictionary Module\nAccess the Data Quality Section\nAdjust the Exceptions Record Limit\nSave Your Changes\nAdditional Reference\nTo configure the number of exception records for a specific table, follow the steps outlined below:\nStep 1: Navigate to the Data Dictionary Module\nOpen the\nDictionaries\n>\nDatabase Tables\nmodule from your system.\nLocate the relevant table associated with the rule for which you want to adjust the exception record count.\nStep 2: Access the Data Quality Section\nWithin the table settings, locate the\nData Quality\nsection.\nSteps 3:\nAdjust the Exceptions Record Limit\nUnder the\nData Quality\nsection, click on\nSettings\n.\nIn the\nExceptions Record Limit\nfield, enter the desired number. The maximum allowable limit is\n75,000\nrecords. You can set the limit anywhere between 300 and 75,000 based on your needs.\nSteps 4: Save Your Changes\nAfter entering the desired record limit, click\nSave\nto confirm and apply the changes.\nNote:\nMinimum Limit: The minimum number of exception records you can set is 300.\nMaximum Limit: The maximum number of exception records you can set is 75,000.\nAdditional Reference:\nRefer the article\nRule detail page\nto know more about exporting exception records.",
    "scraped_at": "2026-02-02 15:36:01"
  },
  {
    "title": "How to Reset Password on the DI Platform",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/36793238343060-How-to-Reset-Password-on-the-DI-Platform",
    "content": "In this article:\nScope and When to Use This Guide\nNon-SSO Flow: Resetting Your Password\nInitiate Password Reset\nVerify Your Identity\nRetrieve Reset Code\nCreate a New Password\nComplete the Process\nSSO Flow: Resetting Password After Disabling SSO\nOverview\nDisabling SSO (Admin Action)\nTemporary Password Delivery\nLog In & Reset Password\nScope and When to Use This Guide\nThis guide walks users through the process of resetting their password in both standard (email/password) and SSO (Single Sign-On) environments.\nYou should use this guide if:\nYou've forgotten your password and can't log into your DvSum account.\nYou're using email/password-based authentication.\nYour organization has disabled SSO and you need to access your account.\nThe guide is divided into two main sections:\nNon-SSO Flow\nâ€“ For standard email/password users.\nSSO Flow\nâ€“ For users switching from SSO to password-based login.\nNon-SSO Flow: Resetting Your Password\nFor users with email/password authentication, follow the steps below to reset your password securely.\n1. Initiate Password Reset\nNavigate to the DvSum login page:\nhttps://app.dvsum.ai\nClick on the\n\"Forgot Password?\"\nlink located below the password input field.\n2. Verify Your Identity\nEnter the\nemail address\nassociated with your DvSum account.\nClick\n\"Reset my password\"\n.\nTip:\nEnsure the email address is an exact match with what's registered in DvSum.\n3. Retrieve Reset Code\nOpen your inbox and find the email from\nDI Support\n.\nLocate the\n6-digit verification code\nin the email body.\nIf you donâ€™t see the email:\nCheck your spam or junk folder.\nIf still not found, click on the â€œResend Verification Emailâ€ button to receive a new code.\n4. Create a New Password\nReturn to the DvSum\nPassword Reset\npage.\nEnter the 6-digit\nverification code\n.\nSet a\nnew password\nthat meets the following criteria:\n8â€“12 characters in length\nAt least\none uppercase\nletter\nAt least\none number or special character\n(\n!@#$%\n)\nCannot be the same as any of your\nlast 3 passwords\nConfirm the new password by entering it again.\n5. Complete the Process\nClick\n\"Change Password\"\nto save changes.\nYouâ€™ll be\nautomatically redirected\nto the login page.\nSign in using your\nemail address and new password\n.\nSSO Flow: Resetting Password After Disabling SSO\nIf your organization has disabled Single Sign-On (SSO), users must log in using standard password authentication. The steps below outline how to reset your password after this transition.\n1. Overview\nSSO allows users to log in via their organizationâ€™s authentication provider. However, if SSO is disabled, users need to reset their password manually to regain access.\n2. Disabling SSO (Admin Action)\nNote:\nThis step is typically performed by an administrator.\nStep 1: Navigate to SSO Settings\nLog in to the\nDvSum Application\n.\nGo to\nAdministration â†’ Account Settings â†’ SSO.\nStep 2: Disable SSO Live Mode\nLocate the\nSSO Live Mode\ntoggle switch.\nClick the toggle to\ndisable\nSSO.\nStep 3: Confirm the Action\nA confirmation pop-up will appear.\nClick\nOK\nto proceed.\nOnce disabled, all users will be required to log in using passwords instead of SSO.\n3. Temporary Password Delivery\nAfter SSO is disabled, a system-generated email with a\ntemporary password\nis sent to each registered user's email address.\nDidnâ€™t receive the email?\nCheck your\nspam/junk\nfolder.\nVerify that the correct email address is used.\n4. Log In & Reset Password\nUse the\ntemporary password\nfrom the email to log in at\nhttps://app.dvsum.ai\n.\nUpon login, youâ€™ll be prompted to set a\nnew password\n.\nFollow the same password rules mentioned earlier.\nClick\n\"Save\"\nto update your credentials and complete the login process.\nSSO Flow: Resetting Password when SSO is Live\nOnce SSO is enabled in\nLive Mode\n, DvSum no longer stores user passwords. All authentication is handled by your organizationâ€™s Identity Provider (IdP), such as Microsoft Entra (Azure AD), Okta, or Ping.\nPassword reset flow:\nThe\nâ€œForgot Passwordâ€\nlink on the DvSum login page will no longer work.\nUsers must reset their password\ndirectly in the Identity Provider (IdP) system\n.\nAfter resetting the password at the IdP, users can log in to DvSum using their updated SSO credentials.\nDvSum support cannot reset passwords while SSO is Live.\nIf SSO is later disabled:\nDvSum reverts to the standard login mechanism.\nUsers will need to set up new DvSum passwords using the regular password reset process described\nHow to Reset Password on the DI Platform\n.\nReferences:\nOkta Support:\nPassword reset for users created via external IdP must be performed on the IdP.\nOkta Support Article\nMicrosoft Entra (Azure AD) Support:\nUsers must reset their work or school passwords through Azure AD security info or mobile device if locked out.\nMicrosoft Support\nAWS SSO with External Identity Provider:\nPassword reset is not allowed via AWS SSO when using an external identity source.\nAWS Support\nThese references confirm that when SSO is active, password management is handled at the Identity Provider, not within the DvSum application itself. The linked DI Platform article provides step-by-step instructions for standard password resets when SSO is not active.\nNote:\nFor more details on enabling SSO refer\nEnabling SAML-Based Single Sign-On (SSO)\naticle.",
    "scraped_at": "2026-02-02 15:36:06"
  },
  {
    "title": "How to Link Terms to Columns",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/36043768656020-How-to-Link-Terms-to-Columns",
    "content": "In this Article:\nOverview\nAdd a Term to a Column\nOverview\nAssociating terms with columns enhances data governance and traceability by ensuring consistency in definitions. The Business Glossary allows users to link terms to relevant columns, making it easier to maintain standardized terminology. Follow the steps below to add a term to a column.\nAdd a Term to a Column\nGo to the\nDictionary\ndropdown and select the\nGlossary Terms\ntab.\nLocate and click on the term you want to associate with column(s).\nOpen the Term Overview Page\nOn the Overview page, select\nEdit\n.\nScroll down to the\nRelationships\nsection and click the\npencil\nicon to add a relationship.\nClick on\nAdd\nbutton\nAfter clicking on\nAdd\n, choose the following options:\nRelationship Type\n:\nRepresented By\nAsset Type\n:\nDatabase Column\nAsset\n: Search and select the relevant columns (e.g., Address, AddressLine1, AddressLine2, etc.).\nOnce selected, multiple columns can be associated with the term.\nClick on Add\nThe chosen terms are displayed in a tabular form\nScroll to the top of the page and click Done\nBy following these steps, you can efficiently manage term-column relationships.",
    "scraped_at": "2026-02-02 15:36:12"
  },
  {
    "title": "How to Manage Relationships Between Assets",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35968933714196-How-to-Manage-Relationships-Between-Assets",
    "content": "Relationships help define associations between different assets, ensuring better traceability and governance. The Assets Dictionary allows users to establish these connections, making it easier to navigate related data elements. Follow the steps below to add a new relationship to an asset.\nAdd a Relationship:\nOn the Overview page, select\nEdit\n.\nScroll down to the\nRelationships\nsection and click the\npencil\nicon to add a relationship.\nClick on\nAdd\nbutton\nEnter the required details and add the relationship.\nThe relationship type may vary based on the Asset Type, such as \"Governed by\" or \"Represented by.\"\nNote:\nOnly column assets support adding a Reference Dictionary as a relationship.",
    "scraped_at": "2026-02-02 15:36:17"
  },
  {
    "title": "How to Execute Selective Rules in DvSum",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35301308681876-How-to-Execute-Selective-Rules-in-DvSum",
    "content": "In this Article:\nOverview\nStep 1: Navigate...\nStep 2: Apply Filters...\nStep 3: Select Rules\nStep 4: Execute Rules\nOverview\nIn\nData Intelligence (DI)\n, rules help validate, cleanse, and monitor data quality. Sometimes, instead of running all rules at once, you may need to execute specific rules explicitly based on certain conditions.\nBy applying filters in the DQ Dictionary, you can isolate and execute only the necessary rules.\nSteps to Run Rules Explicitly Using Filters\nStep 1: Navigate to the DQ Dictionary Page\nGo to the\nDictionaries\n>\nDQ Dictionary\nsection within your DI environment.\nThis section contains all configured\nData Quality (DQ) rules\nin DI.\nStep 2: Apply Filters to Isolate Specific Rules\nUse the\nsearch bar or filtering options\nto locate the specific rules you need to run.\nYou can filter rules based on various attributes like\nRule ID, Rule Name, Rule Type,\nor\nStatus.\nExample: If you need to run\nDQ-0001234 and DQ-0005678\n, apply a filter to display only these rules.\nAfter filtering, only the relevant rules will be listed.\nStep 3: Select the Rules\nOnce the desired rules are displayed,\nselect them\nby clicking the checkbox next to each rule.\nEnsure that only the required rules are selected before proceeding.\nStep 4: Execute the Rules Using 'Run Rule'\nAfter selecting the rules, click the\n\"Run Rule\"\nbutton located at the\ntop right\nof the page.\nA dropdown will appear with two options:\nRun Online\n:\nExecutes the rule instantly and provides real-time results.\nNote:\nIf you change the tab while the rule is running, the process will get\naborted\n.\nRun Offline\n:\nRuns the rule in the background (via a scheduler).\nResults will be available after execution is complete.\nExecution status can be checked later in the\nJob section\nNote:\nThis option is Recommended for\nlarge datasets\nor when running multiple rules simultaneously.",
    "scraped_at": "2026-02-02 15:36:22"
  },
  {
    "title": "How to re-set password after disabling SSO?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35213710872468-How-to-re-set-password-after-disabling-SSO",
    "content": "To disable SSO, the user should switch off SSO Live mode.\nA confirmation pop-up will be displayed, Click OK.\nAfter SSO is disabled, an email notification with a temporary password will be sent to the user. The temporary password can then be reset to a new one.",
    "scraped_at": "2026-02-02 15:36:27"
  },
  {
    "title": "How to delete a table in Data Dictionary?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35166590464404-How-to-delete-a-table-in-Data-Dictionary",
    "content": "In This Article:\nOverview\nSteps to Enable Table Deletion\nOverview:\nTables can only be deleted if their status is set to 'Deleted'. If an attempt is made to delete tables with any other status, as shown in the screenshot below, the system will display a pop-up message:\n\"No table(s) has been deleted.\"\nTo delete a table successfully, follow the steps below to change its status to 'Deleted' before proceeding with the deletion.\nSteps to Enable Table Deletion\nNavigate to the Administration >Data Source\nOpen the\nData Source\noption from the left-hand menu.\nSelect the Tableâ€™s Source\nLocate and select the source that contains the table you want to delete.\nGo to\nSettings\nand select the\nConnection\noption.\nModify Schema Selection\nClick the\nEdit\nbutton to open the\nSchemas\npage.\nIdentify the schema that contains the table.\nMove the schema from\n'Selected'\nto\n'Available'\nto remove it from the data source.\nPerform a Scan\nInitiate a\nscan\nto update the system.\nOnce the scan is complete, the tableâ€™s status will be updated to\n'Deleted'\n.\nDelete the Table\nOnce the status is set to\n'Deleted'\n, proceed with the table deletion.\nNote:\nFor the Amazon S3 data source, updating the S3 buckets is required instead of moving the schema from 'Selected' to 'Available.'",
    "scraped_at": "2026-02-02 15:36:32"
  },
  {
    "title": "How to Create View in Jobs and Job Executions",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/35132550978452-How-to-Create-View-in-Jobs-and-Job-Executions",
    "content": "In this Article:\nNavigating to Definition and Job Executions\nCreating a Custom View in Jobs\nClone View\nShare View\nDelete View\nExport View\nOverview\nDvSum enables users to create custom views for Jobs and Job Executions, allowing better tracking, management, and analysis. Views help users focus on relevant data by filtering and organizing job records efficiently.\nNavigating to Definition and Job Executions\nOpen the\nAdministration\ntab on the left and choose\nJobs\nmenu.\nFrom the drop down, click on\nDefinition\ntab to access the Job Listing page.\nCreating a Custom View in Jobs\nClick on\nDefinition\nin the left menu to open the Job Listing page.\nSelect\nCreate View\n.\nEnter a\nView Name\n.\nCustomize the view by:\nAdding or removing columns.\nSpecifying criteria for filtering records.\nClick\nSave\nto store the custom view.\nFor details on job creation, refer to the\nCreating a New Job\narticle.\nNote:\nThe user will not be able to Edit/ Share and Save the default view. These buttons will remain disabled.\nThe columns available in the current view will be automatically pre-selected.\nClone View:\nTo create a clone of any view, users can click on the Edit pencil icon next to the view name. This action will reveal the clone button, allowing users to duplicate the selected view.\nShare View:\nThe view can be shared with other users by granting \"Editor\" or \"Viewer\" access.\nUsers with \"Editor\" access have the capability to Edit, Clone, Delete, Share, Save, and Export operations with a view.\nUsers with \"Viewer\" access can perform Share, Save, and Export operations with a view.\nDelete View:\nTo delete a particular view, click on the Edit button, and you will find a delete button available.\nExport View:\nSimply click on the download button, and users can download the records available in the grid as an Excel file.\nNote:\nUsers can also create their own view in Job Executions, following the same procedure as creating a view in Jobs.",
    "scraped_at": "2026-02-02 15:36:38"
  },
  {
    "title": "How to create a Tag",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10673319844884-How-to-create-a-Tag",
    "content": "In this Article:\nOverview\nStep 1: Access Tags\nStep 2: Create a New Tagset\nStep 3: Configure Tagset Information\nOverview\nUser can classify data assets ( data dictionary, field dictionary, analytics dictionary and business glossary) using tags. User can link single and multiple tags to data assets. Lets see how tags can be created and link to data assets:\nStep 1: Access Tags\nTags can be accessed by navigating to\nAdministration > Asset Management > Tags\n.\nBy default following tags are available. You can also use the already-created tags.\nPII classification\nSensitivity\nCustome Tags\nDataset Type\nCertification Tag\nDataset Quality tags\nNOTE\n: Certification tags and Dataset quality tags are non-editable by default.\nStep 2: Create a New Tagset\nProvide Tagset name, and description and click on Add button. The user will be redirected to the tagset configuration page.\nThe tagset detail page would have two section:\nInformation\nTag\nStep 3: Configure Tagset Information\nNow let's see each option in the information section for tagset:\n- Name & Description:\nTagset name and definition would be pre-populated as this information was already provided while creating a tagset.\n- Applicable To\nUsing the Applicable To option, the User can enable/disable tagset for the Dataset, Fields, terms , Model & Report. If applicable To is unchecked for any Data asset, then the tag will not be shown for unchecked data assets.\n- Only Single value Allowed\nUsers can select only one tag for tagset. Multiple selections of tags is not allowed if this option is enabled.\n- Allow New Values During Tagging\nUser can also add his own custom tag in the tagset from Data asset detail page.\nRequire Note when Tagging:\nWhile linking a tag to any data asset user has to provide a note. Note will be shown when user hover over the tag.\n- Make Tags Prominent:\nProminent tags with selected colors will be shown with the Data assets name on top of the detail page and Enterprise search.\nWatch this quick video tutorial on how creating a tag works:",
    "scraped_at": "2026-02-02 15:36:45"
  },
  {
    "title": "Configure Amazon S3 permissions: manage data source in DvSum (Elevated Permissions Required)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360055209472-Configure-Amazon-S3-permissions-manage-data-source-in-DvSum-Elevated-Permissions-Required",
    "content": "In This Article:\nOverview\nDetailed Steps\nStep 1: Add a New User\nStep 2:Â Add Role\nStep 3: Generate Access Key\nNext Steps\nOverview\nDvSum Data Quality can perform data quality checks on data stored in Amazon Simple Storage Service (S3). DvSum uses Amazon Athena to provide access to the files, and it uses AWS Glue Data Catalog to crawl the files and to gather metadata.\nIn order to define a data source in DvSum you must first configure an AWS user with appropriate permissions for\nS3\n,\nAthena\n, and\nGlue\n. Follow the steps below to create a new user with the required permissions or to verify that an existing user has the permissions needed.\nThe detailed steps below explain the process to create a new user in the AWS Console. A command-line version of the same steps is provided as well. Users should follow either the Detailed Steps or the the Command-Line Instructions.\nDetailed Steps\nStep 1: Add a New User\nTypically a new user is created to be used with DvSum. Using an existing user is of course valid as well, but it's important to validate that the user has all permissions documented below.\nTo add a new user, open the AWS Console, navigate to\nIAM â†’ Access Management â†’ User\n, and click on the 'Create user' button.\n1.1 Set User Details\nSet the user name to any valid name, and click on the Next Button.\n1.2 Set Permissions\nSelect \"Attach policies directly\"\n1.3 Create a Policy named \"dvsum-s3-source-policy\"\nClick on the \"Create policy\" button and you will be redirected to the Create Policy page.\nThen click on the JSON button, and paste the JSON provided below. Click the \"Next\" button.\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"ConfirmPoliciesAndPassRole\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:PassRole\",\n\"iam:GetUser\",\n\"iam:ListAttachedUserPolicies\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n1.4 Review and create the Policy\nImportant: the policy name must be exactly the name provide here. The description is optional.\nPolicy name:\ndvsum-s3-source-policy\nPolicy Description: Policy used by DvSum to confirm permissions\nClick the \"Create policy\" button.\n1.5 Add Permissions Policies\n1.5 Add Permissions policies\nReturn to the Create user wizard already in progress from earlier. Select the following permission policies:\nAmazonS3FullAccess\n(AWS managed)\nAmazonAthenaFullAccess\n(AWS managed)\nAWSGlueServiceRole\n(AWS managed)\ndvsum-s3-source-policy\n(customer managed)\nClick the \"\nNext\n\" button.\n1.6 Review and Create User\nReview User details and Permissions Summary. Then click the \"Create user\" button.\nStep 2:Â Add Role\nAWS Glue requires a role with appropriate permissions which will be passed to the crawler when it runs.\n2.1 Create role\nNavigate to\nIAM â†’ Access management â†’ Roles\nand click the \"Create role\" button.\n2.2 Define trust policy\nIn the first step of the create role wizard, select \"\nCustom trust policy\".\nPaste the code provided below. Click the \"Next\" button.\nCopy from here and paste in the Custom trust policy:\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"glue.amazonaws.com\"\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}\n2.3 Add permissions\nAdd the following permission policies to the role.\nAWSGlueServiceRole\nAmazonS3FullAccess\nClick the \"Next\" button.\n2.4 Name, review, and create\nImportant: the role name must be exactly the name provide here. The description is optional.\nRole name:\ndvsum-glue-service-role\nRole Description: Role used by DvSum to grant the Glue Crawler permission to access files\nClick the \"Create role\" button.\nStep 3: Generate Access Key\n3.1 Navigate to Security Credentials\nNavigate to\nIAM â†’ Access Management â†’ Users\n. Select the user you just created, and click the \"Security credentials\" tab.\n3.2 Create Access Key\nClick the \"Create access key\" button.\nSelect \"Command Line Interface(CLI)\". Click the \"Next\" button.\n3.3 Add a description Tag\nA description for the access key is optional, but setting it is a best practice.\nDescription tag value: Access key used by DvSum to access S3, Athena, and Glue services\nClick the \"Create access key\" button.\n3.3 Retrieve access keys\nSave the Access key and Secret access keys. You will use these values when configuring the S3 data source in DvSum.\nNext Steps\nNow that you have an AWS Access Key associated with a user that has all required permissions, the next step is to follow the instructions on how to\nConfigure Amazon S3 as a Data Source in DvSum.\nCommand Line Instructions\nThe steps below achieve the same results as the detailed steps above using the AWS CLI.\n# Create a new user (use any valid name for the user)\naws iam create-user --user-name dvsum-user\n# Attach these 3 AWS managed policies\naws iam attach-user-policy \\\n--policy-arn arn:aws:iam::aws:policy/AmazonAthenaFullAccess \\\n--user-name dvsum-user\naws iam attach-user-policy \\\n--policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \\\n--user-name dvsum-user\naws iam attach-user-policy \\\n--policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole \\\n--user-name dvsum-user\n# Create dvsum-s3-source-policy (Use this exact name)\naws iam create-policy \\\n--policy-name dvsum-s3-source-policy \\\n--policy-document \\\n'{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"GetUserInfo\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:GetUser\",\n\"iam:ListAttachedUserPolicies\",\n\"iam:PassRole\"\n],\n\"Resource\": [\n\"*\"\n]\n}\n]\n}'\n# Attach the policy (your policy-arn will be different from this example)\naws iam attach-user-policy \\\n--policy-arn arn:aws:iam::<account-id>:policy/dvsum-s3-source-policy \\\n--user-name dvsum-user\n# Create role for crawler\naws iam create-role \\\n--role-name dvsum-glue-service-role \\\n--assume-role-policy-document \\\n'{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"glue.amazonaws.com\"\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}'\n# Attach AWSGlueServiceRole policy to role\naws iam attach-role-policy \\\n--role-name dvsum-glue-service-role \\\n--policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n# Attach AmazonS3FullAccess policy to role\naws iam attach-role-policy \\\n--role-name dvsum-glue-service-role \\\n--policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n# Create an access key\naws iam create-access-key --user-name dvsum-user",
    "scraped_at": "2026-02-02 15:36:50"
  },
  {
    "title": "Tracking Data Set Usage in Data Catalog",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/11795434280212-Tracking-Data-Set-Usage-in-Data-Catalog",
    "content": "In This Article:\nOverview\nData Usage Statistics in Data Catalog\nUsage Classifications\nCapturing Data Usage Statistics\nEnable Query Logging (SQL Server / Azure)\nEnable Query Logging (PostgreSQL)\nQuery Logging in Other Sources\nOverview\nDvSum Data Catalog (DC) manages lots of information about data sets--from metadata like column definitions to governance information like the data owner and data steward to data classifications like 'restricted' vs 'public' and much more. Data sets include tables and views in relational data sources, reports in business intelligence platforms, and objects in other applications like SAP and Salesforce.\nOne piece of information that DC tracks about relational data sets (tables and views) is data usage metrics documenting how often the data set is accessed by users via SQL queries. Query logs are maintained by the underlying database, and these statistics are summarized in DC to help users to understand which data sets are used most frequently.\nThese data set usage statistics are tracked in a property called \"Popularity\". This article explains where popularity is available in DC and how to configure databases to allow DC to gather this information.\nData Usage Statistics in Data Catalog\nThe data popularity is displayed in several locations in DC:\nData Dictionary table view\nData Set popup window\nData Set details page\nSearch results for tables\nExamples of data popularity available in DC:\nUsage Classifications\n3 Stars - Heavily used\nTable is used daily over the last 30 days\n2 Stars - Moderately used\nTable is used at least weekly over the last 30 days\n1 Star - Lightly used\nTable is used less than once per week over the last 30 days\nIf no stars are displayed it means either DvSum is unable to read query logs or the table has not been accessed in the past 30 days.\nNote:\nThe popularity score is typically calculated for the last 30 days. The data retention policy for Snowflake tracks user query history for only 7 days. Keep in mind that retention policies and query logging details can be set differently by your corporate database administrators.\nExamples in DvSum Data Catalog (DC)\nData Dictionary table view\nNote: The field \"Popularity\" is an optional field, so add it to your view if it is not visible.\nData Set popup window\nData Set details page\nSearch results for tables\nNote: If search results have multiple tables with the same name, the table with the higher popularity score will be shown first.\nCapturing Data Usage Statistics in Data Sources\nEnable Query logging in SQL Server, A\nzure SQL, Azure Synapse\nUse the\nQuery Store\n.\nThe Query Store is enabled by default for new Azure SQL Database and Azure SQL Managed Instance databases.\nQuery Store is not enabled by default for SQL Server 2016 (13.x), SQL Server 2017 (14.x), SQL Server 2019 (15.x). It is enabled by default in the\nREAD_WRITE\nmode for new databases starting with SQL Server 2022 (16.x). To enable features to better track performance history, troubleshoot query plan related issues, and enable new capabilities in SQL Server 2022 (16.x), we recommend enabling Query Store on all databases.\nQuery Store is not enabled by default for new Azure Synapse Analytics databases.\nEnabling the Query Store can be done in multiple ways.\nEnable Query Store using the Query Store page in SQL Server Management Studio\nIn Object Explorer, right-click a database, and then select\nProperties\n.\nNote: Requires version 16 or later of Management Studio\nIn the\nDatabase Properties\ndialog box, select the\nQuery Store\npage.\nIn the\nOperation Mode (Requested)\nbox, select\nRead Write\n.\nEnable Query Store usingÂ Transact-SQL statements\nUse the\nALTER DATABASE\nstatement to enable the query store for a given database. For example:\nALTER DATABASE <database_name>\nSET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE);\nIn Azure Synapse Analytics, enable the Query Store without additional options, for example:\nALTER DATABASE <database_name>\nSET QUERY_STORE = ON;\nEnable Query logging in PostgreSQL\nTo enable query logging in PostgreSQL, you can modify the configuration file (postgresql.conf) and use SQL statements for more fine-grained control.\nModify postgresql.conf\nYou need superuser privileges to edit this file.\nYou can locate\npostgresql.conf\nin your PostgreSQL data directory.\nCommon locations include:\n/etc/postgresql/<version>/main\n(Linux)\nC:\\Program Files\\PostgreSQL\\<version>\\data\n(Windows)\nEdit\npostgresql.conf\nSet\nlogging_collector\nto '\non\n'\nSet\nlog_statement\nto 'all' to track SQL queries. For reference, these are the valid values:\nnone\n(default): Log no statements.\nddl\n: Log data definition language (DDL) statements like CREATE, ALTER, and DROP.\nmod\n: Log moderate-level statements, including DDL and most of the data manipulation language (DML) statements.\nall\n: Log all statements, including SELECT, INSERT, UPDATE, DELETE, and more.\nUse SQL Statements for Fine-Grained Control\nIf you want to enable or disable query logging for specific databases or sessions, you can use SQL statements. This can be especially useful for debugging or auditing purposes. You can change logging settings on a per-session or per-database basis using the following SQL commands:\nTo enable query logging for the current session only:\nSET log_statement = 'all';\nTo enable query logging for a specific database (replace your_database with the actual database name):\nALTER DATABASE your_database SET log_statement = 'all';\nTo disable query logging for a specific database:\nALTER DATABASE your_database SET log_statement = 'none';\nThese SQL statements will take effect immediately for the current session or database.\nRemember to be cautious with query logging in production environments, as it can generate large log files and potentially impact performance. Always monitor your log files and adjust the log level as needed to balance the need for more information and the need for better performance.\nQuery logging in other sources\nQuery logging is enabled by default for these databases, so DC will typically gather popularity statistics with no additional configuration needed.\nOracle\nSnowflake",
    "scraped_at": "2026-02-02 15:36:56"
  },
  {
    "title": "How to Use Global Filters",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10534035730836-How-to-Use-Global-Filters",
    "content": "In This Article:\nOverview\nHow to apply Global Filters\nEnterprise Search\nListing Pages\nHome Page\nTurn Off Global Filter (Note)\nOverview\nIn the Data Catalog application, users have the catalog of all the sources, but when users want to look into the details of certain sources and data domains, t\nhey have to apply filters again and again on different sections in the application to view the desired data so in order to save the time and effort we have\nGlobal filter where users can choose the sources and data domains as user settings at the global level to view the data in the application.\nHow to apply Global Filters:\nTo apply global filters navigate to the My settings Page, Select the Data sources and Data Domain that you want to add as Filter, and click on the save button.\nGlobal filter will be applied on Enterprise search, Listing pages ( Data Dictionary, Analytics Dictionary, Field Dictionary & Business glossary), and Home page. Let's see each one by one:\nEnterprise Search\nWhen users search any keyword using enterprise search, a global filter would be applied to selected Data sources and domains. Global filters will applicable on all tabs ( Tables, columns Terms, Model,s and reports) of enterprise search.\nNOTE\n: The user can also apply other filters available in enterprise search.\nListing Pages:\nOn Listing pages ( Data Dictionary, Analytics Dictionary, Field Dictionary & Business glossary) global filters gets applied and results will be shown on the basis of filters. Users can remove the Filter using the\nClear All\nbutton.\nHome Page:\nWhen global filters settings are turned on, Feed on the home page will be shown on the basis of global filters.\nNote:\nUsers can turn off the global filter anytime by un-checking the global filter flag for Data sources and data domains on the My settings page. Filter will not get applied on Enterprise Search, Listing pages and Home page\nWatch this quick video tutorial on how to apply and use Global filters in the DvSum app.",
    "scraped_at": "2026-02-02 15:37:05"
  },
  {
    "title": "How To: Mass Update",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10582893963924-How-To-Mass-Update",
    "content": "",
    "scraped_at": "2026-02-02 15:37:13"
  },
  {
    "title": "How to Schedule a Job for Data Sources",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18727787422228-How-to-Schedule-a-Job-for-Data-Sources",
    "content": "In this Article:\nOverview\nScheduling a Scan Job\nConfigure Scheduler Settings\nScan Frequency Options\nDaily Scans\nWeekly Scans\nMonthly Scans\nEnd Conditions\nScan Summary\nVideo Tutorial\nOverview:\nJob schedulers automate and streamline the execution of tasks within web applications, ensuring timely and efficient processing of background jobs, thus enhancing overall system performance and user experience. In the Dvsum Data Catalog application, the scheduler plays a pivotal role in automating the data source scanning process. By setting predefined schedules, the scheduler ensures that scans occur without manual intervention, maintaining data accuracy and relevance. This automation optimizes resource utilization by enabling scans during off-peak hours, enhancing overall system efficiency. The automated scans also provide timely insights into data changes, allowing prompt identification of trends or anomalies.\nWithin the Data Catalog application, users are presented with two methods to initiate a scan. The first involves an immediate on-demand scan achieved by clicking the \"Scan now\" button on the data source's Scan History page.\nThe second option grants users the ability to schedule scans using a scheduler, a process that we will explore in more detail below.\nScheduling a Scan Job:\nNavigate to the Data Source general settings from the detail page. There, the user will see the Scanning Information section which has the scheduling details. By default, it is set to empty as shown below.\nConfigure Scheduler Settings:\nNavigate to Data Sources > Any Source > Settings > General\nIn order to configure scheduler settings, open the edit mode by clicking the \"\nEdit\n\" button. Within this mode, users will find and be able to adjust the desired scheduler settings as shown below.\nScan Frequency Options\nFirst of all, the user has to set the Scan Frequency. Click on the Scan Frequency field dropdown and it will show the following options:\nOne Time:\nThis will create and execute just one job on a selected date and time.\nDaily:\nThis will result in the creation and execution of jobs on a daily basis at a chosen time.\nWeekly:\nThis will create and execute jobs on a weekly basis at a chosen day, date, and time.\nMonthly:\nThis will create and execute jobs on a monthly basis at a chosen date, and time.\nNow from the \"\nStarts On\n\" field, select the Date and Time at which job(s) will be executed.\nDaily Scans:\nOnce the Daily scan frequency is set and the user selects the date and time of the frequency of the scan, the user can also set the day frequency at which job(s) will be scheduled and executed from the days dropdown as shown below.\nBy default, it is set as 1, which means jobs will be executed daily at the selected time. If the user selects it to 2 then jobs will be executed after every 2 days. If I start the scheduling on Monday and set the day frequency to 2, then jobs will be executed on Monday, Wednesday, Friday, and so on every second day.\nIn addition to this, users have the option to repeat the task multiple times within a single day. To enable this feature, users need to click on the checkbox labeled \"Repeat tasks every.\" This action triggers additional text fields to appear, prompting the user to input the frequency in minutes and the total duration in hours for the task repetition. For instance, if a user enters 20 minutes for 1 hour and schedules it for 9:00 PM, a new task will be executed every 20 minutes. This means the initial task will start at 9:00 PM, followed by subsequent tasks at 9:20 PM, 9:40 PM, and concluding at 10:00 PM for the day.\nWeekly Scans:\nOnce the Weekly scan frequency is set and the user selects the date and time of the frequency of the scan, the user can also set the days on which that scan will be repeated. Days checkboxes will appear in the \"\nRepeat on\n\" field and whichever days the user selects, a new scan will be executed on those days at the desired time.\nMonthly Scans:\nOnce the Monthly scan frequency is set and the user selects the date and time of the frequency of the scan, the user can also choose to select the repetition of the scan. Once the Monthly scan option is selected, the \"\nRepeat by\n\" field will populate which will have two options \"\nDay of the month\n\" and \"\nDay of the week\n\". If \"\nDay of the month\n\" is selected, then a new job will be scheduled and executed on the selected date and time of every month.\nEnd Conditions:\nNow closing out the scheduling settings, select the end date/time of the scheduler from the \"\nEnds\n\" field. It has three options in the form of radio buttons which have the following purpose:\nNever:\nThis means that the jobs will always be running on the desired date and time which is set and it will never stop.\nAfter:\nThis means that after certain occurrences, jobs will stop getting auto-created and executed. The user has to enter a certain number of occurrences.\nOn:\nThis means that on a particular date that the user has selected, jobs will stop getting auto-created and executed.\nScan Summary:\nOnce all the settings are done, the Scan Summary will appear at the bottom as shown below. Save the General settings to start the execution of the scheduler.\nVideo Tutorial:\nWatch this quick video tutorial of how to s\nchedule a Job for Data Sources:",
    "scraped_at": "2026-02-02 15:37:20"
  },
  {
    "title": "How to find help using our in-product Help Center in Dvsum App",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/11525283492500-How-to-find-help-using-our-in-product-Help-Center-in-Dvsum-App",
    "content": "In this Article:\nOverview\nHow to Interact with Message Support\nHow to Give Feedback to Different Guides\nOverview:\nLearning about DvSum has always been easy. But we have made onboarding on the Dvsum Data Intelligence app a lot easier and simpler. Users can find help quickly using our in-product help center.\nClicking on \"Help\" within Dvsum App (at the top-right-hand corner) will give you access to interactive guides. There are some detailed guides from adding a new source and scanning it, to searching tables, columns, models & reports, and more. Guides are in the form of Quick Tips, Tutorials, Videos, and Support Documentation (like the article that you're reading right now). All these bits of help can be reached directly within the application by searching a keyword inside the Help center as shown above.\nHow to interact with Message Support?\nIn order to interact with Dvsum Message Support, open the application and click on the HELP menu in the upper right-hand corner of the page. Then select \"Message support\" from the Additional Resources section. Use the Message support widget that pops up to tell us how we can help. Say \"HI\", submit your name & email, and our intake tool will attempt to find a self-service answer from our Help Center to assist you.\nHow to give feedback to different guides?\nDvsum support values your user experience and feedback. In order to give feedback on any of the Quick tips or videos, click on it. A modal will open which will guide you. If the guidance was helpful then select the \"Yes, thanks\" button and submit it. Otherwise, if you have any suggestions or concerns regarding the guidance, click on the \"Not really\" button, type your feedback in the comment box and submit it.\nIn case of giving feedback on a tutorial, select the tutorial and follow all the step-by-step guides. At the end when the tutorial finishes, a feedback section will appear from there you can follow the same steps as shown above to give feedback.",
    "scraped_at": "2026-02-02 15:37:26"
  },
  {
    "title": "Data Import",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/11197801100820-Data-Import",
    "content": "In this Article:\nOverview\nSteps to Data Import\nStep 1: Starting the Import\nStep 2: Preparing the Import File\nNote on Subdomain Handling\nAdditional Notes for Sample File\nNote on Status\nSupported Statuses\nImportant Notes\nImage: Existing Import Status Table Example\nExample: Summary Report for Invalid Status\nExample: Email Notification of Import Failure\nImporting/Updating Values for Custom Attributes\nStep 2.1: Download and Reuse Data for Import\nStep 3: Field Mapping\nStep 4: Import Completion\nStep 5: Import Summary Email\nStep 6: Post-Import Actions (Domain-Specific)\nOverview\nThe Data Import option is available to users with Admin role and to users with Edit access. While the Administration menu option is visible only to Admins, Editors. Users without Editor privileges cannot import.\nWhether you're working with system asset types (e.g., Glossary Terms, Data Dictionary, Analytics Dictionary, Fields & Lineage) or custom asset types, the Data Import functionality enables efficient population of the platform using structured Excel files.\nAdmins and Editors can import up to 50,000 records at a time using\n.xls\n,\n.xlsx\n, or\n.csv\nfile formats. This capability significantly reduces manual effort and accelerates the onboarding and management of metadata assets.\nLetâ€™s explore how the Data Import process works in detail.\nSteps to Data import\nImportant:\nEnsure that the Domain and Sub-domain are created in the application before starting the import process. The import will use these as a reference when uploading data.\nStep 1: Starting the Import\nYou can start an import in two ways:\n1. From the Administration menu â€“ Go to Administration â†’ Data Import, select the Asset Type from the dropdown, and click Next.\n2. Directly from an asset view â€“ Open the listing page for the asset you want to import (for example, Glossary Terms) and click the Import button at the top right. This method works for any supported asset type.\nStep 2: Preparing the Import File\nTo begin, download the sample file and fill in the data as per the instructions.\nImportant:\nField Dictionary Imports\n:\nThe Entity field must now include both the\nData Domain\nand\nSubdomain\nin the format:\nDD.SD.EntityName\nFor example:\nDefault.Default.US_PASSPORT\n.\nThis structure ensures each entity is uniquely identified during the import.\nMore information on above, refer to\nthis article\n.\nNote on Subdomain Handling\nThe Field Dictionary import template does not include a separate Subdomain column. Subdomain is not a required field in the Field Dictionary and is automatically inherited. A separate Subdomain column is available only in the Business Glossary import, where it is mandatory.\nAdditional Notes for Sample File:\nTags\n:\nThe\nTags\ncolumn in the sample file allows multiple tags to be added using the format\n{Tagset Name}={Tag}\n. Multiple tags can be separated by a comma. For example:\nData Classification=Public, Custom Tags=PKR\n.\nData Steward and Data Owner\n:\nThese fields in the sample file must contain valid email addresses of users already added in the DvSum tool.\nFor Glossary Terms file imports only\nThe\nTerm Type\ncolumn must contain one of the\neight predefined term types\n.\nThe applicability of\nBusiness Rules\nâ€”such as Category Data Rule, Pattern Rule, Regex Pattern Rule, Value Range Rule, and Manualâ€”depends on the selected\nTerm Type\n.\nIf the Term Type is Numerical\n:\nOnly\nValue Range Rule\nand\nManual\nare applicable.\nFor\nValue Range Rule\n, use the format:\n{Min Value} - {Max Value}\nExample\n:\n1 - 20\nIf the Term Type is Categorical Attribute\n:\nUse\nCategory Data Rule\nin the format:\n{Categories.Value 1}={Categories.Description}, {Categories.Value 2}={Categories.Description}\nIf the Term Type is Computed Measure\n:\nThe\nTerm Formula\nmust be provided in the same file.\nOnce the file is filled correctly,\nupload the sample file\nthrough the Data Import screen.\nBy default, the\n\"Update Existing Records\"\noption is enabled for\nData Dictionary\nimports, allowing you to update existing records during the import process.\nNote on Status\nIf you include a\nStatus\ncolumn in your import file, the import process validates the value to ensure it is recognized by the platform.\nStatus Values Recognized and Processed During Import\nIf you include a\nStatus\ncolumn in your import file, note the following:\nThe following Status values are\nrecognized and processed\nduring Data Import:\nPublished\nDraft\nModified\nWhen these supported values appear in the import file:\nThe import\ndoes not fail due to Status validation\nThe Status value is\naccepted and processed\naccording to the domainâ€™s configuration (workflow or non-workflow)\nIf your file includes\nunsupported or invalid status values\n, such as:\nDeleted\nNew\nSuggested\nDefault\nAny custom status\nThen:\nThe import will\nfail for those records\nThe import view will\nnot accept those rows\nThe summary report will display the remark:\nStatus has Invalid data\nThis validation helps ensure only valid and supported Status values are processed during import.\nImportant Notes\nYou\ndo not need to include\na Status column in the import template. The system will assign the correct status automatically.\nIf included, the Status column should only contain values recognized by the platform. Any other value will cause the row to fail validation.\nImage: Existing Import Status Table Example\nCaption: Import view showing records with different status outcomes.\nExample: Summary Report for Invalid Status\nCaption: Summary report showing failed record due to unsupported Status value (\"Deleted\") with remark \"Status has Invalid data\".\nExample: Email Notification of Import Failure\nCaption: Email notification from DvSum indicating that the import failed due to invalid data.\nImporting/Updating values for Custom Attributes\nThe Data Import process also supports importing values for custom attributes. Any custom attributes applied to an asset type should be added as a column in the downloadable import templates.\nMove the required custom attribute column from\nAvailable\nto\nAssigned\nso that it appears in the view, allowing users to download the template and populate values for those attributes.\nTo import custom attribute values:\nDownload the import template for the asset type.\nLocate the custom attribute columns (these are added dynamically based on the attributes applied to that asset).\nEnter the values in the respective custom attribute columns.\nUpload the completed template through the Data Import tool.\nExample:\nIf a custom attribute called â€œCredit Card Numberâ€ is applied to your assets, the template will include a â€œCredit Card Numberâ€ column. Filling in this column with values that match the defined format (e.g., 16-digit numbers like\n1234-5678-9012-3456\n) will assign the values to the correct assets once the import is processed.\nAfter entering the values, follow the same steps to import.\nNote:\nValues that do not match the required format (defined when creating the custom attribute) will not be accepted.\nFor more details on creating and managing custom attributes, refer\nCreating and Managing Custom Attributes.\nExcel (.xlsx) File Requirements\nWhen importing data using an Excel (\n.xlsx\n) file, ensure the file is clean before uploading.\nThe Excel file must contain\nonly one worksheet\n.\nAny formatting applied to rows beyond the actual data records must be removed.\nRows that appear empty but contain formatting are treated as records during import.\nNote:\nFailure to meet these requirements may result in additional or incorrect records being imported.\nStep 2.1: Download and Reuse Data for Import\nUsers can also download data from the listing page and import the downloaded file using Data import:\nStep 3: Field Mapping\nFields from the uploaded file will get mapped with Field in dvsum. Mapped Tab will show all fields which are mapped. Once All Required Fields are mapped. Users can Click on\nImport\nbutton and start import\nIf a field is unmapped, the user can map it individually by selecting the appropriate option from the dropdown available in the Fields section under the DvSum column. Similarly, to remove a single field, the user can click the remove icon provided for that field.\nFor bulk actions, the user can utilize the\nReset Field Mapping\nand\nApply Auto Mapping\nbuttons.\nStep 4: Import Completion\nA success screen will be shown to the user when the file is imported successfully.\nI'm Done\nButton will redirect the user to the Data import step1.\nGo to Assets Dictionary\nbutton will redirect the user to the Assets dictionary listing page with respective Asset Type.\nStep 5: Import Summary Email\nAfter 3-5 minutes the user will receive an email with a complete summary of imported Data. Users can also download the summary of uploaded data using the Download button.\nStep 6: Post-Import Actions (Domain-Specific)\nAfter completing the import,\nrecords are not automatically published\n.\nPlease follow these additional steps to publish your records. The post-import steps vary depending on whether your domain has a workflow enabled or not.\nPost-Import Actions â€“ Workflow-Enabled Domains\nPost-Import Actions â€“ Non-Workflow Domains\nPlease refer to the relevant guide to complete your import process appropriately.",
    "scraped_at": "2026-02-02 15:37:32"
  },
  {
    "title": "How to do Target Scan of Data Sources",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10860307282580-How-to-do-Target-Scan-of-Data-Sources",
    "content": "How do you limit your scan to specific schemas?\nInitially, you must add and configure a data source in Dvsum. Once successfully configured and authenticated, a Database field will be visible, containing a list of all available databases. From this list, you'll then proceed to select the specific database intended for the scanning process.\nAfter selecting the database, the next step involves targeting specific schemas for scanning. To achieve this, you can focus your scan on particular schemas to retrieve metadata from the selected ones.\nIf the goal is to scan all available schemas within the chosen database:\nNavigate to the connection settings tab of a source.\nEnsure that the checkbox for \"Limit to specific schemas\" remains unchecked.\nSave the settings.\nInitiate the scan for the source.\nThe scan will comprehensively cover all schemas in the selected database.\nIf the goal is to scan specific schemas:\nCheck the \"Limit to specific schemas\" checkbox.\nSelect the particular schemas intended for scanning.\nIt is possible to choose either a single schema or multiple schemas for the scanning process.\nDvSum supports different Data Sources including RDBMS sources such as\nOracle\n,\nSnowflake\n,\nMy SQL\n,\nSQL Server\n,\nAzure SQL\n, &\nAzure Synapse.\nAdditionally, it includes different Reports-related sources like\nTableau\n&\nPower BI\n. Comprehensive articles are available for each of these sources, providing step-by-step guides on how to add and configure them within the application. To access the detailed instructions for a specific source, simply click on the corresponding link mentioned above, and it will direct you to the relevant article.",
    "scraped_at": "2026-02-02 15:37:37"
  },
  {
    "title": "How to create Governance Views",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10027321551380-How-to-create-Governance-Views",
    "content": "In this Article:\nOverview\nCreate View\nClone View\nShare View\nSaved View\nDelete View\nExport View\nVideo Tutorial\nOverview:\nDvSum enables users to create views on the Data Dictionary, Field Dictionary, Analytics Dictionary, and Business Glossary. Let's begin exploring this functionality.\nCreate View:\nNavigate to the Dictionaries > Data Dictionary option in the left menu. This will take the user to the Data Dictionary listing page, where the Default Table view is initially shown. Choose the option to create a view of your preference.\nOn the \"Create View\" page:\nProvide view name\nAdd/remove column for the view\nSpecify criteria\nNote:\nThe user will not be able to Edit/ Share and Save the default view. These buttons will remain disabled.\nThe columns available in the current view will be automatically pre-selected.\nMulti-Sort\nYou can now sort Governance Views using multiple columns. For example, first by\nTable Name\nand then by\nAssigned To\n.\nThe\nDefault Sort Order\ncan be configured directly in the UI, as shown below:\nTo apply multi-sort, hold the\nCtrl\nkey (or\nCmd\non Mac) and click on the columns you want to sort. Users will then see the rows ordered based on the hierarchy of columns selected, first by the primary column, then by the secondary, and so on.\nExample:\nCtrl+Click on the Business Name column header now includes it in the sorting:\nClone View:\nTo create a clone of any view, users can click on the Edit pencil icon next to the view name. This action will reveal the clone button, allowing users to duplicate the selected view.\nShare View:\nThe view can be shared with other users by granting \"Editor\" or \"Viewer\" access.\nUsers with \"Editor\" access have the capability to Edit, Clone, Delete, Share, Save, and Export operations with a view.\nUsers with \"Viewer\" access can perform Share, Save, and Export operations with a view.\nSaved View:\nThe saved view will appear in the \"Favourites\" section of the views drop-down.\nDelete View:\nTo delete a particular view, click on the Edit button, and you will find a delete button available.\nExport View:\nSimply click on the download button, and users can download the records available in the grid as an Excel file.\nNote\n:\nFor details on refining views using filters, refer to the article:\nHow to Apply Filters in Governance Views.\nVideo Tutorial:\nWatch this quick video tutorial of how to create Governance Views:",
    "scraped_at": "2026-02-02 15:37:45"
  },
  {
    "title": "How to use Certification and Quality Tagset?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/8946385941140-How-to-use-Certification-and-Quality-Tagset",
    "content": "In this article:\nOverview\nRequire Note When Applying Tag\nMake Tags Prominent\nCertification Tagset\nDataset Quality Tagset\nOverview\nCertification and Dataset quality tags have been introduced for Datasets. By default, users are unable to alter these tags, and the checkboxes for 'Require Note When Applying Tag' and 'Make Tags Prominent' come pre-selected. The Certification tag is indicated by the green color, and the Data Quality Tag is represented by the red color. Tags for both tagsets have been pre-configured.\nRequire Note When Applying Tag:\nChoosing this option means users must include a Tag note when adding a tag, and adding a tag without a note is not allowed.\nMake Tags Prominent:\nWhen this option is chosen, tags will appear at the top of the page alongside Dataset/Term/Field names, highlighted in the specified color (Red, Green, Blue). The default color for the new tagset is Blue, but users can customize the tag color based on their preferences.\nCertification Tagset:\nThe certification tagset has only one tag:\nCertified\nOn the Dataset detail page, Certify Button shows when the button is clicked, the user will be directed to the Classification section, and Certified Tag will also get selected automatically with a single click.\nNOTE\n:\nUsers can also select a Classified tag from the classification section.\nEnabling \"Require Note\" prompts a text box for the Certified Tag note when clicking Apply. After entering the note, publishing the Dataset results in the certified tag displaying alongside the Dataset name on the listing reference page.\nDataset Quality Tagset:\nDataset Quality Tagset has the following tags:\nDeprecated\nUnder Maintenance\nWarning\nEdit the classification section and any tag from available tags in Data Quality tags. As Require Note is enabled, click on Apply button a text box will appear to enter the note for Under Maintenance Tag.\nSave the changes and publish the Dataset after adding the note. The Under Maintenance tag will be shown with the Dataset name and listing ref page\nWatch this quick video tutorial on how to use Certification and Quality Tagset:",
    "scraped_at": "2026-02-02 15:37:52"
  },
  {
    "title": "Catalog and Datasets/Reports Approval",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/19390168472852-Catalog-and-Datasets-Reports-Approval",
    "content": "Introduction\nCatalog Tables and Datasets/Reports can be scanned and edited using an approval workflow in the DvSum Data Catalog. Approval workflows are configured for each data sub-domain. When an approval workflow is configured, then Data Stewards are able to edit the tables and datasets/reports and submit the changes for approval. Approvers are then able to approve or reject the updates. The Approval flow already exists in the case of Business Glossary Terms. You can click\nhere\nif you would like more information about the Glossary Approval flow.\nPrerequisites\n1. An administrator must enable data governance.\nAdministration â†’ Account â†’ Module Settings â†’ Governance\n2. Data domains should already be defined.\nThe approval workflows will be defined for a data domain.\nLet's check the step-by-step guide on how this functionality works. So, first of all, we will be adding the data domain and then attaching the data domain to a table or catalog/report.\nAdding a Domain:\nGo to the Administration tab, and select \" Organizations\", and then choose \"Data Domains\" from the dropdown. On the domain listing page, click the\nAdd\nbutton.\nOn Add Domain form provide the Domain name, Description, and Data Steward. And then click on the \"\nEnable Workflow\n\" checkbox.\nThis will enable the workflow for this particular domain. Choose the approvers: There can be single or multiple approvers. Further, choose the workflow type, either \"\nAnyone can approve\n\" or \"\nAll need to approve\n\".\nClick on the Save button, and the domain will be saved. In the listing, it can be seen that the newly added domain will appear on the top with an icon that represents this domain has workflow enabled as shown below.\nThe steward who is selected while creating the Domain will be the one who will be editing the table/dataset/reports and making the changes. Then, changes will be sent for approval to the approver(s). The approver's job will be to review the changes made by the Steward and approve or reject them.\nWorkflow for Tables\nWorkflow can be enabled for Tables by attaching a Domain for which workflow was enabled. Users can add data domain by going on to the detail page of the table from data dictionary tab:\nThe Data domain can be added to the table by clicking on \"Edit,\" which opens the Edit mode of the table. The Data domain for which workflow is enabled should be added in order for the workflow process to be started:\nNote: The Domain for which workflow is enabled will have the workflow symbol in front of it\nOnce the Domain is added, the changes will be needed to publish so that the domain is now attached to the table:\nOnce the changes are published for the workflow to be initiated user is required to edit the table and make any changes in the table once the change is made then in order to publish the change, the table would be required to be published by the approver.\nOnce the changes are made, the user can click on the \"Done\" button. After clicking \"Done,\" there will be an option of \"Submit for Approval\". Once the user clicks \"Submit for Approval,\" then the workflow process can be initiated, and the table will be submitted for approval.\nThe Submitter has the option of Cancelling the submission or otherwise waiting from the approver's end.\nThe approver will receive an email once the table is submitted for approval:\nOn the Approver's screen, the approver can \"Review changes\" that have been made by the submitter.\nAll the fields will appear, and the edited ones will show up as highlighted.\nIf the approver wants to reject the changes, then the \"\nReject\n\" button can be clicked. It will ask the user to add a reason for rejection as a comment. The changes will get rejected.\nTo approve the changes, the user needs to click on the \"\nApprove\n\" button, and the changes that the Steward made will be approved, and the term will get Published.\nSubmitting/Approving multiple Tables using Mass Update:\nIf there are more tables to be submitted for approval, then the user can submit them all for approval at once by using mass update:\nNote: In order to initiate the workflow from listing first tables should have the workflow domain and must be edited (change the data domain, add entity from mass update, or add tags).\nOnce the Tables are submitted for approval, the approver can approve multiple tables as well at once from the Mass update. Go to the Data Dictionary listing again, and there will be a tab \"Assigned to me,\" which will show all the tables that are assigned to the logged-in approver.\nFrom the \"Assigned to me\" tab, select the tables and click on mass update and from the mass update modal, click on the status field and select \"\nApprove\n\" and apply. This will approve all the selected tables. That's how the approver can approve multiple tables at once.\nWorkflow for DQ Rules\nWorkflow for DQ Rules is automatically applied to any Rule associated with a Table that belongs to a Data Domain where workflow functionality is enabled. This ensures that any modifications to DQ Rules follow a controlled approval process, maintaining data governance and change auditability across governed domains.\nFor Tables or DQ Rules that belong to a workflow-enabled Data Domain, a workflow symbol appears on both the Database Table and DQ Rule tabs..\nThe newly created rule can be executed in either\nOnline\nor\nOffline\nmode where both the\nData Steward\nand\nData Owner\ncan initiate the rule execution.\nAfter initial creation, the Steward can modify the rule definition by selecting\nEdit option\n. Changes can be made on theÂ  Rule Definition (Overview), Thresholds, and Workflow. However, any changes made to a workflow-enabled rule require\napprover authorization\nbefore being published.\nFor Example:\nWhen changes are made to the Rule Definition, the\nSubmit for Approval\noption automatically appears.\nOnce all modifications are complete, select Submit for Approval to initiate the workflow process.\nThe rule then enters the approval stage.\nOn the DQ Rule > Overview page, the submitter can view the current workflow status.\nAt this stage, the submitter can either Cancel the Submission or wait for the approverâ€™s response.\nApproval Process\nOnce a rule is submitted for approval, the approver receives an email notification indicating that the DQ Rule is awaiting review.\nThe approver can click Review Changes to view all modifications made by the submitter. All updated fields are highlighted for easier comparison.\nIf the approver decides to reject the changes, the Reject option can be selected. A reason for rejection must be entered as a comment. Once rejected, the submission is discarded, and the rule remains in its previous state.\nTo approve the changes, click Approve. The updates made by the Data Steward are then accepted, and the DQ Rule is published.\nWorkflow for Dataset/Reports\nDatasets and Reports can be found on the BI Models and reports tab, and just like the tables for the workflow to be started, a data domain for which workflow is enabled should be added for the dataset or report:\nOnce the workflow domain is added, the user will be required first to publish the changes and then again edit the dataset/report to initiate the workflow:\nOnce the Dataset/Report is submitted for approval, user can see the workflow on the top right, and email will be received by the approver about the dataset/report that is sent for approval:\nThe approver can \"Compare changes\" for the dataset/report and afterward approve or reject the dataset or report. Once the approver approves the dataset/report, the dataset/report will be published\nJust like the Tables, if there are more than one dataset/reports then they can be submitted for approval through the mass update, and the approver can accept the dataset/report through the mass update:\nSubmitter's account\nApprover's account\nThe overall functionality of the workflow is the same for Tables, Datasets, and reports, so for that reason, the dataset/report part is not covered like the Tables part.\nHow to check the Activities history of the Tables/Reports/Datasets?\nOn the Table/Report/Dataset detail page \"\nManaged by\n\" section, there will be a chat icon. Click on that, and it will show all the history of activities that have been performed on the table/report/dataset. Comments can also be added to the table/report/dataset activity.",
    "scraped_at": "2026-02-02 15:37:58"
  },
  {
    "title": "Approving Glossary Terms",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/15536676968724-Approving-Glossary-Terms",
    "content": "In this Article\nOverview\nPrerequisites\nAdding a New Term\nApproving multiple Terms using Mass Update\nWorkflow for Assets\nApprover Review\nApproving multiple Assets using Mass Update\nVideo Tutorial\nOverview\nGlossary Terms can be created and edited in DvSum Data Catalog using an approval workflow. Approval workflows are configured for each data sub-domain. When an approval workflow is configured, then Data Stewards are able to propose new terms and suggest edits to existing terms. Approvers are then able to approve or reject the updates.\nPrerequisites\n1. An administrator must enable data governance.\nAdministration â†’ Account â†’ Module Settings â†’ Governance\n2. Data domains and sub-domains should already be defined. The approval workflows will be defined for a data sub-domain.\nLet's check the step-by-step guide on how this functionality works. So, first of all, we will be adding the data domain, sub-domain, and then finally the term.\nFor detailed information on adding data domain and sub-domain refer article\n\"Data Domains & Data Sub-Domains\"\nAdding a New Term\nFrom the left navigation bar select\nDictionaries >Glossary\nTerms\nwhich will navigate to the terms listing page. From the listing click on the \"Add Asset\" button.\nFrom the Add Term modal, select the same sub-domain that we created above. Enter the Name, Title, and Description, and select the Term Type. Click on the Save button and the term will be saved redirecting the user to the term detail page.\nOn the term detail page, it can be seen that this newly created term is in the draft status. The term will only get published once it is submitted for approval and gets approved.\nNote:\nThe terms in the Draft state will not appear in enterprise search results. Only Published terms will appear in enterprise search.\nAt the top of the Term detail page, the workflow stages will appear. Currently, it will be in the first stage as Steward would need to make some changes.\nNow below is the Steward's view which they can edit in this term. The below image shows that the Steward has changed the definition.\nTo edit a term, click on the \"Edit\" button. Click on the pencil icon for whichever section needs to be edited. Once changes are made to the particular field, click on the circular check mark button on the same section and then the Done button.\nOnce the Steward edits the term, then the \"\nSubmit for Approval\n\" button needs to be clicked which will send the term for approval to the approver.\nOnce the Steward submits the term for approval, the workflow will move to the second stage which will show the pending approval status along with the approver's name as shown below\nNow next step is for Approver who needs to review the changes made by the Steward. The below image shows the approver's screen.\nClick on the \"\nCompare Changes\n\" button to review the changes that have been made. All the fields will appear and the edited ones will show up as highlighted.\nIf the approver wants to reject the changes, then the \"\nReject\n\" button can be clicked. It will ask the user to add a reason for rejection as a comment. The changes will get rejected.\nTo approve the changes, the user needs to click on the \"\nApprove\n\" button, and the changes that were made by the Steward will be approved and the term will get Published.\nApproving multiple Terms using Mass Update\nOther than that, Approver can approve multiple terms as well at once from the Mass update. Go to the Business glossary listing again and there will be a tab \"Assigned to me\" which will show all the terms that are assigned to the logged-in approver.\nFrom the \"Assigned to me\" tab select some term and click on mass update.\nFrom the Mass Update modal click on the status field and select \"\nApprove\n\" and apply. This will approve all the selected terms. That's how the approver can approve multiple terms at once.\nHow to check the Activities history of the term?\nOn the term detail page \"\nManaged by\n\" section, there will be a chat icon. Click on that and it will show all the history of activities that have been performed on the term. Comments can also be added to the term activity.\nWorkflow for Assets\nFor any asset associated with a workflow-enabled data domain, if the Data Steward makes changes to the asset, the\nSubmit for Approval\nbutton must be clicked. This action sends the asset to the designated approver for validation and approve.\nAfter the Steward submits the asset for approval, the workflow status is displayed at the top of the asset detail page, indicating\nPending Approval\nalong with the approverâ€™s name, as shown below..\nApprover Review\nThe next step is for the Approver to review the changes made by the Steward. The image below shows the Approverâ€™s view.\nClick the\nCompare Changes\nbutton to review the modifications. All fields will be displayed, with any edited fields highlighted for easy identification.\nTo reject the changes, the Approver can click the\nReject\nbutton. A prompt will appear to provide a reason for the rejection as a comment, and the changes will then be declined.\nTo approve the changes, click the\nApprove\nbutton. The modifications made by the Steward will be approved, and the asset will be published.\nApproving multiple Assets using Mass Update\nThe Approver can also approve multiple assets at once using the\nMass Update\nfeature. On the asset listing page, the\nAssigned to Me\ntab displays all assets assigned to the logged-in Approver.\nFrom the\nAssigned to Me\ntab, select the desired assets and click\nMass Update\n.\nIn the\nMass Update\nmodal, click the\nStatus\nfield, select\nApprove\n, and then click\nApply\n. This action will approve all the selected assets, allowing the Approver to process multiple assets at once.\nVideo Tutorial\nWatch this\nTutorial\nto understand the Glossary Security Flow in which the following points are discussed:\n1. Enable Workflow for Sub-Domain\n2. Add New Term and Submit for Approval\n3. Approve the Term\n4. Edit the Term and submit for Approval again\n5. Compare changes and Approve the term\n6. Approve multiple terms",
    "scraped_at": "2026-02-02 15:38:05"
  },
  {
    "title": "Execution Modes, Model & Workflow Configurations in Tool",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/41054809831572-Execution-Modes-Model-Workflow-Configurations-in-Tool",
    "content": "In This Article:\n1. Instruction-Based Logic\n2. Flow-Based Logic\nWhen configuring a Tool, you can choose between two execution modes:\nInstruction-Based Logic\nand\nFlow-Based Logic\n. Each mode defines how the assistant interprets data, applies logic, and integrates workflows.\n1. Instruction-Based Logic\nOverview\nInstruction-Based Logic allows you to configure the assistant using natural language instructions along with dataset descriptions. The assistant interprets these instructions in real time to generate recommendations. This mode is ideal when flexibility and human-readable explanations are needed.\nConfiguration Fields\nExecution Mode\nSelect\nInstruction-Based Logic\nto enable this mode.\nAI Assistant Type\nA text field to define the role or job title of the assistant.\nData Description\nA text field to provide a detailed explanation of the dataset.\nLogic\nA plain-text field to describe the rules and conditions the assistant should apply.\nGuidelines\nA text area for specifying rules or constraints that the assistant should follow when generating outputs.\nModel & Workflow Configuration\nModel Version\nâ€“ Dropdown to select the model version for execution.\nWorkflow JSON\nâ€“ Editable JSON field that must include:\n\"source_configs\"\n\"state_machine\"\nThe JSON editor supports copy, download, and full-screen expand options for easier editing.\n2. Flow-Based Logic\nOverview\nFlow-Based Logic requires a structured JSON flow to define all conditions, steps, and outcomes explicitly. This ensures deterministic execution where outputs are consistent and predictable.\nConfiguration Fields\nExecution Mode\nSelect\nFlow-Based Logic\nto enable this mode.\nAI Assistant Type\nA text field to define the role or job title of the assistant.\nLogic JSON\nA structured JSON editor field to define the Flow-Based Logic (steps, conditions, outcomes).\nThis JSON determines the issue category and drives troubleshooting analysis.\nUI Response Format\nA text field specifying how the assistantâ€™s output should be presented in the UI.\nModel & Workflow Configuration\nWorkflow JSON\nâ€“ Editable JSON field that must include:\n\"source_configs\"\n\"state_machine\"\nThe JSON editor supports expand-to-full-screen for easier editing.\nKey Notes\nInstruction-Based Logic\nincludes a\nModel Version\ndropdown, while\nFlow-Based Logic\ndoes not.\nThe\nWorkflow JSON\nfield is common to both modes and must always contain:\n\"source_configs\"\n\"state_machine\"\nThe Workflow editor supports expand/full-screen mode, copy, and download for ease of use.",
    "scraped_at": "2026-02-02 15:38:10"
  },
  {
    "title": "Co-Pilot in Data Intelligence",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25996549175828-Co-Pilot-in-Data-Intelligence",
    "content": "In This Article:\nDvSum configuration\n1. Add Source\n2. Configure New Chatbot\n3. Analyzing Customer Data from Train Bot\nHow Chat History Works with Copilot\nThis article describes the steps needed to configure Rest API as a source in DvSum Data Intelligence (DI) to use CADDI Chatbots.\nDvSum configuration:\n1. Add Source\nTo create a data source, navigate to Administration â†’ Data Sources â†’ âŠ• Add Source.\nSelect Rest API.\nGive the source a name, and save it.\n2. Configure New Chatbot\nClick on \"New Chatbot\" which will redirect the user to the new chatbot detail page.\nFrom the Definition tab click on the \"Download sample file\" button to download the sample file. Configure the workflow JSON according to the requirements and upload the file.\nThere will be a default prompt already being set but the user can also update the prompt of the bot accordingly. The prompt directly impacts the quality of the results.\nOnce the workflow JSON file is uploaded and the prompt is updated, save the chatbot.\n3. Analyzing Customer data from the Train bot\nNavigate to the\nTrain bot\ntab and type a question containing the customer ID.\nWhen the question is executed, Co-Pilot displays a\nprogress indicator\nshowing the following four steps\nUnderstanding your question\nâ€“ Co-Pilot interprets the query, identifies parameters, and validates inputs.\nSelecting relevant tool\nâ€“ The most appropriate analysis tool is automatically chosen based on the question type and detected parameters.\nFetching data\nâ€“ Relevant records or data sets are retrieved for processing.\nAnalyzing data\nâ€“ The data is examined to extract key insights and generate a summarized result.\nOnce all four steps are completed,\ninsights are displayed automatically\n, giving you a clear summary of findings and any key observations derived from the data.\nThe results display key facts, observations, and recommendations derived from the data. The prompt set from the Definition tab influences these results.\nInsights from the data would show up in different grids that can be analyzed.\nHow Chat History Works with Copilot:\nUsers can ask a variety of questions related to telecom troubleshooting beyond analyzing customer IDs. Copilot utilizes a Q&A chain specifically designed for telecom troubleshooting queries. Users can also ask follow-up questions to receive recommendations or additional troubleshooting assistance for specific customer IDs.\nMult-Language Support in Copilot:\nCopilot supports multiple languages. This functionality enables users to ask questions and receive answers and explanations in their preferred language, enhancing the platform's accessibility and usability for a diverse global user base.\nThe questions shown below were asked in Spanish, and the results and analysis also appear in Spanish.",
    "scraped_at": "2026-02-02 15:38:15"
  },
  {
    "title": "CADDI Chat (Talk to your Data)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18534685180948-CADDI-Chat-Talk-to-your-Data",
    "content": "In this Article:\nIntroduction\nViewing Execution Progress\nSample Questions\nDetailed Questions\nProfiling & Filtering\nSaving the Question as Context\nGenerating Answers Based on Different Tools\nBasic CRUD Operations\nLeft Navigation Bar\nIntroduction\nArtificial Intelligence (AI) plays a crucial role in data analysis by providing a simple interface to extract insights from large and complex data sets. With its ability to process natural language quickly, AI helps people make better-informed decisions, identify patterns and trends, and uncover hidden insights that might otherwise go unnoticed. In DvSum users can talk to their data using\nC\nonversational\nA\nI for\nD\nata-\nD\nriven\nI\nnsights (CADDI). DvSum CADDI is a user-friendly platform where users can ask different questions from their datasets and extract useful insights. The following article explains how CAADI chat can be used.\nThis article is linked to Agents. For more information on Agents, click\nConfiguring CADDI - Creating Agents\n.\nHow to talk to the data in DvSum Data Catalog:\nBefore we start chatting with data, first, we need to create a agent and share it with some users.Â  Here I have a agent shown below, which I have shared with the user. On the agent go to \"Manage Access\" and share the agent with any user.\nOnce the agent is shared,\nclick on \"Talk to Your Data\" from the left Nav. It will open the Conversational page on a new tab.\nNow in the Conversational tab, all the agents that are shared with some users will appear as agents. So we need to select some agent to start the conversation. I'm selecting the agent which I shared above.\nOnce the agent is selected, an About section of that chat will appear on the new chat, which will contain details of the agent i.e. the tables that were added to the agent .\nViewing Execution Progress\nWhen a question is executed in CADDI Chat, a\nprogress indicator\nappears showing the following four steps of processing:\nUnderstanding the question\nâ€“ The query is interpreted, language detected, and validated for clarity.\nIdentification of relevant tables\nâ€“ CADDI determines which tables contain the required data.\nCrafting SQL query\nâ€“ A SQL query is generated to ensure accurate and relevant data retrieval.\nExplanation of how the query works\nâ€“ The system explains the query logic, including filters, joins, and calculations.\nAfter execution, the\nfinal output and query explanation\nare displayed, offering transparency into how the results were derived.\nOn clicking the book icon located on the top right of the chat bar, some sample questions show up. Users can use these questions for analysis:\nOn the CADDI, two types of questions can be asked which are:\nSample Question\nDetailed Question\nSample Questions\nClick on the \"Sample Data\" button, and it will show the sample data of that data set. The Sample question will not have any SQL code generated, and there will be no visualized charts for it. The Sample Data will have the Pivot View.\nFrom the sample data, users can have some basic insights into what data is about and what it looks like. Users can select different columns and apply filters to check the results.\nUsers can see the profiling of the different columns in sample questions by simply clicking on the column heading in the grid:\nUsers can apply some filters and set up that question as a context also by clicking on the context icon:\nNote: For Sample Questions, context can only be set up when some filter is selected\nThe Concept of Profiling and Context will be explained further in this article.\nDetailed Questions\nA detailed question is one for which:\nSQL Code is generated\nVisualized Grid, Charts and Pivot Information is generated\nContext can be set up, and Profiling Information is available\nSQL Query\nWhen the question is asked about any dataset, then for that question, SQL code is generated, it can be seen when users select the \"Show Code\" option below the answer output:\nDifferent Generated Views\nFor the question that is not typed, 3 different views are generated which are:\nChart View (located next to Chart Settings)\nGrid View\nPivot View\nThe Chart View, Chart Settings, and Pivot View are separate detailed topics, and they are not included in the scope of this article. For additional information, you can visit the articles below:\nChart Settings in CADDI\nPivot Tables in CADDI\nThe Grid View shows the data in tabular form of the columns that are fetched from the SQL query:\nAlong with the 3 different views there will be a gear icon of Settings which is present when this is clicked settings tab opens up with further three options:\nGeneral\nChart (Learn more in\nChart Settings in CADDI\n)\nField (Explained in Profiling & Filtering below)\nThe General tab contains the toggle button of \"Human readable format\". By default, this option is turned off:\nWhen this option is turned on then it helps in better reading of the data. For example, if there are large values like in thousands or millions then it will show in a much more readable format:\nProfiling & Filtering\nProfiling shows the distribution of different values of a column which helps in a better understanding of the data. Users can apply filters on the Grid through profiling by selecting particular values. The profiling of any column is opened by clicking the column heading in the grid. When the profiling of any column is opened then the \"Settings ---> Field\" tab is opened up.\nWhen no column is selected and from \"Show settings\", the \"Field\" option is selected then no profiling of the column will be shown. Instead, it will show this:\n2.3.1- Date type column\nFor any date type column user can set different date formats according to the requirement.\nOnce any date format is selected then in the grid that particular date format will be applied:\nNote: When any date format is selected then it applies to every question that was previously asked or will be asked\nThe Grid can be filtered from the visual distribution section. A handle is provided which helps in applying filters according to different values. Once the values are selected then the grid on the left will be filtered accordingly:\nDecimal type column\nThe Profiling of any column that has the data type can be opened by just clicking on the column heading in the grid. The difference is that there are some additional changes provided in its profiling. Users have the option to set decimal values after the point according to the requirement:\nFilters can be applied from the visual distribution section. For this particular example, the handle is not present below the distribution bars because if the distinct values for the columns are less than 20 then there will be no handle provided, users can click on the bars to fill in \"Minimum\" and \"Maximum\" values.\nInteger type column\nThe Profiling can be opened by clicking on the column heading. For the columns having data type there are no additional options, there will be just visual distribution present from where \"Minimum\" and \"Maximum\" values can be selected:\nHere in the above example, the handle is provided below the distribution bars because, for this particular column, the distinct count exceeds 20. The logic that handle should be present or not is the same for columns having data type \"Decimal\" and \"Integer\".\nString type column\nThe Profiling can be opened similarly by just clicking on the column heading present in the grid. The visual distribution for the string-type columns is a little different from other data types. Filters can be applied by selecting different bars and on the grid the filters will be applied accordingly:\nSaving the Question as Context\nThe question that is asked that can be added as context or\nany specific filter applied on the grid can also be added from the Context icon on the top right of the question:\nThe added context can be seen on the top left of the chat bar below. Users can add more than one context from different datasets. If the context is no longer needed, then it can be removed from Clear Context.\nNext Time when any question related to the dataset is added as context this context will be incorporated into the questions also:\nGenerating Answers Based on Different Tools\nOften times users will require some questions to be answered in a visualized form and there would be scenarios where users would want just typed answers. For this purpose before asking a question user can select the tool that will generate the answer based on the selected tool. The three tools are located at the top right of the chat bar:\nThere are 3 different types of Tools that can be selected before writing up any question:\nSQL Query Tool\nData Understanding Tool\nGeneral Knowledge Tool\nSQL Query Tool\nBy default when any CADDI chat is opened the selected tool is the \"SQL Query tool\":\nWhen the selected tool is \"SQL Query Tool\" then when the user asks a question from any dataset then for that answer, SQL code and answer in visualized form (Charts and Grid) will be given:\nData Understanding Tool\nThe Data Understanding Tool can be used where the user wants some information about some columns present in the dataset e.g. range of values or different distributions of column. In this tool SQL Code and Visualized Grid will only be generated if necessary otherwise the answer is usually given in typed form where visualization is not required:\nGeneral Knowledge Tool\nThis tool can be used if the user wants to ask questions that are not related to the datasets present in the Data Catalog application. Any sort of general knowledge questions can be asked by using this tool. One use case of this tool can be for example there are some definitions for some columns for a particular dataset but user wants to know the industrial definition then this tool can be handy. This tool is located next to the Data Understanding tool:\nBasic CRUD Operations\nOn the top left of the Chat, where the name is written, some basic crud operations can be performed:\nUpdate Chat Name & Objective\nRefresh Insights\nDelete Chat\nCollapse All/Expand All Questions\nThe Chat Name can be updated by clicking on the \"Edit\" icon. Users can update the chat name as well as the objective for that chat:\nThe \"Refresh Insights\" button will refresh all the questions present in the chat:\nThe \"Delete\" button will delete the Chat and all the questions that are asked in it:\nThe \"Expand All/Collapse All\" will expand and collapse all the questions present on the chat:\nOn the top left of the chat bar, the agent that is opened will be shown. When a user clicks on it the view is expanded, and all the agents that are shared are shown here. Users can switch from one agent to another:\nLeft Navigation Bar\nWhen any agent is opened, if \"New Chat\" is clicked, then it takes to the Default CADDI window. Users can also scroll around the agents that were created:\nThe \"Find Data\" tab opens up the Home page of the Application on a new tab, and the \"Logout\" tab logs out of the Application:\nChat Preferences\nBelow the \"Find Data\" tab there will be a \"Chat Preferences\" tab. A modal opens up when \"My Preferences\" is clicked.\nUsers have the flexibility to configure a wide range of settings according to their preferences, allowing them to tailor the generated insights to their specific needs and requirements.\nNote:\nFor more details, refer\n\"How to talk to your data\".",
    "scraped_at": "2026-02-02 15:38:21"
  },
  {
    "title": "How CADDI Agents Interpret Questions and Follow-Up Requests",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/45239604841108-How-CADDI-Agents-Interpret-Questions-and-Follow-Up-Requests",
    "content": "Table of Contents\nOverview\nTypes of Questions Supported\nIntent Logic and Processing Chains\nAnalysis Chain Flow\nQA Chain Flow\nAction Chain Flow\nData Elements Extracted from Questions\nAction Execution and Agentic Tools\nSupported Actions\nOverview\nCADDI agents are designed to understand different types of user questions and respond appropriately based on intent. Depending on how a question is phrased, the agent may analyze customer data, answer a general question, or perform a system action.\nThis article explains the types of questions supported, how intent is evaluated, and when each processing chain is used.\nTypes of Questions Supported\nCADDI agents support the following types of user questions.\nCustomer-specific questions that include a unique identifier (e.g. customer ID).\nGeneral or follow-up questions related to a domain or previous conversation.\nRequests to perform actions such as resetting a modem or creating a Jira ticket.\nThe agent determines how to handle each question by evaluating intent flags.\nIntent Logic and Processing Chains\nIntent logic is defined within the agent configuration and controls how incoming questions are routed. Based on the detected intent, the agent selects one of three processing chains.\nAnalysis Chain Flow\nThe analysis chain is used for customer-specific data analysis.\nWhen this chain is used\nThis chain is selected when the question contains a unique identifier. which are values. The question may also consist only of the unique identifier.\nRules\nIf no unique identifier is detected, this chain must not be used. This chain focuses on analyzing customer data across connected systems and does not perform actions.\nExample inputs\nAnalyze\n<unique identifier>\nWhat issue is being faced by\n<unique identifier>\nQA Chain Flow\nThe QA chain is used for general questions and follow-up queries.\nWhen this chain is used\nThis chain is selected when no unique identifier is present, no action is requested, and the question is informational or refers to previous conversation context.\nExample inputs\nWhat does HFC mean in the telecom context?\nCan you identify the issue affecting customer\n<unique identifier>\nand suggest appropriate actions?\nAction Chain Flow\nThe action chain is used when a user explicitly asks the agent to perform an action.\nWhen this chain is used\nThis chain is selected when the user requests execution of a task that involves a real-world operation or a system update. If the request is a follow-up to a previous interaction, the agent internally rephrases the question to ensure the action is executed correctly.\nWhen an action is triggered, the agent executes one action at a time and verifies the outcome before proceeding. If additional steps are required, the agent confirms the result and asks the user how they would like to continue.\nExample actions\nCreate a Jira ticket.\nReset modem.\nCheck modem status.\nUpdate CRM database.\nData Elements Extracted from Questions\nTo process requests correctly, the agent extracts specific data elements from user questions.\nRequired data elements\nA unique identifier represents an identifier for a telecom customer.\nThe agent can extract a minimum of one data element and multiple data elements as needed, based on the userâ€™s question.\nExample\nFor questions that require specific analysis, a unique identifier must be included in the request. This identifier allows the agent to identify the correct entity and retrieve the relevant data. Without it, the agent responds at a general level.\nAction Execution and Agentic Tools\nActions performed by the agent are enabled through\nAgentic Tools\n.\nThese tools define what actions the agent can perform and how those actions interact with external systems such as APIs and databases.\nAgentic Tools are built on predefined connections and commands. Connections define how the agent securely communicates with external systems, while commands define the specific actions that can be executed, such as rebooting a modem, checking modem status, updating customer records, or creating support tickets.\nEach agentic tool includes details such as the tool name, purpose, required inputs, connector type, and execution method. Tools may interact with systems through API-based integrations or database operations, depending on the nature of the action.\nActions are executed only when explicitly requested by the user and follow controlled execution and verification logic to ensure safe and predictable behavior.\nSupported Actions\nThe agent supports multiple predefined actions, including resetting a modem, checking modem status, creating Jira tickets, and updating CRM databases.\nEach action is executed only when explicitly requested and follows strict execution and verification rules.",
    "scraped_at": "2026-02-02 15:38:25"
  },
  {
    "title": "Feedback Analytics",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/28138720331796-Feedback-Analytics",
    "content": "In this Article:\nOverview\nAnalytics Tab Views\nReview Feedback\nActivities\nStatus\nOverview\nFeedback Analytics is designed to manage and understand user feedback easier and more efficiently. All user feedback is displayed in one organized table, making it easy to see everything in one place.\nAnalytics Tab Views\nThe Analytics tab in the agents offers two distinct views:\nChats Analytics\nand\nFeedback Analytics\n. By clicking on\nFeedback Analytics\n, the system will display all questions that have received feedback in chats.\nHere is what to expect from Feedback Analytics:\nIn\nFeedback Analytics\n, the top section includes a \"View\" selector that allows toggling between different views. By default, there is an\nAll Feedback Analytics view\n, but users can create new views based on their preferences.\nBelow this, a comprehensive feedback table displays all the questions that have received feedback in chats.\nTo begin with, there are four tiles named as follows:\nAll Feedback:\nIt displays all the feedback\nApproved:\nIt filters all the questions with Approved Feedback\nRejected:\nIt filters all questions with Rejected Feedback\nTo Review:\nIt displays all questions with the status set to New.\nFollowing that, there are columns:\nQuestion:\nIt displaysÂ the question text\nResponse:\nIt displaysÂ the generated query\nUser:\nIt displaysÂ user name who asked the question\nChat Name:\nIt displays the chat name in which the question is asked\nFeedback Time:\nIt displays the time when the feedback was given\nFeedback:\nIt displays the feedback on the question: Accepted or Rejected\nStatus:\nIt displays the current status of the question\nFeedback Category:\nIt displays the category selected at the time of feedback\nFeedback Text:\nIt displays the feedback text if provided\nReview feedback\nClicking on any question within the table opens the\n\"Review Feedback\"\nmodal, providing more details about the individual feedback items. This modal is divided into two panels:\nThe left panel displays a list of all questions within the current view, enabling users to navigate quickly between different feedback items.\nThe right panel offers information about the selected question:\nFeedback Type\n: Displays the type of feedback, with the full text available on hover.\nCurrent Status\n: Displays the current status of the feedback.\nChat and User Name\n: Provides context by displaying the chat where the feedback was given and identifying the user who submitted it.\nQuestion and Results\n: Displays the specific question and the feedback results.\nActivities\nThe activities icon displays a view of all actions taken regarding the feedback, including status updates and added comments. This ensures that users have a comprehensive history of interactions.\nThe activities icon is also accessible in the feedback listing table.\nStatus\nThe Update status dropdown allows users to update the feedback status directly from the modal.\nAdditionally, there is a mass update option that allows bulk status changes across multiple feedback items.",
    "scraped_at": "2026-02-02 15:38:30"
  },
  {
    "title": "Agent Analytics",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/27034936667924-Agent-Analytics",
    "content": "In this Article:\nOverview\nVisibility of the Analytics Tab\nAnalytics Tab\nAgent Engagement\nTop Users\nChats Listing\nCopying and Exporting Data\nManaging Custom Views\nOverview\nThe new feature in our agent interface: the Analytics tab. This enhancement offers deep insights into agent engagement, top users, and detailed chat metrics.\nVisibility of the Analytics Tab\nThe analytics tab appears only when the agent is shared with other users. To enable the analytics tab, share the agent with at least one other user. Once shared, the analytics tab becomes visible in the interface.\nAnalytics Tab\nThe Agent Analytics is divided into three main sections:\nAgent Engagement\nTop Users\nChats Listing\nAgent Engagement\nThe Agent Engagement section provides a detailed overview of user interactions with the Agent. Here, you will find metrics on how many questions were asked by users and at what times. This section includes a graph that visualizes user activity over time and trends in user engagement.\nTop Users\nIn the Top Users section, you can see a list of users ranked by their activity. This section displays the names of users and the total count of questions each has asked.\nChats Listing\nThe Chats Listing section is a overview of all chat interactions. It starts with four summary tiles:\nTotal Chats:\nTotal number of chats created.\nQuestion:\nQuestions asked in the chats\nWith Feedback:\nQuestions with feedback\nUsers:\nUsers who have created the chat\nBelow these tiles is a detailed listing that includes the following columns:\nChat Name\nTotal Questions\nApproved Feedback\nRejected Feedback\nUser\nLatest Chat Time\nCreated On\nFeedback Count\nObjective\nCopying and Exporting Data\nThe Analytics tab provides options to copy and export data from the Chats Listing section. Users can:\nCopy:\nSelect and copy responses directly.\nCopy with Headers:\nCopy data along with column headers.\nExport:\nDownload chat data for further analysis.\nTo use these options, right-click on any row in the Chats Listing table and choose the desired action from the context menu.\nManaging Custom Views\nThe Analytics tab is designed to be customizable. By default, it features the Chats Analytics view, but users can create and manage custom views to suit their specific needs.\nTo create a new view click on the view, a drop-down will appear click on the Create View.\nA modal will open. Add a name and details for the new view, and it will be created and displayed in the Analytics tab.\nAll the views created by the users will be shown in the drop-down.",
    "scraped_at": "2026-02-02 15:38:36"
  },
  {
    "title": "Training of CADDI Agents",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/27022713765524-Training-of-CADDI-Agents",
    "content": "Training of Agents:\nIn the CADDI Agent detail page, users can refine and enhance the agent's performance through the Training tab. This feature allows users to add and manage concepts that impact the agent's responses, as well as handle feedback to ensure continuous improvement. Here's a step-by-step guide on how to train your CADDI agent effectively.\nIn the CADDI agent detail page, there is a Training tab where users can train the agent. From the Training tab, users can add different concepts that impact the results of CADDI. Users can add concepts as shown below:\nGo to the Training tab in the agent detail page and click on the 'Add Concept' button.\nOnce the 'Add Concept' button is clicked, a modal will open where the user can add and save the concept.\nOnce the user enters the concept, they need to set the scope of the concept. There are three scopes for concepts:\nAgent Level:\nBy default, the concept is set at the Agent level. This means the concept will only be applied to questions asked within this specific agent.\nSource Level:\nThis option can be selected if the concept needs to be added at the source level. This means the concept will be incorporated into all questions asked in any agent within this source.\nAccount Level:\nIf the concept needs to be added at the account level, this option can be selected. This will apply the concept to all agents within the entire account, making it a global scope.\nOnce the scope of the concept is set, click on the Save button, and the concept will be added to the Training tab listing.\n.\nMoreover, when the user provides negative feedback on any of the question results, that feedback shows up in the Training listing as well. Users can edit and delete the feedback and concepts.",
    "scraped_at": "2026-02-02 15:38:41"
  },
  {
    "title": "Question level Menu items for CADDI Agents",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/25334871881364-Question-level-Menu-items-for-CADDI-Agents",
    "content": "In Agents and chat interfaces, each question will be accompanied by a set of menu items, each featuring distinct action buttons for interaction. The functionality of each button is elaborated below.\nUpon selecting a specific question block within the agent interface, a menu of items will be displayed below the results, as illustrated below:\nThe functionality of each menu item is explained below:\nApprove icon\n:\nClicking on this button signifies approval of the question's results, indicating user satisfaction with the generated outcomes. Additionally, a notification confirming the submission of feedback will be displayed upon clicking.\nReject icon\n:\nWhen the reject icon is clicked, a feedback modal will open, allowing the user to provide feedback on why the results appeared incorrect to them. Users can submit the feedback accordingly.\nRefresh Data\n:\nClicking on this button will trigger the re-execution of the generated query, providing the user with fresh data and insights.\nShow code\n:\nWhen clicked, this icon will display the query above the results, representing the query generated for this question. If the query is already visible and the user clicks the icon again, the query block will hide.\nShow Explanation\n:\nClicking this button will display the explanation of the question and the query. Once the explanation appears, clicking the button again will hide the explanation section.\nIf the user seeks further information regarding the content displayed in the Query Explanation, they can refer to the following article:\nQuery Explanation in CADDI\nShow log\n:\nThe generated log will appear once this button is clicked, as shown below.\nDelete question\n:\nThe user can delete the question by clicking this button.\nSave Context\n:\nClicking this button allows the user to save the context, adding the question as a context.\nIf the user seeks further information on how context is added and its end-to-end functionality, they can refer to the following article:\nAdding a context in CADDI and Agents\nUsers can configure default answer view settings from Chat Preferences, such as whether the Query code, Explanation, and Log will appear by default or not. All these settings can be adjusted within Chat Preferences. For detailed information on configuring these default settings, please refer to the following article:\nChat Preferences for CADDI Agents",
    "scraped_at": "2026-02-02 15:38:47"
  },
  {
    "title": "Chat Preferences for Agents",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/24639112445716-Chat-Preferences-for-Agents",
    "content": "Chat Preferences\nThe Chat Preferences option is available in the General Settings, allowing users to configure various settings that will impact the generated insights.\nNavigate to My Settings from the top right corner.\nSelect \"Chat Preference\" from My settings page\nDefault Answer View\n:\nWithin these settings, users can specify their preferred default view for generated insights: either\nChart\nview or Table view. Note that if the data set cannot be displayed in a chart, then it will fall back to a table view of the data.\nDefault Chart Sorting\n:\nIn the Default Chart Sorting settings, users have the option to set the default sorting order for generated insights on charts. They can choose between Ascending or Descending order, which will determine how charts are sorted by default. The image below shows these Default Chart Sorting.\nBy default, the Descending option will be set for users, which can be changed at any time. If the user selects the Ascending option, the chart will be sorted in Ascending order as shown below.\nAscending option was selected and settings were saved\nChart shows up sorted in Ascending order on first Y-Axis\nIf the user selects the Descending option, then the chart will be sorted in descending order as shown below.\nChart shows up sorted in Descending order on first Y-Axis\nConditions under which Default Chart Sorting works\n:\nThere are certain conditions under which this default sorting will work. These default chart sortings will only apply to those questions for which the bar chart shows up by default and the generated query of the question doesn't have the Order By clause. Note that the bar chart will be sorted based on the first Y-axis which is Y1 axis. However, if a chart has been modified by the user, such as changing the sorting or altering the X or Y axis, then the default sorting will be removed.\nThese default sorting settings do not apply to line charts, as sorting is automatically applied to the time series column on the X-axis by default for line charts. Additionally, if a question displays a bar chart by default but the generated query includes an Order By clause, the default sorting settings will not apply to that question\nDefault Answer Outputs\n:\nIn the default answer output, users can specify their preferred output settings, including '\nShow Log\n', '\nShow Code\n', and '\nShow Explanation\n'. By default, admins have 'Show Code' and 'Show Explanation' enabled, while other users default to 'Show Explanation' only.\nWithin these settings, users can select different Answer outputs. If a user toggles on the 'Show Log' option and saves the settings, logs will subsequently appear for each newly asked question.\nOnly log details are displayed for the question.\nIf a user wants the Query block to appear by default with insights, they can toggle on the 'Show Code' option\nOnly the Query code is displayed.\nLikewise, if a user prefers the Query Explanation to appear by default for each answer, they can toggle on the 'Show Explanation' option.\nOnly Query Explanation is displayed.\nAny changes made in the \"Chat Preferences\" will require the user to save the changes at the end.\nRegardless of the settings chosen for Default Answer outputs, users can hide/show Logs, Query code, and Query Explanation at the individual question level. This means that users can adjust these settings for each specific question as per their need, without affecting the default settings. For example, even if Code is set to appear by default according to the chat preferences, a user can choose to hide Code for a particular question if desired. These changes are specific to each question and do not impact the settings for other questions or the default settings.\nThis chat preference page is also available in the Talk to Your Data interface.\nThe chat preferences pages in both 'My settings' and 'Talk to Your Data' are synchronized.",
    "scraped_at": "2026-02-02 15:38:53"
  },
  {
    "title": "Agent Reliability; How Query generation re-attempt works",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/23892690705684-Agent-Reliability-How-Query-generation-re-attempt-works",
    "content": "Sometimes in CADDI, we encounter issues where a question triggers an error state due to an incorrectly generated query. To address this problem, if a question enters an error state due to query generation issues, CADDI will attempt to rectify the error by identifying the mistake in the query and re-attempting query generation. If the initial attempt fails, it will make another try to generate a correct query. Once a correct query is successfully generated and executed, it will provide insights. If the question still fails, the user can provide feedback to help train the agent.\nWhen example shows how query generation re-attempt will look on UI:\nOnce the question goes into a successful state then it will show the insights:",
    "scraped_at": "2026-02-02 15:38:57"
  },
  {
    "title": "Query Explanation in CADDI",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/23891989976084-Query-Explanation-in-CADDI",
    "content": "While we use CADDI to communicate with the database and gain meaningful insights from our data, there's an interesting feature that provides insights into the generated query. After a query is generated for any question, an Explanation section appears below the query block. This section explains the various keywords of the query and how they are used, as well as providing the expected output of the query.\nThe Query Explanation section will have the following details:\nWhat we are looking at:\nThis section will indicate the action the query will perform, such as its purpose and the results it will produce upon execution.\nTables included in the Query:\nThis section will display all the tables that will be used in the query execution, which can range from one to many\nJoin Data:\nIf the generated query employs the \"JOIN\" keyword, indicating that a join operation is being utilized, this section will specify which tables are being joined.\nAggregation and Counting:\nThis section explains which aggregate functions are used in the query and how they are applied to specific columns.\nGrouping Data:\nThis section will tell how data is being grouped in the generated query.\nSorting Order:\nThis section will specify the column for which the \"ORDER BY\" clause is being used and indicate whether the data will be sorted in ascending or descending order.\nFiltering Conditions:\nThis section will detail any filtering conditions utilized in the query, specifying the conditions applied, such as which filters are being used.\nExpected Output:\nThis section will provide an explanation of the expected output of the query, outlining the results it will return upon execution.\nFollowing is an example of how the Query Explanation section will look like:",
    "scraped_at": "2026-02-02 15:39:02"
  },
  {
    "title": "Adding a context in CADDI and Agents",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/22955304388628-Adding-a-context-in-CADDI-and-Agents",
    "content": "In this Article:\nOverview\nAdding context to the questions\nRemoving the added context\nOverview\nIn this article, we'll be covering how we can use contexts in the questions that we ask in CADDI. Adding the context will have a significant impact on our data analysis and insights. Let's discover more about the context in detail.\nWhat is the importance of Contexts and how does it impact the insights which are generated s\nubsequently\n?\nWhen a user adds some context, the system stores this information. Subsequently, when a new question is asked, the system seamlessly integrates the contextual input into the generated queries, facilitating the generation of insights aligned with the provided context. This will help users get more precise insights from the data, contributing to an enhanced data analysis experience.\nAdding context to the questions\nClick on the \"Saved Contexts\" button which is located just underneath the Question text field as shown below.\nOnce the button is clicked, the Context panel will open from the right side of the screen. From that panel click on the \"Add Context\" button which will open up a text field where context can be added.\nThe example below shows the context that is added. Users can also add multiple contexts. Click on the Save button and the context will be saved.\nFrom there on whatever question is asked, will incorporate the saved context. The below example shows how added context is used in the generated query\nResults and insights as shown below are presented in alignment with the user's context.\nQuestions can also be used as a context. After a question is asked, click on the context button located with the actions buttons of the question.\nAfter clicking the context button, the selected question will be added as a context. If any filters are applied from the grid view using profiling, these filters will also be incorporated into the context after the question is set as the context.\nNote\n: Users can add multiple contexts.\nRemoving the added context\nThe user has the option to remove a single context or clear all contexts. If a single context is to be removed, simply click on the cross icon located with each context as shown below.\nOnce it is clicked, that context will be removed from the \"Context in Use\" section but it will still appear in the \"Saved Context\" section as shown below.\nAn essential point to note is that only the contexts in the \"Context in Use\" section are active and will be included in the upcoming questions. Contexts that show up in the \"Saved Context\" section are only there to be added back to the active contexts section if needed.\nThe other option to remove the existing context from the \"Context in Use\" section is by clicking on the \"Clear Context\" button, it will remove all the contexts and move them to the \"Saved Context\" section.",
    "scraped_at": "2026-02-02 15:39:10"
  },
  {
    "title": "Pivot Tables in CADDI",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18540310425748-Pivot-Tables-in-CADDI",
    "content": "In this Article\nIntroduction\nPivot View\nRow Groups\nValues\nColumn Labels\nIntroduction\nImagine that you have a dataset that contains some numerical values, and you want to know the sum of a column, the maximum value of a column, or the minimum value of a column. Then, for every answer, you are required to ask different questions on Agent.Â  Now, instead of asking different questions about maximum value, minimum value, or sum, if you can get all the answers in one place, then it can save a lot of time. This Article explains how Pivot View can be of great help. Now, for this article, you should have the know-how of Agents. The Article on Agents can be found here:\nConfiguring CADDI - Creating Agents.\nPivot View\nWhen any question is asked on Agent from any dataset, there are different views generated, which are:\nChart View\nGrid View\nPivot View\nOur Focus in this article will be related to Pivot View only. The Pivot view can be found next to the Grid View:\nBefore analyzing the pivot view, let us see the grid view so we can see the data that has been brought up in the answer. The Grid View button is located to the left of the Pivot View button.\nIn the Grid, we have Transaction Amounts and Different Transactions. Now, Imagine that for different Transaction Names, I want to know the sum of the Transaction amount or maximum value, so for that, we will be required to ask different questions. Pivot View solves this problem.\nWhen Pivot View is opened, then on top, we have the search bar and the columns that are fetched for that question:\nBelow that, we have 3 options which are:\nRow Groups\nValues\nColumn Groups\nRow Groups\nIn the Row Groups, the columns that will be added will be shown as rows on the left side. The columns can be dragged from the dots present before the column name. In the Row groups, metric columns can not come under the Row Groups\nValues\nUnder the \"Values,\" only metric columns are allowed to be dragged. Metric columns are the columns whose data type is double, int, or decimal. Example of these types of columns is the amount, cost revenue, etc.\nIf dots are clicked on the columns under \"Values,\" then there are multiple options like sum, average, count, max, min, etc. In this way, the user can shift between them, and the grid on the left side will be changed accordingly:\nHere, in one place, the user can view the average, sum, minimum value, and maximum value of a metric instead of typing 3 to 4 different questions.\nColumn Labels\nJust like in Row groups where we added columns as rows, in the column labels, columns added here will be added on top as columns:\nNow, here for different dates and Transaction Names, the user can see the average, maximum, minimum, count, etc. of the transaction amount. That is how Pivot view saves the users from asking multiple questions. All of that information can be seen here. One thing is to be noted: just like Row Groups, metric columns are also not allowed to be dragged here.\nThe Pivot View is provided in both Agents and on CADDI chat. For more information on Agents and CADDI chat their articles can be found here:\nConfiguring CADDI - Creating Agents\nCADDI Chat (Talk to your Data)\n.\nThe example explained in this article is a simple one. Users can customize the pivot view according to their requirements.",
    "scraped_at": "2026-02-02 15:39:15"
  },
  {
    "title": "Chart Settings in CADDI",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18506870411284-Chart-Settings-in-CADDI",
    "content": "In this Article:\nCharts in CADDI\nCharts Settings\nHuman-readable format\nAttributes & Dates section\nMetric Fields section\nCharts in CADDI\nPresenting data in charts or graphical forms enhances understanding and analysis. Visual representations simplify complex information, allowing users to grasp trends, patterns, and outliers quickly. This visual approach fosters more efficient decision-making and enables deeper insights from the data. In CADDI we show data results in the form of charts so that users can have more valuable insights of the data.\nWithin CADDI, three distinct chart types are available: Bar charts, Line charts, and Tile charts. When working with data results that include a Datetime column, the Line chart will be the default visualization, providing insights into temporal trends. Conversely, when the dataset lacks a Datetime column, the Bar chart will be automatically generated, aiding in the visualization of categorical comparisons. The Tile chart, a specialized representation, will exclusively manifest when the data output consists of a singular row and column intersection. Refer to the images below for a visual representation of each chart type.\nLine chart\nBar chart\nTile chart\nCharts Settings\nBoth Line and Bar charts feature customizable settings that empower users to conduct more comprehensive data analyses. To access these settings, simply click on the designated icon located on the charts, as depicted in the image below. Upon clicking, the chart settings panel will seamlessly appear on the right-hand side of the grid, offering users a convenient and intuitive way to fine-tune their analytical experience.\nChart settings panel will appear as shown below.\nOn the settings panel, X-axis and Y-axis fields appear which show which attribute is drawn on X-axis and which Metric fields are drawn on Y-axis. Remember that X-axis will have only one attribute whereas Y-axis can have up to 2 metric fields as there can be Y1 and Y2-axis.\nIf the attribute that is there on the X-axis is clicked, the data on the X-axis will get sorted in ascending order. If it is clicked for the second time, data will get sorted in descending order as shown below.\nClick on the X-axis field.\nData will be sorted in ascending order on the x-axis and on the drawn chart as shown below:\nIf that attribute field is clicked again, data be sorted in descending order on the x-axis and on chart:\nNow talking about the\nY-Axis field\non chart settings. There will be 2 metric fields for Y-Axis. One will be on the Y1 axis and the other one will be on the Y2 axis. So on the\nY-Axis field\non chart settings, if both the checkboxes are checked then both the fields will be drawn on the Y2 axis. And if both the checkboxes are unchecked, then both the fields will be drawn on the Y1 axis. If only one of the metric field's checkboxes is checked then that field will be drawn on Y2 whereas the one whose checkbox is not checked will be drawn on the Y1 axis.\nHuman-readable format\nA toggle labeled \"Human-readable format\" is present. When enabled, it indicating that significant values within both the grid and chart will be presented in a user-friendly manner. This entails rendering values such as 10,000,000 as 10M and 24,000 as 24K for enhanced readability. Should users wish to revert to unformatted values for precise examination, they can conveniently deactivate this feature by toggling it off.\nAttributes & Dates section\nThis section will have all the attributes and dates that can be drawn on X-axis. It will contain the string, date, and number type attributes. Users can click on any of the attributes and select it. Since there can only be one attribute on X-axis so if any of the attribute is selected from this section, then it will be drawn on the chart's X-axis.\nMetric Fields section\nThis section will be disabled in case 2 metrics are already selected. In order to select some metric from this section, a maximum of one metric field should be pre-selected. Once it is enabled then user can select any of the metrics and it will be drawn on Y1-axis.\nIf 2 metric fields are selected then all the fields will be disabled as shown below:\nSo in order to select some metric field, first remove one or both fields from the Y-axis section:\nThen it will get enabled and the user will be able to select any of it:\nOnce changes are made in the chart from settings and the user seeks to revert the chart to its original default configuration, the \"Reset to default\" button can be employed. By clicking this button, the chart will be reset to its initial state, restoring the original settings.",
    "scraped_at": "2026-02-02 15:39:20"
  },
  {
    "title": "Configuring CADDI - Creating Agents",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18497031217428-Configuring-CADDI-Creating-Agents",
    "content": "In this Article:\nIntroduction\nDetailed Steps\n1. Creating an Agent\n2. Train Agent\n3. Detailed Question\n4. Add a Concept\n5. Basic CRUD Operations\n6. Manage Access\nIntroduction\nConversational AI for Data-Driven Insights (CADDI)\nmakes it effortless to interact with your data, understand it, and extract reliable insights\nby using plain English. This article explains how to configure a CADDI Agent.\nDetailed Steps\n1. Creating a Agent\nAgents can be found on the Sources Detail page. Any Source that will have Chat Enabled will have the Agent Tab. Chat Access can be given to any source through the Module Settings. For more information, click on\nUser Role and Module Settings\n.\nAgent Tab can be found on the detail page of the source for which Chat is enabled.\n`\nThe \"Agents\" tab can be seen next to the \"Settings\" Tab. On clicking on Agents, a new Agent can be created:\nWhen a new Agent is created, the user is landed on the Definition Tab of the Ageny. In a Data source, there can be several Tables. Users can either use all the tables in the agent or user can limit the agent to some specific sources by adding tables from the Available Tables.\nAny added table shows up in \"Tables For Chat\". Users can also preview the Table to see the data inside it:\nUser can also see individual columns and their profiling by clicking on the \"Manage Fields\" option. Here Users can select or unselect columns of any Table:\nAfter Adding the necessary Tables, the user can click on the \"Done\" and \"Save\" button to save the changes:\n2. Train Agent\nAfter the user has selected specific tables and saved the changes, along with the \"Definition\" Tab, the \"Train Agent\" tab can be seen. On clicking the \"Train Agent,\" user can see the \"Generate Questions\" button up front and the Chat bar below that where questions can be typed:\nOn clicking Generate Questions, some sample questions show up. Users can use these questions for analysis:\nOn the Agent, two types of questions can be asked which are:\n1- Typed Question\n2- Detailed Question\n3. Detailed Question\nA detailed question is one for which:\nSQL Code and Logs are generated\nVisualized Grid, Charts, and Pivot Information are generated\nContext can be set up and Profiling Information is provided\nPositive or Negative feedback can be given for further improvement (Training)\nLogs Generation & SQL Query\nThe Logs, in simpler words, are the information related to when a question is asked and what goes to the Backend in textual form. By default, the logs are not turned on. To turn the logs on, the user will have to enable them from the Chat Preferences:\nOnce the logs are turned on any question is asked the Logs will be printed, and SQL query generated for that question will be printed below the Logs:\nThe Log details and the SQL code can be hidden from the Hide Code/Log icon located below the answer output:\nDifferent Generated Views\nCADDI generates three different views:\nChart View (learn more in\nChart Settings in CADDI\n)\nGrid View\nPivot View (learn more in\nPivot Tables in CADDI\n)\nThe Grid View shows the data in tabular form of the columns that are fetched from the SQL query:\nAlong with the 3 different views there will be a gear icon of Settings which is present when this is clicked settings tab opens up with further three options:\nGeneral\nChart (Learn more in\nChart Settings in CADDI\n)\nField (Explained in Profiling & Filtering below)\nThe General tab contains the toggle button of \"Human readable format\". By default, this option is turned off:\nWhen this option is turned on then it helps in better reading of the data. For example, if there are large values like in thousands or millions then it will show in much more readable format:\nProfiling & Filtering\nProfiling shows the distribution of different values of a column which helps in a better understanding of the data. Users can apply filters on the Grid through profiling by selecting particular values. The profiling of any column is opened by clicking the column heading in the grid. When the profiling of any column is opened then the \"Settings ---> Field\" tab is opened up.\nWhen no column is selected and from \"Show settings\", the \"Field\" option is selected then no profiling of the column will be shown. Instead, it will show this:\nDate type column\nFor any date type column user can set different date formats according to the requirement.\nOnce any date format is selected then in the grid that particular date format will be applied:\nNote: When any date format is selected then it applies to every question that was previously asked or will be asked\nThe Grid can be filtered from the visual distribution section. A handle is provided which helps in applying filters according to different values. Once the values are selected then the grid on the left will be filtered accordingly:\nDecimal type column\nThe Profiling of any column that has the data type can be opened by just clicking on the column heading in the grid. The difference is that there are some additional changes provided in its profiling. Users have the option to set decimal values after the point according to the requirement:\nFilters can be applied from the visual distribution section. For this particular example, the handle is not present below the distribution bars because if the distinct values for the columns are less than 20 then there will be no handle provided, users can click on the bars to fill in \"Minimum\" and \"Maximum\" values.\nInteger type column\nThe Profiling can be opened by clicking on the column heading. For the columns having data type there are no additional options, there will be just visual distribution present from where \"Minimum\" and \"Maximum\" values can be selected:\nHere in the above example, the handle is provided below the distribution bars because, for this particular column, the distinct count exceeds 20. The logic that handle should be present or not is the same for columns having data type \"Decimal\" and \"Integer\".\nString type column\nThe Profiling can be opened similarly by just clicking on the column heading present in the grid. The visual distribution for the string-type columns is a little different from other data types. Filters can be applied by selecting different bars and on the grid the filters will be applied accordingly:\nSaving the Question as Context\nThe question that is asked that can be added as context or\nany specific filter applied on the grid can also be added from the Context icon on the top right of the question:\nThe added context can be seen on the top left of the chat bar below. Users can add more than one context from different datasets. If the context is no longer needed, then it can be removed from Clear Context.\nNext Time when any question related to the dataset is added as context this context will be incorporated into the questions also:\nGiving Feedback\nOften times there will be some questions that may not get answered, or the answer was not according to the user expectations. To tackle this problem, the concept of giving Feedback is introduced. The feedback can be positive if the user is satisfied with the answer:\nBut there will be times when the user will not be satisfied with the answer or the query generated for that question is wrong, due to which the question may give an error. In this case, the user can give negative feedback:\nIn the negative feedback, the user can select an option like whether the Didn't get the right data. Didn't visualize properly or Other. In addition, the user can explain in plain English about how the question should be approached, or the user can write the correct SQL query that should have been generated:\nWhen the feedback is submitted, this feedback is taken into consideration when the next question will be asked. This is the kind of training that we are doing, so the accuracy of results can be improved over time.\nGenerating Answers Based on Different Tools\nOften times users will require some questions to be answered in a visualized form and there would be scenarios where users would want just typed answers. For this purpose before asking a question user can select the tool that will generate the answer based on the selected tool. The three tools are located at the top right of the chat bar:\nThere are 3 different types of Tools that can be selected before writing up any question:\nSQL Query Tool\nData Understanding Tool\nGeneral Knowledge Tool\nSQL Query Tool\nBy default when any agent is opened the selected tool is the \"SQL Query tool\":\nWhen the tool chosen is \"SQL Query Tool\" then when the user asks a question from any dataset then for that answer, SQL code and answer in visualized form (Charts and Grid) will be given. The questions asked by using the \"SQL Query Tool\" are detailed type questions that were explained in the article above.\nFor Technical users, they can verify which tool is being used. If the selected tool is \"SQL Query Tool\" then on the Log details it can be seen:\nData Understanding Tool\nThe Data Understanding Tool can be used where the user wants some information about some columns present in the dataset e.g. range of values or different distributions of column. In this tool SQL Code and Visualized Grid will only be generated if necessary otherwise the answer is usually given in typed form where visualization is not required:\nGeneral Knowledge Tool\nThis tool can be used if the user wants to ask questions that are not related to the datasets present in the Data Catalog application. Any general knowledge questions can be asked by using this tool. One use case of this tool can be for example there are some definitions for some columns for a particular dataset but user wants to know the industrial definition then this tool can be handy. This tool is located next to the Data Understanding tool:\n4. Add a Concept\nIn the CADDI agent detail page, users can refine and enhance the agent's performance through the Training tab. This feature allows users to add and manage concepts that impact the agent's responses, as well as handle feedback to ensure continuous improvement. In the CADDI agent detail page, there is a Training tab where users can train the agent. From the Training tab, users can add different concepts that impact the results of CADDI. For a step-by-step guide on how to train your CADDI agent effectively go to the following article\nTraining a CADDI Agent\n.\n5. Basic CRUD Operations\nOn the top right of the Agent, some basic operations can be pretty handy for the users. There are two filters which are:\nAsked By\nStatus\nThe \"Asked By\" gives the option of filtering the questions that are asked by different users in the agent. All the different users who have asked the questions in the agent can be seen in this filter:\nThere is another filter \"Status\" that will filter the questions according to different question types. The following are the filter values:\nAll\nTo Review\nFailed to Generate\nApproved\nRejected\nConcepts\nWith the two filters, there are three dots, users have the option of \"Refresh All\", which refreshes all the questions and Expand/Collapse All, which will expand or collapse all the questions present in the agent.\nThe user has the option of Duplicating & Deleting the Agent. If the user wants to work on the same agent, but the user does not want to affect the original agent so the agent can be duplicated. If the user wants to delete the agent, then the option is available along with the duplicate button.\nSome more basic information about the Source and Agent can be found below the Agent name, like questions asked, tables, and Chat exclusion criteria, which are mentioned in the Module Settings for this source. The link to the Module settings article can be found\nhere\n6. Manage Access\nBy default, when any Agent is created, then it is private, and it is indicated by a \"lock\" icon present in front of the name of the agent . Users have the option of sharing the Agent with other users or user groups through Manage Access:\nOn clicking Manage Access, a modal opens up where the user can enter any user group or users with whom he wants to share the Agent. 2 types of Access can be given which are:\nChat Only\nEdit\n\"Chat Only\" access means that the user will only have access to CAADI chat (Talk to your Data). In \"Chat Only\", the user will not have any access to the agent, which is present on the Sources Detail page. Once the user shares the agent and gives \"Chat Only\" Access, another user will receive an email, and a link will be provided.\nThe Link provided in the email takes the user to CAADI chat (Talk to your Data). For more information about CAADI chat (Talk to your Data), go through the article\nhere\nWhen the agent is shared with Access as \"Edit,\" then the other user will have edit access to the Agent that is present on the source detail page. When the agent is shared with other users with access as \"Edit\", then the user will receive an email with a link.\nThe email link takes the user to the \"Definition\" Tab of the Agent. This user can also make changes in the original agent since he has edit access:\nOnce the user has given \"Chat Only\" access or \"Edit\" access, then on Manage access, there is another option of \"Remove Access\", which basically will remove the Chat and Edit access to the other user:\nAll the editors of the agent can change permissions for other users except for the creator of the agent.\nFor here the above user is an Editor of the agent, this user can add new users and remove access from already existing users but the permissions of the agent creator can never be changed",
    "scraped_at": "2026-02-02 15:39:31"
  },
  {
    "title": "How to get a guided set of follow up questions to ask in CADDI",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/16290044151828-How-to-get-a-guided-set-of-follow-up-questions-to-ask-in-CADDI",
    "content": "Before the user starts talking to the data, the user can get a guide of questions that can be asked. The guide depends upon what is the description of the topic and what objective is set by the user. Once you have selected the topic In CADDI, click on the icon on the top right of the question field as shown below.\nOnce the button is clicked, a modal will open which will have the objective field. Enter your objective and click Save.\nOnce the objective is set and saved, the user will get the summary along with the guided set of questions that will relate to the objective.\nUsers can get the guide at any time by clicking the guide icon. The guide questions will generate depending on the description of the chat topic, the objective that is set by the user, and the questions asked (if any) before triggering the guide.\nClick on the \"\nUse this\n\" button and it will copy the guided question into the text field.",
    "scraped_at": "2026-02-02 15:39:35"
  },
  {
    "title": "How to talk to your data",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/15168396488852-How-to-talk-to-your-data",
    "content": "In this Article:\nData Analysis and Artificial Intelligence\nHow Chat History Works with CADDI\nMulti-Language Support in CADDI\nData Analysis and Artificial Intelligence\nArtificial Intelligence (AI) plays a crucial role in data analysis by providing a simple interface to extract insights from large and complex data sets. With its ability to process natural language quickly, AI helps people make better-informed decisions, identify patterns and trends, and uncover hidden insights that might otherwise go unnoticed. In DvSum users have the ability to talk to their data using\nC\nonversational\nA\nI for\nD\nata-\nD\nriven\nI\nnsights (CADDI). DvSum CADDI makes it effortless to interact with your data, understand it, and extract reliable insights.\nNote:\nFor more details on CADDI chat refre \"\nCADDI Chat (Talk to your Data)\n\" article.\nHow to talk to the data in DvSum Data Intelligence\nBefore we start chatting with data, first we need to create a topic and share it with some users. Here I have a chat topic shown below which I have shared with the user. Go to the chat topic detail page and follow the steps below to see how we can share the topic with some other users.\nAfter sharing the chat topic, click on \"Talk to Your Data\" from the left navigation. This will open the Conversational page in a new tab.\nIn the Conversational tab, all the analyses shared with users will appear as topics. Select the desired topic to begin the conversation.\nOnce the topic is selected, an About section of that chat will appear on the new chat which will contain details of the topic i.e. that chat topic is created for which dataset, who are the owners of that dataset, etc.\nClick on the \"Sample Data\" button and it will show the sample data of that data set.\nFrom the sample data user can have some basic insights of what data is about and how it looks like. Users can select different columns and apply filters to check the results.\nOnce sample data is explored generically, now users can move forward by asking different questions to have more to-the-point insights into the data. In the current example I have selected the Order History table so I will try asking questions to explore data further as shown below.\nHere is the result:\nThe result shown above shows a line chart of the insights for which the question was asked. It shows the insights in graphical form of all the orders that were placed recently in the current month. In order to see the results in the form of a grid then click on the \"Toggle View\" button as shown below and it will also show a grid view of the results.\nThe type of chart depends on what query is generated from the question and what results are fetched. If datetime column and metric count in results is greater than or equal to 1 then the line chart will show up as shown in the case above. Whereas if datetime column count is 0 and the attribute and the metric count are greater than or equal to 1 then the Bar chart will show up as shown in the example below.\nWhereas if results just show one row and column then the Tile chart will show up as shown below.\nUsers can also change the topic within the chat. Click on the topics button and it will show up all the topics. Select any and it will show its About section.\nHow Chat History Works with CADDI\nCADDI supports chat history as well, allowing the system to use context from previous questions to provide more accurate and relevant responses to follow-up questions. The system first determines whether a question is a follow-up or a standalone query. If it is identified as a follow-up, it utilizes previous context to provide a more accurate and relevant response, similar to how other advanced agents, such as ChatGPT, operate. The example below illustrates how the system maintains the context of previously asked questions.\nFirstly, the following question was asked: '\nHow many members are from the state of California?\n', and it showed the results.\nAfter that, a follow-up question was asked: '\nShow me how many of them have diabetes and are in their 30s\n.'\nThe second follow-up question didnâ€™t include any specific information about members from California, yet it still identified members from that state. This is because the system used the context from the first question, which mentioned California\nMulti-Language Support in CADDI\nCADDI also supports multiple languages, allowing users to query and receive insights in their preferred language. This feature enhances the platformâ€™s accessibility and usability for a diverse global audience.\nLanguages supported by CADDI include, but are not limited to, German, Spanish, Chinese, French, Portuguese, Japanese, Russian, Italian, and Korean, among others.\nFor example, if the query is submitted in Spanish, both the response and explanation are provided in the same language.",
    "scraped_at": "2026-02-02 15:39:41"
  },
  {
    "title": "Why can I see only 300 exceptions in Analysis tab when Rule Run Result shows more ?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000991833-Why-can-I-see-only-300-exceptions-in-Analysis-tab-when-Rule-Run-Result-shows-more",
    "content": "If you see less records in cleansing workbench than the number of exceptions in the Rule Run Result, below is the reason why.\nPlease refer to the end of this article to know how to view all exceptions in Analysis tab.\nYou may have hundreds of records in your data source, but DvSum will process a limited number of records at a time. Similarly, there is a limited number of exceptions that are displayed in the cleansing workbench.\nWhere do these limits come from?\nEvery table of a source has its own staging configuration from where the administrator can control the number of records to be processed at a time and the number of exceptions which can be displayed at a time. This means if there are 1,241 exceptions (refer to the image above), you will only see 300 of the exceptions. Once these 300 are fixed and the rule is executed again, it will show 941 (1241 - 300) exceptions.\nCan I show higher number of exceptions in extract?\nAllowed Review limit: 300 - 10,000*\n1. When theÂ rule is run in\nonline mode\n(through online execution), it will give the correct exception count in Run Result, but the actual detail is 300 records in cleansing workbench.\n2. WhenÂ the rule is run in\nbatch mode\n(through scheduler), it will still have the same count, and the actual detail extract is up to 10,000 records in cleansing workbench.\n*Note: These are the 2 boundary limits.Â Changing that number below 300 will not decrease and changing from 10,000 to a higher number will not increase the limit.\nAll exceptions in Online Run\nThis small button has the power to show you all exceptions on Analysis tab of rule detail page. By default this button is off and you will see 300 exceptions.\nIf you turn the button on, it will bring upto 10,000 exception records on Analysis tab.",
    "scraped_at": "2026-02-02 15:39:47"
  },
  {
    "title": "How do you select the right Catalog View?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115005116933-How-do-you-select-the-right-Catalog-View",
    "content": "You can determine the information you need in a view and then create one of the following DvSum Catalog views:\nScope View\nRDBMS View\nSAP View\nHere is the detail of all three:\nScope View:\nThis view enables you to specify a logical scope of data relevant to you. You choose an existing data catalog object, set filtering criteria, and arrange fields in a layout that suits your preferences. It's a versatile view applicable to all types of data sources.\nRDBMS View:\nThis view enables you to define a SQL-based view for joining and filtering multiple objects in a Relational Database (e.g., Oracle, SQL Server). It functions similarly to creating a database view, but without the obligation to do so. Subsequently, you can interact with this view in the same way you would with any table.\nSAP View:\nThis view is designed for SAP-ECC and SAP-ISR data sources, allowing you to join various objects within SAP for data profiling and analysis.\nRead more on\nJoining tables to create Blend View",
    "scraped_at": "2026-02-02 15:39:51"
  },
  {
    "title": "Add an IP Address to the White List",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115006294588-Add-an-IP-Address-to-the-White-List",
    "content": "Problem Statement:\nWhen you sign in to DvSum Web Application and you see a warning sign that prevents you from connecting to the Web service.\nReason:\nNot every IP can access the Web Service and use DvSum data quality management application because of the security risks. Your IP must exist in the White listed IP Addresses.\nResolution:\nFor a regular User/Administrator:\nIn case you are a regular User, please visit\nhttps://www.whatsmyip.org/\nand find out your IP Address. Next, request your administrator to add your IP to white list.\nFor an Owner:\nIn case you are an Owner, first you need to know the IP address to be white listed (your own/your companyâ€™s user) or you can simply click on \"Get Current\" and it will fetch the current IP for you.\nStep 1:\nOn the left menu, go to Administrator >> Manage Accounts >> Data Security Tab\nStep 2:\nEnter the IP address in the field below and click on â€œAddâ€\nOnce the IP has been added to the white list, you can see the web service icon on top. You can now run the web service and access DvSum's data quality features.",
    "scraped_at": "2026-02-02 15:39:56"
  },
  {
    "title": "How to add Dataset Group?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/22706867079700-How-to-add-Dataset-Group",
    "content": "Dataset Groups serve as\ntags\nenabling you to categorize your datasets. These tags can be applied to organize datasets, functioning similarly to folders. However, unlike folders, a single dataset can have multiple Dataset Group tags, and conversely, a Dataset Group can include multiple datasets.\nCreate Dataset Group\nA Dataset group can be created by users through Mass Update. Simply follow these steps:\nSelect Dataset -> More Action -> Mass Update.\nAs users input data into the \"Value\" field, previously established Dataset groups will become visible. If the desired Dataset group is not displayed, you can easily create a new one by pressing the Enter key.\nRemove/ Replace Dataset Group\nIn\nMass Update\n, users have the ability to remove or replace existing Dataset groups. You can do this by selecting the Dataset you want, going to \"More Actions,\" and then choosing \"Mass Update.\" From there, you can remove or replace the selected Dataset Group.\nHere are the available choices related to Dataset groups:\nAdd the following Dataset Groups.\nReplace existing Dataset Groups with the following Dataset Group.\nRemove the following Dataset Groups.",
    "scraped_at": "2026-02-02 15:40:01"
  },
  {
    "title": "Guide on using Address Validation API",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360056932812-Guide-on-using-Address-Validation-API",
    "content": "Overview\nThis document is a guide on how to use Dvsum Address Validation API. Dvsum Address Validation API helps the users to validate their address data thus getting a clean and more meaningful and understandable address.\nIn this document, we will learn the following things:\nDvsum Address Validation API endpoint: The REST endpoint to access the amazing Address Validation functionality\nThe HTTP method to ping this endpoint.\nRequest Format: The format in which the data is to be sent to the API.\nResponse Format: The format in which the API will return the validated results.\nError Codes: If some error occurs, error codes help to know what kind of error it is.\nURL\nhttps://apis.dvsum.com/address-validation\nMethod\nThe request type is\nPOST\n.\nAuthentication Header\nRequest header should contain the following header.\nx-api-key: <api-key-value>\nRequest Payload\nThe address data needs to be sent to API in the request body of the POST request. Following is the structure of how the request body is to be created.\n{\n\"addresses\":Â [\n{\n\"address_line1\":Â \"\",\n\"city\":Â \"\",\n\"zip_code\":Â \"\",\n\"country\":Â \"\",\n\"company_name\":Â \"\",\n\"address_line2\":Â \"\",\n\"address_line3\":Â \"\",\n\"state\":Â \"\",\n\"other_address_lines\":Â \"\"\n}\n],\n\"case_format\":Â \"\",\n\"zip_code_format\":Â \"\"\n}\nNow, let us dig deep into each of these fields.\nâ€œaddressesâ€\n(mandatory)\ncontains the address data that is to be validated. It can contain the following fields.\nâ€œaddress_line1â€\nmainly contains the data of the street i.e., street number and street name. However, it can also contain the data that one is supposed to enter in â€œaddress_line2â€ and â€œaddress_line3â€.\nâ€œcityâ€\ncontains name of the city.\nâ€œzip_codeâ€\ncontains zip code. It can be added either in US format or non-US format.\nâ€œcountryâ€\npreferably contains abbreviated name of the country; however, full name can also be provided but in that case matched status will be\nFor example,\nif user wants to enter United States, he/she should enter\nUS\nrather than the full name.\nâ€œaddress_line2â€ (optional)\ncontains the data of Suite/Apartment/Building.\nâ€œaddress_line3â€ (optional)\ncontains c/o information and attention.\nâ€œstateâ€ (optional)\npreferably contains abbreviated name of the state or province, however, full name can also be provided but in that case matched status will be\nFor example, if user wants to enter New York, he/she should enter\nNY\nrather than the full name.\nâ€œcase_formatâ€ (optional)\nrepresents the case format in which the results are to be returned. It can have any of these three values,\nâ€œUPRâ€\n: Converts the results in upper case.\nâ€œLWRâ€\n: Converts the results in lower case.\ncase_format is an\noptional\nfield. Hence, if not provided, its default value will apply which will return results in\nInitCap\nformat. For example, the street address â€œ4000 KRESGE WAY REC DOCKâ€ will be converted to â€œ4000 Kresge Way Rec Dockâ€. Here, you can see that the first letter of each word is in uppercase and the rest are in lowercase.\nFor\nOK\nresult API is doing case sensitive check which means input address is exactly same as matched one then it is considering as ok. Any case change or standardization (Road to Rd) consider as Standardize.\nâ€œzip_code_formatâ€ (optional)\nrepresents the Zip code format in which the results are to be returned.\nFor US, zip code format can have any of these two values,\nâ€œFULLâ€\n: Returns complete zip code which contains 5 initial letters and 4 additional letters in the following format â€œxxxxx-xxxxâ€.\nâ€œINITâ€\n: Returns only the initial 5 letters of the zip code.\nFor non-US, zip code format might not be provided. API will validate it and might add a whitespace. For example, L5T2N7 will get converted to L5T 2N7. In this case, matched status will be standardized.\nzip_code_format is an optional field. Hence, if not provided â€œ\nFULL\nâ€ will be used as its value.\nNote:\nAll the field names are to be provided in lower case.\nOut of these fields, it is mandatory to provide â€œaddress_line1â€, â€œcountryâ€ and either â€œcityâ€ or â€œzip_codeâ€.\nResponse Format\nThe validated address is returned by the API in the form of JSON. Its format is as follows.\n{\n\"results\":Â [\n{\n\"matched_address_line1\":Â \"\",\n\"matched_address_line2\":Â \"\",\n\"extra_information\":Â \"\",\n\"matched_city\":Â \"\",\n\"matched_state\":Â \"\",\n\"matched_zip_code\":Â \"\",\n\"matched_country\":Â \"\",\n\"matched_complete_address\":Â \"\",\n\"matched_location_type\":Â \"\",\n\"matched_status\":Â \"\",\n\"matched_status_name\":Â \"\",\n\"longitude_coordinates\":Â \"\",\n\"latitude_coordinates\":Â \"\",\n\"street_number\":Â \"\",\n\"street_name\":Â \"\",\n\"zip_code\":Â \"\",\n\"zip_code_suffix\":Â \"\",\n\"input_complete_address\":Â \"\",\n\"partial_match\": \"\",\n\"matched_address_types\":Â []\n}\n]\n}\nLet us, dig into each of these fields.\nâ€œresultsâ€\nwill have a list of matched addresses. Address contains the following fields\nâ€œmatched_address_line1â€\ncontains the validated data of the street i.e., street number and street name\nâ€œmatched_address_line2â€\ncontains the validated data of Suite/Apartment/Building.\nâ€œextra_informationâ€\ncontains extra address data, which is usually not part of standard address data, if input address contains any c/o information (phone number, person name etc.).\nâ€œmatched_cityâ€\ncontains validated name of the city.\nâ€œmatched_stateâ€\ncontains abbreviated name of the validated state or province.\nHere, it will still contain abbreviated name even if full name of state or province was provided in the input address. And hence, match status in this case will be Standardized.\nâ€œmatched_zip_codeâ€\ncontains the validated zip code.\nFor\nUS\nIf\nâ€œFULLâ€\nwas provided as zip code format in the input address, full zip code will be provided in the following format 40207-4605.\nIf\nâ€œINITâ€\nwas provided as zip code format in the input address, only initial 5 letters of zip code will be provided. (only 40207 in this case)\nFor\nnon-US\na zip code will either be provided as it is.\nor with an added whitespace will be provided. For example, L5T2N7 will get converted to L5T 2N7. In this case, matched status will be standardized.\nâ€œmatched_countryâ€\ncontains abbreviated name of the validated country.\nHere, it will still contain abbreviated name even if full name of the country was provided in the input address. And hence, match status in this case will be Standardized.\nâ€œmatched_complete_addressâ€\ncontains the complete validated address. It is basically comma separated concatenation of all the above explained matched fields. There is no comma in between\nâ€œmatched_address_line1â€, â€œmatched_address_line2â€\nand\nâ€œmatched_stateâ€, â€œmatched_zip_codeâ€\nâ€œmatched_location_typeâ€\nstores additional data about the specified location. The following values are currently supported:\n\"ROOFTOP\"\nindicates that the returned result is a precise geocode for which we have location information accurate down to street address precision.\n\"RANGE_INTERPOLATED\"\nindicates that the returned result reflects an approximation (usually on a road) interpolated between two precise points (such as intersections). Interpolated results are generally returned when rooftop geocodes are unavailable for a street address.\n\"GEOMETRIC_CENTER\"\nindicates that the returned result is the geometric center of a result such as a polyline (for example, a street) or polygon (region).\n\"APPROXIMATE\"\nindicates that the returned result is approximate.\nâ€œmatched_status_nameâ€\ncontains the complete name of the matched status. It can contain one of these values:\nIncomplete\nOK\nStandardized\nEnrich\nNo match\nEach of these statuses is explained next.\nâ€œmatched_statusâ€\ncontains the status of the matched address in abbreviated format. it may contain the following values based on intelligence:\nINC, OK, SD, ER, NM\nINC (Incomplete)\ni\nndicates that the provided address data is not complete and is returned if any of the below conditions is fulfilled:\naddress_line1 is empty.\ncountry is empty.\ncity and zip_code both are empty.\nOK\ni\ns returned if following conditions are fulfilled:\nmatched_location_type is â€œ\nROOFTOP\nâ€.\nmatched_address_line1 exists.\nmatched_city exists.\nmatched_zip_code exists.\nmatched_country exists.\nInput address and validated address are exactly same without any case difference or if\n\"\ncase_format\"\nis\n\"DEF\"\nthen input address and validated address will be compared as case insensitive.\nSD (Standardized)\ni\ns returned if following conditions are fulfilled:\nmatched_location_type is \"\nROOFTOP\"\n.\nmatched_address_line1 exists.\nmatched_city exists.\nmatched_zip_code exists.\nmatched_country exists.\nThere is only\nZIP Code\ndeference between provided address and validated address (example: 63122-6604, 63122 or 63122, 63122-6604). If\n\"\nzip_code_format\"\nvalue is\n\"INIT\"\nthen it will compare the first 5 characters only.\nThere is\nabbreviated form\nfound in the validated address (example: Henry Street, Henry St).\nER (Enrich)\ni\ns returned if following conditions are fulfilled:\nmatched_location_type is â€œ\nROOFTOP\nâ€.\nmatched_address_line1 exists.\nmatched_city or matched_zip_code exists.\nmatched_country exists.\nValidated address is a bit changed from provided address (example: Henry Road, Henry St).\nThere is only\nZIP Code\ndeference between provided address, validated address (example: 63121, 63122).\nNM (No match)\nis returned if none of the above status is returned.\nâ€œlongitude_coordinatesâ€\ncontains the longitude coordinate of the provided location.\nâ€œlatitude_coordinateâ€\ncontains the latitude coordinate of the provided location.\nâ€œstreet_nameâ€\ncontains the name of the validate street.\nâ€œstreet_numberâ€\ncontains the number of the validated street.\nâ€œzip_codeâ€\ncontains the initial 5 letters of the validated zip code. For example, if 40207-4605 was provided as a zip code, then\nâ€œzip_codeâ€,\nwill contain only 40207.\nâ€œzip_code_suffixâ€\ncontains the last 4 letters of the validated zip code. For example, if 40207-4605 was provided as a zip code, then\nâ€œzip_code_suffixâ€,\nwill contain only 4605.\nâ€œinput_complete_addressâ€\ncontains complete address that was provided to the API. It is basically comma separated concatenation of all the input fields There is no comma in between\n\"address_line1â€, \"address_line2\", \"address_line3\", \"other_address_lines\"\nand\n\"stateâ€, \"zip_code\"\nâ€œmatched_address_typesâ€\narray indicates theÂ typeÂ of the returned result. This array contains a set of zero or more tags identifying the type of feature returned in the result. For example, a geocode of \"Chicago\" returns \"locality\" which indicates that \"Chicago\" is a city.\nThe following types are supported and returned in the matched_address_typeÂ  arrays:\nstreet_address\nindicates a precise street address.\nroute\nindicates a named route (such as \"US 101\").\nintersection\nindicates a major intersection, usually of two major roads.\npolitical\nindicates a political entity. Usually, this type indicates a polygon of some civil administration.\ncountry\nindicates the national political entity and is typically the highest order type.\nadministrative_area_level_1\nindicates a first-order civil entity below the country level. Within the United States, these administrative levels are states. Not all nations exhibit these administrative levels. In most cases, administrative_area_level_1 short name will closely match ISO 3166-2 subdivisions and other widely circulated lists; however, this is not guaranteed as our geocoding results are based on a variety of signals and location data.\nadministrative_area_level_2\nindicates a second-order civil entity below the country level. Within the United States, these administrative levels are counties. Not all nations exhibit these administrative levels.\nadministrative_area_level_3\nindicates a third-order civil entity below the country level. This type indicates a minor civil division. Not all nations exhibit these administrative levels.\nadministrative_area_level_4\nindicates a fourth-order civil entity below the country level. This type indicates a minor civil division. Not all nations exhibit these administrative levels.\nadministrative_area_level_5\nindicates a fifth-order civil entity below the country level. This type indicates a minor civil division. Not all nations exhibit these administrative levels.\ncolloquial_area\nindicates a commonly used alternative name for the entity.\nlocality\nindicates an incorporated city or town political entity.\nsublocality\nindicates a first-order civil entity below a locality. For some locations may receive one of the additional types:Â sublocality_level_1Â toÂ sublocality_level_5. Each sublocality level is a civil entity. Larger numbers indicate a smaller geographic area.\nneighborhood\nindicates a named neighborhood\npremise\nindicates a named location, usually a building or collection of buildings with a common name\nsubpremise\nindicates a first-order entity below a named location, usually a singular building within a collection of buildings with a common name\npostal_code\nindicates a postal code as used to address postal mail within the country.\nnatural_feature\nindicates a prominent natural feature.\nairport\nindicates an airport.\npark\nindicates a named park.\npoint_of_interest\nindicates a named point of interest. Typically, these \"POI\"s are prominent local entities that do not easily fit in another category, such as \"Empire State Building\" or \"Eiffel Tower\".\nIn addition to the above, address components may include the types below.\nfloor\nindicates the floor of a building address.\nestablishment\ntypically indicates a place that has not yet been categorized.\nlandmark\nindicates a nearby place that is used as a reference, to aid navigation.\npoint_of_interest\nindicates a named point of interest.\nparking indicates\na parking lot or parking structure.\npost_box\nindicates a specific postal box.\npostal_town\nindicates a grouping of geographic areas, such as locality and sublocality, used for mailing addresses in some countries.\nroom indicates\nthe room of a building address.\nstreet_number\nindicates the precise street number.\nbus_station, train_station and transit_station indicate the location of a bus, train or public transit stop.\nAn empty list of types indicates there are no known types for a particular address component, for example, Lieu-dit in France.\nâ€œpartial_matchâ€\nindicates that the geocoder did not return an exact match for the original request, though it was able to match part of the requested address. You may wish to examine the original request for misspellings and/or an incomplete address.\nPartial matches most often occur for street addresses that do not exist within the locality you pass in the request. Partial matches may also be returned when a request matches two or more locations in the same locality\nResponse Codes\nFollowing are the response codes that will help user know what kind of response is returned by the API.\nCode\nDescription\n200\n{\n\"results\": []\n}\nIt indicates successful processing of request. The results will have a list of matched addresses.\n400\n{\n\"message\":Â \"InvalidÂ request.Â MissingÂ the\n'addresses' parameter.\",\n\"status\":Â 400,\n\"error\": \"Bad Request\"\n}\nIt indicates a parameter is missing from the request body.\nparameter that will cause this issue is \"addresses\".\n403\n{\n\"message\":Â \"API key is missing or invalid in request header.\",\n\"status\":Â 403,\n\"error\": \"Forbidden\"\n}\nIt indicates that API key was missing or invalid in the request header.\n406\n{\n\"message\":Â \"Expected input format is JSON.\",\n\"status\":Â 406,\n\"error\": \"Invalid input format\"\n}\nIt indicates the request body format is not correct i.e., backend is expecting json and client is sending request in some other format.\n500\n{\n\"message\":Â \"Error while processing request.\",\n\"status\":Â 500,\n\"error\": \"Internal server error\"\n}\nIt indicates that something went wrong on server.\n504\n{\n\"message\":Â \"The server didnâ€™t respond in time.\",\n\"status\":Â 504,\n\"error\": \"Timeout\"\n}\nIt indicates that the request did not finish in time and request got timed-out.\nAPI Request, Response Examples\nExample 1\nLet see an example where the address data is added separately in address line 1,2 and 3.\nRequest Body\n{\n\"addresses\":Â [\n{\n\"address_line1\":Â \"4000Â KRESGEÂ WAY\",\n\"address_line2\":Â \"RECÂ DOCKÂ 1\",\n\"address_line3\":Â \"TAGÂ 0001238279\",\n\"city\":Â \"LOUISVILLE\",\n\"state\":Â \"KY\",\n\"zip_code\":Â \"40219\",\n\"country\":Â \"US\",\n\"company_name\":Â \"BAPTISTÂ HEALTHÂ LOUISVILLE\"\n}\n],\n\"case_format\":Â \"\",\n\"zip_code_format\":Â \"\"\n}\nResponse Body\n{\n\"results\":Â [\n{\n\"matched_city\":Â \"Louisville\",\n\"matched_status\":Â \"ER\",\n\"extra_information\":Â \"RECÂ DOCKÂ 1,Â TAGÂ 0001238279\",\n\"matched_location_type\":Â \"ROOFTOP\",\n\"input_complete_address\":Â \"4000Â KRESGEÂ WAYÂ RECÂ DOCKÂ 1Â TAGÂ 0001238279,Â LOUISVILLE,Â KYÂ 40219,Â US\",\n\"matched_zip_code\":Â \"40207-4605\",\n\"matched_state\":Â \"KY\",\n\"matched_status_name\":Â \"Enrich\",\n\"street_name\":Â \"KresgeÂ Way\",\n\"zip_code\":Â \"40207\",\n\"longitude_coordinates\":Â \"-85.6395549\",\n\"matched_complete_address\":Â \"4000Â KresgeÂ Way,Â Louisville,Â KYÂ 40207-4605,Â US\",\n\"matched_address_line2\":Â null,\n\"zip_code_suffix\":Â \"4605\",\n\"partial_match\": true,\n\"matched_country\":Â \"US\",\n\"street_number\":Â \"4000\",\n\"latitude_coordinates\":Â \"38.2375702\",\n\"matched_address_type\": [\n\"street_address\"\n],\n\"matched_address_line1\":Â \"4000Â KresgeÂ Way\",\n}\n]\n}\nExample 2\nLet see an example where the address data is added only in address line 1.\nRequest Body\n{\n\"addresses\":Â [\n{\n\"address_line1\":Â \"4000Â KRESGEÂ WAYÂ RECÂ DOCKÂ 1Â TAGÂ 0001238279\",\n\"city\":Â \"LOUISVILLE\",\n\"state\":Â \"KY\",\n\"zip_code\":Â \"40219\",\n\"country\":Â \"US\",\n\"company_name\":Â \"BAPTISTÂ HEALTHÂ LOUISVILLE\"\n}\n],\n\"case_format\":Â \"\",\n\"zip_code_format\":Â \"\"\n}\nResponse Body\n{\n\"results\":Â [\n{\n\"matched_city\":Â \"Louisville\",\n\"matched_status\":Â \"ER\",\n\"extra_information\":Â \"RECÂ DOCKÂ 1,Â TAGÂ 0001238279\",\n\"matched_location_type\":Â \"ROOFTOP\",\n\"input_complete_address\":Â \"4000Â KRESGEÂ WAYÂ RECÂ DOCKÂ 1Â TAGÂ 0001238279,Â LOUISVILLE,Â KYÂ 40219,Â US\",\n\"matched_zip_code\":Â \"40207-4605\",\n\"matched_state\":Â \"KY\",\n\"matched_status_name\":Â \"Enrich\",\n\"street_name\":Â \"KresgeÂ Way\",\n\"zip_code\":Â \"40207\",\n\"longitude_coordinates\":Â \"-85.6395549\",\n\"matched_complete_address\":Â \"4000Â KresgeÂ Way,Â Louisville,Â KYÂ 40207-4605,Â US\",\n\"matched_address_line2\":Â null,\n\"zip_code_suffix\":Â \"4605\",\n\"partial_match\": true,\n\"matched_country\":Â \"US\",\n\"street_number\":Â \"4000\",\n\"latitude_coordinates\":Â \"38.2375702\",\n\"matched_address_type\": [\n\"street_address\"\n],\n\"matched_address_line1\":Â \"4000Â KresgeÂ Way\",\n}\n]\n}\nHere, we can see that even if data was only added in address line 1, we were still able to get the expected validated address from the API and non-address related information is added in â€œ\nextra-information\nâ€ property.\nAPI Versioning\n1) All new changes to API will be published as new API Version. API consumer will be able to use both i.e. latest and previous versions of Address validation API using same API key. Currently only one version is available i.e.\nhttps://apis.dvsum.com/address-validation\n2) API consumer will have existing and new versions available for testing on Prod environment\n3) Every new version will be published with version number in URI i.e\nhttps://apis.dvsum.com/address-validation/v1\nhttps://apis.dvsum.com/address-validation/v2\nAddress Validation Rule\nThis is a\nlink\nto Address Validation rules article.",
    "scraped_at": "2026-02-02 15:40:05"
  },
  {
    "title": "How to Configure Email Notifications ?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360045674733-How-to-Configure-Email-Notifications",
    "content": "In DvSum, there are multiple modules which send emails to notify information about respective modules. Most popular modules are Job Scheduling (Rules, Profiling, Batch), Data Quality Workflows and Data Management workflows. Previously, there was no control defined at account level for these emails to be received. With Email Optimization,Â we will be giving the control on user profile page (click user name at top right corner + click Profile option in the drop-down to navigate to User Profile) to set the frequency of the Notification Alert type.\nYou will be provided with the notification settings on the profile page through which respective flags can be controlled.\nNotifications settings are segregated into following parts\nNotification Frequency\nNotification Type\nNotification Frequency\nInstant:\nNotifications will be received as they were previously. Emails will be received right away as the event gets completed.\nHourly:\nNotifications will be received on the hourly basis i.e all the set of actions performed in previous hour will be consolidated in a single email and then sent to the respective user. No emails will be received instantly then.\nDaily:\nNotifications will be received on the daily basis i.e all the set of actions performed in previous day will be consolidated in a single email and then sent to the respective user. No emails will be received instantly or hourly then.\nNever:\nNotifications will never be received on setting Never as frequency from the list.\nNotification Type\nThere are five standard notification types in DvSum application. Below are the details for each notification type.\nData Management Workflow Notifications\nEnable to receive the following notifications for data management workflows\nWorkflow Initiation Request\n:\nEnable to receive notification whenever new Workflow request is started\nWorkflow Completion\n:\nEnable to receive notification whenever workflow gets finished\nWorkflow Cancellation\n:\nEnable to receive notification whenever workflow gets cancelled\nWorkflow Step Assignment\n:\nEnable to receive notification whenever step is assigned to you\nWorkflow Step Assignee Changed\n:\nEnable to receive notification whenever step is reassigned to someone else\nWorkflow Consecutive Steps\n:\nEnable to receive notification to your further next step\nWorkflow Step Rejection\n:\nEnable to receive notification whenever step assigned to you gets rejected\nData Quality WorkflowÂ Notifications\nEnable to receive notifications for data quality workflows\nOn proceeding to your own next step\n:\nEnable to receive notification when you are assigned to in the next step\nScheduled Rule EmailÂ Notifications\nEnable to receive scheduler rule email notifications\nReceive Email only with Exceptions\n:\nEnable to receive notification with exception rule only\nScheduled Batch EmailÂ Notifications\nEnable to receive scheduler batch email notifications\nReceive Email only with Exceptions\n:\nEnable to receive batch notification with exception rules only\nScheduled Profiling Email\nEnable to receive schedule profile email notifications\nAdd on Feature -Â Receive Job Email as Creator\nThere was a request from our customer that they do not want to receive the email if they have scheduled a job unless they are explicitly added in the job as an internal recipient. So if it is disabled and you schedule a job with yourself added as recipient in that job, you will not be receiving the job completion email.\nEnable to receive schedule email if you create the job.\nNote: If you are the job creator and you have turned off the notification and added external user in that job, the external user will not be receiving the job completion email. The resolution is that you must have to select at-least one internal user in that job.",
    "scraped_at": "2026-02-02 15:40:10"
  },
  {
    "title": "Data Security Admin",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/21781811462676-Data-Security-Admin",
    "content": "Overview\nIn an era where data privacy and security are paramount, the need to fortify sensitive information within applications is more critical than ever. This article explores the imperative task of enhancing data security and privacy within the DvSum application.\nLet's start looking into this.\nAdd Data Security Admin\nOwner Account can add Data security admin from Administration -> Manage Account -> User Security.\nOnly Data security admins will have access to Hide/ Unhide Data Catalog and apply masking on columns.\nNote:\nThe user must have Admin permission to the Data Catalog at the Role level.\nNow login with the Data Security Admin account, and you will see the Hide/unhide menu for the Data Catalog Grid.\nHide/ Unhide Table & Columns:\nTable Dictionary:\nUsers can mark any table as hidden. The hidden tables won't show at:\nData Catalog - Table Dictionary\nProfiling Listing page\nProfiling Page search field\nElastic Search\nOn the Rules Creation form\nExported Excel File\nTo view hidden Tables select the checkbox\nShow Hidden Tables\n, all hidden tables in the grid will be shown.\nUsers can unhide the Table, by selecting the row and clicking on the unhide menu\nColumn Dictionary:\nWhen a column is marked as hidden, The hidden column won't show at:\nData Catalog - Column Dictionary\nTable Profiling Detail Page - Profiling Tab\nTable Profiling Detail Page - Data Quality Tab\nTable Profiling Detail Page - Field Configuration Tab\nTable Detail Page - Field Tab\nTable Detail Page - Sample Data\nOn the Rules Creation form\nRule Detail Page - Data Tab\nRule Detail Page - Definition Tab\nExported Excel File\nTo view hidden Columns select the checkbox\nShow Hidden Columns\n, all hidden Columns in the grid will be shown.\nUsers can unhide the column, by selecting the row and clicking on the unhide menu.",
    "scraped_at": "2026-02-02 15:40:15"
  },
  {
    "title": "Data Masking",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/21103301641748-Data-Masking",
    "content": "Overview\nDvSum Data Quality (DvSum DQ) provides the ability to mask sensitive data through the use of tags. Tag values include Public, Internal, Restricted, and Sensitive. Details about the effect of different tags and how to set them are explained in this article.\nConfiguring Data Security Admins\nIn order for a user to specify the tag values that control whether data is masked, the user must be given the Data Security Admin privilege. Steps to enable this privilege:\nWhile logged in as the account owner, navigate to Administration â†’ Manage Account â†’ User Security â†’ Data Security Admins\nSelect and add the relevant users. Likewise, remove users when appropriate.\nMasking Columns\nUsers with the Data Security Admin privilege are able to view and set the value of the Data Classification property.\nData masking in DvSum DQ serves as a privacy shield, obscuring sensitive information with masked placeholder values of \"X\". A data security admin can mask any column using the Data Classification property.\nNavigate to Data Catalog â†’ Column Dictionary.\nSelect the desired columns, then click \"Mass Update\".\nSet the Data Classification property to the desired value.\nSensitive\n: The column data will be masked as \"XXX \" all across the application.\nRestricted\n: The column will be marked as a Hidden Column, so users won't see the column.\nInternal\n/ Public\n: The column data will be visible (default situation).\nWhen the column is marked as sensitive, an icon will be shown with the column name and the data for the sensitive column will be masked as \"XXX\" throughout the application.\nWhen the rule is executed on the table that has a masked column, the exported Excel file will show masked Data for the sensitive Column.",
    "scraped_at": "2026-02-02 15:40:20"
  },
  {
    "title": "Manage Roles",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360052075952-Manage-Roles",
    "content": "There are different set of users who use DvSum application. If Admin wants to provide access to particular set of users/groups or if he wants to restrict the access of certain modules, here's how to do it.\nManage Users\nStep 1:\nIn this section, there is a field \"Module Access Role\". In this column, the default roles will be displayed as same as Roles for existing users. For instance; if an existing user has an 'Admin' role then it will also appear as 'Admin' in Module access role column as shown below:\nStep 2:\nIf you create a new user and wants to give it a different level of access, then, first select the user, click the 'Edit user' button. It will open up an Edit user interface where you can see the new field added as 'Manage Access Role'. It will have all the existing default roles, also the new roles that are added from which you can select. For instance; you select 'Analyst' role for user 'Admin' and hit Save.\nNote:\nNow it will have restricted access that is given at Analyst level. Please note that even though the user role is set as 'Admin' but the preference of access will be granted according to option set in 'Module Access Role'.\nManage Roles\nStep 3: Default Roles and access\nPlease note that for existing users, their default roles i.e Owner, Admin, user, super user level will have no change. Also, the default roles (Owner, Admin, user, super user) cannot be edited or deleted from here and there will be no 'Created by' and 'Modified By' data in the cell. These roles will be generated by default here.\nBy default Owner will have access to all tabs. Admin will have access to Admin tab, Reference dictionary and Batch execution. Super user will have access to reference dictionary and Batch execution. Normal user will have no default access at all.\nBut for custom role(s) (custom role that is created by user), these permissions can be changed as explained in step 7\nOwner\nAdmin\nSuper User\nUser\nAdministration tab\nâœ“\nâœ“\nâœ•\nâœ•\nManage Account Tab\nâœ“\nâœ•\nâœ•\nâœ•\nDelete a user from Admin Tab\nâœ“\nâœ•\nâœ•\nâœ•\nReference Dictionary\nâœ“\nâœ“\nâœ“\nâœ•\nBatch Execution\nâœ“\nâœ“\nâœ“\nâœ•\nStep 4: View Users\nIf you want to view list of users that have a role set as as 'Admin' and their user 'Status' then click this 'View users' button and it will display a new interface consisting list of all users added as an Admin as shown below:\nStep 5: Create Role\nAdditionally, we have provided user a facility to create more roles other than just default roles which later can be assigned to users with different level of role access. These custom roles can be edited or deleted based on the access given which later is explained in\nStep 8\nthat shows how it can be set.\nIn the Create Role interface, there is 'Name', 'Clone from role' and 'description' field. The clone from role will enable you to select role from any existing roles and the permission for this new role will be set accordingly which later can be modified.\nStep 6 : Role Detail Page\nAs soon as the role is created user will land on the Role detail page. User can navigate back to Manage Roles page by clicking it from top. Also, if you want to view list of users, then click 'View Users' link. Under this, all the modules and sub modules of DvSum application are displayed and from here, you can control permissions for these as shown below:\nNote:\nPlease note that the access of Dashboard will always be enabled and cannot be changed. For instance; if some user has no access of any module or sub module in DvSum application then dashboard by default will always be visible.\nStep 7 : Setting up Permission(s)\nLet's say the role 'Access Module' that is just created, you turn off the switch for 'Manage Dashboard' and 'Advanced Analytics' for it. Click the Save button.\nStep 7.1\nNow any user that this role is assigned to will not have access to these two sub modules of Dashboard. Let's say, you select any user from Manage users section and assign this Role to it as shown below:\nNote:\nFrom here you can also verify that, the user role is set as 'Admin' but the permission of access will be preferred based on what role is set in 'Module Access Role'.\nStep 7.2\nTo verify it, login from that user account and check the options right under Dashboard section. Those two sub modules will not be showing down there as shown below:\nStep 7.3\nIf you switch off the main Module permission then that particular module will not be visible on main DvSum application menu. For instance; Administration tab is switched off for above user then on login from that account, it will be hidden as shown below:\nStep 8Â  Manage Role: Viewing/Creating/Editing/Deleting Role\nFurthermore, in this version v1.0 of manage access, we have enabled you to further set permissions of Viewing, Creating, Editing, and Deleting in 'Manage Role' under the Administration module. If you want to enable this role to be able to only view/create/edit/delete or do all of these, it can be done from here.\nFor instance; set the view permission in the Manage role for 'Access Module' and Save the changes as shown below\nStep 8.1\nSpecifically, for Data Dictionary the Read/Write access can be done like this.\nBy selecting the view only access for Table DIctionary, it will allow user to only view all the details from listing view of table / column dictionary respectively. User can not make any changes from listing view pages e.g:\n1.\nRow Level Operations:\nMass Update, Hide/Unhide Rows, Refresh Relationships.\n2.\nColumn Level Operations:\nInline editing for editable columns e.g:\na.\nTable Dictionary:\nTable Type. Table Entity, Data Domain, Subject Area, User Description & Comments.\nb.\nColumn Dictionary:\nColumnn Type, Entity Type, DQ Category, Sensitivity Level, CDE, User Description & Glossary Term.\n3. Table Detail Page:\nPublish/Unpublish, Overview section (User description), Stewards, Primary Attributes, Relationships & comments.\nStep 8.2\nLogin from that user account OR if you have already logged in, just refresh the page. Select that role and you will see that above options do not get enabled. You will be able to only View it but cannot make any modification. That is the reason because we have restricted the access to modify it. Just like that you can set any other permission from Manage role i.e View, Edit/create/Delete.\nNote:\nThis is for customer roles only not for the default roles (Admin, user, super user)\nStep 9Â  Deleting Role\nIn case of Deleting a role, when you click Delete button then it will require you to transfer the control to any other role if that role has user(s) associated with it.\nNote:\nOnce you transfer the role to a different role, keep in mind that permissions will also get updated based on new role assigned here.\nStep 9.1\nIf any role does not have any user(s) associated here, then it will further ask you to confirm and then Delete the role as shown below:\nStep 10 Access denied Message when accessing with in application from a page\nTo understand this, lets simply take an example. If you switch off 'Table dictionary' permission for this user.Â  Save the changes. Login from that account, click profiling and then click any table name.\nStep 10.1\nSince this user has no access to table dictionary, on clicking any table name, it will notify you that Permission is denied for accessing table detail page. If you click OK you will be redirected to Dashboard and if you 'Click here' then you will be redirected to main Profiling page.\nStep 10.2 Access denied Message when accessing using URL\nFor example; you are not allowed to access Profiling module, if you copy and paste the URL \"prod.dvsum.com/profiling\" in new tab and hit enter then it will notify you that Access is denied\nStep 10.3\nIf you click OK then it will redirect you to main Dashboard page as shown below:\nStep 11: Other Associated Modules Permission and Permission Denied Messages\nHere is the list of some other associated modules that will be operational same as above example.\nManage / Analyze Rules\nDashboard (Rule list widget)\nProfiling (Table -> View Rules)\nProfiling (Table -> Data Quality -> Anayze Rule)\nProfiling (Table -> Data analysis -> Add business rule)\nProfiling (Table -> Data analysis -> View business rule)\nTable / Column Data Dictionary (Table detail -> Data Quality section)\nComply (Action Items Grid -> Rule Id)\nManage workflow (Add / edit task -> Rule Id)\nGlossary (Term Detail -> Data policy -> Rule Id)\nTable Dictionary - Table Detail\nProfiling -> Table\nColumn Dictionary -> Dataset\nElastic Serach Results (Table Name)\nScheduler\nManage Sources\nProfiling\nManage / Analyze Rules\nBatch Execution\nData Prep workbench\nTable / Column Data Dictionary (Table detail - > Data Preparation section)\nWorkflow\nGlossary - Term Detail (Process Workflows -> Workflow Id)\nGlossary - Term\nData Prep Workbench (Assigned asset columns -> Term)\nColumn Dictionary listing (Glossary Column -> Associated Term Name)\nElastic Serach Results (Term Name)",
    "scraped_at": "2026-02-02 15:40:27"
  },
  {
    "title": "Schedule Cataloging",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360049346552-Schedule-Cataloging",
    "content": "The article below explains how users can schedule the cataloging process. Let's get started.\nCatalog Scheduling\nStep 1 :\nGo to Administration -> Manage Sources\nChoose a source and click the \"\nSchedule Cataloging\n\" button as shown below:\nNote:\nThe scheduler catalog is applicable to SAWS version 2.4 and above.\nThe user will be directed to the Schedule page as shown below. The scheduler operates in the same manner as it does in the Dvsum tool.\nStep 2 :\nIf you schedule a job and view the details in the Scheduler section, you'll notice a newly introduced Catalog icon. When hovered over, it displays as \"Cataloging,\" as shown below:\nSchedule Cataloging Email Notification\nStep 3:\nTo customize the notification frequency or enable/disable scheduled cataloging notifications, visit your User profile. Click on Email Notification, then scroll down to find the \"Scheduled Cataloging Email Notification\" option, as shown below:\nNote:\nBy default, this option will be enabled.\nStep 4:\nAfter scheduling a Catalog, the user will receive an email according to the set email frequency. Below is the template for the email:",
    "scraped_at": "2026-02-02 15:40:32"
  },
  {
    "title": "User Access at DQ Rules Level",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360050817613-User-Access-at-DQ-Rules-Level",
    "content": "User Access at DQ Rules Level\nNow, users and Super users can be restricted from using Edit functionality in Rule definition, Rule Summary, Users, Workflows, Script configuration, Column Sequence and Rule instruction tabs in Rule detail page. Only Admins, Owners or any authorized users will be able to Edit the rules. For that, this access is managed using the User Groups section. For getting started with it, let us dig into the detail bit by bit.\nStep 1:\nFrom Home page, click Administration section. In drop down options you will see a Manage User Groups option. This option is available for Admins and Owners to limit the access for users or super users at DQ Rule level. Select this option and Manage User Groups detailed page will be displayed. On this page you can Add, Edit and Delete Group(s) as shown below.\nStep 2:\nFrom there, click Add/Edit Group. An Add/Edit Group detailed page will be displayed, scroll down to bottom and there you will see a new section for\nDQ\nRule\nAccess\nis added. The access setting in this section will be applied to all exiting and new DQ-Rules that are present in application. It will have two options, View and Edit as shown below:\nScenario 1:\nEdit Access at User/Super User Level\nStep 3 :\nNow on the same page, select user(s) or super user(s) that you want to limit or give the access to Edit the DQ Rule(s). For example, you select the Edit option from drop down for selected user(s). Save the option and now login into DvSum application using that user credentials. After login, select any rule to open rule detail page. Once rule detail paged is displaced, observe the following changes.\nA- Rule Summary\nUser/Super user will be able to edit Rule summary as shown below:\nB- Rule Definition\nUser/ Super user will be able to edit Rule definition as shown below:\nC- Users\nUser/Super user will be able to remove or add user(s) as shown below:\nD- Script Configuration\nUser/Super user will be able to edit Script configuration as shown below:\nE- Workflow\nUser/Super user will be able to add workflow as shown below:\nF- Rule Instruction\nUser/ Super user will be able to edit Rule instruction as shown below:\nG- Column Sequence\nUser/Super user will be able to edit column sequence and save button will be visible as shown below:\nScenario 2:Â  Â View Access at User/Super User Level\nStep 4 :\nNow on the same page, select user(s) or super user(s) that you want to limit or give the access to View the DQ Rule(s). For example, you select the View option from drop down for selected user(s). Save the option and now login into DvSum application using that user credentials. After login, select any rule to open rule detail page. Once rule detail paged is displaced, observe the following changes.\nA- Rule Summary\nUser/Super user will not see the Edit icon to edit the Rule summary as shown below:\nB- Rule Definition\nUser/Super user will not see the Edit icon to edit the Rule definition as shown below:\nC- Users\nUser/Super user will not be able to Add or Remove user(s) as shown below:\nD- Workflow\nUser/Super userÂ will not be able to Add Workflow(s) as shown below:\nE- Rule Instructions\nUser/Super userÂ will not be Edit in Rule instruction as shown below:\nF- Script Configuration\nUser/ Super user will not be able to Edit Script configuration tab as shown below:\nG- Column Sequence\nUser/ Super user will not be able to make any change in the column sequence tab. Save button will be hidden as shown below:\nScenario 3:\nIf a user/super is added in two different user groups and in each of those groups, user/super user has different access. For instance, user/super user in Group A has \"View\" access and same user/super user in Group B has \"Edit\" access. By business requirement, the least access which is only \"View\" access will be applicable to user/super user.Â  Therefore, user/super user will only be able to view the changes and won't be able to Edit anything.\nScenario 4:\nIf you select any user which has the role type \"Admin/Owner\" and you set the access as \"View\" for that particular user then by default as per Business requirement, it will still have the Edit access at the rule level. Therefore, this access limit will not be applicable on role type of Admin and owner.",
    "scraped_at": "2026-02-02 15:40:40"
  },
  {
    "title": "DvSum Edge Gateway Installation (SAWS) for Data Quality",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360021928854-DvSum-Edge-Gateway-Installation-SAWS-for-Data-Quality",
    "content": "Overview\nThis document describes the installation process for the DvSum Edge Gateway. It includes everything needed for installation, such as required middleware and instructions for deploying the Gateway/Connector on your machine.\nNote: Gateway, Connector, SAWS (Stand-Alone Web Service) and webservice are used interchangeably referring to the DvSum Edge Gateway.\nSystem Requirements\nOperating System\nWindows XP, 7 - 10 or Windows Server 2008, 2012 and above.\nDisk Space\n1 GB (free)\nMemory\n4 GB\nBrowser\nGoogle Chrome, Mozilla Firefox, Microsoft Edge, Apple Safari\nDownloads\nDvSum Data Quality Gateway Windows Installer\nNetwork Configuration\nIf DvSum Edge Gateway is to be installed on a server where outside access is restricted by a firewall, you must white-list the following addresses so the gateway can communicate with the DvSum SaaS web application.\nprod.dvsum.com port 443\nemail-smtp.us-west-2.amazonaws.com\nOpen port for gateway communication\nPort 8183 (default port)\nNetwork Diagram\nInstall System Prerequisites\nCheck and install Java Runtime (JRE)\n1. On your machine, go to\nStart\n>\nRun\n> type \"cmd\" > \"\nRun as Administrator\n\"\n2. Type \"java -version\" and hit enter. This will indicate what version of Java (if any) is installed.\n3. If Java is not installed, please download and install required version of Java.\nFor other sources, download and Install\nJava 8\nNote: You may be asked to create an oracle.com account to download Java.\nInstall Microsoft Access database Engine 2010\n(For EXCEL only)\n1. Go to\nwww.microsoft.com\nand search \"\nMicrosoft Access database engineÂ 2010 Redistributable\"\nIf 64 bit Java is installed on system make sure to install 64 bit Microsoft Access Database Engine 2010\nIf 32 bit java is installed on system make sure to install 32 bit Microsoft Access Database Engine 2010\nFor more details,\nHow to Configure EXCEL/CSV?\nInstall Python\nFind the\nPython 3.7 installation\nfiles.\nDownload the relevant installer from the Files section (e.g. for Windows 64 bit, install Windows x86-64 executable installer)\nRun the installer.\nSelect the checkbox \"Add Python 3.7 to PATH\"\nClick on \"customize installation\"\nClick next, confirm that \"Install for all users\" is checked.\nThe options pre-selected on this step may be left unchanged. Click \"Install\".\nThis will install Python on your system.\nConfirm installations\nConfirm that Java and Python are working well.\njava -version\npython --version\nInstall additional required Python libraries\nTo install python libraries, go to command prompt (\nStart\nâ†’\nRun\nâ†’ type \"cmd\" â†’\nRun as administrator\n)\nType (or paste):\npip install pandas XlsxWriter xlrd Flask beautifulsoup4 \"SQLAlchemy<2.0\"\nThis will install required Python libraries on your system.\nFor more details, read\nStep-by-step Installation of Python\nInstall DvSum Edge Gateway\nDownload SAWS from DvSum\nLog onto\nhttps://prod.dvsum.com\nGo to\nAdministration\nâ†’\nManage Account\nâ†’\nSAWS tab\nNext to the heading \"Stand-alone web service information\" click \"Download\". This will download \"webservice.zip\".\nUnzip webservice.zip.\nOptionally move the folder to a more convenient location.\nThis folder will be the webservice root directory.\nOn a machine dedicated to the DvSum Edge Gateway, this will typically be c:\\dvsum\\webservice or something similar.\nAdd SAWS to DvSum account\nGo to\nhttps://prod.dvsum.com\nGo to\nAdministration\nâ†’\nManage\nAccount\nâ†’\nSAWS tab\nClick \"âŠ•Add\" to register a new Edge Gateway.\nName: any descriptive name\nHost Name: enter either the hostname or the IP address where the Edge Gateway will run\nPort: 8183 by default\nHelp: If installing the Edge Gateway on your own laptop/computer, enter the Host Name as 127.0.0.1\nUpdate configuration.properties file with api.token\nGo to\nAdministration\nâ†’\nManage\nAccount\nâ†’\nSAWS tab\nCopy the communication key.\nOpen configuration.properties in a text editor. You'll find this file in folder created earlier. For example: c:\\dvsum\\webservice\\configuration.properties\nPaste your communication key into configuration.properties as the value for api.token:\napi.token=KnM15U6GsAcQtbXJMtQV51SRciAG8Het\nSave configuration.properties.\nFor more details, read\nAdd and Install SAWS as a service\nInstall DvSum Edge Gateway as a service\nGo to downloaded webservice root directory.\nFor example: c:\\dvsum\\webservice\nRight click \"saws_service_install.bat\" run as administrator.\nThis will install the windows service \"DVSUM SAWS\" along with some closely related services.\nVerify DvSum SAWS is running by\nStart\nâ†’ type\nServices\nâ†’ open Services â†’ scroll to \"DVSUM SAWS\" in the services list.\nClient machine IP white-listing\nIn order to establish the communication channel between the DvSum Edge Gateway (SAWS) that you just installed and the DvSum SaaS Web Application, the end-user public IP must be white-listed in the DvSum Web Application.\nGo to\nAdministration\nâ†’\nManage Account\nâ†’\nApplication Security\nâ†’ IP AddressÂ  White List\nEnter the public IP address of the end-user machine. Click Add.\nAdd SAWS exception to your browser\nGo to\nhttps://prod.dvsum.com\nLook at the SAWS icon on top right corner.\nIf the SAWS cloud icon is green, SAWS is successfully installed and running.\nIf the SAWS cloud icon is red, click on it.\nThis will open a window prompting you to add an browser exception for SAWS.\nClick \"Advanced\".\nClick \"Proceed to [hostname or IP address] (unsafe)\". You are connecting to the SAWS instance that you just installed. It is fully secure.\nFor more details, see\nWhy do I need to Add exception?\nNow the SAWS cloud icon will turn green, and the SAWS installation is complete.\nCopy SAP DLLS to c:/windows/system32\n1. Go to webservice\nroot\ndirectory > click on\nSap Dependencies\nfolder >\nSAP JARS AND DLLS\nFor 32-bit operating system, click onÂ SAP 32bit jars and dll folder.Â Copy 32-bit sapjco3.dllÂ and paste in c:\\windows or c:\\windows\\system32\nFor 64-bit operating system,Â click onÂ SAP 64bit jars and dll folder. Copy 64-bit sapjco3.dll and paste in c:\\windows or c:\\windows\\system32\n2. make sure folder address where dll has been pasted is added to system path variable i.e\nright click on (\nmy computerÂ icon\nor\nComputer\nor\nthis PC icon\n) >\nProperties\n>\nAdvanced System Settings\n>\nEnvironment Variables\n> find 'path' in System Variables.\nIf c:\\windows and c:\\windows\\system32 is added in path leave as it is otherwise add following String in the beginning of path c:\\windows;c:\\windows\\system32;\nHere is a quick video on How to Install DVSUM Edge Gateway:\nDvSum Edge Gateway Logs\nThe DvSum Edge Gateway maintains logs on the machine where it is running. This log file records all actions performed by SAWS. In case of an error, this log can facilitate troubleshooting to pinpoint the root cause of the issue.\nThe logs are found in the logs sub-folder. For example: c:\\dvsum\\webservice\\logs\nThe server is configured to rotate logs daily. Rotated log files follow this naming convention: SAWS_2023-05-15.log.\nThe undated file,\nSAWS.log\n, is the latest.\nSAWS Maintenance and Advanced settings\nAfter the basic SAWS setup, some advanced configuration options are available. You can easily manipulate the settings from SAWS configuration file.\nFor details, read\nSAWS Advanced Settings (Configuration.properties file)\nSAWS Upgrade\nWhen a newer version of SAWS is available on DvSum, you will see the notification for upgrade. It will update SAWS in one click.\nFor details, read\nSAWS Update.\nOther Articles Related to DvSum Edge Gateway (SAWS)\nHow does SAWS work?\nWhy is SAWS showing red cloud even though it is installed successfully?\nCan I add multiple SAWS?\nHow to Schedule Jobs?\nWhat are SAWS Alert notification controls?",
    "scraped_at": "2026-02-02 15:40:47"
  },
  {
    "title": "Advanced Dashboards",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360052125833-Advanced-Dashboards",
    "content": "Dashboard Main Page:\nWhen a user has a default Dashboard, then upon landing on main dashboard page, the default dashboard will be shown.\nNote:\nWhen a user creates an account for first time, by default, Advanced dashboard option will be enabled in user profile\nIf a user has more than 1 default dashboard then the first default dashboard will be selected and displayed\nUser can switch the dashboard from the list of shared default dashboards\nWhen a user switches the dashboard, this newly selected dashboard will be set as default dashboard of the user.\nUser Groups in Share Dashboard\nNow, in sharing feature of Dashboard, you will be able to add \"User Groups/Users\" as well as shown below;\nUser Groups in Create/Edit Dashboard\nUser will be able to share a dashboard with user groups and specific users as well from here. For admins/owners, when creating/editing dashboard, a checkbox will be shown to mark the dashboard as default as shown below:\nNote:\nOnly admin/owner user will be able to make a dashboard as default\nThere can be no more than one default dashboard for one user group\nAdd New User:\nIf a new user is added from manage user and some group(s) are assigned to the user then the default dashboard(s) of groups will be assigned as default dashboard of the user.\nIf the user is not a part of any group then no default dashboard will be set.\nAdd New User in User Groups:\nWhen a new user is made part of the group, theÂ default dashboard(s) of groups will be assigned as default dashboard of the user\nEmail and Bell Icon Notification On Sharing Dashboard\nUpon sharing a dashboard, user(s) will be notified via Email as well as by bell icon notification as shown below;",
    "scraped_at": "2026-02-02 15:40:52"
  },
  {
    "title": "History Trend Widget using Power BI Template Reports",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360049313234-History-Trend-Widget-using-Power-BI-Template-Reports",
    "content": "History Trend Widget - Dashboards\nOne of the requests from our clients, which added great value to the existing functionality for Dashboards, is to be able to see the Trend History Report by selecting the slicer 'ValGroup' as well. Previously, you were only able to see trend history widget report only on the basis of Rule ID as a slicer.\nNow, we will have 3 different power bi template reports against each template widget. They will be different based on the slicer selection. The differentiation will be according to these slicers\nRule Id\nVal Group\nVal Group and Rule Id\n1.1 Rule Id\nOn Add Widget form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.\nBased on your selection as a slicer, as in this case it is 'Rule ID', you will be able toÂ view the Trend History Report.\nYou may then create a view where you can specify a criteria for a particular ValGroup. On saving a view, you will be navigated to Dashboard page where you can view the widget.\nOn clicking the widget, you will be navigated to a new tab where you would see the Report.\nYou will be seeing 2 different charts\n- Average of RunResult by RunDate -\nThis will be the graphical representation of Run Result count (avg.) for all rules in the selected ValGroup. The time-series on the x-axis of the chart is on weekly basis. The count on y-axis is the Run Result count (avg.)\n- RunDate -\nThis will be the list type that contains the following for each rule in the ValGroup.\n- Rule ID\n- Run Date\n- Run Result\n- Val Group\n1.2 Val Group\nOn Add Widget form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.\nBased on your selection as a slicer, as in this case it is 'Val Group', you will be able toÂ view the Trend History Report.\nOn clicking the widget, you will be navigated to a new tab where you would see the Report.\n1.3 Val Group and Rule Id\nOn Add Widget form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.\nBased on your selection as a slicer, as in this case it is 'Val Group, Rule ID Both', you will be able toÂ view the Trend History Report.\nOn clicking the widget, you will be navigated to a new tab where you would see the Report.\n2. Color Persistence - Dashboards\nDashboard widgets, moving forward, will have same color across all widgets for specific data. Also, the color would persist across different widgets i.e if green color is assigned to â€˜Passedâ€™ run status, then it should be same across all the widgets.\n2. Data Analysis (DAE) Rules - Creation From Manage Rules Tab\nWe have now given the flexibility to create DAE rule from Audit -> Manage Rules tab as well. Previously, they could only be created from Profiling -> Show Details -> Data Analysis",
    "scraped_at": "2026-02-02 15:40:58"
  },
  {
    "title": "History Trend Widget â€“ Dashboards",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360045131774-History-Trend-Widget-Dashboards",
    "content": "On â€œAdd Widgetâ€ form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.\nBased on your selection, you will be able to view the Trend History Report for a ValGroup.\nYou may then create a view where you can specify a criteria for a particular ValGroup.\nNote: You can only create a view where selected ValGroup contains no more than 10 rules, else you will be notified to select a ValGroup having rules not more than 10.\nOn saving a view, you will be navigated to Dashboard page where you can view the widget.\nOn clicking the widget, you will be navigated to a new tab where you would see the Report for the selected ValGroup\nYou will be seeing 3 different charts\nAverage of RunResult by RunDate - This will be a graphical representation of Run Result count (avg.) for all rules in the selected ValGroup. The time-series on the x-axis of the chart is on weekly basis. The count on y-axis is the Run Result count (avg.)\nAverage of RunResult by RunDate and RuleID - This will be a graphical representation of Run Result count (avg.) for each rule in the selected ValGroup.\nRunDate - This will be the list that contains the following details for each rule in the ValGroup.\n** Run Date\n** Run Result\n** Val Group",
    "scraped_at": "2026-02-02 15:41:02"
  },
  {
    "title": "Advanced Analytics - Integrating Power BI reports into DvSum",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360021231094-Advanced-Analytics-Integrating-Power-BI-reports-into-DvSum",
    "content": "We can create interactive reports using Power BI webapp tool and integrate them in DvSum for data visualization.\nOn Dashboards > Advanced Analytics tab, all reports will be visible. Upon clicking, it will open the detailed report with widgets. You can apply filters to the widgets and since they are all dependent, the data of all widgets will be refreshed according to the selected filter.\nFor each widget you will have a few more options like sort, have a spotlight on a certain widget.\nYou can also export data on your machine as an excel/csv file.",
    "scraped_at": "2026-02-02 15:41:08"
  },
  {
    "title": "Workflow Digest Emails",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360021222614-Workflow-Digest-Emails",
    "content": "If you have turned on Workflow Email Digest from User profile for Daily or weekly, you will receive email with following information.\nThere are 2 sections in this email; Execution and Step summary will contain information of last 24 hours or last week depending on the setting you chose.\nWorkflow Execution Summary\nInitiated Execution(s) - Executions that just launched\nIn-Progress Execution(s) - Currently taking place executions in which you are involved\nCompleted Execution(s) - Completed and closed by the day (or week)\nCanceledÂ Execution(s) - Terminated executions\nWorkflow Step Summary\nSubmitted Step(s) - Steps which were completed by you today (or in current week)\nOpen/In-Progress Step(s) Steps which you are working on or have yet to work on\nRe-assigned Step(s) - Steps re-assigned to you.\nRejected Step(s) - When an execution is rejected and a Step is assigned back to you.\nNote: If there is no information regarding any steps or executions, you will not see the table. For example, if no steps were Re-assigned to you then this section will not appear.",
    "scraped_at": "2026-02-02 15:41:12"
  },
  {
    "title": "DvSum export options for reports",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360007921813-DvSum-export-options-for-reports",
    "content": "DvSum supports export report capability for the following:\n1. Dashboard (Views)\nYou can create customized dashboard with different criteria and filters. These dashboards appear in the drop down list. Here's how to proceed:\nSelect the dashboard you want to export\nClick on export icon\nAn excel file consisting the data of all the widgets present in the selected dashboard will be downloaded on your computer\n2. Audits (All rules)\nTo download the list of created rules, navigate to Manage Rules > More Actions > Export Rules.\n3. Analyze Rules (Views)\nYou can create customized view of rules as follows:\nNavigate to Review > Analyze Rules page.\nFrom \"Analyze Rules\" drop down, click on \"Create View\".\nHere you can apply specific criteria and choose columns to appear in a view.\nOnce the view is created, it appears in the drop-down list.\nSelect the view and click on \"Export Results\". An excel report of the selected view will be downloaded.\n4. Exceptions report\nWhenever a rule is executed and there are exceptions, DvSum generates an exception report which can be downloaded from the rule detail page. Go to Analysis tab and click on \"Export\".\n5. Glossary Categories\nGlossary category export downloads all the terms within that category. You can download multiple categories at a time. From Manage Categories page, click on \"Export Results\"\n6. Assessment Report\nDvSum provides the GDPR assessment report. You can take the\nsurvey\nand create a\ndata model\nrequired to generate the\nassessment report\n.\nGo to assessment report detail page. The \"Download PDF\" option will download the report.",
    "scraped_at": "2026-02-02 15:41:20"
  },
  {
    "title": "Configure Oracle as Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/23084323535124-Configure-Oracle-as-Source",
    "content": "Overview\nThis article describes the steps needed to configure Oracle as a source in DvSum Data Quality (DQ).\nDetailed Steps\nOracle Configuration\nStep 1:\nCreate a user to be configured in DvSum, if the user does not already exist.\nStep 2\n: Grant readonly access to the User for schemas and tables that you would like to catalog and profile. (The username and password will be used below when configuring the data source in DvSum)\nStep 3:\nGrant readonly access to the user for the following system tables:\nALL_USERS\nALL_OBJECTS\nALL_TABLES\nALL_SYNONYMS\nALL_TAB_COMMENTS\nALL_EXTERNAL_TABLES\nALL_TAB_COLS\nALL_COL_COMMENTS\nALL_CONSTRAINTS\nALL_CONS_COLUMNS\nIn order to grant access the below statement can be used:\nGRANT SELECT ON (\nTABLE_NAME\n) TO (\nUSER\n)\nReference Article\nDvSum configuration\n1. Add Source\nTo create a data source, navigate to Administration â†’ Manage Sources â†’ Add Source â†’ Select Oracle.\n1.1 Basic Information\nAdd\nSource Identifier and select the Webservice for the source.\nTo configure the web service, you can refer to this\nArticle\n1.2 Host Information\nAdd the following information in host information section:\nHost\nPort\nDatabase SID or Service Name\n1.3 Credentials\nAdd DB login and DB password and click on the save button.Â  The source will be created successfully.\n2. Test the Connection\nThe source will now appear in the Manage Sources Listing. Select the source and Click on Edit Source:\nClick on the Source and in the bottom left corner click \"Test Connection\"\n3. Catalogue the data source\nNavigate to Manage Sources and click the \"Run Cataloging\" button. When the cataloging is completed, the status will change to \"Tick Mark\".\nAfter the Catalog completion, click on Go to Catalog and it will open the Table Dictionary page of this source.\nThe Table dictionary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this catalog.\nYour Oracle connection is now fully configured and functional.",
    "scraped_at": "2026-02-02 15:41:26"
  },
  {
    "title": "Configure Oracle ADW as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/18826674587796-Configure-Oracle-ADW-as-a-Source",
    "content": "In this article, we'll guide you through the process of leveraging DvSum's capabilities within Oracle ADW to transform your data into a valuable asset. Let's embark on this journey together and unlock the full potential of your data.\nAdding Oracle ADW source in the DvSum tool\nStep 1:\nOpen the Dvsum application, select Administration, and click on the Manage Sources option.\nClick on Add Source, and Select Oracle ADW source as shown below\nStep 2:\nIn the \"Basic Information\" section, provide the following details:\nSource Name\nDescription\nSelect the web service on which the Oracle ADW source is enabled.\nOther fields are optional, as shown below.\nStep 3:\nIn the host information section, you will be required to provide:\nTNS name: TNS names denote the available service levels and are accessible within the Database Connection Tab of the Autonomous Database Details.\nWallet Path: Absolute directory path of the extracted wallet folder located on the same machine as the SAWS.\nCredentials:\nWallet Username:\nThe wallet username corresponds to the username of the database user.\nWallet password:\nThe wallet password refers to the password provided by the user during the download of the wallet zip file from the Database Connection Tab within the details of the Autonomous Database.\nStep 4:\nClick on Test Connection, and a success message will be displayed.\nNow click on the save button.\nStep 5:\nNow Oracle ADW is added as a source, and the\nuser will be able to Catalog it, profile it, Execute Rules, and cleanse it.",
    "scraped_at": "2026-02-02 15:41:33"
  },
  {
    "title": "Configure Azure Synapse Analytics as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/12893404920980-Configure-Azure-Synapse-Analytics-as-a-Source",
    "content": "A data source refers to a specific location or system where data is stored and can be accessed. It can be a database, a file, an application, or any other entity that holds information. Data sources can be internal, such as a company's database, or external, such as a government website. The data from these sources can then be used to support decision-making, analyze trends, and perform other tasks.\nAdding Azure Synapse Analytics as a Data Source:\nTo add a data source, you have to navigate to the â€œ\nAdministration\nâ€ tab on the left panel.\nAfter clicking on the â€œ\nManage Sources\nâ€ tab, you will see â€œ\nAdd Source\nâ€ on the window as shown below in the screenshot:\nOnce you have clicked on\nAdd Source,\nyou will see all the supported sources by DvSum, you have to select â€œ\nAzure Synapse Analytics\nâ€\nAfter selecting â€œ\nAzure Synapse Analytics\nâ€, you will be redirected to the configuration window where you have to enter the following details step by step as shown in the screenshot:\nStep 1\n: Enter Host\nStep 2\n: Enter Port\nStep 3\n: Enter Instance Name\nStep 4\n: Enter DB Login\nStep 5\n: Enter DB Password\nStep 6\n: Click on Authenticate\nAfter entering the details, you need to click on â€œ\nAuthenticateâ€\n. On Authentication, it should now prompt for adding the â€œ\nDatabase\nâ€. After selecting the Database, the last step to successfully add the source is to Test the Connection. As shown in the screenshot below, you have to click on â€œ\nTest Connection\nâ€.\nNow that you have successfully added Azure Synapse Analytics as a source, it is time to Run the Cataloging. In the screenshot below you will see in the\nGreen\nbox â€“ â€œ\nRun Cataloging\nâ€, it will take a few minutes to complete the cataloging of the source that you have added. You will also get a notification in the\nBlue\nbox with the arrow on the top right corner after the cataloging is completed.\nIn order to view the cataloged source, you will have to go to the\nData Catalog\ntab and select\nâ€œTable Dictionaryâ€\nas shown in the\nRed\nbox.\nAfter the Cataloging is completed and you have received the notification, you can view the catalog in the Table Dictionary tab. In the screenshot below you can see the cataloged source details. It is now time for\nProfiling\n. In order to do that, you have to navigate to the\nProfile\ntab on the left panel and select â€œ\nProfilingâ€\nas shown in the\nGreen\nbox.\nIn the Profiling tab, you have to select the source and the table name respectively that you want to run the Profiler on. As shown in the screenshot below, you have to click on â€œ\nRun Profiler\nâ€ in the\nGreen\nbox. It will take a few minutes to run the profiler, once completed you will receive a notification in the notification bar on the top right corner.\nIf you want to view the catalog, you can click on the table name or â€œ\nView Catalog\nâ€ as shown in the\nRed\nbox.\nCongratulations, you have successfully:\nAdded Azure Synapse Analytics as a source.\nRun the Cataloging.\nRun the Profiler\nFinally, this is how your Catalog will look like:",
    "scraped_at": "2026-02-02 15:41:38"
  },
  {
    "title": "Configure Databricks as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/9830486369428-Configure-Databricks-as-a-Source",
    "content": "Azure Databricks is optimized for Azure and tightly integrated with Azure Data Lake Storage, Azure Data Factory, Azure Synapse Analytics, Power BI, and other Azure services to store all your data on a simple, open lakehouse and unify all your analytics and AI workloads.\nEnabling Databricks Source in DvSum\nStep 1:\nOpen the Dvsum application, select the Administration tab and click on, the Manage sources option,Â  Click on 'Add Source' and Select Data Bricks source'. Following error messages will be displayed if the DataBricks source is not enabled for users 'Owner' and 'Admin'.\nNote\n:\nOnly the owner is authorized to add a source.\nOwner:\nAdmin:\nStep 1.2\nOwner will click the 'Manage account' link and gets redirected to this page from where the source can be enabled. On other hand, Admin will request the owner to get the source enabled for the account. Click the 'Saws' tab, select the saws, and click the 'Enable source' button.\nStep 1.3\nFrom the list of available sources, select DataBricks and click the\nUpgrade\nbutton as shown below\nStep 1.4\nOn returning back in the SAWS tab, it will take some time to process and after that, Databricks Icon will appear in the enabled sources column which means that the source is successfully enabled as shown below\nScenario 1: SAWS Error\nOn upgrading, if there's any issue with SAWS, an error message will be displayed \"Please check if your SAWS is working correctly\".\nScenario 2:Â  Pending State\nOn upgrading, if any job(s) is running, it will go to a 'pending' state.\nAdding Databricks source\nStep 2.1\nOpen the Dvsum application, select 'Administration' and click on the 'Manage sources' option,Â  Click on 'Add Source' and Select Databricks source as shown below\nStep 2.2\nIn the Basic information section, provide the source name, and description, and select web service on which Databricks source is made enabled, other fields are optional as shown below;\nStep 2.3\nIn order to get the Server hostname, HTTP path, and personal access token go to Databricks Dashboard.\nStep 2.3.1\nClick on Compute >> Cluster name >> Cluster configuration >> Advance Options >> JDBC/ODBC. And Youâ€™ll get â€œServer Hostnameâ€ and â€œHTTP pathâ€.\nStep 2.3.2\nTo get Personal Access Token, click on â€œUser settingsâ€. Add Name for Token and set Days limit for token and click on Generate.\nNote\n:\nMake sure to copy the token now. You won't be able to see it again\n.\nScenario 1: Authentication using Access Token\nStep 1\nAdd server hostname, HTTP path and personal access token in Host information and click on Authenticate button.\nStep 2\nDatabase Name will be shown select the database from the dropdown and click on the save button.\nStep 3\nEdit the source and verify the â€œTest connectionâ€\nScenario 2: Authentication using Client Secret\n~Prerequisites for Configuring Azure Databricks (\nService Principal Service\n):\nPlease refer to the article to configure\nAzure Databricks (Service Principal Service).\nStep 1\nFor Authentication using\nClient Secret\n, enter the correct\nServer Hostname\n,\nHTTP path\n(\nServer Hostname and HTTP path used for authenticating via Access Token will be the same here),\nAzure Subscription ID\n,\nAzure Resource Group\n,\nAzure Workspace\n,\nAzure Tenant ID\n,\nAzure Client Id\n,\nAzure Client Secret\nand\nOAuth Secret\n. Click the Authenticate button.\nNote:\nOAuth secret is a confidential key used to authenticate and authorize applications when integrating securely with external services. Admin can generate it from Service Principle's secret tab.\nNote: OAuth Secret is a required field when the checkbox is enabled. Using OAuth secret, Databricks will use an updated driver which eliminates the need for an additional rule to wake up the cluster and scheduled jobs will be executed faster\nStep 2\nDatabase Name will be shown select the database from the dropdown and click on the save button and \"Test Connection\"",
    "scraped_at": "2026-02-02 15:41:45"
  },
  {
    "title": "Configure Google BigQuery as a source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/4403282434324-Configure-Google-BigQuery-as-a-source",
    "content": "We continue to roll-out connectors to support bi-directional connection with Google Bigquery, where user can catalog, audit rules and the cleanse data without need for any external data pipelines. User will be able to add Google Bigquery as a source in DvSum and make customer's data insightful by Cataloging, Profiling , Executing rules and cleansing it.\nNote:\nTo add Google bigquery as a source following are the required conditions to be fulfilled.\na) SAWS version should be 2.4.0 and above\nb) Google bigquery as a source type should be enabled (explained below)\nStep 1:Â  Enabling Google bigquery Source in DvSum\n1\nOpen the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Google bigquery source. Following error messages will be displayed if the Google bigquery source is not enabled for users 'Owner' and 'Admin'.\nNote:\nOnly owner is authorized to add the source.\nOwner\n:\nAdmin\n:\n1.2\nOwner will click the Manage account link and gets re-directed to this page from where the source can be enabled. On other hand, Admin will request the owner to get the source enabled for the account. Click the Saws tab, select the saws and click\nEnable\nsource\nbutton;\n1.3\nFrom the list of available sources, select Google bigquery and click\nUpgrade\nbutton as shown below;\n1.4\nOn returning back in SAWS tab, it will take some time to process and after that Google bigquery Icon will appear in enabled sources column which means that source got successfully enabled as shown below\nScenario 1 : SAWS Error\nOn upgrading if there's any issue with SAWS then an error message will be displayed \"Please check if your SAWS is working correctly\".\nScenario 2:Â  Pending State\nOn upgrading if there's any job(s) running then it will go in pending state.\nStep 2 : Adding Google bigquery source in DvSum tool\n2.1\nOpen the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Google bigquery source as shown below;\n2.2\nOnce landed on source detail page, you will be required to provide following info to add Google bigquery as a source;\nChoose and upload JSON file in\nService account key file\nIt will fetch\nProject ID\nBased on that\nData set\noptions will be populated in the drop down option\nNote\n: In order to set up service account, please follow this\nlink\n:\n2.3\nAfter that Success message will be displayed and now click the save button;\n2.4\nNow Snowflake is added as a source and user will be able to Catalog it, profile it, Execute Rules and cleanse it.",
    "scraped_at": "2026-02-02 15:41:51"
  },
  {
    "title": "Configure Snowflake as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360050674571-Configure-Snowflake-as-a-Source",
    "content": "We continue to roll-out connectors to support bi-directional connection with Snowflake, where user can catalog, audit rules and the cleanse data without need for any external data pipelines. User will be able to add Snowflake as a source in DvSum and make customer's data insightful by Cataloging, Profiling , Executing rules and cleansing it. Let's get started.\nNote:\nTo add Snowflake as a source following are the required conditions to be fulfilled.\na) SAWS version should be 2.4.0 and above\nb) Snowflake as a source type should be enabled (explained below)\nGetting Started with Snowflake as a Source\nBefore starting the configuration process, ensure that query history is properly set up. Query history is essential for enabling seamless data lineage and effective data analysis within the application. For a detailed guide on enabling query history refer to the\nEnabling\nQuery\nHistory\nfor\nData\nSources\narticle.\nStep 1:Â  Enabling Snowflake Source in DvSum\n1\nOpen the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Snowflake source. Following error messages will be displayed if the Snowflake source is not enabled for users 'Owner' and 'Admin'.\nNote:\nOnly owner is authorized to add the source.\nOwner\n:\nAdmin\n:\n1.2\nOwner will click the Manage account link and gets re-directed to this page from where the source can be enabled. On other hand, Admin will request the owner to get the source enabled for the account. Click the Saws tab, select the saws and click\nEnable\nsource\nbutton;\n1.3\nFrom the list of available sources, select SnowflakeÂ and click\nUpgrade\nbutton as shown below;\n1.4\nOn returning back in SAWS tab, it will take some time to process and after that Snowflake Icon will appear in enabled sources column which means that source got successfully enabled as shown below\nScenario 1 : SAWS Error\nOn upgrading if there's any issue with SAWS then an error message will be displayed \"Please check if your SAWS is working correctly\".\nScenario 2:Â  Pending State\nOn upgrading if there's any job(s) running then it will go in pending state.\nStep 2 : Adding Snowflake source in DvSum tool\n2.1\nOpen the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Snowflake source as shown below;\n2.2\nOnce landed on source detail page, you will be required to provide following info to add Snowflake as a source;\nHost\nInformation\nURL (will be used by user to access database. This link is given in the email that you will get on activating account for snowflake)\nDatabase\nWarehouse\nCredentials\n(Same as which are use to login into Snowflake account)\nUser\nPassword\n2.3\nAfter that Success message will be displayed and now click the save button;\n2.4\nNow Snowflake is added as a source and user will be able to Catalog it, profile it, Execute Rules and cleanse it.",
    "scraped_at": "2026-02-02 15:41:55"
  },
  {
    "title": "Catalog Table Detail Page",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/17457787570452-Catalog-Table-Detail-Page",
    "content": "Overview\nDvSum DQ's Table Detail page provides an efficient look at table metadata, data, current data quality rules, suggested data quality rules, and more. Users can easily edit table details and manage rules, enabling seamless decision-making and data quality maintenance.\nSample View:\nFinding the Table Detail Page\nData Catalog â†’ Table Dictionary:\nProfile â†’ Profiling â†’ View Catalog:\nFunctionality\nRun Profiling\nData profiling will query the data source to gather metrics about the data. It will also generate suggested DQ rules based on the data and metadata that it discovers.\nProfile Table â†’ Run Online\nThis will promptly initiate the profiling process. A progress bar will display the ongoing progress. It's crucial to stay on the page during profiling, as it operates synchronously. Leaving the page will result in the cancellation of the profiling process.\nProfile Table â†’ Run Offline\nThis will schedule a profiling job to run in the background. The job will typically start within the next minute, and this is the recommended approach for profiling large tables that may require several minutes to complete. You can navigate to other pages without affecting the job, and the updated statistics will be available once the job finishes. Check the job status here:\nReview â†’ Scheduler.\nThe\nProfile Table button will be\nYellow\nif profiling has never run. Afterwards, it will change to\nBlue.\nEdit Table\nEdit the table details by clicking on the\nEdit Table\nbutton.\nThe following fields can be edited:\nDescription\nTable Type\nLoad profile\nAll Data - Select this load profile if data is fully refreshed regularly.\nIncremental Data - Select this if data is normally loaded incrementally (appended).\nIf the load profile is set to incremental, then you should also set the Metric Time field.\nMetric time - Select from the Data and Timestamp fields in the table. This field will be used in relevant DQ rules. For example, when calculating daily load volumes this field will be used to see which records were loaded in the last 24 hours.\nTable Detail Page\nThe Table detail page primarily contains 4 sections as shown in the screenshot below:\nColumns\nDisplays metadata (e.g. data types) and data (e.g. unique count).\nRules -Â Available Rules\nCreate new rules and review existing rules.\nSelect an existing rule to run it online or offline. For details refer to the article\nRun Online/Offline.\nMore Actions:\nUpdate Schedule -\nupdate the existing schedules or add a new schedule.\nRemove Schedule -\nThis option allows the user to remove any existing schedule of the rule(s)\nDelete rule\n- This option allows the user to Remove the rule permanently\nClone rule\n- This option allows the user to clone the current rule\nMass Update\n- Mass update allows the user to select multiple rules and update them at once\nRules - Recommended rules\nRecommended Rules\nThese are the rules that are recommended based on the profiling of the source's tables\nIf the User wants to add a recommended rule they can simply click on the\nADD\nbutton\nThe following rule types are\nRecommended\n:\nBlank\nValue Range\nCount\nMetric\nSample Data\nThis has info same as Profiling >> Data Analysis. The grid will have all columns that are added in that table and will display sample data for each column.\nRelationship\nIt shows the relationship of the table between different tables, their cardinality/relationship types, Constraint/Key names, database status, recommended status, accepted status, and Zoom in/out. The rejected state will not be shown. If you click any table name from here then it will open up its table detail page separately.\nView Comments\nUsers can view the comments and add a new comment",
    "scraped_at": "2026-02-02 15:42:02"
  },
  {
    "title": "Data Catalog",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360052123973-Data-Catalog",
    "content": "User will be able to see the data of all Sources/Columns/Tables in Data Dictionary section. There will be one enterprise dictionary for one account. Letâ€™s get started step by step.\nA) Table Dictionary\nB) Column Dictionary\nC) Table Detail Page\nTable Dictionary\nStep 1\nClick on Data dictionary and it will display sub item as \"Table Dictionary\" under it. Select this option and it will take user to detailed page of Table dictionary. On this page, all the table specific details will be displayed as below;\nSource Name\nSource type\nEntity type\nTable type\nTable group\nData set\nData set description\nUser description\nRecord identifier exist\nRecord count\nProfile status\nNo. of Rules\ncomments\nColumn Dictionary\nStep 2\nOn the left side menu of DvSum application, click on \"Data Dictionary\" it will display sub item as \"Column Dictionary\" under it. Select this option and it will take user to detailed page of Column dictionary as shown below;\nNote:\nIn Data dictionary page, only those sources will be visible that are connected and cataloged.\nOn this page, all the column specific details will be displayed as below;\nSource\nEntity type\nData set\nColumn Name\nColumn Description\nColumn type\nColumn group\nSensitivity level\nCompliance Category\nCDE\nData type\nLength\nPrimary Key\nForeign Key\nCheck Constraints\nUnique Constraints\nUser description\nProfiling Info\nGlossary Info\nStep 3Â  Profiling Info and Distribution data\nAt the end of the grid view, you will see a field name as \"Profile Info\". If column is profiled, then a profiling icon will be shown. Otherwise it will not be shown. Refer to the screenshot below;\nIf user clicks the icon then it will show the profiling info for specific column or table that is being profiled. Also, if any source or column has distribution data info available then it will also be shown there on clicking the icon. Otherwise, only profiling info will be displayed i.e source name, table name, max value, min value, unique count and null count.\nNote:\nOn default Data dictionary view page, data across all sources/Column will be displayed. Also, this default view cannot be shared or marked as favorite. The favorite and share icons on top right side will appear as disabled.\nFrom here, following steps listed below are commonly used features in both Table and Column dictionary.\nStep 4Â  Â Filters\nFilters are also provide against all fields of columns. User can click on any field, apply filter and sort the data as shown below;\nStep 5Â  Mass Update\nMass update will be applicable on Entity field. The purpose of this functionality is to be able to group the the different sources/columns available.\nSelect sources, click the Entity field, a new interface will open upfront. For the first time, create an entity, provide entity name and hit the apply button as shown below;\nYou will see that it gets mass updated against all the selected sources. User can apply filter on this field as well.\nStep 6Â  Picker List\nNow, user will be able to select an existing entity type from the picker list available in the Entity type field which are fetched from DB. Click on cell, picker list will appear that will contain all the available entities (already existing in DB) from which user can select.\nFollowing are the picket lists:\n1) Entity type\n2) Table type\n3) Table group\nStep 7 : Hide/Unhide Rows\nNow, in column dictionary, select the rows that you want to hide, click on the Hide Row(s) button and then rows will be hidden. In table dictionary, same functionality will be applicable on tables.\nClick on Show Hidden Rows button on top right side, then the hidden rows will start appearing with slightly grayish background color to differentiate it. Select that row and click the Unhide Row(s) button given on top as shown below;\nNote:\nOne hide/unhide Rows, the sequence of data won't change. After un-ride, the rows will appear in their current sequence from where they were hidden.\nFollowing field is editable\nUser Description (Column + Table dictionary)\nStep 8 : Drag/Drop and Copy/Paste\nUser can copy entity field from one cell and paste it into another cell by Ctrl+c and Ctrl+v. Also, drag and drop any entity type from one cell to another by selecting the cell(s) and pressing Ctrl+d.\nStep 9 : User Description as Editable text field\nThis field is fully editable. User can delete, copy, paste, drag and drop text inside it.\nStep 10Â  Add New or Clone Existing Data dictionary\nUser can also create a new Data dictionary as shown below;\nUser can also clone the default data dictionary as shown below;\nStep 11Â  Cloning the Data Dictionary\nOn cloning detail page, provide name, select column fields, Specify criteria. In specific criteria section, user can also apply criteria for Entity field that will show the data w.r.t that particular entity. A 'Share with' section is also added to share this cloned Data dictionary with specific users or groups. Now, click the save button as shown below as shown below;\nA cloned Data dictionary will be displayed w.r.t specified criteria. On the top right corner, Favorite and share icon will also become enabled as shown below;\nStep 12Â  Fine Grain Control\nUser can create a view, add sources and give access to User, Super user and Admin. By default, Admin and Owner users will have complete access to the data of all sources added in a view. But for user and super user, data will be shown on column/table dictionary listing/detail page according to the user/super access given.\nStep 13 Export View\nClick on the \"Export icon\" to export records given in Table/Column grid (w.r.t record visibility on current page) as shown below;\nTable Detail Page\nStep 14\nDataset column is a hyperlink in column/table dictionary. By clicking the hyperlink, user will be navigated to Table Detail page.\nAlso, from\nprofiling page\n, View Catalog on hover will appear as well as on top of the grid, it is added as button from where user will be navigated to Table detail page.\nStep 15\nOn top it is displaying following;\nSource name\nTable/Column name\nView comments and its count\nEndorsing (Accept and Reject)\nThis is just dummy and not functional yet.\nMark Favorite\nPublish/Unpublish:\nPublish\n(Unhide row)Â -\nButton on Table detail page will appear as 'Publish' if this not published and its is hidden on Table/Column dictionary page. If user clicks on 'Publish' button, then it will automatically unhide the row on table/column dictionary page and text on button will be updated as 'Unpublished'. Similarly, if user unhides row from table/column dictionary page then on navigating to table detail page, text on button should be updated as 'unpublished'\nUnPublish\n(Hide row)-\nButton on Table detail page will appear as 'UnPublish' if this is published and its is unhide on Table/column dictionary page. If you click 'UnPublish' button then it will automatically hide the row on table/column dictionary page and text on button will be updated as 'Publish'. Similarly, if you hide row from table/column dictionary page then on navigating to table detail page, text on button should be updated as 'Publish\"\nThen under this there are main navigation menus including\nOverview\nFields\nSample data\nStatistics\nRelationships\nQuality\nData preparation\nComments\nStep 15Â  Overview\nIt has two parts;\nSystem description: Defined at DB level and it cannot be Editable\nUser description : Defined by user just like at table/column listing page. It can be edited by clicking edit icon given.\nStep 16Â  Fields\nThis represents that column names of table/column, Raw datatypes, data types, PK, sample value (min-max values), description (user description). Only Description field will be editable.\nStep 17Â  Sample Data\nThis has info same as Profiling >> Data Analysis. The grid will have all columns that are added in that table and will display sample data for each column.\nStep 18Â  Â Statistics\nThis section is same as main Profiling tab on Data profiling page. On clicking Distribution icon from here, it will open up column distribution data. Similarly, on clicking profiling pattern it will open up data pattern here.\nDistribution Chart\nProfiling Pattern\nStep 19Â  Â Relationships\nIt shows relationship of table (whose table detail page is opened i.e CUSTOMER2) between different tables, their cardinality/relationship types, Constraint/Key names, database status, recommended status, accepted status, Zoom in/out. The rejected state will not be shown. If you click any table name from here then it will open up its table detail page separately.\nView Types:\nIt has two views;\nERD view\nGrid view\n(Same information as ERD view). Action column will be functional. From here you can update their statuses.\nComplexity Level:\nIt has two complexity types\nLevel 1 :\nMain table (i.e CUSTOMER2) which has relation with 7 other tables as shown below;\nLevel 2 :\nIn case those 7 tables have relation with further other tables as shown below;\nStep 20 Data Quality\nIt shows rule sets that are created on this table (CUSTOMER2) and this info will be as same as Analyze rule page.\nStep 21Â  Data Preparation\nIt will show all datasets here that are created on this particular table/column.\nStep 22Â  Â Comments\nAll comments will be shown here. If you click comments from top header, you will be directed down here. Also, you cannot delete other user's comment(s). You can delete your comment only.\nStep 23Â  Stewards\nYou can add and delete stewards from here as shown below;\nStep 24Â  Top Users\nIt will show list of users here based on most visited percentage.\nStep 25Â  Primary Attributes\nIt will be as same as Mass update fields shown in table detail page. User can edit from here as well.\nStep 26Â  Other Information\nIt will display same information as of Profiling detail page.",
    "scraped_at": "2026-02-02 15:42:11"
  },
  {
    "title": "Blend Views",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360016874754-Blend-Views",
    "content": "Overview\nA Blend View combines data from two or more tables of the same source. It's useful in cases the data model is normalized, and a data steward reviewing and correcting errors needs to see fields from multiple related tables. It can also be useful to aggregate data saved at different granularities in different tables.\nBlend Views are available for most data sources, but the exact details will depend on the source technology. If your relational database technology does not support full outer joins, then you may not use a full outer join in your blend view--though DvSum will not prevent you from defining such a view.\nCreate a Blend View\nBegin by selecting Profile â†’ Profiling â†’ \"âŠ• Create View\" (visible in the upper-right corner of the page)\nIn the\nCreate View\nwizard, specify the View Type as shown below.\nIn Step 2 of the\nCreate View\nwizard, select 2 or more tables. To create a join between them, click and drag from one table to the other.\nThe Join configuration pop up will provide you the option to select the join type and the columns. DvSum supports join types of inner, outer, left, or right joins, but you should confirm that your relational database technology supports your selected type. DvSum intelligently guesses which columns should be used for the join, but you may add columns or alter the columns used.\nOnce you save the join configurations, the translated join condition will appear.\nAdvanced Option:\nThe FUI interaction provides a simpler user experience in most cases. But an advanced user can provide the Join Query. As soon as you start typing query in the \"Translated Join Condition\" box, DvSum will try to determine the tables for you and automatically reverse engineer the specified join into graph above.\nYou may not see a success message. Sometimes DvSum will not be able to translate the join condition you have provided into a graph. It does not mean there is necessarily a mistake. If you are confident your join condition is correct, then these three steps will suffice to define the joins:\nSelect the Join Tables.\nEnter the Translated Join Condition.\nValidate the join condition.\nIn Step 2 of the\nCreate View\nwizard, the fields of all tables will be displayed on the left. Select the desired fields for your view.\nSave the Blend View.\nThe detail page for this view will open. From here chose \"Run Profiler\" to populate field values. You may use this Blend View as you would use any other table.",
    "scraped_at": "2026-02-02 15:42:16"
  },
  {
    "title": "Reference Dictionary",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360005669534-Reference-Dictionary",
    "content": "A Reference Dictionary is like a pre-organized reference book where users can create their own dictionaries, associating useful information with sets of values. This is helpful for DvSum validation audits, allowing users to choose a reference dictionary with a set of data values instead of listing individual ones.\nFor example, a user might create a \"Hired Personnel 2017\" dictionary with names and hiring dates of employees who joined in that year.\nWhen setting up a \"Value Range\" audit to ensure only hired personnel are included in a column, you can easily select the reference dictionary you've created, rather than manually entering the range.\nWhen you run the audit, it checks if your data matches the values in the reference dictionary you selected.",
    "scraped_at": "2026-02-02 15:42:21"
  },
  {
    "title": "Creating and editing a Catalog View",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115005116733-Creating-and-editing-a-Catalog-View",
    "content": "Data profiling provides you the basic information summary about a particular table of source which might not be enough for effective profiling and rule definition. Whereas, Catalog view generates customized information from multiple tables of sources. You can personalize these views depending on how you want to evaluate the data. That data is shown in the form of a table.Â You can then run profiling on that table just as on any other table of a source.\nFrom Profile tab in the left Navigation, go to profiling and click on \"Create View\". There are 3 steps in creating a view.\nStep 1:\nEnter View Name, type, source and table name.\nRead here to see\nhow to select the right Catalog View Type?\nStep 2:\nNow you can apply Conditions to filter the data in the required form.\nStep 3:\nNow you can select the table fields of your source. Only these selected fields will appear as columns of catalog view table.\nYou can change the name of the columns as well from the Fields Information section. That is it. Now you can \"Save\" the view.\nThis will take you to the View detail page where you can profile or create rules on this view.\nEdit View\nIf you want to edit any view, you will go back to the Profiling page and select the check-box for the view. Go to \"More Actions > Edit view\"\nThis will open the Edit View dialogue where you can make changes and save.",
    "scraped_at": "2026-02-02 15:42:26"
  },
  {
    "title": "Review Profiling Results",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/202846854-Review-Profiling-Results",
    "content": "You can access Data Profiling from Left Navigation, and then selecting the Source and Tables you want to view, of simply selecting a View from your history that is automatically displayed. Profiling information is also available when reviewing nodes in the Data Exploration section.\nThe initial Table detail is a good view to identify basic data exceptions such as Blanks, or Values that may be out of the expected or desired range. As outlined above, Rules can be created within Profiling by clicking on the Data Quality and Data Analysis tabs.\nSelecting the specific Column Name displays the distribution of the data to enable further analysis.\nSelecting the specific Column Name displays the distribution of the data to enable further analysis.",
    "scraped_at": "2026-02-02 15:42:31"
  },
  {
    "title": "Run Online and Run Offline",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/17472157533076-Run-Online-and-Run-Offline",
    "content": "Overview\nUsers can execute many different tasks in DvSum Data Quality (DQ). Testing a DQ rule might take just a few seconds. But some rules could take several minutes or even longer. Therefore DQ offers the ability to \"\nRun Online\n\" and \"\nRun Offline\n\" to give users the flexibility to run jobs as required.\nExecuting a rule synchronously and waiting to see the results is often the most efficient way to test a new rule. Executing a long-running job in the background and reviewing the results later is sometimes preferred.\nDetails\nThe option to \"\nExecute Now\n\" or \"\nRun Offline\"\napplies in several places.\nSource cataloging\nTable profiling\nRule execution\nThe options are complementary and give developers the flexibility to work as they prefer.\nRun\nOnline\n- Will immediately execute the action. Note that if the user navigates to another page while the task is executing, then the task will be aborted.\nRun Offline\n- Will schedule the action to run soon. Typically the task will begin execution within a few seconds. The job will be visible in the\nScheduler\ntab (Review â†’ Scheduler).\nSource cataloging\nAdministration â†’ Manage Sources\nTable Profiling\nTable profiling can be executed from multiple places.\nData Catalog â†’ Table Dictionary\nProfile â†’ Profiling\nTable Detail Page:\nRule Execution\nAudit â†’ Manage Rules\nRule Detail Page:\nReview â†’ Analyze Rules",
    "scraped_at": "2026-02-02 15:42:36"
  },
  {
    "title": "DQ Rule Details",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/17468996404756-DQ-Rule-Details",
    "content": "The latest update to our rule management system introduces offline and online execution capabilities, providing users with greater flexibility. The alert status of each rule is displayed as either \"Healthy\" or \"Alerting\" based on specified thresholds. Users can now easily edit rule definitions, descriptions, priorities, and scope, including window types and lookback days. Data aggregation into buckets enables detailed data quality checks. Various threshold types allow tracking and alerting based on exceptions. Rule notifications, action statuses, and scheduling options have been enhanced for improved rule management. The new history tab provides execution charts and grids for comprehensive performance analysis. Overall, these updates enhance rule customization, monitoring, and analysis.\nSome of the major changes that have been implemented in the User Interface are highlighted in the\nthe screenshot below for a quick review.\nThe Rule Detail page has undergone significant changes, both in its interface and the introduction of new functionalities. Let's explore these enhancements in detail:\nNot Scheduled\n-\nA notification at the top of the page is shown which indicates whether the rule is scheduled. If the rule is not scheduled it will not run automatically.\nAlert status\nfor the rule will be shown as\nHealthy\n- if the metric value is within specified threshold limits, then the alert status will be Healthy\nAlerting\n- If the metric value is not within specified threshold limits, then the status will be Alerting\nRun\nOnline & Offline rules can be run directly from the Rule detail page\nFor Details please check the article\nRun Online/Offline\nAction\nUsers can set the Action status for non-cleansable rules using the Action button. The action status button will be available for all rules, but in data cleansing, the user can enable workflow from the workflow tab and there won't be an action button since we are tracking using workflow.\nThe\nAction\nbutton has the following statuses:\nNew -\nWhen the rule is created, default status is set to New\nAcknowledge\nâ€“ When the user acknowledges the rule execution/ exception details\nResolve\nâ€“Â Once the exception has been resolved, the user has the option to perform an action and mark it as resolved upon completion\nTrack action item\nMore\nUpdate Schedule\n- The User can now have the ability to update the existing schedules of the rule and able to create a new one.\nRemove Schedule\nâ€“ This option allows the user to remove any existing schedule of the rule(s)\nDelete\n- This allows the user to permanently remove Rule\nClone\n- This allows the user to clone the current Rule\nEdit\nThis button will allow the user to update the following:\nOverview\nRule Description\n: Users can update the description for the rule\nPriority\n: Users can update the priority for the rule\nOpen Rule Definition:\nUsers can directly open the rule definition page from here\nScope\nIf the\nMetric Time\nfield is selected on the table level. Selected field name will be inherited to the rule as well\nOtherwise, the user can also select / update the metric time field at the rule level as well\nUsers can set the window type: - Used to define what is the scope of data to be selected.\nAll Data\n- By Default all the rules will window type as All data, which will consider all the data in the table during theÂ  execution of the rule\nData Max Time\n- In the case of incremental data, we can choose this option to run validation only on newly added data based on the timestamp available on the table\nClock Time\n-Â  In the case of incremental data, we can choose this option to run validation only on newly added data based on the current timestamp available on the table\nUsers can set the\nLookback days\nfor Data max time & Clock time\nUsers can optionally aggregate data into\nBuckets\nand DQ checks will be performed on each bucket.\nNo bucket\n1 Day\n1 Hour\nUsers can select available\nSlicer\noptions\nThreshold\nFor the Metric type, we can choose whether we want to track and alert based on the number of exceptions or percentage of exceptions.\nNo Threshold\n- Metric will not alert.\nConstant\n- Metric will be compared against constant thresholds.\nUser can set the Upper bound and Lower Bound\nRelative\n- Percentage change (increase or decrease) in metric compared to the previous bucket or execution. For DQ exception checks, the decrease will not alert.\nUser can set the percentage for the relative threshold type\nAdaptive\n- Thresholds auto-adjust based on observations using outlier detection techniques. It uses the Interquartile range technique to detect if the metric is an outlier.\nUser can choose the threshold bounds from the 3 available options:\nUpper and Lower\nUpper\nLower\nNotifications\nAssign a rule to the user - The user is able to assign a rule to any user\nAdd a rule group to the rule - The user is able to add a rule group to the current rule\nAdd/update the schedule for a rule -A new functionality is added to manage the scheduling of the rule\nOn the Lower Section of the Rule detail page, the user will be able to see only the 3 tabs by default:\nOne important change is the modification of the first tab in the lower panel:\nExisting\n: Currently the first tab is called the \"\nAnalysis\"\ntab\nNew\n: In the new update we have replaced the \"\nAnalysis\"\ntab with \"\nData\n\"\nUsers can click on\nShow More\nto view the other tabs available:\nAfter clicking on the\nShow More\ntab, user will be see the following tabs and if the user wants to Hide them they simply need to click on\nHide Others\nHistory\nThe history tab will show the execution history for the rule in the form of a chart\nExisting:\nIn the existing case the Trend was shown in the Insights tab as shown in the screenshot below:\nNew:\nA new tab has been introduced at the bottom of the Rule detail page named \"\nHistory\n\" which now shows two views that are as follows:\nChart View -\nIn this view, the data is shown to the user in the form of a chart\nWhen I select the Slicer, the data in the History Chart will get distributed based on Slicer Field\nGrid View -\nIn this view, the same data is shown to the user in a tabular form\nUsers can view execution history for:\nCurrent\n: It will show the metric value for the current execution\n30 Days\n: It will show metric value for 30 days\n90 Days:\nIt will show metric value for 90 days\nALL\n: It will show the metric value for All data",
    "scraped_at": "2026-02-02 15:42:43"
  },
  {
    "title": "Rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/17457922936468-Rules",
    "content": "Introducing a range of exciting updates to our rule management and analysis features. We are thrilled to enhance your experience by introducing offline rule execution capabilities and expanding the Action Grid Menu with more options. Users can now run rules both online and offline, ensuring continuous rule execution even in disconnected environments. The updated menu offers functionalities like updating and removing schedules, deleting and cloning rules, and performing mass updates. Additionally, the system now highlights recommended rules, providing improved visibility. These updates empower users to efficiently manage and analyze rules, enabling better decision-making and data quality control.\nManage Rules\nThe Manage Rules page has undergone significant changes, both in its interface and the introduction of new functionalities. Let's explore these enhancements in detail:\nIn the User interface, the following changes have been introduced:\nAvailable and Recommended Rules\nThere are 2 options regarding the rules:\nAvailable Rules\n- These are the existing rules that are created and can be run\nRecommended Rules\n- These are the rules that are recommended based on the profiling of the source's tables\nAlert Status -\nThis column has been added in the new release to show the alerting status of the rule based on the threshold limits\nThere are 2 alerting statuses:\nHealthy\n- if the metric value is within specified threshold limits, then the alert status will be Healthy\nAlerting\n- If the metric value is not within specified threshold limits, then the status will be Alerting\nNew Rule -\nUsers can create new rules based on their requirements from the following rules available\nFoundational DQ\nPipeline Checks\nCross-System DQ\nMDM\nData Diffs\nRun Rule\nRun\nOnline\n- This feature will immediately execute the reference action\nRun Offline\n- This feature will schedule the reference action to run ad-hoc and will be\nFor Details please check the article\nRun Online/Offline\nMore Actions\nUpdate Schedule\n- The User can now have the ability to update the existing schedules of the rule as we can able to create a new one.\nRemove Schedule\n- This option allows the user to remove any existing schedule of the rule(s)\nDelete rule\n- This option allows the user to Remove the rule permanently\nClone rule\n- This option allows the user to clone the current rule\nMass Update\n- Mass update allows the user to select multiple rules and update them at once\nAnalyze Rules\nReRun Rule\nRun\nOnline\n- This feature will immediately execute the reference action\nRun Offline\n- This feature will schedule the reference action to run ad-hoc and will be\nMore Actions\nUpdate Schedule\n- The User can now have the ability to update the existing schedules of the rule as we can able to create a new one.\nRemove Schedule\n- This option allows the user to remove any existing schedule of the rule(s)\nDelete rule\n- This option allows the user to Remove the rule permanently\nClone rule\n- This option allows the user to clone the current rule\nMass Update\n- Mass update allows the user to select multiple rules and update them at once\nUsers can\nExport\ngrid data - This allows the user to export the current grid\nFavorite\n- This allows the user to mark the current view as favorite\nShare\nthe view - The user can share the current view with any user\nMore\n:\nEdit\n- Allows the user to edit the current view settings\nClone\n- Allows the user to clone the current view\nDelete\n- Allows the user to permanently remove the current view\nAlert Status -\nThis column has been added in the new release to show the alerting status of the rule based on the threshold limits\nThere are 2 alerting statuses:\nHealthy\n- if the metric value is within specified threshold limits, then the alert status will be Healthy\nAlerting\n- If the metric value is not within specified threshold limits, then the status will be Alerting\nWorkflow Status\nA workflow status column has been added to the Default Analyze Rules Page",
    "scraped_at": "2026-02-02 15:42:48"
  },
  {
    "title": "Creating a CUSTOM QUERY DQ Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/15714519528980-Creating-a-CUSTOM-QUERY-DQ-Rule",
    "content": "Custom Query Summary\nThe \"CUSTOM QUERY\" Rule Type is a powerful feature that empowers users to apply custom logic using a SQL query. This query should be a SELECT statement designed to identify invalid records.\nDetails\nPreparation\nOutside of DvSum, users must create a SQL query to find invalid records. For instance, in a data model where STUDENTS can have multiple STUDENT_ADDRESSES, the query should identify cases where students have more than one address marked as primary (IS_PRIMARY set to true).\n/* query identifying students with too many primary addresses */\nselect s.STUDENT_ID, GIVEN_NAME, FAMILY_NAME\nfrom\nDEMO_DB.EDUCATION.STUDENTS s\nleft outer join DEMO_DB.EDUCATION.STUDENT_ADDRESS sa on ( sa.STUDENT_ID = s.STUDENT_ID )\ngroup by\ns.STUDENT_ID, GIVEN_NAME, FAMILY_NAME\nhaving\nsum(case when sa.IS_PRIMARY then 1 else 0 end) > 1\nSteps\nCreate the rule\nAudit â†’ Manage Rules â†’ âŠ•Add Rule â†’ Process Quality â†’ CUSTOM QUERY\nDefine the details\nDefine the Rule Description.\nSelect the relevant Data Source and Table Name.\nPaste in the Custom Query.\nSet the Threshold Min and Max to 0.\nSave.\nIn some advance cases you may use other thresholds. When the rule is intended to find all records in error, use zeros.\nRun the rule\nInitially, the summary information will be mostly empty.\nClick \"â–ºRun\" in the top right corner.\nAfter running the rule once, the summary information will contain more details.\nView the errors\nIf your query found any errors, then these errors will be visible on the Analysis tab.\nThe fields available in Exceptions Deep Dive are determined by the fields returned in your query.\nNext steps\nYour CUSTOM QUERY Data Quality rule should now be working. Common next steps include:\nEnable writeback\nin the rule to allow data stewards to fix error records.\nSet Rule Groups for this rule.\nSchedule the rule to run regularly.",
    "scraped_at": "2026-02-02 15:42:53"
  },
  {
    "title": "How to Create/Assign/Remove Rule Group or Val group & Batch Scheduling",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/13070366457620-How-to-Create-Assign-Remove-Rule-Group-or-Val-group-Batch-Scheduling",
    "content": "What are Rule Groups?\nRule Groups serve as categorizing \"labels\" for organizing your audits into meaningful groups. By applying a Rule Group label, you can categorize your Audits according to your desired grouping. Unlike folders, Rule Groups allow for multiple labels to be assigned to a single audit and vice versa, where an audit can belong to multiple Rule Groups.\nHow to Create a new/ Assign existing Rule Group?\nIn case you want to add a new Rule Group or Assign an existing one to your Audit you can do that from multiple places.\n1. Rule Detail Page\nAdding a New Rule Group\nSelect any rule, Go To the Rule detail page, Click on the Edit Button, and navigate to the Notification Section. In the screenshot below you can see the option of \"Rule Group\". You can simply type in the name you want to give to the new Rule Group and press enter and the new rule will be added and assigned to your audit after you have saved it.\nAdding single or multiple Existing Rule Groups\nIn case you want to add single or multiple rule groups that already exist to a new Audit you can simply select them from existing rule groups from the dropdown as shown in the screenshot below.\n2. Mass Updating Rule Groups\nMass update is another way of adding/removing and replacing single/multiple Rule groups to a single or multiple Rules. There are multiple operations that you can perform with Mass Update as follows:\n1. Adding one or multiple new Rule Groups to a single Rule\n2. Adding one or multiple existing Rule Groups to a single Rule\n3. Adding one or multiple new Rule Groups to multiple Rules\n4. Adding one or multiple existing Rule Groups to multiple Rules\n5. Replacing/ Removing one or multiple Rule Groups from a single or multiple Rules\nMass update operations can be performed from both Manage Rules (For Legacy Accounts only) and Analyze Rules:\nManage Rules:\nAnalyze Rule:\nBatch Execution\nBatch execution is an operation that allows a user to Run a single or more Rule Groups at the same time.\nDetails of the Run History are shown at the bottom of the page.\nExecute Via a Schedule\nYou can create a\nSchedule\nby following the steps:\nSelect Group -> Add group -> Schedule Rule\nNext, you need to enter the following details if you want to share an email alert:\n1. Internal Recipients\n2. Recipient Groups\n3. External Recipients\n4. Select the Repeat configuration from dropdown. (One-time/ Daily/ Weekly/ Monthly / Monitor)\n5. Select the start date & time for the schedule\n6. Click on the Save button in the bottom right corner:\nNote\n: \"\nSend schedule completion email\" will send an email on job completion\n\"Send email only on alerts\" will send email only rule is\nAlerting\nExecute Via a Script\nYou can also schedule them\nVia a Script\nby following the steps:\nSelect Group -> Add group -> Schedule Rule\nNext, you need to enter the following details if you want to share an email alert:\n1. Internal Recipients\n2. Recipient Groups\n3. External Recipients\n4. Click on Generate Script in the bottom right corner:\nNote\n: \"\nSend schedule completion email\" will send an email on job completion\n\"Send email only on alerts\" will send email only rule is\nAlerting\nYou will now receive the\nScript information\nand\nDynamic Source Script\nin the respective blocks:\nYou can copy the script and execute it to run the rule.",
    "scraped_at": "2026-02-02 15:42:58"
  },
  {
    "title": "Executing the rule API via ADF",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/10311781361556-Executing-the-rule-API-via-ADF",
    "content": "Rules in dvsum can be executed using the azure data factory. To run a rule or set of rules through a script, you can generate a script for those rules using the Schedule Rule Menu.\nThe script takes the form of\nhttps://<saws\naddress>:<saws_port>/runJob?jobId=<jobid>\nUsers can generate the script for the rule and use the script URL to execute the rule. Let's get started step by step:\nStep 1:\nOpen the Dvsum application, go to Audit >> Manage Rules >> select any existing rule >> click More action >> select Schedule Rule as shown below:\nStep 1.1:\nDynamic Source Script Generation\nGo to Generate Script tab and click on the button at the bottom to generate the script as shown below;\nStep 1.2:\nA job is created against the rule. Now go to the main scheduler page, search the rule, and a newly created job will be visible with a description and status as â€œstand byâ€.\nStep 2:\nGo to Azure Data Factory and paste the copied URL in api call. Click on Debug button to start the execution.\nStep 2.1:\nOnce the execution is completed. it will print the remarks in API call output same as shown on the rule detail page in Dvsum.\nOn the Scheduler page, the job status changes to \"Completed\".\nThe return_code always contains 3 values\n0Â  -> PASS if a rule has 0 exceptions or exceptions are less than the lower bound of the threshold\n-1 -> WARN if the rule has exceptions but exceptions are greater than the lower bound but less than the upper bound\n-2 -> FAIL if the rule has exceptions greater than the upper bound limit.\nThe return_remark contains text that will provide the name of the rule and additional information that you typically see in the system remark field on the Rule Detail page.\nRelated Articles:\nDatabricks as a Source\nIntegrating Rules into batch workflow",
    "scraped_at": "2026-02-02 15:43:03"
  },
  {
    "title": "Execute/run Scheduled Jobs",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115000205853-Execute-run-Scheduled-Jobs",
    "content": "The jobs that are scheduled using DvSum Web Application are executed by SAWS. ItÂ continuously interrogates the web application using the API token to check the readiness or state of the scheduled jobs that are to be executed.\nThere are two prerequisites for running the scheduled jobs.\nAPI token needs to be added in the configuration file when SAWS is downloaded on a system. You can ask your administrator to provide you the API key. (If required, please refer to\nHow to install and Configure SAWS file\n)\nYour IP address needs to be white listed as DvSum Application blocks connections from remote locations.",
    "scraped_at": "2026-02-02 15:43:07"
  },
  {
    "title": "File upload to DvSum and creating rules on file",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360014113613-File-upload-to-DvSum-and-creating-rules-on-file",
    "content": "File upload requires user to create a folder where a file can be dropped. The idea behind file upload is the reuse-ability of rules on a dataset. In a real life scenario, a user uploads a file and create a source in DvSum which uses that file. That file is then profiled and certain rules are made and executed on it. The report generated by these rules provides important stats to make day-to-day business decisions.\nWhen the data in the file changes, a user simply uploads the new file to the folder and runs the same rules again. This saves user the time of repeating the process of profiling and creating rules again.\nNote:Â This folder should be in the same machine as the SAWS associated with it.\nFollow the steps below to upload file and get started:\nStep 1: Create Folder\nFrom Administration tab, go to Manage Account > Folders and add a folder. You need to specify folder name, folder path and the associated SAWS.\nOnce a folder is added, it will show on the grid and on the\nfile upload page\n.\nStep 2: Upload File\nFile Upload page shows the folders added by all users. If you hover over the folder you created, it will show you the path set for it. Simply add the files in drop-zone and they will be uploaded to the folder. (only excel, csv and txt files are allowed)\nStep 3: Add File as Source\nNow you can create a source using this excel file from Administration > Manage Sources page.\nSpecify the path for where the file is (This could be remote location as well where remote SAWS is running). After saving, \"Configure\" the source and it will open all sheets of excel file.\nNow that the source is added, you can profile it and create DvSum rules on it.\nAdvanced Option:\nIf you want to apply preprocess properties to file in order to cleanse it before creating rules on it, read the article\nExcel Cleansing Upon File Upload\n.",
    "scraped_at": "2026-02-02 15:43:13"
  },
  {
    "title": "Excel Cleansing Upon File Upload",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360013799514-Excel-Cleansing-Upon-File-Upload",
    "content": "EXCEL Default Cleansing\nWhen a file is uploaded, it is automatically cleansed to make it compatible for DvSum use. DvSum always reads the first row as \"header\" and rest of the rows as \"data\".\nThe default cleansing includes:\nRemoving blank rows\nReplacing white spaces within a column name with underscore e.g. First Column will be changed to First_Column\nSpecial characters from column name are removed\nDuplicate columns are dropped\nHowever in some cases, the default cleansing is not enough and the user needs to explicitly specify if any rows needs to be skipped or specify the format for number columns where the data automatically truncates the values up to required decimal precision, for that purpose, a properties file associated with every uploaded file is available for user to edit.\nEXCEL Pre-process properties\nWhen an excel file is uploaded to folder, properties file is created with the same name as the uploaded file name. A user can manually set the pre-process options to change file formatting. So whenever the same file is uploaded again to the folder, DvSum will automatically apply the pre-process options and cleanse the file accordingly.\nFor example, below is the un-processed file.\n1. There is some extra information in row 1, 3 and 4 which cannot be read by DvSum. User manually needs to inform DvSum to skip these.\n2. Any spaces in between column names will be replaced by underscore and special characters will be removed automatically.\n3. Any blanks rows within the data will be removed automatically.\n4. For number fields, User can inform DvSum to cleanse the values up to a specific decimal precision.\nOnce this file is uploaded to folder, basic cleansing (as mentioned before) will be applied by DvSum and pre-process properties for upload file will be created.\nOpen the pre-process properties file with the name as the original file (AES2test)\nSave the properties file. An important step is to upload the file again to the target folder. This will format the excel source file according to the changes made in preprocess properties.\nNow the file is cleansed and ready to use in DvSum. If you configure this excel source from Manage Sources page, it will show you the cleansed file.\nValidate JSON - Verify syntax of pre-process.properties file\nAs a good practice, we recommend you to validate json of preprocess file to make sure it has no syntax errors. You can validate using\nhttps://jsonlint.com/\nCopy all the content of preprocess.properties file and paste it in text field of jsonlint.com and click on \"Validate JSON\". If there is no syntax error, it will say Valid JSON.\nIf JSON is invalid, it will show you the line where there is an error. Here, you can see there is error on line 11 because there is semi-colon (;) added instead of comma (,)\nIt is recommended to validate json every time you make changes to your pre-process file.",
    "scraped_at": "2026-02-02 15:43:19"
  },
  {
    "title": "Data Preparation Workbench",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360035713813-Data-Preparation-Workbench",
    "content": "Introducing\nDvSum\nData Preparation Workbench\n, a place where you can access data from source conveniently, prepare and perform cleansing actions and commit validated data back to source. Let's start.\nOn your DvSum homepage, you will see Data Preparation in left navigation. Go to \"Datasets\"\nYou can create multiple datasets on a single table. With every dataset you create, the idea is to filter some part of data for analysis that is according to your requirements.Â That filtered data will open in a workbench.\nNote: For now, Data Preparation Workbench functionality is only applicable for\nOracle and SQL Server\nsources only\nCreate Dataset\nThere are 3 simple steps to create a dataset.\nStep 1\n- Click on Create button and provide source and table name on which dataset is to be created.\nStep 2\n- Choose Columns and Apply filters (if required)\nHere, you can choose the columns you want to see on workbench and the order by which you want to view them on Workbench. You can also apply filters to dataset according to your requirements.\nStep 3\n- Provide a Unique Identifier, Sync All Dataset User Views (optional), Execute Stored Procedure(optional) and Sharing.\n1. A \"\nUnique identifier\"\nof the table needs to be selected.\n2. \"\nSync All Dataset User Views\n\", will update and sync all the views that you have created on the workbench that are linked.\n3. \"\nExecute Stored Procedure\n\", this will execute stored procedure on commit or on consolidate, write procedure including parameter name.\n3. If you want to \"\nShare\"\nthis dataset with others, select the required option.\nThis is it. You will see the created dataset on the dataset page and when you click on the name, it will open a workbench fetching the data accordingly.\nWorkbench Operations\nThere are multiple operations you can perform on workbench. Here is a list of few:\n1. Inline Editing\nClick on a cell and start editing. The changes will automatically be saved and you can commit the changes to write-back to source.\n2. Drag And Copy\nYou can select a cell and drag cell-selection outline to the place where you want the copy to appear and release the mouse button. Pressing CTRL + D will copy the value to all selected cells.\n3. Find and Replace\nThis is a bulk-update operation performed on a single column. You can even choose to fill all blanks with a particular value across the data source.\nSelect a column and you will see Find and Replace Option.\n4. Delete Rows\nThis is a row level operation. Once you select row(s), you will see the option to Remove Rows. You can remove single or multiple (at max 100) rows at a time. Please note that this will permanently remove the records form your data source and you will not be able to Undo after Committing.\n5. Apply Validations for Write-back\nSometimes there might be certain validations required on a column for write-back operation. For example, Customer_ID should have no blanks, or for a string Column \"Country\" the input value should have only 2 characters (US, UK etc)\nSuch validations can be applied from Profiling page using Master Completeness Ruleset (MDC Ruleset) on a table.\nThere are 3 types of data constraints you can apply from MDC ruleset\n1. Blanks Check\nOn a column where blanks check is applied, workbench will highlight null fields are red and it will not take \"Blank\" as an input\n2. Value Range\nAllow only a specific range of values as valid\n3. Data Format\nConstraints\nValidations on string (character length limit), Number (only integers, whole numbers or decimal values) are to be allowed.\nThese rules are reflected on Workbench. You will see a small badge for each of the rules above (B, VR, DF) below the column name on which rule is applied.\nWhen you hover on the DF badge, it will show you the validation and highlight the cells which do not meet the criteria.\n6. Apply DvSum Rules as Filters\nThe DvSum rules, like Custom Query, Blanks, Value Range, MDC Ruleset, Orphan Records, Integrity Check created on a particular table can help you filter out the data on workbench. You can select one rule at a time on workbench and it will bring only those records which are shown as exception on rule detail page.\nClick on the \"Select a Rule\" drop-down where you will see only the above mentioned rules. If the rule has not been executed, it will show as disabled. Similarly, rules other than the listed will be disabled.\nThe selected rule will start to show on top and it will fetch the records according to rule definition.Â  This is helpful when you want to perform cleansing actions on selected data.\n7. Reference Column - Data Preparation Workbench\nIf a column has a reference column defined in Field configuration, when we update key column value in Prep workbench, its reference column would also get updated accordingly.\nProfiling - Field Configuration Update -\nWe will now have a Reference Mapping section where user can define Reference column for a Key column\nReference Dictionary Update -\nCurrently, you were able to create reference dictionary by manually adding values from UI (by adding values for the fields) i.e name and description.\nNow, there will be a checkbox 'Is Key Value Pair', if marked this as checked, user will be able to create a Key Value based dictionary.\nOn marking this checkbox checked, you will be able to add key, value and description. Else you can import values\nOn importing values for a column whose reference is not defined in Field Configuration, it will prompt you to select a key column for it and the Reference column will be set in the Field Configuration automatically\nOn importing values for a column whose reference is already defined, it will prompt you. On importing values, it will create a reference dictionary containing distinct values of both columns\nOn saving, it will create a Key Value Based reference dictionary with Is Key Value Pair\nNote:\nWhile creating a MDC rule, on Reference column defined in Field configuration, it will not allow you to create a Value Range rule with normal dictionaries. It will give you a warning message to select a Key Value Pair Dictionary from the list\nData Preparation Workbench Updates-\nOn updating a value of a reference column (if defined in Field Configuration) in the Workbench, it should automatically update the value in its key column accordingly.\nAll those columns having write back enabled will have icon 'Write Back' enabled for them.\nThe defined unique Identifier Column, Key Column will have write back disabled with grayed out column header.\nKey Column will also have a chain icon in the column header. On hover, it will show the related information i.e the reference column defined for it.\nOn updating a value in the Reference Column, it will automatically update the value in the key column accordingly\nThe same operation can also be performed via Find and Replace. Clicking the reference column header checkbox, it will enable the Find and Replace button and values can be updated as required",
    "scraped_at": "2026-02-02 15:43:24"
  },
  {
    "title": "Off-line Write-back to Source in DQ workflow",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360013797874-Off-line-Write-back-to-Source-in-DQ-workflow",
    "content": "Among the Workflow Configuration Steps, there is a setting of commit parameter as \"Off-line Write-back to source\".\nBy default, when a DQ Workflow executes, DvSum writes back the fixes of exceptions to the source. With off-line write-back, user can cleanse the exception records on DvSum cleanse workbench UI and save changes but it would be required to manually commit fixes to source outside DvSum.",
    "scraped_at": "2026-02-02 15:43:28"
  },
  {
    "title": "Cleansing Work Bench and Audit Trail",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/219786788-Cleansing-Work-Bench-and-Audit-Trail",
    "content": "Issue:\nIf I run a rule and immediately click on Cleanse to open workbench. I notice that the Write-back shows as disabled, even though I have it enabled. Additionally, my old cleansing audit trail shows up. If I want for a few seconds after running the rule and open it, I don't see the above issues. Why?\nReason:\nThe exception cleansing information experiences a brief delay in loading after the completion of rule execution due to the asynchronous generation of Cleansing data following audit execution. Typically, when the rule is run in the background, users are notified of rule completion, and upon logging in to resolve, the exception data is already loaded.",
    "scraped_at": "2026-02-02 15:43:32"
  },
  {
    "title": "How to create a Workflow with excel file as input ?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360033768933-How-to-create-a-Workflow-with-excel-file-as-input",
    "content": "Follow along the steps to use Excel file as an input in a Workflow;\n1. Create a target Folder\nFrom Administration > Manage Account > Folder tab, create a folder. Provide the path of the folder where you want to upload excel file.\n2. Upload file to folder\nFrom File upload on left navigation, select the folder you created. Upload the unprocessed excel file in this folder. This is how an unprocessed file will look like.\nOnce uploaded to target folder, DvSum will run pre-processing on the file and apply basic cleansing.\nBasic cleansing includes removing blank rows, replacing white spaces within a column name with underscore e.g. First Column will be changed to First_Column. Special characters from column name are removed. Also, the duplicate columns are dropped. Below is the image how the file looks like after basic cleansing.\nPre-processing file is created against the uploaded excel source in target folder. To do so, go to the target folder and there will be a new folder called \"ExcelPreProcessProps\".\nOpen the folder to access the preprocess.properties file. It a note-pad file with the same name as the excel source file with \"_preProcess\" appended at the end.\n3. Custom Cleansing with Pre-process.properties file\nIn some cases, the default cleansing is not enough. Sometimes a user needs to tell DvSum to skip rows which do not contain actual data. Or specify the format for number columns where the data automatically truncates the values up to required decimal precision. For that purpose, a properties file associated with every uploaded file is available for user to edit.\nAs an example, lets change the data type of one column to \"str\".Â A particular use case when your data source has numeric field values but you want DvSum to read it as string. For example, in the case below where you want the field value \"0000000000000000043\" to be read as it is and not as \"43\", you would have to mention the data type \"str\" in pre-process file. DvSum source requires to explicitly mention the dataType in such case.\nFor DvSum to read \"Article_ID\" as string and not as number, open pre-process propertiles file and set the dataType as \"str\". Save the pre-process properties files.\nFor more details, please read\nEXCEL Pre-process properties\n4. Upload file again to apply latest preprocess.properties\nOnce you have saved the changes in preprocess properties file, you will be required to upload the file again (same as step 2). This will apply the latest changes to the file.\nThis will clean and format the excel source the way it is required. Now it is ready to be configured in DvSum.\n5. Add Excel in Memory Source\nFrom Administration > Manage Sources > Add Source.\nProvide a name and select the source type as Excel in Memory. You would be required to provide File System Path (root folder path where the excel file is placed)\nSave to add this source in DvSum.\n6. Configure Source\nClick on \"Configure\" button on the bottom left corner. It will show you all the tabs/sheets of your excel source. Select a tab and select the option \"My Data has Headers\". This will show you the name of the columns. Select the columns you want to configure in DvSum.\nNote: The columns that you specified as \"str\" in pre process.properties file are to be explicitly set as \"String\" from configure page.\nWhen you click on \"Add\", those columns will show the status as \"Cataloged\". Click on Finish.\n7. Profile Source\nFrom Profile > Profiling page, select the source and table - Run profiler. Once completed, go to \"Show Details\" to view the data which will now be appearing according to the pre-process properties.\n8. Create Rules\nFrom Profiling, you can create MDC ruleset and DAE rules. You can also create any other rules like Value Range, Blanks check, Orphan Records or Integrity check rules on your excel source.\nExecute the rules to extract exception records. The idea is to use these rules in a workflow and cleanse these there.\n9. Create a Workflow\nFrom Workflows > Manage Workflows, Create a workflow.\nOnce you Save the workflow, you will land on Workflow Designer page. Complete the Workflow by adding Step widgets and making their end to end connection.\nNow in order to add tasks to a step, go to detail page of that step and click on \"Add Task\"\n9.1 File Upload Task\nProvide a name to task and choose type as File Upload Task. In the section File Upload Details, select the same folder that you created in Step 1 (where the file was uploaded). Also, make sure you have checked \"Upload any file\" and \"Apply pre-processing\" options.\n9.2. Validation Task\nProvide a name to task and choose type as \"Validation Task\". In the section Rule List, select the rules created on the configured excel source. You can also select them by filtering rules by source name or Rule group. Also, make sure you have enabled \"Allow dynamic file selection\" check in order to run rules against the dynamic file.\nIf you want the task to be Mandatory, enable Mandatory checkbox.\nThe mandatory step in rule validation can sometimes cause hard-stop in workflow execution. In some cases, it may be acceptable to have exceptions for rule and a user may want to be able to over-ride the failed and mark the validation step as complete by first getting them \"Approved\"\nEnable \"Select Approver\" and select from the group of users or a single user for this role.\nNote\n: Approvers can be appointed only for mandatory Validation Tasks.\n10. Publish workflow\nOnce you have defined the tasks within the steps of workflow, go back to the Workflow Designer window and click on \"Publish\" button. This will change the status of workflow from Draft to Published.\n11. Execute Workflow\nFrom New Workflow Request on the top right corner, create an execution for this workflow.\nProvide the required details and click on Initiate.\n12. Upload dynamic file from workflow\nOnce the execution starts, the step owner will be notified about progress. He/She will then complete the tasks and submit them. User is required to upload a dynamic file, execute validation rules against the uploaded file and cleanse the exceptions.\n12.1 File Upload task\nFrom the task detail page, Click on Upload file and select the file to be uploaded.\nUpon clicking Save, DvSum will apply the pre-process properties to file and cleanse it. Now you can execute the validation rules on it.\n12.2 Validation Rules Task\nFrom the task detail page, Click on Execute. This will show you a dialogue to select a data source against which the rules are to be executed. The file uploaded in previous step will be automatically selected.\nIf the execution of rules passes, then you mark the step as complete. If it fails, you will have the option to cleanse the exceptions or assign step to \"Approver\"\n13. Fix exceptions and assign Step to Approver\nClick on the Validation rule task to see the Rules. You can go to the rule detail page and fix the required exceptions.\nIf there are still exceptions left and you want to be able to over-ride these exceptions and move to the next step of Workflow, assign the step to \"Approver\" to take a decision whether the exceptions fixed are acceptable or not.\nThe Approver can approve the changes and the step can now be marked as complete.\nThe execution will go back to the Step Owner to submit and move to next step of workflow.\nOther Related Articles\nHow can I add new fields in a Excel upload Template?\nData Management Workflow Alerts\nWorkflow Digest Emails\nHow to appoint an Approver for Validation Task in a workflow?\nWho are Watchers and how to add them in a workflow?\nWhat are the types of tasks that can be created in a process step of a workflow?",
    "scraped_at": "2026-02-02 15:43:37"
  },
  {
    "title": "Why is Data Analysis tab not showing for the profiled table?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360022912733-Why-is-Data-Analysis-tab-not-showing-for-the-profiled-table",
    "content": "Sometimes when you profile a table of your data source, Data Analysis tab does not appear in table detail page. For Data Analysis tab to be enabled, there must be at least 2 fields marked as attributes.\nStep 1 -\nFrom\nProfiling pageÂ  > Show details of table you cannot see Data Analysis tab\nStep 2 -\nFrom Field Configuration tab, mark at least 2 fields as attributes.\nStep 3 -\nGo to previous page and profile the table again.\nStep 4 -\nGo to Show details and now Data Analysis tab will be enabled.",
    "scraped_at": "2026-02-02 15:43:45"
  },
  {
    "title": "Staging Configuration: How to set up staging table ?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360021780333-Staging-Configuration-How-to-set-up-staging-table",
    "content": "For rules like Address Validation and Match-Merge, DvSum works on staging tables i.e copy of the original tables. The changes committed back to source will be written on staging table.\nStep 1:\nTo create Staging Table, go to Profiling page and select source and table > Show Details\nStep 2:\nEnable Match Merge and enter description, click SAVE.\nStep 3:\nClick on \"Generate Script\"\nStep 4:\nRun the script on your Datasource to create staging table and then Refresh catalog. This table will start showing on profiling page.\nYou can profile it and create Address validation rules on it.",
    "scraped_at": "2026-02-02 15:43:52"
  },
  {
    "title": "How can I send Scheduler job emails to other users?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360021367293-How-can-I-send-Scheduler-job-emails-to-other-users",
    "content": "When a job is scheduled, sometimes it is required to send email alerts containing summary of rule execution to multiple users. These users may or may not exist in DvSum.\nSchedule a rule, you will see the option to send the email to Internal Recipients, Groups or External Recipients. This option is available for On demand scheduling, schedule for later and Generate Script.\nInternal Recipients\n- Users signed up on DvSum application\nRecipient Groups\n- Group of DvSum users from Administration > Manage Groups page\nExternal Recipients\n- Users not signed up on DvSum but added as external users from Administration > Manage Users > External tab\nFor detailed help, read our article on\nHow to add External Users as Recipients?\nNote: Email is addressed to the person who creates the schedule and a carbon-copy is sent to all other users. This way it is easy for recipients to communicate on the same email thread.",
    "scraped_at": "2026-02-02 15:43:57"
  },
  {
    "title": "Can I add new columns to my already cataloged and profiled EXCEL source?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360013703693-Can-I-add-new-columns-to-my-already-cataloged-and-profiled-EXCEL-source",
    "content": "If an excel source is already cataloged and profiled and a user wishes to add new columns, he can add, catalog and profile them individually and it will not have an impact on profiling of old columns or affect any rules created on source previously.\n- Navigate to the Administration tab\n- Configure the Excel source\n- Add the desired column\n- Click on Finish\nThis will catalog the new column:\nUpon visiting the profiling details for this source, the added column will be visible. You can then individually profile and establish rules for this column without any impact on your existing rules.\nNote:\nMerge Catalog feature is available for both EXCEL and CSV.",
    "scraped_at": "2026-02-02 15:44:02"
  },
  {
    "title": "How to create Custom Governance Views for Rules?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360008059154-How-to-create-Custom-Governance-Views-for-Rules",
    "content": "You can create custom view in DvSum by selecting the fields and defining filtering criteria according to your requirement, once created, these views will appear in \"Created by me\" section.\nThis capability is available for Rules list and Workflow execution list. To create a custom view, follow the steps below.\n1.\nGo to Review > Analyze Rules > Analyze rules drop down > Create View\n2.\nA dialog will open where you enter relative information.\nEnter name for your view and drag the available fields to selected. These field will appear in your custom view.\n3.\nSpecify the criteria\n4.\nFor \"Shared with\" section, choose who can see the view.\n5.\nSave this view and it will appear in \"Created by me\" section of drop down.\nYou can mark the view as favourite, edit, clone or delete a view from the ellipsis button on top tight corner.\nYou cannot edit a view that is created by someone else and shared with you. You can however clone it and then edit it to create your own version.\nNote: You can create a custom view for Workflow execution list in a similar way from Workflows > Execution list > Workflow Exec List drop down > Create View.",
    "scraped_at": "2026-02-02 15:44:08"
  },
  {
    "title": "How can I export/download Rule Exception Report?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360006879773-How-can-I-export-download-Rule-Exception-Report",
    "content": "DvSum rule execution allows you to see the potential bad records in your data source. It also saves these exceptions in to an excel file which can be downloaded from Analysis tab on the rule detail page.\nThe \"Export\" button will download an excel file containing exceptions for this rule but this button is only enabled when there are\nexceptions\nin the run result. If the executed rule passes, which means there are no bad records identified in your data source, then, there will not be any report to download. Hence, the Analysis tab will be disabled.",
    "scraped_at": "2026-02-02 15:44:12"
  },
  {
    "title": "Why don't I see all users in list of owners in glossary?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360002074473-Why-don-t-I-see-all-users-in-list-of-owners-in-glossary",
    "content": "DvSum Owners can practice fine-grain or global access control over the categories of Business Glossary. This means only those users which have given access to a particular category will be able to\nview\nor\nedit\nthe terms belonging to those category.\nFor example, if\nAlice\nmanages (edit rights) the Category\nIncome. Bob\ncarries out some tasks over this category and he has edit rights for this category as well. However,\nJohn\ncan only read (view rights) the terms defined under\nIncome.\nIf\nTerm1\nis defined under\nIncome\n. This means only Alice and Bob can be assigned the role of Term Owner or Term Steward. Bob can view the detail page of the terms but he wont be able to make any edit changes.\nNote: You can learn more about\nWhat is Fine-Grain and Global Glossary Access Control?\nhere.",
    "scraped_at": "2026-02-02 15:44:17"
  },
  {
    "title": "Edge Gateway (SAWS) not accessible",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360001182373-Edge-Gateway-SAWS-not-accessible",
    "content": "Overview\nThe DvSum Edge Gateway (SAWS) always runs over HTTPS. It sometimes uses a self-signed certificate. In cases where it uses a self-signed certificate, you need to add an exception to allow the browser to communicate with the web service.\nTroubleshooting\nClient not white-listed\nSymptom: Warning Icon\nOnly browser clients with a white-listed IP address are able to access the Edge Gateway Servers (SAWS). If your IP address is not white-listed, you will see this warning icon icon:\nSolution:\nAdd your IP address to the White List\n.\nEdge Gateway (SAWS) not accessible\nSymptom: Red Cloud Icon\nThe DvSum application running on prod.dvsum.com uses a valid, secure, trusted certificate. But the gateway server in use is using a self-signed certificate. This means that communication passing between the browser and the gateway server is still encrypted, but the certificate is not trusted by any certificate authority. The situation is indicated visually with the red cloud icon:\nYou may register an exception with your browser to allow this connection.\nTo do so, click on the Cloud icon. It will open the list of edge gateway servers (most customers use only a single server). Click on the web-service with the red cloud icon. It will open the web-service page which will warn you of connection not being private. Click on â€œShow me advanced optionsâ€ â†’ â€œProceed to <<IP Address>>â€.Â  This is an internal company connection, and it is fully secure.\nOnce you click â€œContinueâ€ or â€œProceedâ€, the next page should display the SAWS Management Console as shown below. This means that your browser can now connect to the web service over a secure connection. You do not need to login to DvSum SAWS Console. Simply close this page to return to the DvSum application.\nIn the DvSum Data Quality application, the web service icons will turn green now:\nYou can execute data quality functions on your data sources.",
    "scraped_at": "2026-02-02 15:44:22"
  },
  {
    "title": "How to create a Custom Dashboard?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000260754-How-to-create-a-Custom-Dashboard",
    "content": "Step 1: Create a Dashboard\nGo to Dashboards >> Manage Dashboards Page. Click on \"Add\" button. Specify a name and description. You can also choose to\nShare\nyour Dashboard with others or keep it to yourself.\nWhen you \"Save\" this Dashboard, you will be navigated to \"My Dashboard\" tab where the newly created dashboard opens. Since you need to add widgets to your dashboard, an \"Add Widget\" dialogue will appear.\nStep 2: Add widgets\nYou can add template widgets to your dashboard.\nUpon selecting a template widget, it opens the widget dialogue which allows you to specify on which data source do you want the template widget is to display the information (\nexplained in next step\n). These template widgets auto-populate the preferred choice for\nchart type\nrelated to\nslicers\n(Y-axis) and\nmetric\n(X-axis). You can\nsort\nhow the data appears on the chart and enable\nDatalabels\n(legends) on charts.\nStep 3: Create a View\nClick on \"Create View\". It opens an \"Add Widget - Create View\" dialogue. Based on this View, the widget will display the information.\nEnter a Name for View. Specifying columns wont appear on the chart because thats what slicers and metric do. But that is required to create a view. However the conditions that you apply on a view under the \"Specify Criteria\" section filters the data accordingly before displaying it on the dashboard.\nWhen you save this view, it will be populated in the \"Add Widget\" dialogue of step 2 (image shown below).\nSave this widget to make it appear on Dashboard. This completes Step 2.\nStep 4: Dashboard\nThe widget will start appearing on Dashboard. You can add more widget by clicking on \"\nAdd Widget\n\" Button. You can mark this dashboard as\nFavorite\nor\nEdit\n,\nClone\nand\nDelete\nit.\nOnce you mark the Dashboard as Favorite, it appears in the Dashboard drop-down under the \"Favorites\" section along with \"Created by me\" dashboards and \"Shared with me\" Dashboards.\nNote: You can view the dashboards shared with you but you cannot edit them. You may however clone the dashboard and edit it according to your liking.",
    "scraped_at": "2026-02-02 15:44:27"
  },
  {
    "title": "How do you select the right chart based on your data?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000024413-How-do-you-select-the-right-chart-based-on-your-data",
    "content": "First, identify the purpose of the chart. When you understand why you need to create a chart and with what information, youâ€™ll be able to select the best chart type to make it more readable. There are several chart types available to you while creating a widget.\nIf you want to compare things, you may choose a\nbar chart\n,\nline chart\nor\nlist\n.\nIf you want relationship analysis,\ntable\nis the best option.\nIf you want to show distribution, you may go with a\ncolumn chart\n.\nIf you want to show composition,\nstacked column\nor\nbar charts\nare preferred.\nIf you want to show trends over time, a\nline chart\nis a great option.\nif you want to represent parts of a whole, a\ndonut chart\nwould work well.",
    "scraped_at": "2026-02-02 15:44:32"
  },
  {
    "title": "How do my Scheduled jobs execute with MultiSaws?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000018334-How-do-my-Scheduled-jobs-execute-with-MultiSaws",
    "content": "If you have scheduled a job where\nProfiling tables\nor\nRules\nare associated with multiple SAWS, the scheduler will split this job into separate jobs. Each of these jobs execute independent of the other with the SAWS they are mapped to.\nFor e.g. if a job has 4 Rules to run.Â  2 Rules are associated with two SAWS S1 and the other with S2.\nThe scheduler will give this job a name and split them into two. Letâ€™s say the Job name is JBS â€“ 00951. It will split into JB â€“ 00951 001 and JB â€“ 00951 002.\nJB â€“ 00951 001 is mapped to Saws S1\nJB â€“ 00951 002 is mapped to Saws S2\nThis way even if S2 is not up and running, JB â€“ 00951 001 will have no impact and it will still be executed.\nSimilarly for profiling of two tables associated with two SAWS, the job breaks up into two.\nThe job JBS-000971001 and JBS-000971002 are executed independent of each other.",
    "scraped_at": "2026-02-02 15:44:37"
  },
  {
    "title": "How to reassign responsibilities of a user upon deletion?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115001891934-How-to-reassign-responsibilities-of-a-user-upon-deletion",
    "content": "Once a user is added by an administrator, he can only be soft deleted. i.e. the status of the user will change to â€œDeletedâ€ and upon deletion, all of his responsibilities will be assigned to another user.\nWhen you Delete a User; An administrator can reassign his responsibilities to another user, admin or owner of DvSum account\nWhen you Delete an Administrator; An administrator, his responsibilities are reassigned to another administrator or owner of DvSum account.",
    "scraped_at": "2026-02-02 15:44:41"
  },
  {
    "title": "What is Fine-Grain and Global Workflow Access Control?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115001889873-What-is-Fine-Grain-and-Global-Workflow-Access-Control",
    "content": "Workflow Control\nThe two levels of control provided to users over Workflows are\nWorkflow fine-grain access control\nGlobal (when the checkbox for Workflow fine-grain access control is disabled)\nFine Grain Disabled (Global Access)\nWhen fine grain checkbox is disabled, it means the users of a particular group are given global access to â€œallâ€ the categories of Workflow.\nOn all categories, there are further 3 levels of access. i.e. Execute, Manage, No Access. An admin can set the access for a group from edit/add group dialog.\nExecute Workflows\nOn Workflows Manage and Execution listing pages, all the WFs of all the categories will be visible to the users of that particular group. It means that the users can only view the WF list and details but cannot create or edit them.\nManage Workflows\nOn Workflows Manage and Execution listing pages, all the WFs of all the categories will be editable to the users of that particular group. It means that the users can edit the WF list and create and execute WFs.\nNo Access\nFor no access, User will be not able to see the WF listing on the Manage and Execution List pages.\nFine-Grain Enabled\nWhen fine grain checkbox is enabled, it means the users of a particular group are given privileges to â€œselective or allâ€ categories of Workflow.\nOn all categories, there are further 2 levels of access. i.e. Manage Workflow and Execute Workflow. An admin can set the access for a group from edit/add group dialog.\nExecute Workflow\nOn Workflows Manage and Execution listing pages, all the WFs of the selective categories will be visible to the users of that particular group. It means that the users can only view the WF list and details but cannot create or edit them.\nManage Workflow\nOn Workflows Manage and Execution listing pages, all the WFs of the selective categories will be editable to the users of that particular group. It means that the users can edit and create and execute WFs.\nWorkflow Behavior for both Fine Grain and Global Access\nThere are some actions that a user can perform regardless of the defined access control.\nWorkflow Creation:\nFunctional Owner: Only those users of a group can be Functional Owner for which they have â€œmanageâ€ or â€œexecuteâ€ right over a category of WF.\nData Stewards: Users of a group can Data Stewards for which they have â€œmanageâ€ right over a category of WF.\nStep Owner: Only those users of a group can be assigned a step for which they have â€œmanageâ€ or â€œexecuteâ€ right over a category of WF.\nInitiate, Cancel and Delete Workflow:\nAll these actions can be performed by the Functional Owner or Data Stewards of a particular WF.\nReassign, Reject and Close Reject:\nAll these actions can be performed by the Functional Owner, Data Stewards or Step Owners of a particular WF.",
    "scraped_at": "2026-02-02 15:44:47"
  },
  {
    "title": "Why can User A see more Source Tables than User B?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115000143513-Why-can-User-A-see-more-Source-Tables-than-User-B",
    "content": "Users can't add sources; only administrators can do that. Admins have the authority to grant users permission to work with specific tables on source and table levels. Essentially, users can only see tables that administrators give them access to.\nConsider this example: the administrator adds a source, let's call it Source XYZ, which includes 20 tables. User A is granted \"Source level\" access, while User B is given \"Table level\" access to specific tables (10 out of the 20).\nUser A can view/configure* Source XYZ (20)\nUser B can view/configure* Source XYZ (10)\nIf you need access to a data source or its tables that you currently don't have, please reach out to your DvSum Administrator for assistance. The administrator is responsible for determining whether a user has the ability to view, write, or configure at the Source level or Table level.",
    "scraped_at": "2026-02-02 15:44:51"
  },
  {
    "title": "Error - 'File is corrupted and cannot be opened'",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/225558507-Error-File-is-corrupted-and-cannot-be-opened",
    "content": "Issue:\nWhen attempting to open a downloaded report by clicking on the \"Download report\" link in Rule detail or via the link provided in the email alert, an error occurs where Excel displays the message \"File is corrupt and cannot be opened.\"\nReason:\nExcel prevents opening files downloaded from the internet, impacting not just DvSum but also files from colleagues or other websites.\nResolution:\nChange the settings in Excel Options\nStep 1: Open Excel --> Click on File --> Options\nStep 2: Click on Trust Center on left menu\nStep 3: Click on Trust Center Settings\nStep 4: Click on Protected View\nStep 5: Uncheck the first check-box\nStep 6: Close Excel\nStarting now, your files should open without any issues.\nIf changing your Trust Center settings isn't possible or you prefer not to, you can manually unblock the Excel file to open it.\nStep 1: Navigate to the folder (e.g. Downloads) where the report is downloaded\nStep 2: Right click on the excel file and click on Properties\nStep3: Click on Unblock at the bottom of the dialog\nStep4: Close dialog\nNow this file can be opened in Excel.",
    "scraped_at": "2026-02-02 15:44:56"
  },
  {
    "title": "Address Validation rule is becoming Invalid",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/222595948-Address-Validation-rule-is-becoming-Invalid",
    "content": "Problem: When you run the address validation rule, it shows as invalid. The comments say, unknown error, contact administrator.\nCommon Reasons:\n1. Incorrect configuration: Go to Configure tab of the table by clicking on configuration in the Rules Scorecard or from Profiling --> Show Details. Click to Edit and Save again so the system Â checks the configuration. If there are any errors, it would prompt for the same\n2. Web Service not able to write changes to staging table. (Administrator) go to Web Service log (dvsum\\webservice\\SAWS.log) and look for any errors in processing the request.",
    "scraped_at": "2026-02-02 15:45:01"
  },
  {
    "title": "Scheduler not executing jobs / error in SAWS.log related to SUN PKIX Path Building Failed",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/221853427-Scheduler-not-executing-jobs-error-in-SAWS-log-related-to-SUN-PKIX-Path-Building-Failed",
    "content": "Root-cause\nThis is related to DvSum Web Service (SAWS) not able to make a secure HTTPS connection to DvSum Cloud. This issue occurs due to local java not recognizing the root certificate authority (CA). Â By default Java Security configuration exists under JRE and not under JDK.\nResolution\nExecute SAWS using JRE instead of JDK\nSteps\n1. On the machine (laptop, server) where SAWS is running, open Computer Properties --> Advanced Properties --> Environment Variables\n2. Add JAVA_HOME\\jre\\bin to the beginning of the path\n3. If you are using SAWS as a service, stop and re-start the Service. If you manually started SAWS, end the Java Process in Task Manager and re-start by double-clicking on DvSum.jar",
    "scraped_at": "2026-02-02 15:45:07"
  },
  {
    "title": "Why only certain fields are available in the Data Analysis screen",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/221403867-Why-only-certain-fields-are-available-in-the-Data-Analysis-screen",
    "content": "In DvSum, Data Analysis is a part of the Data Profiling module. It helps discover how data is connected across different fields, such as business unit, product family, and product line. For example, it identifies which combinations exist and how many records are associated with them.\nThe system automatically makes those determinations by reading the statistics of the data. And it does it only for fields it considers as attributes. So for example if a table has 100,000 rows and there are 100,000 unique item ids, then that field will not show up in Data Analysis. On the other hand, if there is Â product family, for which there are 100 unique values, then those will show up in the analysis.\nFor this reason, you only see limited values in the data analysis section.",
    "scraped_at": "2026-02-02 15:45:11"
  },
  {
    "title": "How to Create Rules (Standard, Data Quality Ruleset, Business Rules DAE)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/202846784-How-to-Create-Rules-Standard-Data-Quality-Ruleset-Business-Rules-DAE",
    "content": "Rules are created and added to Audits to be run together with other rules.\nRules can also be run individually. They can be scheduled or run ad-hoc.\nRules in DvSum can be created from a number of different locations within the tool. They can also be added from the Rules Library, or if you have existing rules for your data they can also be uploaded to DvSum.\nWhen a rule is created it is assigned a unique ID. The ID will have a form like \"DQ-000167\". DvSum generates a page at a url in the form https://prod.dvsum.com/rules/{rule_id} which includes the Rule Definition, Summary, Exceptions from the latest Audit run, and trends.\nStandard Rule Creation\nThe majority of rules are created through the Rules Wizards provided within the tool. From the left navigation pane, access Audit â†’ Manage Rules, and click \"âŠ• Add Rule\".\nCompleting Rules Wizards\nFor each of the Rules wizards, information is completed within the following sections. Depending on the type of Rule, options will differ. If the Rule is applicable to a single data source, Reference Inputs will not apply. Doc-matching rules will also require different information.\nFor some rule types, under Basic Inputs, the Variance Check check-box indicates this is a rule that will compare results of the current run to the results of a previous run, and you will be asked to provide a Variance Threshold or %.\nData Quality Rulesets\nCreating Rules from within Profiling â†’ Data Quality\nWhen in Profiling, by clicking the Data Quality tab, you have the ability to create Rules and Rulesets for the data you are reviewing.\nYou must first click â€œEnable Ruleset,â€ complete the required information, and click â€œSaveâ€ before defining the specific checks.\nRules created from the Data Quality tab within Profiling are limited to Completeness and Value Range checks. For example, by selecting the â€œBlanks Checkâ€ check box, the system automatically creates a rule to identify blank or NULL values.\nYou also have the opportunity to establish Filter conditions, to ignore the special scenarios where a blank might be an acceptable value.\nWithin the same Ruleset, or as a standalone Rule, you can also define Value Range Checks. These can be defined based on a Min/Max Threshold, or based on values from a Picklist defined either within the Field or in a reusable Reference Dictionary.\nYour specified configuration is automatically saved as a rule which can be viewed and edited under Audit â†’ Manage Rules.\nData Analysis Exceptions (DAEs)\nCreating Rules from within Profiling â†’ Data Analysis\nMore complex data rules can be created from within Profiling, under the Data Analysis tab. When exploring data through the filtering analysis, you may identify combinations of data that you want to define as Exceptions or that you want to create a query for to review the record details.\nFor example if the combination is invalid and should not exist, a Rule can be created from this view. As seen below, for a Product/Item Category of Accessories (Accy) sourced from Asia, a user may know that the Plan Level (planning category or priority) should not be \"4.\" You can create a rule by clicking \"Add Business Rule\", entering a Description, and clicking Create. A Rule template (or shell) is created, which you may then edit as desired.\nClicking Create takes you Audit â†’ Manage Rules with the Rule template created. From here you can Edit, add detail, Action Items, Create a Workflow, etc. before finalizing the rule.\nYou may also simply choose to click, \"Add Rule\" which takes you to the Rule Editor. Here you will need to recreate the filtering conditions for the Rule you want to create and click \"Add Rule\" which notates the condition under the \"Exceptions Rule List\".\nHere you can specify whether the conditions are valid or invalid. If valid, the rule will be displayed under \"Valid Rule List\". It will consider the condition as valid and bring all other records as exceptions. If invalid, the rule will bring only those records as exception which meet the condition. This can be selected from top right corner.\nWhen creating rules from this view, you are able to create multiple filtering conditions, within the selected Attributes, and also add them as Rules. This creates a compound rule that returns all qualifying exceptions or records within the query, and allows you to view them together. At the bottom, there is Cleanse Column list. Here you can check the columns which you want to make cleanse-able and unchecked any column which serves only as a filter.\nIn the cleanse workbench, the filtered columns with be selected by default and only the cleansable columns will be visible to be fixed.\nAlternatively, DAE rules can also be created from Audit â†’ Manage Rules tab as well.\nSelect \"âŠ• Add Rule\" â†’ \"Business Context\" â†’ \"DATA ANALYSIS EXCEPTIONS\".",
    "scraped_at": "2026-02-02 15:45:16"
  },
  {
    "title": "How to Install Python service for Excel, Excel in Memory Source?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360021118474-How-to-Install-Python-service-for-Excel-Excel-in-Memory-Source",
    "content": "Note: This is a part of\nSAWS installation\n. If your SAWS is already running, uninstall it and follow the steps below\nTo download and install python with its libraries, please make sure you follow all the steps below.\n****************Python Installation****************\n1\nGo to https://www.python.org/downloads/release/python-370/\n2\nFrom \"Files\" section, install the executable file according to your operating system. (e.g. for windows 64 bit, install Windows x86-64 executable installer)\n3\nRight click and run as administrator the exe file.\n4\nSelect the checkbox \"Add Python 3.7 to PATH\"\n5\nClick on \"customize installation\"\n6\nSelect check box \"Install for all users\"\n7\nClick on Browse and select driectory accessible to all users i.e \"C:/Program Files/Python\" for python installation and Click on install\nThis will install Python on your system.\n****************Python Libraries****************\n1. To install python libraries, go to command prompt (Type \"Cmd\" on run, make sure to run cmd as administrator)\n2. Type pip install pandas, press Enter.\n3. Type pip install XlsxWriter, press Enter.\n4. Type pip install xlrd, press Enter.\n5. Type pip install Flask, press Enter.\n6. Type pip install SQLAlchemy, press Enter.\nThis will install required Python libraries on your system.\nNow go to SAWS folder on your machine and click on\nsaws_service_install\n. Run it as an administrator. This will install webservice and you will be able to see DvSum Python Service running in your machine along with SAWS.\nNote: This python service is required to avail features like excel preprocessing and for Excel in memory source.",
    "scraped_at": "2026-02-02 15:45:22"
  },
  {
    "title": "Is there a limit of how much data we can extract to a file on azure blob/S3 bucket or to folder?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360018196733-Is-there-a-limit-of-how-much-data-we-can-extract-to-a-file-on-azure-blob-S3-bucket-or-to-folder",
    "content": "You can control the data extraction limit on Azure blob or s3 by specifying the number of records to be uploaded.\nThis is the storage level option provided on Cloud storage tab. From Administration > Manage Account > Cloud storage > Add/edit\nThe Export Limit field is optional. If a user does not specify a value, by default 75,000 records will be uploaded. Valid range for this is 1 to at max 10 million records.\nNote: If there are more exceptions than the limit, exception count will show on rule detail page but at the max 10M exception file will be uploaded to Azure/s3.",
    "scraped_at": "2026-02-02 15:45:26"
  },
  {
    "title": "On Premise Gateway (SAWS) Advanced Settings (Configuration properties file)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360015361094-On-Premise-Gateway-SAWS-Advanced-Settings-Configuration-properties-file",
    "content": "Following the basic gateway setup, additional features are provided to enhance the experience for Dvsum users. You can conveniently adjust the settings through the gateway configuration file. This file is located within the webservice folder.\nNote: Read here if you are not sure\nhow to install SAWS and run as a Service\nThe configuration file allows gateway on your machine to communicate with DvSum application.\nSAWS Error Identification\nThe flag \"enable.log.sync\" is false by default. If you set it to true, the activity logs of SAWS will be uploaded on S3 bucket which makes it easier for the DvSum team to monitor SAWS logs in case of errors during rule/thread execution.\nIt is to be noted that your data remain private. We will not have access to your data but only the SAWS logs.\nScheduled SAWS Reboot\nIf you are concerned that SAWS process might be consuming a lot of memory, you can refresh/restart it to release un-used resources to ensure smooth running of SAWS.\nFor this purpose, configuration file has rebootTime flag. Here you can give time in 24 hour format i.e. 18:45.\nEveryday at the specified time, SAWS will reboot and flush the memory.\nPort\nBy Default, SAWS runs on port 8183. If you want to run it on some other port, you can simply change it from here and save file.\nSAWS Auto-Restart\nIf for any rare reason SAWS crashes, it will restart on its own within 5 minutes. You may need to add SAWS exception in your browser.Â Otherwise the\nSAWS cloud will appear red.",
    "scraped_at": "2026-02-02 15:45:31"
  },
  {
    "title": "Deleting the data source. (Owner and Administrators)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360005670514-Deleting-the-data-source-Owner-and-Administrators",
    "content": "In order to delete a data source, first, navigate to Administration, then proceed to Manage Sources in the left-hand navigation menu. Select the data source labeled as EXCEL_DEMODATASET, and click on \"Delete Source.\"\nPerforming this action marks the source as deleted, and it will be removed from the Source list after 24 hours. Once deleted, it won't appear anywhere in the application, including profiling tables, rules, or related workflows, as they will also be deleted.\nAdministration section is only available for Owner and Administrator roles. If you are a user or Super-User, you will not be able to delete the demo dataset.\nAll data for the selected demo datasets will be deleted. This includes profiles, audits, results and action items. The data is also at an account level. So once the data sources are deleted, they are deleted at account level and no user can use it. It is recommended that you delete the demo dataset after confirming with any users in the system who might be using the demo dataset.",
    "scraped_at": "2026-02-02 15:45:36"
  },
  {
    "title": "Export options for Rule Results (Administrators and Super Users)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360005093774-Export-options-for-Rule-Results-Administrators-and-Super-Users",
    "content": "When you execute a rule on DvSum, you see the run result in Analysis tab. You can also download the exception file on your computer.\nThere are further advanced options available for you to manage rule result. You can export rule data to cloud storage like AWS S3 and AzureÂ  or return it to calling API. This can be done only on a Custom Query rule as it covers almost all rules.\nOn the custom rule detail page, there is a tab \"Script Configuration\". You can set the export configuration to send the data to either AWS S3 Bucket/Azure Bucket or send the data to calling API.\nAWS S3 Storage/ Azure Storage\nFrom Administration > Manage Account > Cloud Storage, click on Add and select the Storage type and the web-service. Depending on the Storage type, you will provide the Access key and fill out the mandatory fields.\nExport limit allows you to set the number of records to be uploaded to Azure/s3 bucket. (Default limit 75,000)\nOnce you save this, you can test your connection with Cloud Storage.\nNow from rule detail page you can select this storage, give a file name and type, say the file name is \"S3 file\" and the type selected is CSV.\nWhen this rule is executed, the results will be uploaded to AWS S3 bucket in a file named \"S3 file\".\nSimilarly for Azure, you can set up Azure storage from Cloud Storage tab and select the same Azure Storage on rule detail page.\nSend Data to Calling API\nThis option allows you to execute rule and fetch the results of that rule outside the DvSum domain. As the name suggests, the run result will be sent to the calling API.\nTo generate script, schedule a rule and go to Generate Script tab. Copy the script and run from command line. The results for this rule will be returned in command line.\nExport to Folder\nAnother option to manage rule report is to export it to a folder. This folder can be on your local machine or wherever the webservice is running. Folder is created from Administration tab > Folder tab where user needs to specify folder path and SAWS associated with it.\nWhen the option â€œExport to Fileâ€ is selected, you will be able to see the folders which are associated with the same SAWS as the source on which rule is created. i.e. The SAWS executing the rule and uploading the report to the folder should be same.\nNOTE: when there are no exceptions in rule executions, no file will be created or uploaded.",
    "scraped_at": "2026-02-02 15:45:41"
  },
  {
    "title": "SAWS Alert Notification controls (Owners only)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360002717393-SAWS-Alert-Notification-controls-Owners-only",
    "content": "SAWS Alert is the Capability to Send Email Notification. These Notifications let Account Owner and optionally Administrators know when a specific SAWS is not able to communicate to DvSum web application and also when it resumes communication after failure.\nThese controls can be changed from Add/Edit SAWS dialogue.\nEnable SAWS Alert ( Default is Yes)\nAlso Notify Admins ( Default is No )\nIf\nEnable SAWS Alert\nis set to Yes and\nAlso Notify Admins\nis set to No Notification will only be sent to Owner of the account.\nIf\nEnable SAWS Alert\nis set to Yes and\nAlso Notify Admins\nis set to Yes Notification will only be sent to Owner as well as Admins of the account.\nIf\nEnable SAWS Alert\nis set to NO, Application won't send alert Notification for this specific SAWS.",
    "scraped_at": "2026-02-02 15:45:46"
  },
  {
    "title": "What is a \"Not-Connected DataSource\"?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360002610433-What-is-a-Not-Connected-DataSource",
    "content": "Not-connected sources are the ones which are not connected to the DvSum Web-service for the purposes of cataloging, profiling, executing rules or cleansing data.\nIf you want to create a blueprint of a source or may require to create lineage, you can select this option. This means you would manually add the tables and the columns of your source.\nOnce you have added the Not-Connected Source, you can go to Profile tab >> Profiling and select your source to define the tables manually. Only for a not-connected source you will see the \"Add Table\" button.\nWhen you add the table, it starts appearing in the tables drop down.\nSelect this table and go to \"show details\" where you can define the columns manually.\nYou can give name to your columns and select their data-type\nThis completes the blueprint of your Database.",
    "scraped_at": "2026-02-02 15:45:51"
  },
  {
    "title": "DQ SAWS Update (For Owner Account)",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000764854-DQ-SAWS-Update-For-Owner-Account",
    "content": "Whenever there is a new version available for SAWS, a pop up will appear informing you about it after you log on to DvSum. You will also see a notification on your SAWS cloud icon.\nIf you decide to upgrade right away by clicking on \"Upgrade Now\", it will take you to the Administration >> Manage Account >> SAWS tab.\nThe \"Upgrade\" button is\nenabled\nfor the SAWS that is running and the communication key matches with the api token in the configuration file.\nThe \"Upgrade\" button is\ndisabled\nfor the SAWS that is not currently running.\nIf your SAWS is busy in running a job, it will not be able to upgrade right away but don't worry. We have got it covered. The application will show \"Upgrade Pending\" status and the SAWS will upgrade itself automatically once the SAWS is free.\nAfter the successful download of the latest version, you will see a blue tick and the version will be updated. If your already have the latest version installed, you will not see the \"Upgrade\" button. (like for the \"Remote ca ws\" in the image below)\nNote:\n1. Please be patient while the SAWS upgrades. It takes around 1-2 minutes.\n2. . SAWS upgrade notification appears only when SAWS is running and showing green cloud\n3. SAWS upgrade from UI is supported by SAWS version 1.1.0 and above. If you have older version of SAWS running, you will manually have to download SAWS from Administration > Manage Account > SAWS tab > Download.",
    "scraped_at": "2026-02-02 15:45:58"
  },
  {
    "title": "Configure one Data Source with Multiple SAWS",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000247253-Configure-one-Data-Source-with-Multiple-SAWS",
    "content": "One Data source can only be configured with one web-service\nat a time.\nHowever, You can edit the source and map it to another web-service by clicking on \"Edit Source\"",
    "scraped_at": "2026-02-02 15:46:03"
  },
  {
    "title": "Updates to database source data catalog",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/225102967-Updates-to-database-source-data-catalog",
    "content": "Overview\nFrom time to time, your database source data definition changes. The metadata changes could be as small as the data type being modified or as big as new tables being added. It can include dropping tables and adding or removing columns. All of these changes can be seamlessly synched to DvSum.\nSteps to Refresh the Data Catalog\nStep 0.\nMake changes in your source systems (outside of DvSum).\nStep 1.\nLogin to\nhttps://prod.dvsum.com\nand navigate to\nAdministration â†’ Manage Sources.\nNote: Only users with Admin or Owner roles have access to this.\nStep 2.\nSelect a source and click \"Run Cataloging\" or \"Schedule Cataloging\". DvSum will\nread your current database definition and synchronize it with the catalog in DvSum.\nNote: \"Schedule Cataloging\" can be executed immediately and runs in the background. This is what most users should select.\n\"Run Cataloging\" runs while you have the page open. It's better for smaller environments.\nResults of Data Catalog Updates\nThe synchronization will result in following possible updates.\nTables or Views\nIf new tables or views are added, they will automatically be imported.\nIf tables or views that previously existed were dropped, then they will be\nmarked for deletion\nin DvSum. Under the catalog list in Profiling page, they will show up with a prefix (deleted). The administrator can manually review and delete them from Profile â†’ Profiling.\nColumns or Fields\nNew fields: automatically imported and become visible.\nDropped fields: automatically removed from DvSum.\nNote that if there were rules that used those fields, those rules will become invalid and may have to be re-configured.\nRenamed fields: treated as combination of drop and add. So the new fields must be re-profiled to generate statistics.\nExtra steps when using staging configuration\nThese steps are needed only when you are using the staging configuration and you added new columns or changed the names of any columns.\nNew fields are not automatically included in the staging configuration. Therefore the domain data type and pre-processing and post-processing logic must be configured. Go to Staging Configuration\nStep 1.\nGo to Staging configuration of your data source by navigating from Profiling Main Page and clicking onÂ Show Details.\nStep 2.\nSelect the Staging Configuration tab and click Edit. Then press Save.\nThis action will re-initialize the configuration. Click Edit a second time.\nStep 3.\nScroll to the bottom of the screen. The new fields are available, but they are not included in the mapping.\nStep 4.\nCheck the box to include the field in staging workflow. Click on the pencil icon to set its match data type and any pre-process validation or standardization settings.\nStep 5.\nClick Save. The system will warn you if there are any rules associated with the configuration; those rules will be reset due to the change in structure. Click OK.\nObject maintenance is complete and you can continue using DvSum with the fully updated Data Catalog metadata.",
    "scraped_at": "2026-02-02 15:46:08"
  },
  {
    "title": "Change of language in DvSum",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360003300593-Change-of-language-in-DvSum",
    "content": "DvSum application can adapt according to the language preferences by translating the source language (English) in to the targeted language.\nYou can change the display language to your preferred language from Profile Page\nUnder the Change Settings section, choose your language from the drop down box and Save.",
    "scraped_at": "2026-02-02 15:46:13"
  },
  {
    "title": "Dashboard templates and blank widgets",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000024353-Dashboard-templates-and-blank-widgets",
    "content": "Template Widgets\nTemplate WidgetsÂ are dashboard charts pre-designed to show the meaningful information that you can otherwise view by going through the DvSum application. There is a button \"Add Widget\" on dashboard which shows a list of template widgets you can select from.\nFor example, when you add a template Rule widget â€œRule Count by Source and Statusâ€, it populates the input fields related to the widget with the preferred Chart type better suited to display the information. All you need to do is provide the Data Source so the widget can fetch the data.\nBlank Widget\nBlank widget is the most powerful widget because it allows you to create a widget from scratch with any information and any chart.",
    "scraped_at": "2026-02-02 15:46:18"
  },
  {
    "title": "What do I see on Advanced Dashboard based on user roles?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000017313-What-do-I-see-on-Advanced-Dashboard-based-on-user-roles",
    "content": "DashboardÂ gives you quick access to frequently used info.\nOn a\nUser level\n, dashboard widgets help you see your involvement in the action items of DvSum application without going to individual functionality sections. For e.g. you can create a dashboard with a Rule Widget showing â€œRule Countâ€ by â€œRule Statusâ€ in the form of a list. This shows all Rules created by you with their status: Pass, Fail or Exceptions.\nOn an\nAdmin/Owner level\n, it is easier to track all user activity over the DvSum application on a single interface. Admins can filter out the data for a single user or view it for everyone using\nData for:\nfilter. (Shown below)",
    "scraped_at": "2026-02-02 15:46:23"
  },
  {
    "title": "What is new in Advanced Dashboards?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360000018374-What-is-new-in-Advanced-Dashboards",
    "content": "DvSum Advanced Dashboards\nare made up of tables, charts and lists used to view, analyze and track the overall activity on DvSum application.\nThese easy-to-read customized Dashboards help you to organize and present the information the way it is required.\nUnlike the basic data quality dashboard, now you can create a dashboard of your own, share it with others or clone someone elseâ€™s dashboard and edit it accordingly.\nYou can add multiple widgets to a single dashboard or create multiple dashboards. You can also add the widgets for\nData Management\nWorkflows\nand\nBusiness Glossary\nto your advanced dashboard.\nHere is what the old dashboard looks like:\nThis is our revamped Dashboard:",
    "scraped_at": "2026-02-02 15:46:29"
  },
  {
    "title": "Configure Salesforce as a Source",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360049829631-Configure-Salesforce-as-a-Source",
    "content": "Overview\nDvSum Data Quality (DQ) supports a bi-directional connection with Salesforce, where a user can catalog data assets, author data quality rules, and even cleanse data without need for any external data pipelines.\nDetailed Steps\nSalesforce configuration\n1. Connected App configuration\nOpen browser, navigate to salesforce.com, and log in.\nhttps://login.salesforce.com/\nNavigate to Setup â†’ Apps â†’ AppManager to create an app.\nTip: use the Quick Find box.\nIn the Connected Apps section, click \"New Connected App\".\nEnter a name to be displayed to users when they log in to grant permissions to your app, along with a contact email address.\nEnable OAuth Settings, and enter the relevant value in the Callback URL box:\nFor Data Quality:\nhttps://prod.dvsum.com/cdata/saveVerifierCode\nFor Data Catalog:\nhttps://apis.dvsum.ai/data-sources/sources/salesforce/saveVerifierCode\nSelect the scope of permissions that your app should request from the user. Save the changes and Continue to the next screen.\nClick your app name to open a page with information about your app. Note the OAuth client credentials. These properties are needed to add Salesforce as a source in DvSum DQ:\nConsumer Key\nConsumer Secret\n2. Permission Set configuration\nThe user connecting to Salesforce must have the ability to call Salesforce APIs. This is done by setting the relevant properties for the relevant Permission Set.\nAccess the relevant Permission Set via either of these 2 paths (the GUI layout will differ depending on whether Lightning Force or Classic view is active, but the logical path remains identical):\nADMINSTRATION â†’ Users â†’ Permission Sets â†’ <permission_set>\nor\nADMINSTRATION â†’ Users â†’ Users â†’ <user> â†’ Permission Set Assignments â†’ <permission_set>\nFrom here you need to activate System Permissions â†’ API Enabled (Access any Salesforce.com API.)\nWith the Connected App and Permission Set configured, then you're ready to configure DvSum.\nDvSum Configuration\nCreate connection in DvSum\nOpen\nDvSum DQ\n. Navigate to Administration â†’ Manage Sources â†’ âŠ•Add Source. Select Salesforce.\nPaste the copied values:\nConsumer\nKey\nâ†’ Client ID\nConsumer\nSecret\nâ†’\nClient\nSecret\nThen Authenticate.\nWhile authenticating, following message will be displayed.\nYou will be re-directed to Salesforce. Provide user credentials and log in.\nDepending on your Salesforce configuration, a verification code will typically be sent to you via email. Paste the code here and verify.\nNow, click \"Allow\". This will allow the DvSum application to access Salesforce data.\nIt will trigger authentication of DvSum's connection to Salesforce.\nA success message should be displayed. Then click \"Save\".\nYour Salesforce connection is now fully configured and functional.\nDetails when using a Salesforce Sandbox\nBe sure to enable the checkbox\nUse Sandbox\nas shown in the screenshot below.\nOnce you have enabled that, you must enter the\nSandbox Login URL\n. then enter the\nClient ID\nand the\nClient Secret\n.\nThen\nAuthenticate\nthe connection. You will be redirected to the Salesforce Sandbox login page andÂ will need to follow these steps:\nEnter Login credentials.\nEnter the Verification code received on the registered email address.\nAllow Access.\nAfter the\nAuthentication\nis done, you can Test the connection to confirm that the source has been added successfully.",
    "scraped_at": "2026-02-02 15:46:35"
  },
  {
    "title": "SQLEXPRESS connection to SAWS",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360033961454-SQLEXPRESS-connection-to-SAWS",
    "content": "Sometimes DvSum Gateway is unable to make connection to SQLEXPRESS. If you are sure you have provided the correct information and credentials and \"Test Connection\" still fails, please check the following settings on your computer.\nStep 1:\nFrom Windows button, click on SQL Server Configuration Manager.\nStep 2:\nClick on Network Configuration in the Manager\nStep 3:\nDouble click on Protocols. Confirm if TCP/IP is enabled.\nIf not enabled, right click and enable. (Donâ€™t re-start service yet)\nStep 4:\nDouble click TCP/IP and in the dialog box, click on IP Address tab\nStep 5:\nScroll to bottom of page. Confirm that under IPAll, TCP Port isÂ  set to 1433.\nIf not, add it and apply.\nStep 6:\nClose and get back to Configuration Manager. Click on SQL Server Services on left side.\nStep 7:\nClick on SQL Server (Running) and Click on Restart button in action bar.\nStep 8:\nOnce re-started, DvSum will connect to this database. Confirm the following in Manage Sources page.",
    "scraped_at": "2026-02-02 15:46:40"
  },
  {
    "title": "Configure Excel/CSV files?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115000641314-Configure-Excel-CSV-files",
    "content": "Prerequisites:\nJava version 7\nYou can configure Excel files after creating a new Source on DvSum app. But before that can be done, you have to make sure that you have â€œAccess Database Engineâ€ Drivers installed wherever SAWS is running (local/cloud). If you donâ€™t, following error message will appear when trying to configure Excel/CSV file.\nFollow the steps below if you require ADE drivers installed on your system\nStep 1:\nGo to\nwww.microsoft.com\nand download Microsoft Access Database Engine exe file.\nStep 2:\nInstall Access Database Engine\nStep 3:\nSearch Windows for ODBC Data Source Administrator and Click on â€œDriversâ€ tab to see if the Access and Excel drivers are installed.\nNow go to DvSum Web Application and follow the steps below to configure Excel/CSV files\nStep 1:\nGo to Manage Sources >> Add Source\nStep 2:\nFill out the mandatory fields with Source Type as Excel or CSV\nStep 3:\nEnter the File System Path. It can be local or Cloud file system location. If your files are on Cloud, please ask your administrator to provide you the path.\nStep 4:\nClick on â€œSaveâ€ to add the source\nStep 5:\nEdit the source you just added\nStep 6:\nClick on the â€œConfigureâ€ button appearing on the bottom left corner.\nNow you can Configure the columns of the Excel sheet.",
    "scraped_at": "2026-02-02 15:46:45"
  },
  {
    "title": "Job Execution Using Dynamic Schema",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360061493952-Job-Execution-Using-Dynamic-Schema",
    "content": "We have introduced a feature where user can execute rule created in Dvsum on another source just by providing information of source with same metadata. A new field has been added up 'Generate Script form'. User have to copy curl Script from Dynamic source Script section given on Generate Script tab of create Scheduler page. Let's get started step by step.\nStep 1 :\nPrerequisite - Installation of curl\nClick the\nlink\nhere.\nDownload curl w.r.t machine's compatibility\nExtract the files from zip folder and double click on application main file to run the curl\nStep 2 :\nRules, Profiling, Cataloging Execution using Dynamic schema\nOpen Dvsum application, go to Audit >> Manage Rules >> select any existing rule >> click More action >> select Schedule Rule as shown below;\nStep 3 :\nDynamic Source Script Generation\nGo to Generate Script tab and click on the button at the bottom to generate the script as shown below;\nStep 3.1\nCopy the Dynamic source script and click OK as shown below;\nStep 3.2\nJob is created against the rule. Now go to the main scheduler page, search the rule and a new created job will be visible with description and status as 'stand by' as shown below;\nStep 3.3\nOpen notepad and paste the copied source script there. It is recommended to make changes in the script using notepad. Replace the information which is different or which you may want to update i.e. Schema name/port/sourceID/Host. Once the modifications are done, copy the script.\nStep 3.4\nOpen command prompt, run it as Administrator and paste the copied script and hit enter. It starts the execution and at Dvsum application side, the Job description and its status gets updated as shown below;\nStep 3.5\nOnce the execution is completed then in cmd, it will print the remarks as same as shown in rule detail page in Dvsum.\nand job status changes to completed after execution\nStep 3.6\nSimilarly for Profiling and Batch execution same steps can be performed to execute jobs using dynamic schema.",
    "scraped_at": "2026-02-02 15:46:49"
  },
  {
    "title": "Address Validation Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360054484771-Address-Validation-Rule",
    "content": "Address validation, or address verification, is a rule ensuring that street and postal addresses are valid. It can be done upfront when a user searches for an incomplete or incorrect address, or by comparing and formatting data in a database with reference postal information.\nIn the application, there are changes in:\nStaging Configuration (Profiling section)\nRule Detail (Workbench)\nStaging Configuration for Address Validation Rules\nNote\n:\nThis setup is necessary only for the initial configuration on Oracle source. It's mandatory to complete this step; otherwise, during the execution of Address Validation rules, the user will receive an error message in the rule remarks stating \"MM configurations missing.\"\nStep 1 :\nNavigate to Profile -> Profiling -> Select an Oracle data source, select main table and staging configuration table. In the below screenshot, main table is CUSTOMER_ADDRESSES and staging table is CUSTOMER_ADDRESSES_DVSUM.\nNote:\nIt is required to make sure that Staging table exists for Address Validation rule to be run.\nStep 2 :\nSelect main table and click on show details\nStep 3 :\nSelect Staging configuration tab and click on Edit\nSteps 4 :\nSetting up Configuration, Additional Configuration, Staging Configuration fields\nConfiguration setup\nChoose the Subject Area, set Golden Record Criteria, tick the Enable Validation checkbox, and provide a Description.\nAdditional Configuration setup\nVerify the \"Write-back Post Process\" option, meaning enable it if the user intends to activate write-back on the main table. Additionally, specify the \"Case type\" preference, indicating whether the user wants to maintain the format of the Suggested Address as fetched by default, either in lower or UPPER case.\nStaging Configuration\nsetup\nChoose the Staging table, Match key, Match count, Match rule, Record identifier, Golden record ref, Record type, Merge status, Time stamp identifier, and then click the Save button located at the top, as illustrated below:\nStep 5 :\nUpon saving the changes, a success notification will inform the user that the modifications have been saved successfully. Simultaneously, an error message will appear, indicating that \"Fields are not mapped to address data types Address Line 1 or Country.\" At the bottom, the Edit Mapping grid will be displayed as illustrated below:\nEdit Mapping\nStep 6 :\nAfter configuring the staging settings, click on the Edit button to modify the mapping. Initially, Match Data Types are assigned randomly to column names, so the user needs to update these Address Match Data Types based on the required columns. The user can choose all columns by clicking Select All or include individual columns, as depicted below:\nStep 7 :\nColumns are mandatory to be mapped against following Address Match Data Types\nCity\nCountry\nState\nUS ZIP code\nADDRESS LINE 1\nFollowing address match data types will be mapped as:\nAddress Line 1 (Street Address)\nAddress Line 2 (Suit/Apartment)\nAddress Line 3 (Info/Atten)\nOther Address Lines\nGoogle response of street address will writeback to Address Line 1. Likewise suit/apartment, Info/Atten will writeback to Address line 2 and Address Line 3 respectively. In this way, the quality of addresses data will be improved.\nStep 8 :Â  Completeness Rule\nThe default completeness rule is as follows:\n\"address_line1\" is mandatory.\n\"country\" is mandatory.\nEither \"city\" or \"zip_code\" must be mandatory.\nUsers have the option to add this rule for all records or apply it selectively to specific filtered records. For instance, in the example provided, the `CUST_STATE` column is chosen as a mandatory field for all addresses where the country is set to 'US'.\nAddress Validation Rule\nStep 9 :\nNavigate to \"Manage Rules,\" click on \"Add Rule,\" and choose the Address Validation rule from the Process Quality category.\nStep 10 :\nProvide all the required fields here.\nNote\n: Table name field should be Main table not the staging configuration table as Rule will be created on Main table.\nStep 11:\nVisit the Rule Detail page and execute the rule, as shown below:\nStep 12:\nAfter the rule is executed, the following data is retrieved:\nRun Status\nScanned Records (Total records got scanned)\nReview Status\na. Pending (requires review)\nb. Modified (if user accepts suggested address OR current address fetched from DB gets updatedÂ  Â  Â  Â  and Saved)\nc. Skipped (If user skips suggested address)\nCommit Status\na. committed\nb. commit failed\nReadiness Score\nRemarks\nStep 13 :\nVisit the Addresses tab, where the matched addresses will be visible in the grid. To edit or update any record, simply click on the Review button, as shown below:\nAddress Line 1, Address Line 2, Address Line 3, Other address lines, City, state, country are mapped together in Complete Address column.\nComplete Address detail information is fetched from User's DB\nMatch Statuses\nStep 14\nFollowing are the Match statuses\nINC (Incomplete)\ni\nndicates that the provided address data is not complete and is returned if any of the below conditions is fulfilled:\naddress_line1 is empty.\ncountry is empty.\ncity and zip_code both are empty.\nOK\ni\ns returned if following conditions are fulfilled:\nmatched_location_type is â€œ\nROOFTOP\nâ€.\nmatched_address_line1 exists.\nmatched_city exists.\nmatched_zip_code exists.\nmatched_country exists.\nProvided address and validated address are exactly same without any case difference.\nSD (Standardized)\ni\ns returned if following conditions are fulfilled:\nmatched_location_type is not empty.\nmatched_address_line1 exists.\nmatched_city exists.\nmatched_zip_code exists.\nmatched_country exists.\nThere is only\nCase\ndifference between provided address and validated address (example: HENRY STREET, Henry Street).\nThere is only\nZIP Code\ndeference between provided address and validated address (example: 63122-6604, 63122 or 63122, 63122-6604).\nThere is\nabbreviated form\nfound in the validated address (example: Henry Street, Henry St).\nER (Enrich)\ni\ns returned if following conditions are fulfilled:\nmatched_location_type is â€œ\nROOFTOP\nâ€.\nmatched_address_line1 exists.\nmatched_city exists.\nmatched_zip_code exists.\nmatched_country exists.\nThere is at the very least 50% match difference between provided address_line1 + address_line2 and matched_address_line1+ matched_address_line2.\nValidated address is a bit changed from provided address (example: Henry Road, Henry St).\nThere is only\nZIP Code\ndeference between provided address, validated address (example: 63121, 63122).\nNM (No match)\nis returned if none of the above status is returned.\nRecord Statuses\nStep 15\nThere are following record statuses:\nUpdate\n(If user Accept Suggestion)\nSkip\n(If user Skips Suggestion)\nModified update\n(Once current address fetched from DB gets updated and Saved by user)\nStep 16 :\nIn the presented screenshot, this icon indicates the specific address chosen for writeback from the suggested or complete address.\nUsers can edit the current address in the review window, validate it, and receive a success or error message accordingly. The label next to the current address will show \"Edited,\" and users can also revert to the original value if needed.\nReview Window:\nStep 17\nWhen the record statuses are modified to Update, Skip, or Update, the count for pending records decreases, while the count for reviewed records increases.\nStep 18\nUsers have the option to select all records and take actions such as accepting or skipping suggestions. They can also clear the selection for all records or choose to discard the changes.\nStep 19\nOnce user clicks Finish button given on bottom, then\nCommit\nbutton appears in Address tab on rule detail page.\nStep 20\nOnce commit is done, then Fixed records and Reviewed records count gets updated in Rule summary\nStep 21\nUser can also create view\nStep 22\nUsers can add comments to the Match Status column by clicking on any status, entering a comment, and saving it. A comment icon with a count will start to appear on the main grid, and the count increases with each new comment. If there are no comments, hovering over the comment icon will display it.\nPostal Addressing Standards supported in Address Validation rule (Bag of words)\nPrimary Street Suffix Name\nCommonly Used Street Suffix or Abbreviation\nPostal Service Standard Suffix Abbreviation\nAVENUE\nAV\nAVE\nAVE\nAVEN\nAVENU\nAVENUE\nAVN\nAVNUE\nBOULEVARD\nBLVD\nBLVD\nBOUL\nBOULEVARD\nBOULV\nCAUSEWAY\nCAUSEWAY\nCSWY\nCAUSWA\nCSWY\nCIRCLE\nCIR\nCIR\nCIRC\nCIRCL\nCIRCLE\nCRCL\nCRCLE\nCOURT\nCOURT\nCT\nCT\nDRIVE\nDR\nDR\nDRIV\nDRIVE\nDRV\nHIGHWAY\nHIGHWAY\nHWY\nHIGHWY\nHIWAY\nHIWY\nHWAY\nHWY\nLANE\nLANE\nLN\nLN\nPARKWAY\nPARKWAY\nPKWY\nPARKWY\nPKWAY\nPKWY\nPKY\nPLACE\nPL\nPL\nROAD\nRD\nRD\nROAD\nSTREET\nSTREET\nST\nSTRT\nST\nSTR\nTRAIL\nTRAIL\nTRL\nTRAILS\nTRL\nTRLS\nFull Form\nAbbreviation\nApartment\nAPT\nBuilding\nBLDG\nDepartment\nDEPT\nFloor\nFL\nUnit\nUNIT\nSuite\nSTE\nRoom\nRM\nGuide on using Address Validation API\nHere\nis a link to an article for a complete guide on address validation API.",
    "scraped_at": "2026-02-02 15:46:54"
  },
  {
    "title": "Allow Blanks in SAP Writeback",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360051535313-Allow-Blanks-in-SAP-Writeback",
    "content": "This enhancement in the DvSum application enables the write-back of blank values to the source data. Let's proceed with the following steps to comprehend how this functionality operates.\nUsers need to ensure that two checks are applied:\nRule Level\nTable level\nNote:\nPlease note that this configuration will be applicable to all rules except\nBLANKS RULE.\nRule Level Check\nStep 1 :\nValue Range Rule\nLets consider an example of VR rule in which\nfirst the rule definition will be checked for blanks and then table level configurations will be checked.\nFor the rest of\nrules,\nonly table level configurations will be checked to allow blanks.\n1. Go to the \"Manage Rules\" tab in the DvSum application.\n2. Search for any VR (Value Range) rule and click on the rule to open the rule detail page.\n3. Once on the rule detail page, navigate to the \"Definition\" tab.\n4. Click the \"Edit\" button.\n5. Under the \"Specify Range value\" section, find the \"Include Blanks\" checkbox.\n6. Check the \"Include Blanks\" box.\n7. Ensure that the \"Valid\" option is selected for values.\n8. Click the \"Save\" button to apply the changes.\nBy following these steps, you can configure the VR rule to include blanks and ensure the \"Valid\" option is selected for values.\nNote:\nIf \"Include Blanks\" is not checked and \"Valid\" option is selected above, still it won't allow to write back for blank values.\nStep 2 :\nNow click Analysis tab and see on which table the exceptions are showing up as shown below;\nTable Level Check\nStep 3 :\nGo to Profiling section, select source, table name and click the Select button. Once the record is displayed, select it and click Show details option as shown below;\nStep 4 :\nOnce the details page is displayed, select Field Configuration tab. In this tab, \"Allow Blanks\" column is added. Now, search for the table name and verify Allow blanks option is checked for instance; in this case it was COUNTRY, re-run the rule as shown below;\nStep 5 :\nOnce the rule is re-run, go back to rule detail page and click Cleanse button. On Cleanse detail page, note the Remaining Exception count shown above, click on Edit option for any field, remove the value, leave it blank and apply the change as shown below;\nStep 6 :\nAs soon as you apply the change, observe that it allows the blank value and Remaining Exception count also gets reduced. Save it and Commit the change. as shown below;\nStep 7 :\nAfter change is committed, the field color turns Green which confirms that the write back was successful as shown below;\nScenario 1 :\nWhen Rule level check is enabled but Table level check is disabled, in this case the Blank won't be write back-able and user will be notified at Cleanse detail page as shown below;\nScenario 2 :\nWhen Table level check is enabled but Rule level check is disabled,Â in this case the Blank won't be write back-able and user will be notified at Cleanse detail page as shown below;",
    "scraped_at": "2026-02-02 15:47:00"
  },
  {
    "title": "Clone Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360050083754-Clone-Rule",
    "content": "In the past, the functionality of cloning rules did not exist, requiring users to manually create rules each time. However, users can now easily duplicate existing rules with minor adjustments and assign them to other users. This eliminates the need for manual rule creation, saving time and allowing users to create rules quickly using the new clone functionality. For more information on this feature, follow the steps outlined below.\nCloning From Manage Rules Page\nSingle Rule Cloning\nSteps 1 :\nNavigate to the \"Manage Rules\" section and choose a specific rule. Click on \"More Actions\" at the top, then select \"Clone Rule\" from the drop-down menu, as illustrated below:\nStep 2 :\nUpon selecting \"Clone Rule,\" the user will be taken to the Clone Rule detail page. At the top, there will be a prefix and all the clone properties, which will be inherited by default. Users have the flexibility to unselect any property as desired. The cloned rule will have a target source, initially set to the same as the current source by default. However, users can modify it to any other source of their choice, as depicted below:\nStep 3 :\nNow, let's assume the user chooses a different target source for this rule and clicks the \"Clone\" button, as demonstrated below:\nStep 4:\nAfter cloning the rule, the user will be taken to the Clone Summary detail page. In the grid below, you will find the Rule ID, the new ID for the cloned rule (which remains blank in case of failed cloning), the default source of the rule, the source on which the rule has been cloned, the status indicating whether the cloning was successful or failed, any remarks in case of unsuccessful cloning, and finally, a description. In the top right corner, there is a \"Download Summary\" button to download a report of this screen, as illustrated below:\nMultiple Rules Cloning From Different Sources\nStep 5:\nIf the user selects multiple rules belonging to different sources, the same step 1 will be executed. Upon redirecting to the clone detail page, you will observe the prefix, all clone properties, and below that, a target sources section. In the case of multiple rule sources, there will be both \"From Source\" and \"To Source\" options. By default, the \"To Source\" will be the same as the \"From Source,\" but users can modify it later, as depicted below:\nStep 6:\nOn the Clone Summary detail page, you will be able to see the number of rules that encountered errors and those that were successfully cloned, as illustrated below:\nMultiple Rules With Same Source\nStep 7:\nChoose rules that belong to the same source. Opt for the \"Clone Rule\" option, as demonstrated below:\nYou'll encounter a familiar screen as before, but this time, as the rules belong to the same source, there's only one target source displayed. Users can modify the target source if necessary. Simply click the \"Clone\" button, as indicated below:\nUpon successful cloning, you will observe that the two rules have been duplicated, and their details will be displayed as shown below:\nNote:\nA rule that has been cloned can be treated independently from the original (parent) rule on which the cloning was performed. This implies that any configurations made in the cloned rule will not affect the parent rule.\nCloning From Rule Detail Page\nStep 8:\nChoose any rule and navigate to its detail page, where you will find a \"Clone Rule\" button on the top right side, as illustrated below:\nStep 9:\nAfter clicking the \"Clone\" button, you will be directed to the clone detail page. In this specific flow, the information remains consistent, with the addition of displaying the target table on which the rule was created. Users have the option to modify both the target source and target table as needed.\nStep 10:\nIf the user selects a different target table to clone the rule and there's an issue with that target table, the user will receive a notification before the cloning process. For example, if there are data type mismatches between the source and target tables or if there is a structural mismatch, the user will be informed about these errors in advance. Various types of errors may be communicated to the user beforehand, as illustrated below:\nS\ntep 11:\nAfter cloning the rule, the user will be redirected to the rule detail page of the rule on which the cloning was performed. Additionally, a prominent glitter message/ confirmation message will be displayed at the top, indicating the new cloned rule ID. Clicking on the ID will open the detail page in a new tab, as depicted below:",
    "scraped_at": "2026-02-02 15:47:07"
  },
  {
    "title": "Rule Status",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360050837013-Rule-Status",
    "content": "Cleansing Information Visibility On Listing Page\nPreviously, when user used to run a rule then it showed the X number of exceptions and it's status which was also exception status. Later on, after cleansing and write-back operation, there was no such visibility of this action on analyze rule listing page and also in rule detail page unless user had opened cleanse workbench page. There was as such no indication that exceptions were getting fixed or write-back was performed.\nIn order to have visibility, without opening the work-bench, we introduced this new enhancement with the help of which now users will have more visibility about the rule(s) on listing/Rule detail page. To get familiar with this enhancement, let us get started step by step.\nManage Rules >> Rule Detail Page\nStep 1:\nGo to Manage Rule section, select any rule and click on it to open Rule detail page. On this page, you will see a new field added as 'Commit Status'. This field has three new statuses asÂ shown below.\n'Modified' : To be committed\n'Committed' : Write-back has done successfully\n'Commit Failed' : Write-back has failed\nCleanse Bench Detail Page\nStep 2 :\nNow, go to Analysis tab, hit the cleanse button there which opens up the cleanse bench detail page. On top you will see three different sections, 'Suggestions', 'Progress', 'Change History'. In Progress box, you will see total number of Exceptions, remaining ones and under that modified count, committed count and commit failed count is displaying as shown in image below:\nStep 3 : Modified Rule Status\nNow, update any value and you will see that Modified count gets updated from 0 to 1. Also, remaining Exceptions count reduces from 54 to 53 as shown below:\nStep 4: Committed Rule Status\nSave the changes and then hit commit button. After commit gets successful then modified count changes from 1 to 0 and commit count increases from 0 to 1 as shown below:\nStep 5 : Commit Failed Rule Status\nNow, we will try to perform this status scenario. For that, provide any input which is invalid and Save the changes. The modified count gets updated again from 0 to 1Â  and Remaining count reduces from 53 to 52 as shown below:\nStep 6 :\nAfter saving, now hit the Commit button and observe the following change. Remaining count stays as 52 but modified count changes again from 1 to 0 and since it was an invalid input so commit failed count gets increased by 1 as shown below:\nStep 7 :\nNow we will modify few more records to see the visibility of its status on other different screens. Make sure that you just Save the changes but do not commit it. After this, Modified count gets updated as 5 and Remaining count reduces to 47. The sum of these 3 status and remaining count will be equal to total Exception count as shown below:\nStep 8 :\nNavigate back to Rule detail page and check the count update there. It will be same as it was on cleanse bench detail page. Also, Run status field will show Modified status and icon since we did last modify it as shown below:\nAnalyze Rules >> Rule Listing Page\nStep 9:\nGo to analyze rule page and observe the count update there as well which will be as same as shown above. See image below :\nDashboard Page\nWe have added 2 more widget under Rule History category\nStep 10 :\nSame rule statuses will be shown in all existing widgets on Dashboard created on any particular rule. A new widget is added\nException detail History\nas shown below: It will show the commutative count for run result and other 3 rule statuses.\nStep 11 :\nNow click on \"\nRun status of Rules by weekly Run date\n\" to view report. The weekly filter is applied. Select the rule on which cleanse was done. You will see the recent rule statues count in the top record which lies with in the current week. The other records shown under it are previous weeks rule status results:",
    "scraped_at": "2026-02-02 15:47:12"
  },
  {
    "title": "SAP Writeback Configuration Setup For Writeback from Flat to Classification table Mapping",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360047799993-SAP-Writeback-Configuration-Setup-For-Writeback-from-Flat-to-Classification-table-Mapping",
    "content": "Scenario: Writeback from Flat to Classification table Mapping\nStep 1:\nIn Order to Writeback to SAP Table using DB Source, First Verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below\nNow Verify the Record Identifierâ€™s in the below location. If the Record Identifierâ€™s are not set, please set them here and Save.\nStep 2:\nNow, navigate to the DB Source Table (Ex: DVSUM.VW_ZMAT_ATTRIBUTE_FLAT) as shown below for which User wants to create a rule and do the writeback to SAP Table (Ex: ZMAT_ATTRIBUTE).\nStep 3:\nVerify the Record Identifiers are same as the SAP Table. If not, please select the record Identifiers for DB Source Table (Ex: DVSUM.VW_ZMAT_ATTRIBUTE_FLAT)\nStep 4:\nSelect the SAP Table (Ex: ZMAT_ATTRIBUTE) To which user needs to writeback using DB source Table (Ex: DVSUM.VW_ZMAT_ATTRIBUTE_FLAT) as shown below, and Click on â€œNextâ€\nStep 5:\nMap the Key Identifier columns and Write-back columns of DB Table and SAP Table by using below screen. Perform the mapping as shown the below screens.\nHere, As there are two Key Identifierâ€™s(MATNR, ATTRIBUTE) in the Target (ZMAT_ATTRIBUTE) table, and in the source we have only one key identifier (MATNR_ARTICLE), The Field â€œATTRIBUTEâ€ is by default selected as Qualifier.\nIn Case of ZMAT_ATTRIBUTE_FLAT table, there will be only One Writeback Column. That Field Needs to be selected under â€œWrite Back Column Mapping Selectionâ€.\nThe â€œQualifier Column Valueâ€ will be the â€œATTRIBUTEâ€ Column Value Where the Writeback will be performed in ZMAT_ATTRIBUTE table.\nOnce the mapping is completed for the Required/All columns that are used in the rule, Save the mapping by clicking on â€œSaveâ€ button.\nAddition configuration for Custom Query\nStep 6:\nStarting from creating a custom query rule, go to Manage Rules, click on Add Rule button, select Process Quality from drop down options and lastly click on Custom Query\nas shown in image below\nStep 6:\nUser will be redirected to Add Rule detail page. In Basic Input section, under Custom Query box you will see the\nEnableÂ Write-back\ncheck box\nas shown in image below:\nProvide all the details for the Rule and Enable Write-back by clicking on the check box. Save the rule by clicking on â€œSaveâ€ button\nStep 7:\nRun the Rule First, then to setup the Writeback Configuration Fields.\nAfter running the rule, User will see the rule as below.\nStep 8:\nSetup the Record identifiers mapping by navigating to â€œWriteback Configurationâ€ tab as shown below and click on â€œNextâ€\nStep 9:\nSetup the mapping for remaining columns as shown below. Click on check box under â€œEnable Write-backâ€ column for the columns where we need to writeback the data. After Enabling Writeback, User can verify the configuration where the field is going to writeback if SAP configuration is available.\nClick on â€œSaveâ€ to save the configuration as shown below\nStep 10:\nRe-Run the Rule after Saving to see the Editable Fields and Cleanse Button.\nAfter the running the rule, now user can see the Cleanse button and Writeback Fields marked with colour.\nStep 11:\nNow, click on Cleanse Button, Modify the Data as shown below.\nAfter Modifying, Save the data. Here, user can see the Writeback is going to Perform on SAP Table (ZMAT_ATTRIBUTE)\nStep 12:\nOnce the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.\nAs soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.\nWait for the Jobs to complete by Navigating to Scheduler Page.\n13:\nVerify the changes on the Rule Page by refreshing the page Once the Job(s) are completed.",
    "scraped_at": "2026-02-02 15:47:18"
  },
  {
    "title": "Column Sequencing",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360046573114-Column-Sequencing",
    "content": "Now, users can control the sequence of columns when profiling a source's table/view. Additionally, the sequence of columns can be customized at the rule level too. To learn more about this feature and understand how it works, let's start step by step.\nTable Level Column Sequencing\nStep 1\nNavigate to Profiling. On the Profiling detail page, choose a table associated with any source where you want to apply sequencing. If you observe the Configuration column, you'll notice that no icon is currently displayed, indicating that no column sequencing is applied. Once you apply customized sequencing at the table level, an icon will appear (which we will verify in the next steps). Finally, click on the \"Show Details\" button, as depicted in the image below:\nThe user will be redirected to the Table's detail page. In the Profiling tab, you will notice a\n\"Default Sequence\"\nlabel right next to the Table's name in the lower grid, serving as a secondary identification to indicate that no customized sequencing has been applied. Now, click on the \"Field Configuration\" tab, as shown in the image below:\nStep 2\nIt's important to note that the \"\nField Configuration\"\nis the main tab where column sequencing will be applied. Similar to the previous steps, the user will observe a \"Default\" label displayed next to the Table's name. Directly below that, in the lower main grid, two buttons will appear: \"\nColumn Sequencing\n\" and \"\nDefault Sequencing\n,\" as illustrated in the image below.\nThe \"Default Sequence\" button will initially appear disabled. This button is designed to reset the sequence of columns to its default behavior. It becomes enabled once the user applies customized sequencing to the table. Hover over the button to view additional information, as depicted in the image below:\nThe \"Column Sequencing\" button is where the user will click to set the sequence of columns. Hover over the button to read the following information, as illustrated in the image below:\nStep 3\nNow, click on the \"Column Sequencing\" button. This will open up a new interface listing all the column names. Begin arranging the sequence of column(s) through drag and drop. Once the sequence is set, click the \"Save\" button, as shown in the image below:\nCustomized Sequencing Confirmation I\nIn the image shown above, the user has placed the \"CUSTOMER_ID\" column at the top. Upon saving the changes, the user will be redirected to the Table's detail page, where the following updates will be visible, as shown in the image below:\nA - In the \"Field Configuration\" tab, \"CUSTOMER_ID\" is now displayed at the top in the Column name.\nB - The \"Default Sequence\" label is updated to \"Custom Sequence.\"\nC - The \"Default Sequence\" button is now enabled.\nClicking the \"Default Sequence\" button will reset the applied customized sequencing to its default state.\nStep 4\nCustomized Sequencing Confirmation II\nAfter setting the customized sequence in the \"Field Configuration\" tab, when you go to the \"Profiling\" tab, you'll see the same sequence applied. The \"Custom Sequence\" label will be visible on this screen as well.\nCustomized Sequencing Confirmation III\nClick on \"Profiling\" from the left navigation menu, and you will see an icon appearing against the table on which the user has applied a Custom Sequence, as depicted in the image below:\nStep 5:\nIf the user wants to reset the custom sequencing to its default sequencing, there are two ways to do that, as shown in the image below:\nStep 5.1:\n- Go to the Profiling main page.\n- Select the table on which custom sequencing is applied.\n- Click the \"More Actions\" button.\n- Finally, click on the \"Reset Column Sequence\" option.\nStep 5.2:\nUsers can also apply default sequencing on multiple records with the following actions:\n- Select multiple records on which custom sequencing is applied.\n- Click the \"More Actions\" button.\n- Then select the \"Reset Column Sequence\" option.\nNote: In case of multiple records, if there exists any single record with the default sequence, then the \"\nReset Column Sequence\n\" option in the \"More Actions\" button will remain disabled until the user un-selects that particular record.\nThe second way to reset custom sequencing to default sequencing is as follows:\n1. Select any record on the Profiling detail page.\n2. Click \"Show Details.\"\n3. Navigate to the \"Field Configuration\" tab.\n4. Click the \"Default Sequence\" button, as shown in the image below:\nOnce the user has reset the custom sequence to the default sequence, the applied changes will revert back to their default settings on the following sections:\nA - The \"Custom Sequence\" label will be replaced back with the \"Default Sequence\" label.\nB - The \"Default Sequence\" button will be disabled again.\nC - The column(s) sequence will be set to the default sequence.\nD - The custom sequence icon will disappear.\nRule Level Column Sequencing\nNote:\nColumn Sequencing at Rule level is only applicable for following rules as listed below:\nA- Blank Rule\nB- Value Range Rule\nC- Data Analysis Exception Rule\nD- Master Data-set Rule\nE- Data Format Rule\nF- Orphan Records Rule\nG- Integrity Check Rule\nStep 6\nTo begin with Column Sequencing at the Rule Level, we'll consider three scenarios, starting with Scenario I. For each of these scenarios, let's create a rule from scratch. Follow these steps:\n1. Go to \"Manage Rules.\"\n2. Click on the \"Add Rule\" button.\n3. Select the \"Business Context\" option.\n4. Then, click on \"Value Range\" rule.\nScenario IÂ  -Â  Table Specific :\nWhen the user selects the data source and table name on which custom sequencing is applied, follow these steps:\n1. Provide a rule description.\n2. Select the data source.\n3. Choose the table name.\n4. Select the field name.\n5. Enter the maximum and minimum threshold values.\n6. Hit the \"Save\" button.\nIn Scenario I, when the user lands on the Rule detail page, a new tab called \"Column Sequence\" becomes visible. This tab appears for rules where Rule-level column sequencing is enabled. Under this section, three options are displayed.\nA- Suggested\nB- Table Specific\nC- Rule Specific\nIn the current scenario, Option B will be selected by default. This represents the logic that when a rule is created on a Table/Source that already has Custom Sequencing applied, the sequence of columns in the rule will be the same as the Table-level sequencing. In the \"Selected Columns\" section, the column(s) on which the rule is created or the Primary Key column will always appear as disabled. Additionally, the \"Analysis\" tab initially appears as disabled. Re-run the rule to enable the Analysis tab.\nAfter the user has re-run the rule, the \"Analysis\" tab gets enabled. Click on the \"Analysis\" tab and verify that the sequence of columns is the same as it was on the table level, as shown in the image below. Click on the \"Cleanse\" button, and you will see the same sequence as it is on the \"Analysis\" tab and \"Column Sequence\" tab.\nStep 7\nIf the user changes the column sequence at the Table level, upon navigating back to the rule(s) in the \"Column Sequence\" tab, a warning message will notify the user to re-run the rule in order to see the updated sequence in the \"Analysis\" tab, as shown in the image below. Once the user re-runs the rule, the sequence gets updated in the \"Column Sequence\" tab, \"Analysis\" tab, and the \"Cleanse\" section.\nScenario II - Rule Specific :\nWhen user apply custom sequence at Rule level\nNow, go back to the \"Column Sequence\" tab. Change the sequence of the column(s). As soon as you change the sequence, the option changes automatically from Table specific to Rule Specific. Hit the \"Save\" button. The user will be notified through a Warning message to re-run the rule to see the updated sequence in the \"Analysis\" tab, as shown in the image below. Once the user re-runs the rule, the sequence gets updated in the \"Column Sequence\" tab, \"Analysis\" tab, and the \"Cleanse\" section.\nScenario III - Suggested :\nUser creates a rule on Table on which No custom sequencing is applied.\nCertainly! Here are the steps to complete Step 6:\n1. Provide a rule description.\n2. Select the data source.\n3. Choose the table name.\n4. Select the field name.\n5. Enter the maximum and minimum threshold values.\n6. Hit the \"Save\" button.\nIn Scenario III, when the user lands on the Rule detail page, in the \"Column Sequence\" tab, the \"Suggested\" option will be selected by default. However, the \"Table Specific\" option will appear as disabled.\nA- Suggested\nB- Table Specific\nC- Rule Specific\nAdditionally, the \"Analysis\" tab appears as disabled initially. Re-run the rule to enable the \"Analysis\" tab.\nNote:\nColumns that has exceptions will always appear upfront in case of Suggested option.\nMeanwhile, if the user applies custom sequencing at the Table level for the same table on which the above rule is created, then upon navigating back to the Column Sequencing section, the \"Table\" option gets enabled, as shown in the image below:\nStep 8\nNow, from here, select the \"Table Specific\" option. The column sequencing below gets updated with respect to the Table level. Also, if rule level sequencing is already applied to this rule, then on selecting the \"Rule Specific\" option, column sequencing gets updated with respect to the Rule level. However, to see the results in the \"Analysis\" and \"Cleanse\" tabs for Table level and Rule level changes, the user will always have to re-run the rule.\nCase 1:\nFor instance, if the user switches the option from 'Suggested' to the 'Table Specific' option, the sequence gets updated. After clicking the \"Save\" button, it notifies the user through a Warning message to re-run the rule in order to see the updated sequence in the \"Analysis\" and \"Cleanse\" tabs.\nCase 2:\nNow, in \"Table Specific,\" if the user changes the column sequence further, then the option switches from 'Table Specific' to 'Rule Specific' instantly. After clicking the \"Save\" button, it notifies the user through a Warning message to re-run the rule in order to see the updated sequence in the \"Analysis\" and \"Cleanse\" tabs.\nNote:\nIf the user has created multiple rules against a single table, each rule can have its own unique column sequencing.\nStep 9\nAs per the functionality, a maximum of 30 columns can be displayed on the Analysis tab. For instance, if a user selects a table that has 50 columns and creates a rule on the column that comes at the 40th number, the following behavior will be observed:\nA-\nSuggested:\nThe column name gets displayed upfront in the Column Sequencing/Analysis/Cleanse tab.\nB-\nTable/Rule Specific:\nThe column name gets displayed on the 31st number.",
    "scraped_at": "2026-02-02 15:47:23"
  },
  {
    "title": "Enable Writeback on Custom Query DQ Rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360045992074-Enable-Writeback-on-Custom-Query-DQ-Rules",
    "content": "Custom Query Rules\nCustom Query data quality rules are powerful rules that give users the ability to monitor any sort of data quality rules that they require. The article\nCreating a CUSTOM QUERY DQ Rule\nexplains how to create a rule. This article builds on that foundation and explains how to support writeback so that data stewards can take corrective action.\nEnabling writeback prerequisites\nYou have a working DQ rule of type \"CUSTOM QUERY\".\nThe table that you want to allow writeback on has an ID field configured.\nConfirm that an ID field is configured for the table\nProfile â†’ Profiling â†’ [tick relevant table] â†’ More Actions â†’ Edit Configuration\nConfirm the value of \"Field(s) that can uniquely identify a data record\".\nConfirm that \"Allow Write-Back\" is ticked.\nEnabling writeback steps\nOverview\nEdit rule definition to enable writeback.\nRun the rule.\nConfigure writeback fields.\nRun the rule.\nDetails\nEdit the rule definition and tick \"Enable Writeback\".\nNotes:\nThe tab \"Write-back Configuration\" is grayed out when writeback is not enabled.\nThe Analysis tab displays error records. They may be exported, but there is no \"Cleanse\" option to correct the errors.\nAfter saving the change, a warning is displayed.\nNote: the tab \"Write-back Configuration\" should now be enabled. But you must run the rule before it's possible to configure the writeback details.\nRun the rule.\nConfigure the Key Identifier on the \"Write-back Configuration\" tab. Then click Next.\nIn this example the Key Identifier is CUSTOMER_ID, and the field returned by the query is also CUSTOMER_ID. So DvSum has automatically guessed the mapping. This is typical, and it's a best practice to use field names that match. But in some cases you'll need to map fields with different names.\nConfigure the Write-back Column Mapping.\nEnable Write-back for the columns which data stewards should be allowed to make changes.\nRun the rule.\nCleanse is now enabled in the Analysis tab.\nThe fields enabled for writeback will be highlighted in red. Clicking \"Cleanse\" will start a workflow allowing a data steward to fix the data.",
    "scraped_at": "2026-02-02 15:47:28"
  },
  {
    "title": "SAP Writeback - Overview",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360045900614-SAP-Writeback-Overview",
    "content": "User will be able to perform write-back for SAP source using DvSum application. On opening cleanse workbench for the rules configured with SAP source, resolving the exception and performing commit, data will be written back to SAP source.\nSAP User Based Credentials\nDvSum application allows users to cleanse the datasets and write back directly on the source database. SAP database is a new addition as a source in DvSum application. For allowing user to write back on SAP source, the user needs to configure the SAP user credentials in DvSum Application. DvSum application will use those credentials for connecting to the source and enabling user to perform different actions on the source data.\nThere is a section in user profile page where user can setup the sap credentials for sources which have user-based write-back enabled. User will select the source and add the username and password for that source. User can also use Test connection button to test the connection with sap source with entered credentials.\nFor enabling, user will have to enable user based writeback in writeback parameters.\nWhen a user enters the credentials, these will be saved. Only one credentials can be added for one sap source for each user.\nAuthorization needs to be provided for a user to be able to do writeback. He should have access to to ZDVSUM* functional module in SAP.\nSAP Writeback Configuration Setup\nUser will now be able to Writeback Data into SAP Application System using Database Source (Ex: SAP MD) by doing certain configuration in DvSum Application at Source Level and Table Level. To know how to setup the configuration one by one, let us get started step by step.\nSource Level Configuration:\nStep 1:\nTo link an existing source connection to SAP, user should navigate to 'Administration' > 'Manage Sources' > select the source to be linked to SAP and click on 'Edit Source'\nStep 2:\nOn Clicking 'Edit Source' (Other than SAP), user will be able to see the following page.\nIn the 'Source Write back Mapping' section, Select the SAP Source to which Data should be Writeback. 'Write back On parameter' must be selected as \"Linked SAP source\" in order for write-back to be performed on SAP source.\nAfter selecting the â€œLinked SAP Sourceâ€ and â€œWriteback Onâ€, Save the Details by clicking on Save button.\nNote: Â 'Write back On' parameter selected as \"Original Source\", DvSum will write back to Original source.\nTable Level Configuration:\nThere can be multiple scenarios for setting the Table level configurations and do the writeback\nScenario A: Reading from SAP and writing to SAP\nStep 1:\nIn Order to Writeback to SAP Table using SAP Source, First Verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below\nStep 2:\nIn Order to Writeback to SAP Table using DB Source, First Verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below\nNow Verify the Record Identifierâ€™s in the below location. If the Record Identifierâ€™s are not set, Please set them here and Save.\nStep 3:\nStarting from creating a rule, Value Range rule type in this case, go to Manage Rules, click on Add Rule button, hover on Business Context from drop down options and lastly click on Value Range\nas shown in image below\n:\nAfter the running the rule, now we can see the Cleanse button and Writeback Fields marked with colour.\nStep 4:\nNow, click on Cleanse Button, Modify the Data as shown below.\nStep 5:\nOnce the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.\nAs soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.\nWait for the Jobs to complete by Navigating to Scheduler Page.\nStep 6:\nVerify the changes on the Rule Page by refreshing the page, once the Job(s) are completed.\nScenario B: Reading from MSSQL Database and writing to SAP\nStep 1:\nNow, navigate to Profile > Profiling page. Select the DB Source Table (Ex: SAP_MD QA , DBO>MARADBO.TVARTICLEPURCHASING_GROUP) as shown below for which User wants to create a rule and do the writeback.\nStep 2:\nVerify the Record Identifiers are same as the SAP Table. If not, please select the record Identifiers for DB Source Table and click Next.\nMake sure the key identifier is set against Source Identifier and the column which we want to use for cleansing purposes, enable writeback must be checked for it in Column Mapping section as shown in the image below. After setting the configuration, press Save.\nNote: Please make sure to set the Key identifier for the Target SAP source as well like we set for the above data source\nStep 3:\nStarting from creating a rule, Value Range rule type in this case, go to Manage Rules, click on Add Rule button, hover on Business Context from drop down options and lastly click on Value Range\nas shown in image below\n:\nStep 4:\nAfter the running the rule, now we can see the Cleanse button and Writeback Fields marked with colour.\nStep 5:\nNow, click on Cleanse Button, Modify the Data as shown below. You will also be able too see the target source in the cleanse workbench where writeback would happen\nStep 6:\nOnce the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.\nAs soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.\nWait for the Jobs to complete by Navigating to Scheduler Page.\nStep 7:\nVerify the changes on the Rule Page by refreshing the page, once the Job(s) are completed.\nScenario C: Reading from MSSQL Database and writing to SAP using Custom Query Rule\nStep 1:\nIn Order to Writeback to SAP Table using DB Source via Custom Query rule, first verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below\nNow Verify the Record Identifierâ€™s in the below location. If the Record Identifierâ€™s are not set, please set them here and Save.\nStep 2:\nNow, navigate to the DB Source Table (Ex: DBO.TVARTICLEPURCHASING_GROUP) as shown below for which User wants to create a rule and do the writeback to SAP Table (Ex: MARC).\nStep 3:\nVerify the Record Identifiers are same as the SAP Table. If not, please select the record Identifiers for DB Source Table (Ex: DBO.TVARTICLEPURCHASING_GROUP)\nStep 4:\nSelect the SAP Table (Ex: MARC) To which user needs to writeback using DB source Table (Ex: DBO.TVARTICLEPURCHASING_GROUP) as shown below, and Click on â€œNextâ€\nStep 5:\nIf the field names are same, they will be mapped automatically. If not, user will have to manually map the Key Identifier columns and Write-back columns of DB Table and SAP Table by using below screen.\nPerforming the mapping for the Fields as shown the below screens. Here you can choose the fields you want to perform writeback on by selecting the Enable Write-back as checked for the respective fields\nOnce the mapping is completed for the Required/All columns that are used in the rule, Save the mapping by clicking on â€œSaveâ€ button.\nAddition configuration for Custom Query\nStep 6:\nStarting from creating a custom query rule, go to Manage Rules, click on Add Rule button, select Process Quality from drop down options and lastly click on Custom Query\nas shown in image below\n:\nStep 7:\nUser will be redirected to Add Rule detail page. In Basic Input section, under Custom Query box you will see the\nEnableÂ Write-back\ncheck box\nas shown in image below:\nProvide all the details for the Rule and Enable Write-back by clicking on the check box. Save the rule by clicking on â€œSaveâ€ button.\nStep 8:\nRun the Rule First, then to setup the Writeback Configuration Fields.\nAfter running the rule, User will see the rule as below.\nStep 9:\nSetup the Record identifiers mapping by navigating to â€œWriteback Configurationâ€ tab as shown below and click on â€œNextâ€\nStep 10:\nSetup the mapping for remaining columns as shown below.\nClick on check box under â€œEnable Write-backâ€ column for the columns where we need to writeback the data\n.\nAfter Enabling Writeback, User can verify the configuration where the field is going to writeback if SAP configuration is available.\nClick on â€œSaveâ€ to save the configuration as shown below\nStep 11:\nRe-Run the Rule after Saving to see the Editable Fields and Cleanse Button.\nAfter the running the rule, now we can see the Cleanse button and Writeback Fields marked with colour.\nStep 12:\nNow, click on Cleanse Button, Modify the Data as shown below.\nAfter Modifying, Save the data. Here, user can see the Writeback is going to Perform on SAP Table (MARC)\nStep 13:\nOnce the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.\nAs soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.\nWait for the Jobs to complete by Navigating to Scheduler Page.\nStep 14:\nVerify the changes on the Rule Page by refreshing the page, Once the Job(s) are completed.\nHow the Commit Works\nOn setting up the configurations are set based on any of the above mentioned four scenarios, user will run the rule and see the pink colour for exceptions. User will click on Cleanse button and make the changes. While making changes, there is an ability where you can group the fields like the one mentioned below\nOn saving the changes and pressing commit, it will create a job. User will have to wait for the job to get completed. On job completion, user will receive the email template like below\nIf user wants to see the status of ongoing job, user can navigate to Review > Scheduler section to view the current status of the job.\nControl Options\nUser can run the mass update in multiple batches. In order to configure batches, go to Manage Sources and define Batch Size.\nBatch Size Variable\nThere is another variable called Writeback Batch Size that will limit the number of records that can be included in a single Batch Job at any given time.Â  Currently, in SAP QA1, this is set to 1,000.Â  Thus, if the data teams were to submit a job to cleanse the 75,000 records mentioned above, the DVSum system would create 75 executable jobs under that Batch Job, each with 1,000 changes.\nthe DvSum process to commit the jobs for changing in SAP, the 75 jobs would be placed in DvSumâ€™s job queue with 1 job marked with the status â€˜Scheduledâ€™ and 74 jobs marked with the status â€˜Stand byâ€™.Â  The job in status â€˜Scheduledâ€™ will wait until the scheduled time to begin and then will switch to status â€˜Runningâ€™ until the time that SAP returns after the completion of that work process.\nUpon receiving the return from SAP, DvSum will mark that job with the status â€˜Completedâ€™ and will mark the next job with the status â€˜Scheduledâ€™ with a start time set for the next change in minute. This process will continue until all 75 jobs have been sequentially executed until completion or until one of the following events occurs:\nDvSum user places the Job on\nhold\nâ€“ This action in DvSum will place all jobs\nnot currently in execution within SAP\nto the status â€˜On Holdâ€™ and will prevent DvSum from sending the remaining jobs to SAP for execution until such a time as the DvSum user releases the jobs to continue their execution.Â  This action will not stop the job currently being executed by the SAP System, but it will prevent any subsequent jobs from being sent.\nDvSum user deletes the Job in Running\nâ€“ This action in DvSum should place the current job to the status â€˜Completed (Aborted)â€™, and all subsequent Stand by jobs for this batch will be put on â€˜On Holdâ€™ job status.\nSAP Basis user cancels the work process\nâ€“ This action should terminate the current work process in SAP, and set the status of running job to errored and any subsequent jobs scheduled for that Batch should be changed to status â€˜On Holdâ€™ until such a time that the SAP Basis team allows the continuation of Batch Job processing.\nWhen the status of all the subsequent jobs get completed, user will go to the respective rule details page and refresh it.\nIf the colour of the submitted exceptions is green, this means the writeback happened successfully.\nIf the colour of the submitted exceptions gets orange, this means the writeback operation got failed. To know the possible error, click on cleanse button and hover exclamation mark in Change History section. It will tell you the possible reason of failure and analyse with your SAP team to resolve the error.",
    "scraped_at": "2026-02-02 15:47:34"
  },
  {
    "title": "SAP Connector Installation and Configuration Guide",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360043550373-SAP-Connector-Installation-and-Configuration-Guide",
    "content": "For making connections to SAP ECC, DvSum provides an RFC-enabled BAPI that is optimized to handle data query type of requests utilized in DvSum.\nThe attachments contains the Transport Files and the installation instructions.",
    "scraped_at": "2026-02-02 15:47:39"
  },
  {
    "title": "How is Readiness Score calculated for rules?",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360035985334-How-is-Readiness-Score-calculated-for-rules",
    "content": "Readiness Score Definition\nReadiness Score is a measure that identifies data quality in terms of percentage of clean, meaningful data among all records. It is visible on the \"Data Quality\" tab for profiled data sources, and it is displayed as \"R Score(%)\" in tabular views. It is listed as \"\nDQ Score\" for individual Data Quality rules.\nIt gives a precise overview to users of the quality of their data and helps them to make relevant decisions.\nReadiness Score Calculation\nReadiness score is calculated slightly differently for different types of rules. The chart below describes how the score is calculated for each type of DvSum rule.\nRule Type\nReadiness Score Calculation\nCount\nCount distinct\nSUM\nMANUAL\nIf the threshold minimum is 0 and the result is less than or equal to 0, then readiness score is 100%.\nIf the threshold minimum is not 0 and the result is less than or equal to the threshold minimum, then the readiness score is:\nresult / threshold minimum\nIf the result is greater than or equal to 2 times the threshold maximum, then the readiness score is zero.\nIf the result is greater than or equal to the threshold maximum but less than 2 times the threshold maximum, then the readiness score is:\n(result - threshold maximum) / threshold maximum\nBlank Check\nValue Range\nDAE â€“ Data Analysis Exceptions\nSize Check\nMDC Ruleset\nOrphan Records\nCross Dataset\nIntegrity Check\nOrphan Keys\nRCL -X-System Reconciliation\nDocument\nMatching\nAddress Validation\nData Format\nIf there are no exceptions, then the Readiness Score is 100%.\nIf there are exceptions, then the Readiness Score is calculated as:\n(total records - exception records) / total records\nCUSTOM QUERY\nIf there are no exceptions, then readiness score is 100%.\nIf there are exceptions, then readiness score is:\n(total records - exception records) / total records\nIf the total record count is less than the custom query count, then readiness score is 0%.\nCompare count\nCompare Table\nCompare Custom Query\nMetric Value not selected on either Target or Reference\nIf the calculated result is less than or equal to the reference result, then the readiness score is:\ncalculated result / reference result\nIf the calculated result is greater than or equal to 2 times the reference result, then the readiness score is 0%.\nIf the calculated result is greater than or equal to the reference result, then the readiness score is:\n(calculated result - reference result) / reference result\nRule Count Readiness Score at Dashboard Level:\nThis shows up in widgets including \"Readiness Score by Val Group\" and other similar widgets. It is analogous to the calculations above. But it is different because this is a calculation about how many rules have no exceptions rather than how many records have no exceptions.\nThe \"Rule Count Readiness Score\" calculation:\n(Total rule count - Rules which fail or have exceptions) / Total rule count",
    "scraped_at": "2026-02-02 15:47:43"
  },
  {
    "title": "Data Format Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360013705873-Data-Format-Rule",
    "content": "The Data Format rule is useful for examining the data types, length, and patterns of input data, as well as determining whether a numerical field is a whole number, an integer, or a decimal.\nFor instance, a basic test helps find irregular records in an Excel dataset when a specific column doesn't match the required data format. In Data Format, you can set the data type (String or Number), character length (minimum and maximum column width), numeric precision (for decimals), and specify if white spaces are allowed.\nThis rule is accessible both independently in Manage Rules and as part of the Rule Set, which includes criteria like blanks, value range, and data format rules.\nThis rule created on column SIZE_DIM will bring exception records where field values are less than 5 or greater than 15 characters.\nLikewise, for numeric fields, you have the flexibility to select the data type as decimal, whole numbers, or integers.\nFor the \"WEIGHT\" column, values with a decimal precision greater than 1 will be treated as exceptions. Similar to other exception rules, you have the option to cleanse and write-back fixes to the Excel source.",
    "scraped_at": "2026-02-02 15:47:48"
  },
  {
    "title": "Add an Orphan Records check between two datasets",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360009027514-Add-an-Orphan-Records-check-between-two-datasets",
    "content": "Overview\nOrphan records are records that exist in one dataset but not in a corresponding dataset. For example, consider these three datasets: ORDERS_PLACED, ORDERS_SHIPPED, INVOICES. We may have a business requirement that any orders found in ORDERS_SHIPPED or in INVOICES must also be found in ORDERS_PLACED. If we ship an order but have no record of receiving the order, there is a discrepancy. The converse could also apply: if we have an order in ORDERS_PLACED but no corresponding record in INVOICES, this could also indicate a problem.\nYou can perform this type of check using Orphan Records or Orphan Keys rules in DvSum.\nDetailed Steps\nStep 1. Manage Rules\nLogin to DvSum and navigate to Audit â†’ Manage Rules.\nStep 2. Orphan Records\nSelect \"âŠ• Add Rule\" â†’ Foundational â†’ Orphan Records.\nStep 3: Basic Input\nIn the Rules Wizard, enter a description, e.g. \"Validate that all ORDERS_SHIPPED appear in ORDERS_PLACED.\" Then fill in the Basic Input section with Data Source, Table Name, and Field Name corresponding to the table in which you want to search for orphan records.\nStep 4: Reference Input\nFill in the Reference Input with Data Source, Table Name, and Field Name that you want to check against. The system will automatically select the columns with the same names. You can change them if appropriate.\nStep 5: Validate\nSave the rule. The rule definition will be displayed. Click Run to execute and test the rule.",
    "scraped_at": "2026-02-02 15:47:53"
  },
  {
    "title": "Field Dictionary",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360007001653-Field-Dictionary",
    "content": "The term field dictionary appears in basic Value Range rule (or DQ ruleset, Exploration). Value rangeÂ identifies exception records within a dataset where a specific field is not within a desired range. Range can be a continue range between min and max or could be a set of specific values.\nThese Specific values can be selected from the field's own profile called Field Dictionary. In a Value Range rule, select your data source, table and field. Specify Field dictionary to define valid/invalid values.\nThis will show you all existing values of the field \"Ending_Date\". This makes it easy for a user to pick out the values for fixing data.",
    "scraped_at": "2026-02-02 15:47:58"
  },
  {
    "title": "Features of Value Range Rule",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360006916294-Features-of-Value-Range-Rule",
    "content": "Value Range is one of the most widely used rule to cleanse the data. It helps you to identify the exceptions by specifying valid range or giving a list of invalid records.Â Range can be a continuous range between min and max or could be a set of specific values. Specific values can be selected from the field's own profile or as a reference dictionary.\nLet's look into the detail of this rule. First you select your Data source, table and field. Now you can select the kind of values as valid or invalid.\nValid/Invalid Range\nFrom the second drop down you can specify whether the range is for Valid values or invalid. If you select \"Valid\", the rule execution will show any value that is\noutside\nthe range as an exception. If you select \"Invalid\", the rule execution will show any value is\nwithin\nthe stated range as an exception.\nSingle Value\nAs the name suggests, for a single value there is a single threshold. From the list of operators, you can define the threshold on a column name.\nConsidering an example, where we want to create a VR rule on \"Ending_Date\" which should be greater than equal to the \"Starting_Date\". Specifying this as \"Valid\" range, this rule will show the records where Ending_Date is less than Starting_Date as exceptions.\nThe same case can be started the other way where a user can create a rule on \"Ending_Date\" and provide threshold as less than the \"Starting_Date\" to define \"Invalid\" records. Both the use cases will return same exceptions.\nRange Value\nFor the Range values, you need to provide the minimum and maximum threshold.\nHere you have 3 options. You can provide an absolute value, like 10 or you can use Reference column of the table. Threshold Max would be less than equal to the column Title. You can also opt for a custom expression.\nField Dictionary\nField dictionary\nprovides you with a list of values in the selected column which makes it easier to pick out the records you want as valid or invalid. Lets say we select as 2001-02 as invalid. This rule execution will bring records with Starting_Date as 2001-02 as exception in result.\nReference Dictionary\nFor reference dictionary with Valid values, the rule execution will bring the records with values that don't exist in dictionary as exceptions in result. For the Field \"is_current' any value other than the ref dictionary will be exceptions.\nNote: You can create your own Reference Dictionary from Profile tab or you can see\nHow to create and use Reference Dictionary\nInclude Blanks\nAnother small yet powerful check you can integrate with this rule is \"Include Blanks\". As an example below, a user can specify \"Valid Range\" of values and check-mark the Include Blanks. This mean the blank fields in the column \"is_current\" will not be shown as exceptions in rule result.\nFor the same case, if \"Invalid\" is selected,Â blank fields in the column \"is_current\" will be shown as exceptions along with any value within the range 10-20.",
    "scraped_at": "2026-02-02 15:48:03"
  },
  {
    "title": "Integrating Rules into batch workflow",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/360006820634-Integrating-Rules-into-batch-workflow",
    "content": "Rules in DvSum can be executed from the UI, scheduled through the scheduler and can also be called programmatically using an API. To run a rule or set of rules through a script, you can generate a script for those rules using the Schedule Rule Menu.\nThe script takes the form of\nhttps://<saws\naddress>:<saws_port>/runJob?jobId=<jobid>\nThe script runs the rule exactly like running from the UI or the Scheduler, with the difference being what is returned back from the script. Below is a screenshot of what is returned from the script. The 2 elements areÂ  return_code and return_remark.\nThe return_code always contains 3 values\n0Â  - PASS if rule has 0 exceptions or exceptions are less than lower bound of threshold\n-1 - WARN if the rule has exceptions but exceptions are greater than lower bound but less than upper bound\n-2Â  - FAIL if the rule has exceptions that are greater than upper bound limit.\nThe return_remark contains text that will provide the name of the rule additional information that you typically see in system remark field in the Rule Detail page.\nExample\nThis rule has threshold set between 0 - 300\nSince exception are 256 which is > 0 but less than <300, the return_code is -1 as a Warning.\nExecuting the rule via API:\nUsers can generate the script for the rule and use the script URL to execute the rule. Lets get started step by step:\nStep 1:\nOpen the Dvsum application, go to Audit >> Manage Rules >> select any existing rule >> click More action >> select Schedule Rule as shown below:\nStep 2:\nDynamic Source Script Generation\nGo to Generate Script tab and click on the button at the bottom to generate the script as shown below;\nStep 2.1:\nA job is created against the rule. Now go to the main scheduler page, search the rule, and a newly created job will be visible with a description and status as â€œstand byâ€.\nStep 3:\nCopy the script url and paste it into Google Chrome and press Enter. Execution starts and on dvsum application, the job description and status get executed as well.\nStep 3.1:\nOnce the execution is completed. it will print the remarks same as shown on the rule detail page in Dvsum.\nOn the Scheduler page, the job status changes to \"Completed\".\nExecuting the rule API via ADF:\nStep 1:\nFor the existing Rule, select schedule to Generate the script URL as shown above, and copy the url.\nStep 1.1:\nA job is created against the rule. Now go to the main scheduler page, search the rule, and a newly created job will be visible with a description and status as â€œstand byâ€.\nStep 2:\nGo to Azure Data Factory and paste the copied URL in api call. Click on Debug button to start the execution.\nStep 2.1:\nOnce the execution is completed. it will print the remarks in API call output same as shown on the rule detail page in Dvsum.\nOn the Scheduler page, the job status changes to \"Completed\".",
    "scraped_at": "2026-02-02 15:48:08"
  },
  {
    "title": "Use Rule Groups as tags for better management of Rules",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/115000728633-Use-Rule-Groups-as-tags-for-better-management-of-Rules",
    "content": "What are Rule Groups?\nRule Groups are\ntags\nthat allow you to organize your rules into categories. (They were formerly known as \"Val Groups\", so you'll see this terminology in some documentation.) You can apply a Rule Group tag in order to categorize DvSum Rules. Rule groups are like folders, but unlike folders you can apply more than one Rule Group tag to a single rule. Likewise, a rule can belong to multiple Rule Groups.\nUsing Rule Groups\nFor instance, the user \"Dani DataSteward\" wants to group together a few rules she created. She can create a Rule Group called \"Dani DataSteward Rules\". She can then filter the \"Analyze Rules\" grid by Rule Group. This gives her the flexibility to tag rules that were not created by her as part of this Rule Group.\nAdditionally, under Review â†’ Batch Execution an administrator can execute multiple rules tagged with a specified set of Rule Groups very easily.\nCreating Rule Groups\nWhile creating a rule\nA user can add Rule Groups from the \"Add Rule\" wizard when creating a new rule. As the user starts typing in the field \"Rule Group\", the list of existing Rule Groups appears. If the desired rule group does not already exist, then\nhit Enter to create the new Rule Group\n. (Note that the screenshot below shows the historic\nlabel\n\"Val Group\" rather than the current label \"Rule Group\".)\nWhile editing a rule\nYou can also add New Rule Groups from rule detail page. Edit the rule summary information. The Rule Group drop down displays the list of existing rule groups. To create a new one, simply type the name and press Enter.\nWhile performing a Mass Update\nOn the pages \"Manage Rules\" and \"Analyze Rules\", you have the option to perform a Mass Update. Under the field \"Rule Group\", you can select from the existing list or type to create a new rule group.\nRemove Rules from a Rule Group\nWhile editing a rule summary it's possible to remove any currently assigned rule groups.\nAdditionally, on the pages \"Manage Rules\" and \"Analyze Rules\" you may select rules and click on More Actions â†’ Mass Update. This interface provide the ability to remove (or add) rule groups.\nYou have the following options regarding Rule Groups:\nAdd following Rule Groups\nReplace existing Rule Groups with following\nRemove following Rule Groups (pictured above)",
    "scraped_at": "2026-02-02 15:48:13"
  },
  {
    "title": "Types of Rules Wizards",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/230178488-Types-of-Rules-Wizards",
    "content": "Types of Rules Wizards\nWe encourage customers to begin by identifying whatever data errors are causing impacts to their business and start with rules to identify those exceptions. Some of the rule types are straightforward, and their relevance is obvious. Others were created for specific customer use cases, and the potential application to your environment may not be readily apparent. Over time, potential rules and use cases become more apparent as you build out a more complete data governance approach.\nThe rules wizards are organized into the following categories:\nFoundational DQ\nBasic, logical checks you would typically do in any environment. Includes completeness, consistency, and validity checks.\nPipeline Checks\nValidates data pipelines by verifying data quantities, assessing data quality and performance, and monitoring data timeliness within the data processing workflows. Enhances data flow, quality, and efficiency.\nCross System DQ\nValidations and checks based on a specific business understanding of what the data should be.\nMDM\nIs the data valid or what is expected based on the process or purpose. Includes accuracy checks.\nData Diffs\nCompare and reconcile details and records across systems â€“ without having to consolidate or pull the data. Includes consistency and validity checks.\nWithin these categories are various rule types that can be configured to validate data and generate exceptions. There may also be different approaches to start with different rule types that generate the Exceptions youâ€™re looking for. We encourage users to explore different rules and approaches, as there is an art to determining what works best within your environment and business processes.\nType of Audit Rule\nDefinition\nExample (s)\nFoundational DQ\nBLANKS\nAvailability and completeness of data. Are there missing values?\nAll the records where address field is missing in customer ship-to information.\nVALUE RANGE\nAre the values in data within the expected range? Either within a continuous range, within a pick list, or one of a specific value.\nManufacturing yield should always be between 0.01 and 1\nOrder type in sales orders should only be one of ZOST, ZOCO, ZOFR.\nItem_category in shipments should only be one of the values in the item_category reference\nUNIQUENESS\nAre the values within a data element unique in the dataset?\nDuplicate customers in customer table. Data is not unique. Duplicate keys and their count are displayed.\nUNIQUE VALUES\nAre the Count of Unique Values within an acceptable range or equal to a defined number?\nExpect unique locations to be between 95-100 based on 100 ship-from locations. Over 100 indicates error. Under 95 indicates potential error.\nCUSTOM QUERY\nAbility to write custom SQL.\nAny custom SQL query.\nDATA FORMAT\nAre the values in the data conforming to predefined formats, or users defining their own formats or patterns, such as SSN, Email etc. ensuring consistency and accuracy.\nExpect 2 values after decimal point in employee IDs field , if employee ID violates the rules's expected format, it would be considered a potential error.\nDefining pattern and then providing regex expression in the filter modal.\nPipeline Checks\nCOUNT\nIs the count of records within the expected range?\nCount of sales orders in open order extract should be between 800k and 1M records.\nFreshness\nMonitors if data is not present or updated by a certain time.\nIn a weather forecasting app:\nData initially fetched at \"2023-09-30 08:00:00.\"\nNext execution timestamp: \"2023-09-30 14:30:00.\"\nTime difference: 6 hours.\nDefined threshold: 3 hours.\nif the difference is more than the defined difference, it should be alerting.\nMetric\nIs the aggregated quantity within the expected range?\nSum of open order quantity should be between 75M and 100M units.\nCross System DQ\nORPHAN RECORDS\nRecord exists in one systemâ€™s set of records, but not in another systemâ€™s set of what should be the same records.\nOrder A exists in the ERPâ€™s list of Orders, but not in the Transportation Systemâ€™s list of Planned Orders.\nORPHAN KEYS\nGroup of Records of a certain type exist in one system but not in another. Summarized by type.\nShipments for Customer X not found in list of Shipments by Customer.\nProduct Category B is found in the list of shipments, but it is not found in list of Products.\nINTEGRITY CHECK\nDoes valid data exist in reference systems to execute a process?\nDoes each SKU have a valid Bill-of-Distribution?\nDoes each Item have weight/measure populated?\nMDM\nADDRESS VALIDATION\nValidates existing address against Google Maps. Also enables standardization.\n123 Main Rd should actually be 123 W Main St.\nDOC-MATCHING\nCompare records at a field level and highlight discrepancies.\nFor Planned Orders in ERP vs Planned Orders in Warehouse System: validate dates, quantity, and price and show discrepancies\nData Diffs\nCOMPARE COUNT\nCompare the count of records between 2 different data sources. The test may be at same or different granularities.\nCompare total Orders or Shipments in ERP at transactional level to Order or Shipment summary loaded in data warehouse at aggregated level.\nCOMPARE CUSTOM QUERY\nCreate your own custom validation rule within one data set and compare to the same query in a separate data set or system.\nCompare totals in the operational system of record to a downstream system and to an enterprise data warehouse.\nCOMPARE METRIC\nCompare aggregated quantity between 2 different data sources.\nCompare the total shipment volume for last 3 months in ERP with shipment volume loaded in data warehouse at aggregated level.\nCOMPARE TABLE\nHolistic comparison of the count of records and volume for metric fields across different attributes.\nCompare Open Sales Orders in SAP with Sales Order extract in JDA for count, total of orderquantity, qtyopen by order_type, item_category, plant, key customer accounts, product line.\nCOMPARE SCHEMA\nCompare the schema structure between two sources, highlight differences in tables and columns, include disparities in data types where applicable.\nCompare schema definitions between the operational system of record, a downstream system, and an enterprise data warehouse to ensure consistency and identify any discrepancies in table structures and column definitions.\nRuleset\nRuleset is a predefined set of data constraints, It ensures data accuracy by enforcing rules such as disallowing blank values, restricting values to a specified range, and validating data formats for columns like strings or numbers.\nExample:\nCustomer_ID can't be left blank, and the \"Country\" column should only have two-character inputs like US or UK.",
    "scraped_at": "2026-02-02 15:48:17"
  },
  {
    "title": "Validation Rule Reference Information",
    "url": "https://dvsum.zendesk.com/hc/en-us/articles/203082320-Validation-Rule-Reference-Information",
    "content": "Overview\nReference information about validation rules (DQ Rules).\nRule Types\nCategory\nRule Type\nFoundational DQ\nDATA FORMAT\nBLANKS\nVALUERANGE\nUNIQUENESS\nUNIQUE VALUES\nDATA ANALYSIS EXCEPTIONS\nCUSTOM QUERY\nPipeline Checks\nCOUNT\nFRESHNESS\nMETRIC\nCross-System DQ\nORPHAN RECORDS\nORPHAN KEYS\nINTEGRITY CHECK\nMDM\nADDRESS VALIDATION\nDOC-MATCHING\nData Diffs\nCOMPARE COUNT\nCOMPARE METRIC\nCOMPARE TABLE\nCOMPARE CUSTOM QUERY\nRun Status\nA new rule will have an implicit Run Status of\nValid\n. After the rule is executed, there are a variety of Run Results that the rule may take. \"Run Status\" and \"Run Result\" are used interchangeably in DvSum DQ.\nStatus\nIcon\nDefinition\nPassed\nRule is valid, and no exceptions were found in the data in the most recent run.\nFailed\nRule is valid, and exceptions were found in the most recent run.\nThis applies only to rules which do not identify specific invalid records. For example, a COUNT rule result falls outside of the allowed range.\nException\nRule is valid, and at least one exception was found in the data in the most recent run.\nThis applies to rules which identify invalid records. For example, a failed BLANKS or UNIQUENESS check will result in this status.\nInvalid\nRule is not valid.\nThis does not indicate a problem with the data. Rather, it indicates a problem with the rule definition which must be solved before the rule can be executed.\nMatched\nFor example, a ADDRESS VALIDATION will indicate Matched after successful execution.\nModified\nAfter data steward performs cleansing, but data is not yet committed back to source.\nCommitted\nValid\nRun Status & Readiness Score\nAlong with a status, a Readiness Score is also generated. Readiness score provides a second degree of information on the quality of data. If the data rule is failing, how bad is it. Readiness score creates a common unit for measuring the quality of data and allows result of audits calculating the overall data quality score across various data elements and types of audits.\nSample Audit Result\nAudit Result Icon\nReadiness Score\nCount of finished good items in item master is 6000 which is within the tolerance of 5500 and 7000\n100%\nCount of finished good items in item master is 5000 which is not within the tolerance of 5500 and 7000\n91%\nExplanation: 5000 is 9% short of 5500.\nCalculation:\n1 - (500/5500)\nForecast Name is 100% unique in SALES_FORECAST extract\n100%\nRun over run variance of qtyplanned in PURCHASE_PLAN is 5% for Supplier X which is more than the limit of 3%\n33%\nCalculation:\n1 - (.02/.03)\nThere are 2 routing records where yield is not between 0 and 1. (total records are 100)\n98%\nCalculation:\n1 - (2/100)\nFollowing insights are available with your audit results\nIcon\nInsight Type\nDefinition\nHistory\nHistory for all audits\nException List\nList of Exceptions for master data audits\nDrill-down Analytical\nDrill-down with Variance for aggregation audits\nHistory Trend Insights\nWith history, you can get insights into the changes to the audit result from last run and also the trend of that audit over time. History insights can be useful to identify what audits to focus on.\nHistory Trend\nDefinition\nFocus Required\nFlat. No change\nStatus Quo\nor\nPositive Up\nâ€“ Test was a fail before and now pass. Test has been failing because results too low but now trending up.\nPositive Down\nâ€“ Test has been failing because results were too high and now trending down.\nThings are improving even if the tests are failing. May not need to focus.\nor\nNegative Down\nâ€“ Test was passing before and is now failing. Or it has been passing but values are trending down and will cross the lower tolerance soon. Or it has been failing and things are getting worse\nNegative Up\nâ€“ Test was passing before and is not failing. Or it has been passing but values are trending up to cross the upper tolerance soon\nThings are getting worse for failed tests â€“ requires top priority focus.\nThings are trending in the wrong direction for passing tests â€“ heads-up for future issues",
    "scraped_at": "2026-02-02 15:48:23"
  }
]