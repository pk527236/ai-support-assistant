================================================================================
TITLE: DvSum AWS PrivateLink Setup (Customer Guide)
URL: https://dvsum.zendesk.com/hc/en-us/articles/43220623721876-DvSum-AWS-PrivateLink-Setup-Customer-Guide
================================================================================

In this Article:
Overview
Service Architecture
Information Required from Customer
Information Provided by DvSum
Prerequisites
Step-by-Step Setup Instructions
Step 1: Create VPC Endpoint for REST APIs (apis.dvsum.ai)
Step 2: Create VPC Endpoint for WebSocket (socket-api.dvsum.ai)
Step 3: Create VPC Endpoint for S3 (s3-storage.dvsum.ai)
Step 4: Notify DvSum for Endpoint Acceptance
Step 5: Security Group Configuration
Step 6: DNS and Connectivity Validation
Custom DNS Configuration
Advanced Troubleshooting
Overview
This document provides comprehensive setup guidance for securely connecting to DvSum SaaS services using AWS PrivateLink. It enables private network access to REST APIs, WebSocket services, and S3 storage using the AWS network. All communication remains inside the AWS private backbone, improving security, reliability, and compliance for enterprise environments. The document also details required customer information, DvSum-provided configurations, step-by-step setup instructions, and validation procedures to establish AWS PrivateLink connectivity successfully.
Service Architecture
DvSum provides
three separate VPC Endpoint Services
for comprehensive private access:
Service
Domain
Purpose
Protocol
Port
Endpoint Service
REST APIs
apis.dvsum.ai
Standard API communication
HTTPS
443
Dedicated Service
WebSocket
socket-api.dvsum.ai
Real-time WebSocket communication
HTTPS
443
Dedicated Service
S3
s3-storage.dvsum.ai
S3 Pre-signed URLs Handling
HTTPS
443
Dedicated Service
Important
: Each service requires its own VPC Interface Endpoint in your VPC
Information Required from Customer
To enable PrivateLink connectivity, please provide DvSum with:
1. Customer Account Information
Parameter
Description
AWS Account ID
Your 12-digit AWS account number
Primary AWS Region
The AWS region where your VPC resides
VPC ID
The VPC where interface endpoints will be created
2. Network Configuration
Parameter
Description
VPC CIDR Blocks
Used for security group configuration and network monitoring
Subnet IDs
Subnets where you plan to create interface endpoints (recommend private subnets)
Availability Zones
AZs to use for endpoint deployment (recommend all for high availability)
3. Service Level Requirements
Expected bandwidth
: Helps DvSum provision appropriate infrastructure capacity
Information Provided by DvSum
Before proceeding with setup, DvSum will provide you with the following:
1. VPC Endpoint Service Names
Service
Example Service Name
REST API Service
com.amazonaws.vpce.us-east-1.vpce-svc-yyyyyyyyy
WebSocket Service
com.amazonaws.vpce.us-east-1.vpce-svc-zzzzzzzzz
S3 Service
com.amazonaws.vpce.us-east-1.vpce-svc-xxxxxxxxxxxxxx
2. Health Check Testing Endpoints
Service
URL
REST API
https://apis.dvsum.ai/health-check
WebSocket
https://socket-api.dvsum.ai/health-check
S3
https://s3-storage.dvsum.ai/health-check
Prerequisites
Before starting the setup, ensure you have:
AWS Access Requirements
AWS Console access with permissions for:
VPC endpoint creation and management
Security group configuration
Route 53 private hosted zone management (if needed)
EC2 instance access (for testing)
Network Requirements
A VPC with private subnets in multiple Availability Zones
Existing applications or test environment to validate connectivity
Security groups configured to allow outbound HTTPS (port 443)
Step-by-Step Setup Instructions
Step 1: Create VPC Endpoint for REST APIs (apis.dvsum.ai)
In the AWS Management Console, go to
VPC â†’ Endpoints â†’ Create Endpoint.
Under
Service Category Type
, select: â€œ
Endpoint services that use NLBs and GWLBs
.â€
In
Service Name
, paste the
REST API service name
provided by DvSum and click
Verify service
. It should get successfully verified.
Under
Network settings:
Choose your VPC.
Select at least
two private subnets
across different AZs for high availability.
Security Group
: Create or select the security group with the following Inbound and Outbound rules:
Inbound Rules:
Type
Protocol
Port
Source
HTTPS
TCP
443
Security Group ID of the machine or VPC CIDR
Outbound Rules:
Keep all outbound traffic allowed by default
Type
Protocol
Port Range
Destination
All Traffic
All
All
0.0.0.0/0
Click
Create Endpoint
.
After endpoint is created, its status will be
Pending Acceptance
, at this point, customers need to inform DvSum to accept the connection request.
Once DvSum accepts the connection request, endpoint status will change to
Available.
As the connection is in Available state, select the endpoint and from
Actions,
click on
Modify private DNS name
Under Modify private DNS name settings, select:
Enable private DNS names
and click on Save changes
Step 2: Create VPC Endpoint for WebSocket (socket-api.dvsum.ai)
In the AWS Management Console, go to
VPC â†’ Endpoints â†’ Create Endpoint.
Under
Service Category Type
, select: â€œ
Endpoint services that use NLBs and GWLBs
.â€
In
Service Name
, paste the
Socket service name
provided by DvSum and click
Verify service
. It should get successfully verified.
Under
Network settings:
Choose your VPC.
Select at least
two private subnets
across different AZs for high availability.\
Security Group
: Create or select the security group with the following Inbound and Outbound rules:
Inbound Rules:
Type
Protocol
Port
Source
HTTPS
TCP
443
Security Group ID of the machine or VPC CIDR
Outbound Rules:
Keep all outbound traffic allowed as by default
Type
Protocol
Port Range
Destination
All Traffic
All
All
0.0.0.0/0
Click
Create Endpoint
.
After endpoint is created, its status will be
Pending Acceptance
, at this point, customers need to inform DvSum to accept the connection request.
Once DvSum accepts the connection request, endpoint status will change to
Available.
As the connection is in Available state, select the endpoint and from
Actions,
click on
Modify private DNS name
Under Modify private DNS name settings, select:
Enable private DNS names
and click on Save changes
Step 3: Create VPC Endpoint for S3 (s3-storage.dvsum.ai)
In the AWS Management Console, go to
VPC â†’ Endpoints â†’ Create Endpoint.
Under
Service Category Type
, select: â€œ
Endpoint services that use NLBs and GWLBs
.â€
In
Service Name
, paste the
Socket service name
provided by DvSum and click
Verify service
. It should get successfully verified.
Under
Network settings:
Choose your VPC.
Select at least
two private subnets
across different AZs for high availability.
Security Group
: Create or select the security group with the following Inbound and Outbound rules:
Inbound Rules:
Type
Protocol
Port
Source
HTTPS
TCP
443
Security Group ID of the machine or VPC CIDR
Outbound Rules:
Keep all outbound traffic allowed as by default
Type
Protocol
Port Range
Destination
All Traffic
All
All
0.0.0.0/0
Click
Create Endpoint
.
After endpoint is created, its status will be
Pending Acceptance
, at this point, customers need to inform DvSum to accept the connection request.
Once DvSum accepts the connection request, endpoint status will change to
Available.
As the connection is in Available state, select the endpoint and from
Actions,
click on
Modify private DNS name
Under Modify private DNS name settings, select:
Enable private DNS names
and click on Save changes
Step 4: Notify DvSum for Endpoint Acceptance
Once all endpoints are created, their initial status will appear as
â€œPending acceptance.â€
DvSum must approve these requests before they become active.
Contact your DvSum representative and notify them that the VPC endpoints for the REST API, S3 and WebSocket services have been created in your AWS Account.
After DvSum accepts the requests, the endpoint status will change to
â€œAvailable.â€
Step 5: Security Group Configuration
Configure your security groups to permit traffic between your applications and the new VPC endpoints.
Machine Security Group (Outbound Rules):
Keep all outbound traffic allowed as by default
Type
Protocol
Port Range
Destination
All Traffic
All
All
0.0.0.0/0
VPC Endpoint Security Group:
Inbound Rules:
Type
Protocol
Port
Source
HTTPS
TCP
443
Security Group ID of the machine or VPC CIDR
Outbound Rules:
Keep all outbound traffic allowed as by default
Type
Protocol
Port Range
Destination
All Traffic
All
All
0.0.0.0/0
Step 6: DNS and Connectivity Validation
Perform these tests from an EC2 instance inside your configured VPC.
1. DNS Resolution Test:
nslookup apis.dvsum.ai
nslookup socket-api.dvsum.ai
nslookup s3-storage.dvsum.ai
Expected Result:
Both domains should resolve to the private IP addresses of the interface endpoints within your VPC's CIDR range.
2. REST API and S3 Connectivity Test:
curl -v https://apis.dvsum.ai/health-check
curl -v https://s3-storage.dvsum.ai/health-check
Expected Result:
A JSON response: {"status": "ok"}.
3. WebSocket Connectivity Test:
openssl s_client -connect socket-api.dvsum.ai:443
Expected Result:
A successful TLS handshake, displaying the certificate chain and a "
CONNECTED
" message without errors.
Custom DNS Configuration
Note:
This step is to be followed only if
Private DNS is not enabled
on VPC Endpoints.
The following are the steps to configure DNS settings for the correct routing of traffic through three VPC endpoints via Private Link.
Step 1: Create Private Hosted Zone
In the AWS Management Console
, Go to Route 53 â†’ Hosted Zones
Create Private Hosted Zone
:
Domain name
: dvsum.ai
Type
: Private hosted zone
VPC
: Associate with your application VPC
Step 2: Add DNS Records
Add A-records pointing to your endpoint private IPs:
Record Name
Target
apis.dvsum.ai
Private IPs of REST API VPC Interface Endpoint
socket-api.dvsum.ai
Private IPs of WebSocket VPC Interface Endpoint
s3-storage.dvsum.ai
Private IPs of S3 VPC Interface Endpoint
Advanced Troubleshooting
Network Connectivity Test
# Test from EC2 instance in your VPCâ€¯ 
telnet apis.dvsum.ai 443â€¯â€¯â€¯ 
telnet socket-api.dvsum.ai 443â€¯
telnet s3-storage.dvsum.ai 443
VPC Flow Logs Analysis
Enable VPC Flow Logs to monitor traffic patterns:
Monitor accepted vs. rejected connections
Identify security group misconfigurations
Track bandwidth utilization
DNS Resolution Debugging
# Check DNS resolution chainâ€¯ 
dig apis.dvsum.aiâ€¯ 
dig socket-api.dvsum.aiâ€¯
dig s3-storage.dvsum.ai

--------------------------------------------------------------------------------

================================================================================
TITLE: IP Allow List
URL: https://dvsum.zendesk.com/hc/en-us/articles/42943305742612-IP-Allow-List
================================================================================

In this Article:
Overview
Navigation Path
Configuration Steps
Add IP Address
Edit, Update, or Delete an IP Address
Overview
The
IP Allow List
feature lets administrators control access to the DvSum environment by specifying the network locations that are permitted to connect.
Only the IP addresses or ranges added to this list will be able to sign in or send API requests.
This setting is typically used to:
Restrict access to corporate networks, VPNs, or trusted data centers.
Prevent unauthorized logins from unknown or public networks.
Strengthen overall security and compliance posture.
Navigation Path
To access the IP Allow List feature, follow the below path:
Administration â†’ Account â†’ Application Security â†’ IP Allow List
This section allows administrators to define and manage the list of approved IP addresses or ranges that are permitted to access the DvSum environment.
Configuration Steps
Follow the below steps to configure the IP Allow List.
Add IP Address
Select the checkbox
Allow login only from listed IP addresses
to enable IP-based access control.
Once enabled, only users connecting from the listed IP addresses can access the application.
In the input field below, enter the IP address or range that needs to be allowed (for example,
39.43.171.123
).
Tip:
Click
Get Current IP
to automatically extract your current public IP and insert it in the input field.
Click
Add
to include the entered IP address in the allow list.
Note:
This setting does
not apply to the owner account
, which can log in regardless of the IP allow list.
The added IP appears in the list below and will be considered an approved source for login when enforcement is active.
Edit, Update, or Delete an IP Address
Once IP addresses are added, they appear in a list below the input field.
Each entry in the list includes options to modify or remove the IP configuration.
Edit:
Click the
Edit
icon (âœï¸) next to an existing IP address to modify it.
Update the IP address or range as required, then click the
checkmark
icon to save the changes.
The updated entry is immediately reflected in the list.
Cancel Edit:
To discard an edit before saving, click the
cross
icon next to the field.
Delete:
Click the
Delete
icon (ðŸ—‘ï¸) to remove an IP address from the allow list.
Once deleted, login access from that IP address will be restricted.

--------------------------------------------------------------------------------

================================================================================
TITLE: Audit Trail Logs
URL: https://dvsum.zendesk.com/hc/en-us/articles/42333972028436-Audit-Trail-Logs
================================================================================

In This Article:
Overview
Navigation Path
Audit Trail Log Details
Filtering Records
Exporting Data
Managing Access to Audit Trail Logs
Audit Trail on Asset Detail Page
Overview
The
Audit Trail Logs
feature provides a comprehensive record of all user and system activities performed within DvSum. It captures key events such as data updates, configuration changes, and user actions across various modules, allowing administrators to maintain visibility into platform usage and modifications.
Each log entry records what action was performed, on which entity, by whom, and when. This information supports transparency, accountability, and compliance by enabling detailed tracking of platform activity and changes over time.
Navigation Path
To access the
Audit Trail Logs
, follow these steps:
From the left navigation panel, click
Administration
.
Select
Audit Trail Logs
.
Audit Trail Log Details
The
Audit Trail Logs
page displays a comprehensive record of all user and system activities within the platform. Each entry captures details about the action performed, the entity affected, the user responsible, and the timestamp of the event.
The table contains the following columns:
Entity ID
â€“ Unique identifier of the entity on which the action was performed.
Entity Name
â€“ Name of the specific item (such as a table, glossary term, or data source) affected by the action.
Entity Type
â€“ Type of entity, for example,
Database Table
or
Glossary Term
.
Action Taken
â€“ Description of the operation performed, such as
Created
,
Updated
,
Deleted
, or
Submitted for Approval
.
Action Details
â€“ Additional context or system-generated information related to the action, such as notification or approval details.
Property Changed
â€“ Field or attribute that was modified as part of the action.
When multiple fields are modified during an action, the
Property Changed
column displays a count (for example,
+7
). Hovering over this value reveals a tooltip listing all affected attributes.
For instance, when a user account is created, the tooltip may show fields such as
Email
,
User Group
,
Status
,
Start Date
,
First Name
, and
Last Name
as the attributes that were set or updated.
Modified By
â€“ Name of the user who performed the action.
Modified On
â€“ Date and time when the action occurred.
The log table provides an organized view of all recorded activities, supporting system transparency and data governance tracking.
Filtering Records
To refine the records displayed in the
Audit Trail Logs
table, a set of filters is available. Filters enable focused viewing of specific actions, users, or time periods, making it easier to analyze targeted activity.
To open the filter panel:
Click the
Filter
icon on the top-right of the
Audit Trail Logs
page.
A
Filter By
panel will slide out from the right side of the screen.
Here, you can narrow down the logs using the following criteria:
Entity Type
â€“ Select the specific asset type or module (e.g., Glossary Term, Data Source, etc.) to view activities related to it.
Action
â€“ Filter by the type of operation performed, such as Create, Update, or Delete.
User
â€“ Choose a specific user to view only the actions performed by that user.
Date
â€“ Filter the logs based on when the action occurred. You can select from preset time ranges or define a custom period:
Last 24 hours
Last 7 days
Last month
Last 6 months
Last year
Custom
â€“ Allows you to specify your own date range.
Once your filters are selected, the list updates to display only the relevant records.
Exporting Data
The
Audit Trail Logs
can be exported for offline review or reporting.
Selecting the
Export
icon in the top-right corner generates an
Excel (.xlsx)
file containing all log entries based on the current filters. If no filters are applied, all available records are included.
As noted in the
Audit Trail Log Details
section, when an action involves multiple field updates (displayed in the interface as
+7
, for example), each updated field is recorded as an individual entry in the exported file. This may result in a higher number of records appearing in the export than displayed on-screen.
The exported file retains the same column structure as displayed in the table. A maximum of
100,000 records
can be exported at a time.
Managing Access to Audit Trail Logs
Access to Audit Trail Logs is controlled through user role permissions in the Admin panel. Administrators can assign view or admin-level access to this feature using the Fine-Grained Control settings.
To modify access:
Navigate to
Admin > Edit User Role
In the
Audit Trail Logs
row, select the appropriate permission level:
This allows organizations to tailor visibility and control based on user responsibilities.
Audit Trail on Asset Detail Page
Audit Trail information is also available directly on
Asset Detail Pages
.
Clicking the
three dots menu
opens a
right-side drawer
, similar to the filter panel described earlier
This drawer displays a timeline of actions specific to the selected asset, such as creation, edits, approvals, and notifications
The drawer includes:
Export
option to download asset-specific audit data
Comment field
for adding remarks or context; these comments are also shown on the main Audit Trail Logs page
This feature supports quick access to audit history and annotations without leaving the asset view.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure SAML for Okta
URL: https://dvsum.zendesk.com/hc/en-us/articles/35305937125268-Configure-SAML-for-Okta
================================================================================

In This Article
Introduction
Prerequisites
SAML Configuration Steps
Step 1: Configuring Okta
1.1 Create a SAML App in Okta
1.2 Configure SAML Settings in Okta
Step 2: Retrieve Identity Provider Metadata
Step 3: Setting Up SAML in DvSum
Step 4: Enable & Test SSO in DvSum
Step 5: Assign Users to Okta Application
Introduction
This guide walks you through setting up SAML authentication for Okta in DvSum. By following these steps, you will enable Single Sign-On (SSO), allowing users to log in securely using their corporate credentials.
Prerequisites
Before you begin, ensure the following:
You have admin access to your Okta account.
You have a DvSum Owner account with access to Manage Account > SSO settings.
You have an SP Metadata File from DvSum (will be downloaded during setup).
Please use the following steps to configure SAML integration for your Okta application.
SAML Configuration Steps:
Step 1: Configuring Okta
1.1 Create a SAML App in Okta
Log in
to your Okta admin account.
In the
menu bar
, go to
Applications > Add Application
.
Click
Create App Integration
.
For the Sign-on
method
, select
SAML 2.0
and click
Next
.
Enter an app name
(e.g., "DvSum SSO") and click
Next
.
1.2 Configure SAML Settings in Okta
Log in to DvSum
from your
Owner account
.
Go to
Manage Account > SSO tab
.
Click
Add IdP
.
Download the
SP Metadata File
.
Open the XML file in Notepad.
Extract and enter required values in Okta
:
Copy the
Location
value inside the <AssertionConsumerService> tag from XML.
Paste it into a Single
Sign-On URL
in Okta.
Copy the
entityID
value from XML.
Paste it into
Audience URI (SP Entity ID)
in Okta.
Set
Name ID Format
to
EmailAddress
.
Set
Application Username
to
Email
.
Under
Attribute Statements (Optional)
, add a statement:
Name
: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress
Name Format
: Basic
Value
: user.email
Leave other settings as default and click
Next
.
Choose a feedback response for Okta Support and click
Finish
.
Step 2: Retrieve Identity Provider Metadata
In Okta, go to
Applications > Select your created app
.
Open the
Sign-On
tab.
Find the
Identity Provider Metadata
hyperlink.
Right-click
the link and
Copy the URL
.
This will be used as the
IdP metadata
when adding the IdP in DvSum.
Step 3: Setting Up SAML in DvSum
Log in to
DvSum
with an
Owner account
.
Go to
Manage Account > SSO tab
.
Click
Add IdP
and enter the required details:
Provider Name
: A unique name for the IdP.
Identifier
: Your company's domain (e.g., mycompany.com).
IdP Metadata
: Paste the
metadata URL
copied from Okta.
Click
Save
.
Step 4: Enable & Test SSO in DvSum
Enable SSO
in DvSum.
Try logging in with your
corporate email address
.
Step 5: Assign Users to Okta Application
In Okta, go to the
Assignments
tab.
Click
Assign to People
.
Click
Assign
next to the user you want to add.
If this is a new account, assign yourself (admin).
(Optional) Set a custom
User Name
, or leave it as the
email address
.
Click
Save and Go Back
â†’
Done
.
For more information
Click here

--------------------------------------------------------------------------------

================================================================================
TITLE: Enabling SAML-Based Single Sign-On (SSO)
URL: https://dvsum.zendesk.com/hc/en-us/articles/4403180875156-Enabling-SAML-Based-Single-Sign-On-SSO
================================================================================

In this Article:
Overview
Detailed Steps
Step 1: Log in to Owner Account
Step 2: Add Identity Provider
Step 3: Download DvSum SP Metadata
Step 4: Fill in Required Fields
Step 5: Save and View IdP Listing
Step 6: Enable SSO in Test Mode
Step 7: Enable SSO in Live Mode
Step 8: Adding Multiple Identity Providers
Step 9: Disable SSO
SAML Configuration to Okta
Additional Reference: Microsoft Entra
Overview
SAML-based single sign on (SSO) gives members access to DvSum through an
 identity provider (IdP) of your choice.
SSO, or Single Sign-On, is a service for session and user authentication.
 It enables users to use a single set of login credentials to access multiple
 applications. This simplifies the management of various usernames and passwords
 for both enterprises and individuals.
DvSum enables you to secure your account by providing Web SSO capabilities
 based on popular standards such as SAML-based identity provider, allowing
 your enterprise user directory or third-party IdP to secure your applications
 via standards-based security tokens.
Certified identity providers:
ADFS
Microsoft Entra
Okta
OneLogin
Detailed Steps
Step 1: Log in to your Owner account
Go to the Administration tab â†’ Account â†’ Click on SSO
Step 2: Add Identity Provider
Click the button "Add Identity Provider" to navigate to the following form.
Step 3: Download DvSum SP Metadata
Download the DvSum SP Metadata file and configure your IdP to add DvSum as
 an application.
Step 4: Fill in Required Fields
Fill the form with all the required fields.
Provider Name
â€“ Give a unique provider name in case you
 have multiple IdP's. The Provider Name cannot be updated once configured.
 Provider Name can only be alphanumeric, must be 3 to 32 characters long,
 must start with a letter, and cannot have special characters or spaces.
Identifier
â€“ This is your company's domain name, e.g. mycompany.com.
IdP Metadata
â€“ This is the metadata information from your
 Identity Provider. You can either provide a URL, or upload a local copy of
 the metadata file.
Attributes
â€“ DvSum requires one attribute:
attribute: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress
value:
Email
Step 5: Save and View IdP Listing
Click Save. You will be directed to the IdP Listing page.
Step 6: Enable SSO in Test Mode
When a user enables the SSO Toggle button, they will be prompted with a confirmation.
 After clicking OK, SSO will be enabled in Test Mode.
After clicking OK, SSO will be enabled in Test Mode.
All administrators will receive an email notification informing them of this
 change.
Note:
In Test mode, Single Sign-On applies only to administrators
 who can test login using either corporate email ID or basic authentication
 credentials. Super users and regular users continue to log in with basic
 authentication.
Step 7: Enable SSO in Live Mode
After the Admin successfully verifies the SSO corporate login in Test mode,
 they can enable SSO in Live mode.
When SSO is made live, all active DvSum users will be required to use their
 SSO credentials to sign in. Their existing DvSum passwords will be deleted.
 An email notification will be sent to all active users.
An email will be sent to all active users and they will no longer be able
 to login using basic authentication.
Step 8: Adding Multiple Identity Providers
Add a new identifier and ensure it is different while keeping the Metadata
 URL the same. Save the changes. Both domains should now work with SSO.
Clicking "Sign in" will redirect to the identity provider (Okta).
Once the user is verified by Okta, they will be logged in to the application.
After Okta verifies the user, they will be logged into the application.
Step 9: Disable SSO
If the Owner turns off the SSO configuration from Manage Account, an email
 will be sent to all users.
Note:
When Single Sign-On is disabled, all users will be
 authenticated by the Basic Authentication process and will be required to
 set up a password.
SAML Configuration to Okta
Please use the following
link
to configure SAML integration for your Okta application.
Additional Reference: Microsoft Entra
Reference:
https://learn.microsoft.com/en-us/entra/identity/enterprise-apps/add-application-portal-setup-sso
Note: For more details on reset password refer
How to Reset Password on the DI Platform
article.

--------------------------------------------------------------------------------

================================================================================
TITLE: Power BI Source Permissions
URL: https://dvsum.zendesk.com/hc/en-us/articles/41264497137684-Power-BI-Source-Permissions
================================================================================

In this Article:
Overview
Required Prerequisites
API Testing from Power BI Admin
Authentication Token Acquisition
Test Connection - Retrieve Workspaces using Auth token Generated in Step 1
Scanning APIs using Auth token Generated in Step 1
Workspace Information Request
Scan Status Check
Scan Results Retrieval
Overview:
The Power BI Admin Team needs to complete the required prerequisites and test the specified APIs to resolve the authentication issue. Once these steps are successfully completed, we can proceed with configuring the connection in DvSum.
Note:
For steps to configure Power BI as a source, refer to
Microsoft Power BI as a Source.
Required Prerequisites
Before proceeding to test API, please ensure the following prerequisites are completed:
Metadata Scanning Setup
Create Azure AD Application
Note: The same links are already available in the Source Detail Page.
API Testing from Power BI Admin
1. Authentication Token Acquisition
Endpoint:
POST
https://login.microsoftonline.com/
<tenant-id>/oauth2/v2.0/token
Request Body:
{
"client_id": "<client-id>",
"scope": "
https://analysis.windows.net/powerbi/api/.default
",
"grant_type": "client_credentials",
"client_secret": "<client_secret>"
}
Expected Response:
Status Code: 200 OK (indicates successful connection)
Response Body:
{
"token_type": "Bearer",
"expires_in": 3599,
"ext_expires_in": 3599,
"access_token": "<token>"
}
2. Test Connection - Retrieve Workspaces using Auth token Generated in Step 1
Endpoint:
GET
https://api.powerbi.com/v1.0/myorg/admin/reports?$filter={$filter}
Reference:
https://learn.microsoft.com/en-us/rest/api/power-bi/admin/groups-get-groups-as-admin
Supported Filters: "Personal", "PersonalGroup", "Workspace"
3. Scanning APIs using Auth token Generated in Step 1
The following APIs will be used during the scanning process:
Workspace Information Request:
POST
https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo?lineage={lineage}&datasourceDetails={datasourceDetails}&datasetSchema={datasetSchema}&datasetExpressions={datasetExpressions}&getArtifactUsers={getArtifactUsers}
Documentation:
https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-post-workspace-info
Scan Status Check:
GET
https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scanId}
Documentation:
https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-status
Scan Results Retrieval:
GET
https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scanId}
Documentation:
https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-result

--------------------------------------------------------------------------------

================================================================================
TITLE: Azure SQL & Azure Synapse Analytics User Setup and Query Store Configuration for Metadata Extraction
URL: https://dvsum.zendesk.com/hc/en-us/articles/41243166846740-Azure-SQL-Azure-Synapse-Analytics-User-Setup-and-Query-Store-Configuration-for-Metadata-Extraction
================================================================================

In this Article:
Overview
Prerequisites
Section 1: Query Store Configuration
Section 2: User Creation and Permission Setup
Section 3: Permission Requirements Matrix
Section 4: Complete Setup Script
Section 5: Alternative Contained User Setup
Section 6: Validation and Testing
Section 7: Troubleshooting Guide
Section 8: Security Best Practices
Section 9: Maintenance Recommendations
Conclusion
Overview
This comprehensive guide provides step-by-step instructions for:
Creating an Azure SQL Database user with required permissions.
Enabling and configuring Query Store for lineage data collection.
Validating the setup for metadata extraction queries.
Note:
For steps to configure Azure SQL or Azure Synapse Analytics as a source, refer to
Configure Azure SQL or Azure Synapse Analytics
Prerequisites
Azure SQL Database with administrative access
SQL Server Management Studio SSMS) version 16 or later,
Administrative privileges on the target database
Section 1 Query Store Configuration
-> Understanding Query Store in Azure SQL Database
Query Store is automatically enabled by default in Azure SQL Database, but may need configuration adjustments for optimal metadata collection.
Note:
Query Store cannot be disabled in Azure SQL Database, Single Database, and Elastic Pool.
The Default configuration is optimized for continuous data collection
Minimum permission required: VIEW DATABASE STATE for reading, ALTER DATABASE for configuration
-> Enable Query Store Using SQL Server Management Studio
Steps:
Connect to your Azure SQL Database using SSMS
In Object Explorer, right-click the target database
Select
Properties
Navigate to the
Query Store
page
In the
Operation Mode Requested)
box, select
Read Write
Configure additional settings if needed:
Data Flush Interval Minutes)
: 15 (default: 15
Statistics Collection Interval Minutes
: 60 (default: 60
Max Size MB
: 1000 (default for new databases)
Query Store Capture Mode
: Auto (recommended)
Click
OK
to apply changes
-> Enable Query Store Using T SQL Commands
For Azure SQL Database:
For Azure Synapse Analytics:
-> Verify Query Store Configuration
Check Query Store Status:
Validation Test Query:
Section 2 User Creation and Permission Setup
-> Create Server Login Master Database)
Connect to the
master
database as administrator:
-> Create Database User
Connect to your target database:
-> Grant Essential Permissions
Apply the minimum required permissions for metadata extraction:
Section 3 Permission Requirements Matrix
Query Component
Required Permission
Purpose
INFORMATION_SCHEMA.TABLES
db_datareader
Table metadata extraction
INFORMATION_SCHEMA.COLUMNS
db_datareader
Column metadata extraction
INFORMATION_SCHEMA.CONSTRAINTS
db_datareader
Constraint metadata extraction
sys.extended_properties
VIEW DEFINITION
Extended propertie MS_Description)
sys.query_store_* views
VIEW DATABASE STATE
Query history for lineage
sys.sql_modules
VIEW DEFINITION
View and procedure definitions
sys.objects
db_datareader
Object metadata
sys.schemas
db_datareader
Schema information
Section 4 Complete Setup Script
Section 5 Alternative Contained User Setup
-> For enhanced security, use contained database users:
Section 6 Validation and Testing
-> Test Metadata Access
Execute these validation queries with the new user:
-> Permission Verification Query
Section 7 Troubleshooting Guide
-> Common Issues and Solutions
Issue
Symptoms
Solution
Query Store not collecting data
Empty results from query store views
Verify OPERATION_MODE is READ_WRITE
Extended properties return NULL
Missing descriptions in metadata
Ensure VIEW DEFINITION permission granted
INFORMATION_SCHEMA access denied
Permission errors on schema views
Confirm db_datareader role membership
Login authentication fails
Cannot connect with new user
Verify login exists in master database
-> Query Store Troubleshooting
Check Query Store Health:
Clear Query Store (if needed):
Section 8 Security Best Practices
-> Principle of Least Privilege
The configuration provides:
Read-only access
to metadata and system catalogs
No data modification
capabilities
No schema changes
allowed
Limited to metadata visibility
only
-> Password Security
Use strong passwords with complexity
Requirements: Consider password expiration
policies for production, implement regular password rotation
-> Monitoring and Auditing
Monitor login attempts and failures
Audit permission changes
Review Query Store usage patterns
Section 9 Maintenance Recommendations
-> Regular Monitoring
Check Query Store storage usage monthly
Monitor permission assignments quarterly
Review user access patterns regularly
-> Performance Optimization
Adjust Query Store retention policies based on needs
Monitor Query Store's impact on database performance.
Consider cleanup schedules for large databases
Conclusion
This documentation provides a comprehensive setup for extracting Azure SQL Database metadata, including proper Query Store configuration and user permissions. The setup follows security best practices while ensuring all necessary permissions for comprehensive metadata and lineage collection.
Key Points:
Query Store is essential for lineage data collection
Minimum permissions follow the least-privilege principle
Regular validation ensures continued functionality
The troubleshooting guide addresses common issues
If you need additional support, please refer to Microsoft documentation or contact your database administrator.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure File Upload as a Data Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/35169298078356-Configure-File-Upload-as-a-Data-Source
================================================================================

In this Article:
Overview
DvSum Configuration
Add Source
Add Files
Supported File Formats
File Upload and Scan
Scan Summary
Browsing the Data Dictionary
Sample Files
Overview
This article describes the steps needed to configure File Upload as a source in DvSum Data Intelligence (DI).
Detailed Steps
DvSum configuration
1. Add Source
To create a data source, navigate to Administration â†’ Data Sources â†’ âŠ•Add Source.
Select Upload Files.
Give the folder a name and save it.
2: Add Files
Drag and drop a file into the designated area.
Alternatively, click on the
Upload Files
button to open your system's file explorer and select the desired file.
Supported File Formats:
You can upload the following file types:
Excel
:
.xlsx
,
.xls
,
.xlsm
,
.xlt
,
.xlr
CSV
:
.csv
Note: You can find the sample files attached at the end of this article.
3. File Upload and Scan
Once a file is selected or dragged and dropped, it is uploaded, and a scan is automatically initiated on the file.
File Actions
Click on the
three-dot menu
next to the uploaded file to access additional actions. You can choose to
Delete
or
Download
the file as needed.
File uploads are manual:
Unlike other data sources (databases or APIs) that can fetch new data automatically during rescans, file-based sources do not update automatically.
User action required:
Users must manually upload new files each day to refresh the data; the system will not do this automatically.
After a file is uploaded and scanned, the following tabs are populated accordingly:
Overview Tab
Displays the detected tables and columns extracted from the scanned file.
Scan History Tab
Shows details of the scan, including timestamps and outcomes.
Logs Tab
Records system activities related to the scan for troubleshooting and tracking purposes.
Settings Tab
The
Settings
tab displays folder-related information, including:
Folder Name
and
Description
Access Information
on how to download files
Owner Details
Users can
Edit
folder settings or
Delete
the folder if needed.
Note:
Only profiling tables can be scheduled by creating a job.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
4. Scan Summary
After the scan is completed, click on the
Scan Name
(e.g., SCN-000060) to open the
Scan Summary
page.
The
Scan Summary
page provides insights into the scan, including:
The number of new tables and columns detected.
Other key details related to the scan..
5. Browsing the Data Dictionary
After the scan, you can explore the detected tables and columns. Navigate to
Data Dictionary
from the left sidebar to access the table listing view.
Click on the
Recently Refreshed
tab to view tables discovered in the latest scan.
Select a table name to view its details.
DQ rules:
Data Quality (DQ) rules cannot be created for file upload data sources.
Sample Files
Here are the sample files for reference:

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Amazon S3 as a Data Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/32779279948052-Configure-Amazon-S3-as-a-Data-Source
================================================================================

In this Article
Overview
Connect Your Amazon S3 Data Source
Step-by-Step Configuration
Step 1: Add Amazon S3 as a Source
Step 2: Configure Connection
Step 3: Save Connection Information
Step 4: Scan the Source
S3 Configurations
Configuring Data Retrieval
Scenario 1: Use an Existing Glue Catalog
Scenario 2: Create a New Glue Catalog
Overview
This guide outlines the steps to configure Amazon S3 as a data source within our SaaS application. It details the process of authenticating and adding your Amazon S3 source using one of two distinct methods, followed by running scans to retrieve your data.
Connect Your Amazon S3 Data Source
To integrate your data from Amazon S3, our platform offers two flexible connection methods:
1. Use Existing Resources (Connect to Your Existing S3 and Glue Setup)
Choose this option if you have already independently set up your Amazon S3 environment, including the creation of S3 buckets, AWS Glue databases, and crawlers. In this scenario, our platform will connect directly to your existing Glue Data Catalog to access your data. Because the necessary administrative tasks for your S3 data source have already been completed outside of our system, this method requires only limited permissions for our platform to read the metadata defined in your Glue databases.
Amazon S3 Data Source Connection: Use Existing Resources (Limited Permissions)
2. Create New Resources (Platform Interface Setup)
Select this option to utilize our platform's user interface to create and manage the necessary AWS resources for accessing your S3 data. This allows you to define and build your data infrastructure directly within our application. You will have the ability to create S3 buckets, define AWS Glue databases, and set up crawlers through our intuitive interface. Consequently, this method requires elevated permissions to enable the creation and management of these resources on your AWS account.
Configure S3 Data Source in DvSum (Elevated Permissions Required)
Once you've selected the connection method that aligns with your current setup and desired level of management, proceed with the relevant instructions to configure the necessary IAM user and assign the appropriate permissions.
Step-by-Step Configuration
Step 1: Add Amazon S3 as a Source
Navigate to
Administration â†’ Data Sources â†’ âŠ• Add Source
.
Select
Amazon S3
as the source type.
Provide a name for the source and click
Save
.
Step 2: Configure Connection
After saving, you will be redirected to the connection settings page.
Enable the checkbox for On-premise Web Service and select the Gateway, or use DvSum Web Services (default).
In the Credentials section, enter the AWS Access Key and Secret Key.
Click Authenticate to verify the connection.
Note:
For more information regarding On-premise Web Servic
e installation, click
here
.
1. Note: By default, the Web Service type is set to Cloud. For additional details, refer
DvSum Web Service vs On-Premises Gateway
.
In the
Credentials
section, provide the correct
AWS Access Key
and
AWS Secret Key
, then click
Authenticate
to verify the source.
How to create an AWS IAM User to configure S3 Data Source in DvSum?
S3 Configurations
Once authenticated, configure the S3 connection settings:
Select the
Region
(e.g., US West - Oregon).
Workgroup
and
Catalog
settings align with your AWS Athena configuration. For typical setups, you can leave them blank.
If you've created custom catalogs or workgroups, enter their names as defined in your Athena environment.
Under
Glue Database
, select one or more databases from the
Available
list and move them to the
Selected
list.
These are fetched from the AWS Glue Data Catalog.
Configuring Data Retrieval
Scenario 1: Use an Existing Glue Catalog
Select the Glue
Catalog
from the dropdown menu, and input the
Staging Bucket
and
Folder
.
Scenario 2: Create a New Glue Catalog
Enable 'Create New' button
Create Source Bucket:
Click
+ Create
next to
S3 Source Bucket
.
DvSum recommends using the prefix
"dvsum-s3-source-"
to ensure a unique bucket name.
Enter a valid bucket name (
AWS Bucket Naming Rules
) and click
Save
. The bucket will be created and automatically added to the dropdown list.
How to Create an AWS S3 Bucket?
Create Staging Bucket:
Click
+ Create
next to
S3 Staging Bucket
.
Use the recommended prefix
"dvsum-s3-source-"
to create a unique bucket name.
Ensure that the
Source Bucket
and
Staging Bucket
have distinct names.
Enter a valid bucket name and click
Save
. The bucket will be created and added to the dropdown list.
Set Folders (Optional):
Specify folders for the
S3 Source Bucket
and
S3 Staging Bucket
if required.
Files within the specified folder in the
Source Bucket
will be cataloged.
Temporary staging files will be created in the specified folder in the
Staging Bucket
.
Create Glue Database:
Click
+ Create
next to
Glue Database
.
Define a database name that complies with
AWS Glue Database naming requirements
(
The name must be in lowercase letters and cannot be longer than 255 characters
) and provide a description.
Click
Save
.
How to Create an AWS Glue Database?
Create Glue Crawler:
Click
+ Create
next to
Glue Crawler
.
Define a crawler name that follows AWS Glue Crawler naming requirements (
The name you choose can be up to 255 characters long, but certain special characters and symbols are not allowed.
) and provide a description.
Click
Save
.
How to Create an AWS Glue Crawler?
Note:
If the action fails, the user should verify and check the following:
Certain special characters are not supported in column names. Examples of unsupported characters include:
Comma separator in column names (e.g., A,B)
Control characters (e.g., U+0000)
Ensure that there are no carriage returns within the column data, and it should be presented in a single line.
Step 3: Save Connection Information
After the credentials are authenticated, scroll to the top of the page.
Click the
Done
button in the top-right corner.
Click
Save
to save the source connection.
Finally, click
Test Connection
to verify the setup.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Step 4: Scan the Source
Navigate to the
Scan History Page
and click the
"Scan Now"
button.
A job will be created, and once its status shows
Completed
, the scan for the new Amazon S3 source will be finished successfully.
After the scan is complete, click on the
Scan Name
to open the
Scan Summary
page for this scan.
On the
Scan Summary
page, you will see insights from the scan, including the number of new tables and columns retrieved from the database that was selected earlier.
To gain more insights into the details of the tables, click on
"Data Dictionary"
from the sidebar. A table listing view will appear. Then, click on the
"Recently Refreshed"
tab. This tab will display all the tables fetched in the recent scan. Click on the table names to view more details on the respective table's detail page.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Netezza as a Data Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/31133806684948-Configure-Netezza-as-a-Data-Source
================================================================================

In this Article:
Overview
Pre-Requisites
Detailed Steps
Step 1: Add Netezza as a Source
Step 2: Configure Connection
Step 3: Save Connection Information
Step 4: Scan the Data Source
Overview:
This article outlines the steps required to configure Netezza as a source in DvSum Data Intelligence (DI). It details the process of adding and authenticating the Netezza source, as well as running scans to retrieve data.
Pre-Requisites
Netezza Performance Server Deployment:
IBM Cloud or On-Premises
Server Status:
Netezza server must be operational
SAWS Version:
1.2.9 or above
Required Permissions for System Tables:
_V_TABLES
_V_RELATION_COLUMN
_V_DATATYPE
_V_RELATION_KEYDATA
_V_DATSBASE
_V_SCHEMA
Enabling General Query Log for MySQL
Before configuring MySQL as a source, ensure that the general query log is enabled for your MySQL server. This is essential for tracking data lineage and obtaining insights into usage patterns. For more information, refer to the
Enabling General Query Log for Data Sources
article.
Detailed Steps
Step-by-Step Configuration
Step 1: Add Netezza as a Source
Navigate to Administration > Data Sources > âŠ• Add Source.
Select Netezza as the source type.
Provide a name for the source and click Save.
Step 2: Configure Connection
After saving, you will be redirected to the connection settings page
Enable the checkbox for On-premise Web Service and select the Gateway, or use DvSum Web Services (default).
Note:
For more information regarding Cloud Gateway, click
here.
3. Enter the required Host, Port, DB Login, and Password.
4. Click Authenticate to verify the connection.
Note:
For more information regarding On-premise Web Service installation, click
here
.
5. After authentication, the Database section will appear, allowing you to select the database forÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  scanning.
6. Choose whether to scan all schemas or limit the scan to specific schemas.
If limiting to specific schemas, select one or multiple schemas from the Available Schemas list and move them to Selected Schemas.
When the option to limit to specific schemas is selected, a list of available schemas will be displayed. The user can choose one or multiple schemas from the Available Schemas list and move them to the Selected Schemas tab on the right.
Step 3: Save Connection Information
After authentication and schema selection, scroll to the top.
Click Done > Save.
Click Test Connection to verify the setup.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Step 4: Scan the Data Source
Navigate to the Scan History Page.
Click Scan Now to initiate a job.
After the scan is complete, click on the Scan Name to open the
Scan Summary
page. This page will display all the insights from the scan, including the number of new tables and columns retrieved from the schemas selected earlier.
Next, analyze the tables and columns discovered during the scan by navigating to "Data Dictionary" in the left sidebar. The table listing view will appear. Click on the "Recently Refreshed" tab to view the tables identified in the recent scan. Click on the table names for more details.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure IBM Db2 as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/29763482203028-Configure-IBM-Db2-as-a-Source
================================================================================

In this Article:
Overview
Prerequisites
Step-by-Step Configuration
Step 1: Add IBM Db2 as a Source
Step 2: Configure the Connection
Step 3: Save the Connection
Step 4: Scan the Data Source
Browsing the Data Catalog
Troubleshooting & FAQs
Additional Resources
Overview
This article describes the steps needed to configure IBM Db2 as a source in DvSum Data Insights (DI). The same steps apply to configure a source in DvSum Data Qualtiy (DQ) with only a slight variation.
Prerequisites
IBM DB2 configuration
User
: Create a user to be used in DvSum, or identify an existing user.
Table permissions
: Grant read-only access to the user for schemas and tables that you would like to catalog and profile.
System table permissions:
Grant read-only access to the user for the following system tables:
SYSIBM.SYSDUMMY1
SYSCAT.SCHEMATA
SYSCAT.TABLES
SYSCAT.COLUMNS
SYSCAT.KEYCOLUSE
SYSCAT.REFERENCES
SYSCAT.TABCONST
Step-by-Step Configuration
Step 1: Add IBM Db2 as a Source
Navigate to: Administration â†’ Data Sources â†’ Add Source (+).
Select: IBM Db2.
Enter a Name for the source.
Click Save.
Step 2: Configure the Connection
Once the source is saved, you will be redirected to the connection settings detail page.
Most common
: Connect Using DvSum Web Service
Whitelist the DvSum application by IP address as indicated. Then enter the required connection information:
Hostname
Port
Database
DB Login & Password
Alternative
: You may
install a DvSum Edge Gateway
behind your firewall. Then connect to your IBM Db2 instance using this web service installed on premises. Read more details in
DvSum Web Service vs On-Premises Edge Gateway
.
After entering the credentials, Authenticate the source.
Once the source is Authenticated, the Database section will appear.
Select the schemas which you want to catalog.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
3. Save the Connection
Scroll up and click the Done button.
Click Save to finalize the configuration.
4. Scan the data source
Go to the Settings page or Scan History.
Click Scan Now.
When the scan completes, the status will change to "Completed".
After the scan completion, click on Scan Name (SCN-001071 in the above example) and it will open the Scan Summary page of this scan.
The Scan Summary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this scan.
Your IBM Db2 connection is now fully configured and functional.
Browsing the Data Catalog
To analyze discovered tables and columns:
Navigate to the Data Dictionary from the left sidebar.
Click on the Recently Refreshed tab.
Select a table name to see its details
Troubleshooting & FAQs
Common Issues & Solutions
Issue
Possible Solution
Connection test fails
Ensure the hostname, port, and firewall rules allow DvSum access.
Invalid credentials error
Verify the username and password. Ensure the user has read permissions.
FAQs
Q:
Can I connect multiple IBM Db2 instances to DvSum?
A:
Yes, repeat the steps to add additional sources.
Q:
How do I verify if the connection is successful?
A:
Run a test query in IBM Db2 before connecting: SELECT * FROM SYSIBM.SYSDUMMY1;
Q:
What should I do if my scan doesnâ€™t return any tables?
A:
Ensure the user has access to the required schemas and tables.
Additional Resources
Read more about
DvSum Web Service vs On-Premises Gateway
Read more about
Gateway Installation

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Governance Roles and Responsibilities
URL: https://dvsum.zendesk.com/hc/en-us/articles/41098426049172-Data-Governance-Roles-and-Responsibilities
================================================================================

In this Article
Overview
Governance Roles
Data Owner
Data Steward
Approver
System Roles & Permissions
Admin
Editor
User / Viewer
Overview
This article outlines clear roles and responsibilities to ensure proper governance, accountability, and data quality management. These roles define who is responsible for data, how it is managed, and the level of system access each user has.
Governance Roles
Data Owner
Responsible for data definition, usage, and compliance.
Ensures data follows company standards and policies.
Monitors and manages data quality issues.
Sets data rules and access policies.
Ultimately accountable for the correct and proper use of data.
Data Steward
Maintains day-to-day data quality within assigned domains.
Ensures glossary terms and definitions are accurate.
Edits and updates terms in the glossary.
Monitors and fixes data quality issues.
Works with system owners to make technical updates or corrections.
Approver
Reviews changes made by Data Stewards.
Compares changes to validate modifications.
Approves or rejects terms submitted by Stewards.
Approves multiple terms at once using Mass Update.
Monitors term activity history via audit trail.
Note:
For more details, refer to
Approving Business Glossary Terms
.
System Roles & Permissions
Admin
Full access to all modules and settings, including access to the
Administration tab
.
Can create, edit, and delete user roles.
Configures fine-grained access, governance workflows, and module settings.
Manages data sources, tables, and glossary terms.
Editor
Can view and edit specific datasets, tables, or glossary terms assigned via Fine-Grained Control.
Can submit changes for approval if workflow is enabled.
Cannot change user roles or global module settings.
Access to the
Administration tab
is restricted.
User / Viewer
Can only view assigned datasets, tables, or glossary terms.
Cannot edit or approve changes.
Access is restricted by Fine-Grained Control and module settings.
Note:
For more details, refer to
User Role and Module Settings
.

--------------------------------------------------------------------------------

================================================================================
TITLE: Default Roles Access Matrix
URL: https://dvsum.zendesk.com/hc/en-us/articles/32742836074516-Default-Roles-Access-Matrix
================================================================================

Understanding Default User Roles in DvSum Data Insights
Managing user roles and permissions is essential for maintaining security, efficiency, and collaboration within
DvSum Data Insights
. Our
User Roles Access Matrix
provides a comprehensive overview of the permissions assigned to each default roleâ€”Owner, Admin, Editor, and Userâ€”across various modules, including Administration, Data Dictionary, Field Dictionary, and more.
This matrix is a valuable resource to help you understand how each role interacts with different features and actions, ensuring users have the appropriate access to perform their tasks effectively. You can explore the matrix in detail by downloading the Excel file provided.

--------------------------------------------------------------------------------

================================================================================
TITLE: User Role and Module Settings
URL: https://dvsum.zendesk.com/hc/en-us/articles/17327470244244-User-Role-and-Module-Settings
================================================================================

In this article:
Introduction
How to Add a New User Role
Fine-Grained Control (Admin)
How Section-Level Permissions Work
Fine-Grained Control (System Catalog)
Fine-Grained Control (Business Glossary)
Configuring Module Settings in DvSum
Best Practices for Assigning Roles and Configuring Module Settings
Link Between User Roles and Module Settings
Introduction
With
DvSum Data Intelligence
, users can access specific datasets and glossary terms. In environments with numerous tables and terms, granting individual access is crucial. Simply instructing users to avoid certain tables isnâ€™t a practical approach.
When assigning specific access levelsâ€”such as "View" for some tables or terms and "Edit" for othersâ€”it can become complex. However, DvSumâ€™s
User Roles
and
Fine Grain
settings enable administrators to precisely control access. They can define whether a user has viewing or editing privileges for each table or term.
Another key feature, alongside
User Roles
, is
Module Settings
. Users can configure governance and workflow settings for different data domains or sub-domains. Additionally, at the table level, they can enable or disable features like
Chat with Data
and
Data Quality
. This article will provide further details on these capabilities.
How to Add a New User Role
Go to Administration Tab and then Users Management > User Roles
Click on User Roles.
Click Add Role and enter a name for the new role.
Assign a user group to the role (this group will define which users inherit the roleâ€™s permissions).
Configure access settings under Admin, System Catalog, and Business Glossary Tabs.
Click Save Changes.
From the Admin tab, users can select Admin permissions, which include options for No Admin, Admin, or custom permissions through fine-grained control.
Fine-Grained Control (Admin)
DvSum allows administrators to assign
No Access
,
View
, or
Edit
permissions to various sections of the Administration Panel. These permissions help tailor access for different user groups based on their responsibilities.
How Section-Level Permissions Work
Some sections in the Administration Panel represent a group of related features. When you assign a permission level to such a section,
that level is automatically applied to all features within it
.
Example: Jobs Section
The
Jobs
section includes:
Job Definition
Job Execution
If you assign
View
access to the
Jobs
section, users will be able to view both
Definition
and
Execution
. Similarly, selecting
Edit
will allow users to modify both.
This ensures consistent access control across related features without needing to configure each one individually.
Permissions can be configured directly from the
System Catalog
and
Business Glossary
tabs.
Fine-Grained Control (System Catalog)
Fine Grain allows further control over Data Sources and Tables inside them for Catalog and the same goes for Terms in the Glossary. Imagine you're an Admin, and a colleague wants to edit a Snowflake catalog. With Fine Grain control, you can grant them edit permissions for that specific dataset.
In the above example, edit permissions will be granted to this user role only for the enabled data source. The admin can decide the level of access he wants to give, he can either give "View & Chat" or "Edit" Permissions. Now admin can further filter the data source by using the Table Exclusion criteria below.
Note: There are two options for Table Exclusion criteria which are shown above.
Let us say that in the data source to which you have given access, you do not want to give this user role access to the tables which have sensitive tags then you will need to mention the criteria like this:
This change would have reflected like this:
On the Admin account:
The both highlighted tables above won't be showing in my user account which will have the user role settings that are mentioned above because these tables have Sensitive tags. On the User account, it will show like this:
Here not only did we restrict the access of this user to specific Tables but we further restricted the edit access of this user. Now this user can make edit changes on any of the tables.
One Important thing is that Fine Grain Control will only be available if the permission above selected is either Editor or Viewer, otherwise, Fine Grain Control won't show on the UI.
Fine-Grained Control (Business Glossary)
Imagine you're a Glossary admin with access to all domains and sub-domains. A glossary editor requests access to specific domains and terms within them, but you only want to display terms in this domain with the status "Published." Fine Grain Control allows you to achieve this level of precision.
This can be achieved as follows:
In this instance, Edit Access is granted to the "Securities" domain and its sub-domains, along with the terms inside it. An additional filter is applied, ensuring that only terms with the status "Published" can be edited.
For Example, on the Admin Account, we have three terms that are not published:
Now the User who will have the user role settings above won't be able to see the highlighted terms because they are fulfilling the exclusion criteria. Here is the view of User account:
Note:
For complete details on user roles and responsibilities, refer to the
Data Governance Roles and Responsibilities
article
Configuring Module Settings in DvSum
Module Settings control governance workflows, Chat with Data, and Data Quality features at a global level. These settings affect all users in the organization.
How to Access Module Settings
Go to Administration > Account Settings > Module Settings.
Choose from the three available module settings:
Governance
Chat with Data
Data Quality
Enable or disable these settings as needed.
1. Governance
In the
Governance
tab, activating the
Data Governance
checkbox enables the workflow for domains and sub-domains. If this checkbox is disabled, the workflow option for a domain or sub-domain becomes inaccessible.
Similar to
User Roles
,
Fine Grain Control
in
Module Settings
allows for applying specific filters. When
Fine Grain
is enabled, the
Default
settings appear with two options. If set to
Enabled
, any newly added domain will automatically have its workflow activated, removing the need for manual configuration.
Consider this example: Governance is enabled for the domain
"Default"
and all terms within it. However, terms tagged as
"Restricted"
will remain excluded from governance.
Now once the governance is enabled user can enable workflow from the Data Domain or Sub domain:
After the workflow is enabled every term in this domain will have governance enabled except for the term which has the tag "Restricted" as mentioned in the exclusion criteria:
And the existing Term which has the exclusion criteria won't have governance enabled:
2. Chat with Data
When enabled, Chat with Data allows users to interact with datasets using AI-powered agents.
How to Configure Chat with Data
Navigate to Module Settings > Chat with Data.
Enable the Chat with Data Checkbox.
Select specific data sources for Chat with Data.
Use Fine-Grained Control to exclude specific tables or columns (e.g., exclude columns tagged as "Sensitive").
Click Save Changes.
Below the Default settings, the user can enable Chat with Data for specific tables. For example, if the user wants to enable agent for one specific source then that source can be moved to Enabled:
This can be further filtered by adding Table exclusion criteria or Column exclusion criteria. For Example if the user does not want to include some tables in agents that have the "Restricted Tag" and columns that have the "Sensitive" tag then these tables and columns will not be available on the agents of that particular source.
So the expected behavior would be that all the tables inside the Datasource "Snowflake_4th_August" will be included in agents except for the Tables that will have the tag "Restricted". Now here is an example of a table "Dwh.Nyc City Bikes" that has the exclusion criteria:
And for columns that are excluded from the Agents will be excluded from the chat and shown like this:
3. Data Quality
When enabled, the
Data Quality
module facilitates monitoring of data integrity and quality scores.
How to Enable Data Quality Monitoring
Navigate to Module Settings > Data Quality.
Enable the Data Quality Checkbox.
Select specific data sources for Data Quality monitoring.
Use Fine-Grained Control to exclude specific tables or columns (e.g., exclude columns tagged as "Sensitive").
Click Save Changes.
Below are the Default settings, the user can enable Data Quality for specific tables. For example, if the user wants to enable the Data quality tab for one specific source then that source can be moved to Enabled:
This can be further filtered by adding Table exclusion criteria or Column exclusion criteria. Like if the user wants to disable the DQ tab for tables that have the "Restricted Tag" and columns which have the "Sensitive" tag should be disabled in the chat.
So the expected behavior would be that all the tables inside the Datasource "Snowflake_DWH" will have the Data Quality tab enabled except for the Tables which will have the tag "Restricted":
Columns that are not included in the Data Quality tab will simply not appear under the "Details" heading on the Data Quality tab:
Best Practices for Assigning Roles and Configuring Module Settings
Assign Admin roles only to necessary personnel to prevent unauthorized system modifications.
Use Viewer roles for non-technical users who only need read-only access.
Enable Fine-Grained Control to give selective permissions rather than broad access.
Regularly review role assignments to ensure users have the correct level of access.
Use module settings strategically to enable governance, Chat with Data, and Data Quality only where necessary.
Link Between User Role and Module Settings
User role settings are specific to individual users, while module settings apply globally, affecting all associated users and the admin account. For instance, if the admin grants View and Chat access for certain sources in user roles, enabling Chat with Data in module settings is necessary to activate Chat topics. This establishes a kind of parent-child relationship between Module settings and User roles, but they can also function independently based on specific requirements.

--------------------------------------------------------------------------------

================================================================================
TITLE: Manage Users
URL: https://dvsum.zendesk.com/hc/en-us/articles/203082210-Manage-Users
================================================================================

In this Article
Overview
Adding New Users
Steps to Add a New User
Editing and Deactivating Users
Overview
DvSum provides administrators and owners with a robust set of features for adding and managing users. The
Users
page displays a list of existing users along with key details, including
Email, User Group, Status, Modified By, Modified On, Expiration Date,
and
Last Login.
This guide is designed for administrators responsible for managing user access and permissions in DvSum.
Adding New Users
When adding a new user, administrators can define the
Start Date
and
Expiration Date
for their account. These settings cannot be modified by the user:
The
Start Date
determines when the user gains access to the application.
The
Expiration Date
restricts access after the specified date unless an administrator extends it.
By default, the
Start Date
is set to the current date, and the
Expiration Date
is set one year later. Both fields can be edited by an administrator.
Steps to Add a New User
Navigate to
Administration > User Management > Users
.
Click
Add User
.
Enter the user's
Email
and assign a
User Group
.
Set the
Start Date
and
Expiration Date
.
Click
Save
to complete the process.
Note:
By default, the "Start Date" is set to the current date, and the "Expiration Date" is set to one year later. Both of these fields are editable.
Editing and Deactivating Users
User access is restricted to the period between their designated
Start Date
and
Expiration Date
. Administrators can manage user access through the
Actions
menu, which offers the following options:
Deactivate/Activate Users
â€“ Temporarily disable or restore a userâ€™s access.
Delete Users
â€“ Permanently removes a user, updating their status to
Deleted
.
Reactivating a Deleted User
â€“ If a deleted user is reactivated, they are treated as a newly added user and must be reconfigured accordingly.

--------------------------------------------------------------------------------

================================================================================
TITLE: SSL Certificate Types Supported for Gateway Installation
URL: https://dvsum.zendesk.com/hc/en-us/articles/45315196278804-SSL-Certificate-Types-Supported-for-Gateway-Installation
================================================================================

Overview
During Gateway installation, an SSL certificate is used to secure communication between the Gateway and other systems. DvSum supports two primary SSL certificate types: self-signed certificates and CA-signed certificates.
This article explains the difference between these certificate types, when each is typically used, and how they relate to the SSL configuration options available during Gateway installation.
Certificate Types at a Glance
Gateway SSL configuration is based on how the certificate is trusted.
The two supported trust models are
self-signed certificates
and
CA-signed certificates
.
The SSL options available during installation define how the certificate and keystore are created or reused, rather than introducing different certificate types.
Self-Signed Certificates
A self-signed certificate is generated by the Gateway itself during installation and is not issued by a trusted Certificate Authority.
When this option is used
Self-signed certificates are commonly used when deploying non-production or test environments, running the Gateway
within an internal or controlled network,
or when external trust by browsers or third-party systems is not required.
Characteristics
Generated automatically during installation.
Not trusted by browsers or external systems by default.
May require manual trust configuration if accessed externally.
Related Gateway installation option
This certificate type is generated when selecting
Generate a new certificate
option during the
gateway installation
.
CA-Signed Certificates
A CA-signed certificate is issued by a trusted Certificate Authority, either internal or public. This option is typically used for production or externally accessible environments.
When this option is used
CA-signed certificates are commonly used when deploying the Gateway in production, when external systems or browsers must trust the Gateway, or when security or compliance requirements apply.
Characteristics
Trusted by browsers and external systems.
Issued by a Certificate Authority.
Requires an existing certificate and corresponding private key.
Related Gateway installation options
CA-signed certificates are used when selecting
Create a new keystore with your certificate
,
Use an existing keystore
, or
Add certificates to an existing keystore
option during
gateway installation
. These options allow a trusted certificate to be supplied or reused during installation.
How This Relates to Gateway Installation
The Gateway installer presents multiple SSL configuration options. These options control how certificates are generated, stored, or reused during setup.
From a certificate perspective, the configuration always falls into one of two categories: using a self-signed certificate generated by the Gateway or using a CA-signed certificate provided during installation.
Recommended Approach
In general, self-signed certificates are suitable for development or internal testing, while CA-signed certificates are recommended for production or externally accessible deployments.
Selecting the appropriate certificate type helps ensure secure communication and aligns with deployment and security requirements.

--------------------------------------------------------------------------------

================================================================================
TITLE: Gateway Upgrade Process
URL: https://dvsum.zendesk.com/hc/en-us/articles/30864921622676-Gateway-Upgrade-Process
================================================================================

In this Article
Detailed Steps for Gateway Upgrade
Preparation
Configuration
Installation
Notes on Certificate Handling
Detailed Steps for Gateway Upgrade
1. Preparation
Step 1: Download the New Gateway Version
Download the latest gateway version.
DvSum Gateway Installer Links
.
Step 2: Uninstall the Current Gateway Service
Before installing the new gateway, uninstall the existing version to prevent conflicts. Use the following command to uninstall the service:
# Windows
.\dvsum_gateway_setup.bat uninstall
# Linux
./dvsum_gateway_setup.sh uninstall
Step 3: Verify Uninstallation
Ensure the service is no longer running:
# Windows
.\dvsum_gateway_setup.bat status
# Linux
./dvsum_gateway_setup.bat status
Step 4: Backup the Previous Version
Move the current gateway folder to create a backup:
# Windows
Rename the existing dvsum_gateway directory to <dvsum_gateway_1.2.3>
# Linux
mv
dvsum_gateway/ dvsum_gateway_1.2.3/
2. Configuration
Windows
Step 1: Set up a New Folder for the Latest Gateway
Create a new folder for the latest gateway files.
Step 2: Extract the Gateway Files
Extract the downloaded .zip file to the new folder. Avoid using the Downloads folder.
Step 3: Migrate Previous Settings
For Gateway version
1.3.8 and earlier
You can
copy the configuration and SSL files directly
from the old gateway folder to the new one:
configuration.properties
ssl_config.properties
If you are using a CA-signed certificate, also transfer the relevant certificate or keystore files:
cp
dvsum_gateway_1.2.3/my_domain.jks dvsum_gateway_1.2.4/my_domain.jks
For Gateway version
2.0.0 and above
Do
not
copy the configuration files from an older version (e.g.,
1.x.x
).
Instead, manually create new
configuration.properties
and
ssl_config.properties
files and populate them using the updated format.
Note:
Configuration files can only be copied when upgrading
within the same minor version range
(e.g., from 2.0.x to another 2.0.x version).
Step 4: Migrate existing scan and chat information
Copy the data_analysis folder from the old installation to the new one.
Copy the dq_analysis folder from the old installation to the new one.
Copy the scan_output folder from the old installation to the new one.
Not all installations have all of these folders. Copy the folders that you have, but it's fine if you do not have all of them.
Linux
Step 1: Set up a New Folder for the Latest Gateway
Create a new folder for the latest gateway installation:
mkdir
dvsum_gateway
Step 2: Extract the Gateway Files
Extract the .tar.gz file into the newly created folder:
tar -xzf dvsum_gateway_latest.tar.gz -C dvsum_gateway/
Step 3: Migrate Previous Settings
For Gateway version
1.3.8 and earlier
You can
copy the configuration and SSL files directly
from the old gateway folder to the new one:
configuration.properties
ssl_config.properties
If you are using a CA-signed certificate, also transfer the relevant certificate or keystore files:
cp
dvsum_gateway_1.2.3/my_domain.jks dvsum_gateway_1.2.4/my_domain.jks
For Gateway version
2.0.0 and above
Do
not
copy the configuration files from an older version (e.g.,
1.x.x
).
Instead, manually create new
configuration.properties
and
ssl_config.properties
files and populate them using the updated format.
Note:
Configuration files can only be copied when upgrading
within the same minor version range
(e.g., from 2.0.x to another 2.0.x version).
Step 4: Migrate existing scan and chat information
Copy the data_analysis folder from the old installation to the new one.
Copy the dq_analysis folder from the old installation to the new one.
Copy the scan_output folder from the old installation to the new one.
Not all installations have all of these folders. Copy the folders that you have, but it's fine if you do not have all of them.
3. Installation
Windows
After setting up the new gateway and transferring the necessary files, install the gateway:
.\dvsum_gateway_setup.bat install --ssl-option=4 --non-interactive
Next, start the gateway:
Open PowerShell in Administrator mode.
Navigate to the gateway folder:
cd
c:\dvsum_gateway
Run the install command:
.\dvsum_gateway_setup.bat install
Linux
To complete the installation and start the gateway on Linux:
./dvsum_gateway_setup.sh install --ssl-option=4 --non-interactive
Notes on Certificate Handling
For
self-signed certificates
, if you donâ€™t need to retain previous settings, you can follow the general installation steps in the Gateway Installation article.
For
CA-signed certificates
, itâ€™s recommended to reuse the same keystore to avoid reconfiguring the private key and related settings. Simply copy the keystore from the previous installation to the new gateway folder to maintain secure communication.
Final Remarks
Ensure that
configuration.properties
and
ssl_config.properties
are correctly migrated/configure.
Keeping a backup of the previous version is recommended to avoid potential issues with SSL configurations or other settings.
For CA-signed certificates, reusing the existing keystore simplifies the process, avoiding the need to track down the private key.

--------------------------------------------------------------------------------

================================================================================
TITLE: DvSum Gateway Installer Links
URL: https://dvsum.zendesk.com/hc/en-us/articles/29756078237460-DvSum-Gateway-Installer-Links
================================================================================

In this Article
DI Platform Download Links
DQM Platform Download Links
DI Platform Download Links
Latest Version (v. 2.2.0):
DvSum Data Intelligence Gateway Windows Installer
DvSum Data Intelligence Gateway Linux Installer
Older Versions:
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.8)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.8)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.7)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.7)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.6)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.6)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.5)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.5)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.4)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.4)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.3)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.3)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.2)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.2)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.1)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.1)
DvSum Data Intelligence Gateway Linux Installer(v. 2.1.0)
DvSum Data Intelligence Gateway Windows Installer(v. 2.1.0)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.7)
DvSum Data Intelligence Gateway Windows Installer(v. 2.0.7)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.6)
DvSum Data Intelligence Gateway Windows Installer(v. 2.0.6)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.5)
DvSum Data Intelligence Gateway Windows Installer(v. 2.0.5)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.4)
DvSum Data Intelligence Gateway Windows Installer(v. 2.0.4)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.3)
DvSum Data Intelligence Gateway Windows Installer(v. 2.0.3)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.2)
DvSum Data Intelligence Gateway Windows Installer(v. 2.0.2)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.1)
DvSum Data Intelligence Gateway Windows Installer (v. 2.0.1)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.0)
DvSum Data Intelligence Gateway Windows Installer (v. 2.0.0)
DvSum Data Intelligence Gateway Linux Installer(v. 2.0.0)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.8)
DvSum Data Intelligence Gateway Linux Installer(v. 1.3.8)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.7)
DvSum Data Intelligence Gateway Linux Installer(v. 1.3.7)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.6)
DvSum Data Intelligence Gateway Linux Installer(v. 1.3.6)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.5)
DvSum Data Intelligence Gateway Linux Installer(v. 1.3.5)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.4)
DvSum Data Intelligence Gateway Linux Installer (v. 1.3.4)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.3)
DvSum Data Intelligence Gateway Linux Installer (v. 1.3.3)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.2)
DvSum Data Intelligence Gateway Linux Installer (v. 1.3.2)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.1)
DvSum Data Intelligence Gateway Linux Installer (v. 1.3.1)
DvSum Data Intelligence Gateway Windows Installer (v. 1.3.0)
DvSum Data Intelligence Gateway Linux Installer (v. 1.3.0)
DvSum Data Intelligence Gateway Windows Installer (v. 1.2.9)
DvSum Data Intelligence Gateway Linux Installer (v. 1.2.9)
DvSum Data Intelligence Gateway Windows Installer (v. 1.2.8)
DvSum Data Intelligence Gateway Linux Installer (v. 1.2.8)
DvSum Data Intelligence Gateway Windows Installer (v. 1.2.7)
DvSum Data Intelligence Gateway Linux Installer (v. 1.2.7)
DvSum Data Intelligence Gateway Windows Installer (v. 1.2.6)
DvSum Data Intelligence Gateway Linux Installer (v. 1.2.6)
DvSum Data Intelligence Gateway Windows Installer (v. 1.2.5)
DvSum Data Intelligence Gateway Linux Installer (v. 1.2.5)
DvSum Data Intelligence Gateway Windows Installer (v. 1.2.4)
DvSum Data Intelligence Gateway Linux Installer (v. 1.2.4)
DQM Platform Download Links
Latest Version (v. 3.2.3):
DvSum Data Quality Gateway Installer
Older Versions:
DvSum Data Quality Gateway Installer (v. 3.2.2)
DvSum Data Quality Gateway Installer (v. 3.2.1)

--------------------------------------------------------------------------------

================================================================================
TITLE: Gateway Installation
URL: https://dvsum.zendesk.com/hc/en-us/articles/29188280659988-Gateway-Installation
================================================================================

In this Article
Overview
Download Links
Prerequisites (all platforms)
Network Configuration
Define a Gateway
Windows Installation
Non-interactive Installation
Interactive Installation
SSL Configuration Option
Uninstall the Gateway
Linux Installation
Download the installer
Python
Non-interactive Installation
Interactive Installation
Appendix
Using Port 443
Overview
This document outlines the steps for installing the DvSum Gateway for use with Data Intelligence Platform. It provides detailed instructions for both Windows and Linux systems, including configuration and installation procedures.
Note: Gateway, Connector, SAWS (Stand-Alone Web Service) and webservice are used interchangeably referring to the DvSum Edge Gateway.
Download Links
DvSum Data Catalog Gateway Windows Installer
DvSum Data Catalog Gateway Linux Installer
Prerequisites (all platforms)
Minimum memory: 8GB (16GB recommended)
Minimum disk space: 10GB (20GB recommended)
CPU cores: 2 (4 recommended)
Python 3.x must be installed for the gateway to function properly.
Linux: Ensure that your Linux distribution has systemd support and x86_64 server architecture.
Windows: Windows Server Edition 2012 or later
# Confirm Architecture
lscpu
# Expected output includes:
# Architecture: x86_64
Network Configuration
If the DvSum Gateway is installed on a server with restricted outside access to specific URLs, you'll need to whitelist the following addresses on the network. This will enable the DvSum Gateway to communicate with the DvSum SaaS application and ensure the service functions properly and securely connects to the necessary resources on the network. Please configure the network settings accordingly to allow for this communication.
URLs that must be whitelisted for outbound access on port 443
https://apis.dvsum.ai
wss://3k7tif2dyi.execute-api.us-west-2.amazonaws.com/prod
https://dvsum-app-data-prod.s3.amazonaws.com
Ports that must be open for inbound access from end user computers
Default: 8183
Common alternative: 443
Testing network configuration
The installer includes an option to test the network configuration.
Linux:
./dvsum_gateway_setup.sh --check-network
Windows:
.\dvsum_gateway_setup.bat --check-network
Define a Gateway
You must define a gateway in the DvSum GUI defining the details that you will use when you install the gateway. Navigate to Administration â†’ Account Settings â†’ Gateway.
Click on "Add" Button as shown in the image below. Provide the Web Service name, Host Name and Port (default 8183) and save it.
This defines a Gateway with a unique communication key by which the gateway communicates with the DvSum web application.
Keep track of the communication key; it will be required later during the installation.
Windows Installation
Download the installer
Use the link provided above, or the installer can also be downloaded directly through the Download button available in the GUI.
Note:
Once the installer is downloaded, extract it and move it to the desired folder.
Define a Gateway
You must define a gateway in the DvSum GUI defining the details that you will use when you install the gateway. Navigate to Administration â†’ Account â†’ Gateway.
Click on "Add" Button as shown in the image below. Provide the Web Service name, Host Name and Port (default 8183) and save it.
This defines a Gateway with a unique communication key by which the gateway communicates with the DvSum web application.
Keep track of the communication key; it will be required later during the installation.
Launch PowerShell
Navigate to Start, look for Windows PowerShell, then right-click and select "Run as Administrator."
Navigate in PowerShell to the gateway folder
# In this example, the gateway was unzipped in c:\dvsum_gateway
cd c:\dvsum_gateway
dir
Non-interactive Installation
The non-interactive mode proceeds automatically without the need for manual input during the process. Instead, you enter all relevant information into the two properties files before running the installation.
Edit
configuration.properties
Set the value for
API Key
Confirm the value for
Port
Confirm the value for
Python Port
Review all other properties â€” in most cases, no updates are needed.
version 1.3.8 or previous
- api.key (copy from app.dvsum.com -> Account -> Gateway -> copy communication key of your gateway)
- port (default value: 8183)
- python.service.port (default value: 8185)
version 2.0.0 or later
- dvsum.apiKey (copy from app.dvsum.com -> Account -> Gateway -> copy communication key of your gateway)
- dvsum.port (default value: 8183)
- dvsum.pythonServicePort (default value: 8185)
Edit
ssl_config.properties
The properties that you need to set in this file depend on the SSL option that you select when running the script.
Generate a cert (SSL Option 1): complete sections 1 and 2.
Add cert to existing keystore (SSL Option 2): complete sections 2 and 3.
Add cert to new keystore (SSL Option 3): complete sections 2 and 3.
Use existing keystore (SSL Option 4): complete section 2.
Comments on section 2:
Keystore Configuration
These settings define how the Java keystore is configured for SSL support.
For Gateway version
1.3.8 and earlier
# Settings for the Java keystore that will hold the SSL certificates
keystore_path= # Typically left blank. It will be named based on the domain_name.
keystore_pass= # Use a strong password.
key_store_type= # Typically left blank. Defaults to JKS if not specified.
alias= # Any name can be used. dvsum-gateway-cert is a good descriptive value.
For Gateway version
2.0.0 and above
# 2. Keystore Configuration
# ----------------------------------------
# Settings for the Java keystore that will hold the SSL certificates
dvsum.keystorePath= # Typically left blank. It will be named based on the domain_name.
dvsum.keystorePass= # Use a strong password.
dvsum.keyStoreType= # Typically left blank. Defaults to JKS if not specified.
dvsum.alias= # Any name can be used. A descriptive alias like dvsum-gateway-cert is recommended.
Launch the installer
Use the following command to execute the gateway setup script and proceed with the installation and configuration of the DvSum Gateway automatically, without requiring any user interaction.
# In this example, Generate a cert (SSL Option 1) is selected.
.\dvsum_gateway_setup.bat --ssl-option=1 --non-interactive
Note: On Windows, Python needs to be installed for the python service to be installed successfully.
Validate the Gateway
Follow these steps to verify if the installation succeeded.
. Verify that the Gateway is accessible at: https://<your-domain>:<port>
. Check the logs at ./logs/SAWS.log for any important messages
Interactive Installation
In an interactive installation the installer will prompt you for all options and all properties that you need to provide.
Launch the installer
Use the following command to access the gateway setup file and proceed with installing and configuring the DvSum Gateway.
.\dvsum_gateway_setup.bat
The help command displays all the necessary commands required to operate with the gateway.
# Display detailed help
.\dvsum_gateway_setup.bat help
The API key and the Port Name are required, these must match the properties you used when defining the gateway above.
SSL Configuration Option
Four different options will be offered for SSL configuration. You will choose one of these options. All four options are explained below.
Generate a new certificate
Choice: 1
If you do not have an SSL certificate already, choose this option. This is the most common option for new users.
It will prompt you for all details needed to generate a certificate. Then it will generate a self-signed certificate that you can use immediately as well as a certificate signing request (CSR) that you can use to obtain a CA-signed certificate later.
Add certificates to an existing keystore
Choice: 2
Use this option when you have a DvSum keystore from a previous installation and you have an updated certificate that you want to use.
You will be prompted for the keystore location and password as well as the certificate location.
How to Use Your CA-Signed Certificate:
1. A CSR file is utilized to obtain a CA-signed certificate from a Certificate Authority, such as GoDaddy or Digicert.
2. Once obtained, you will receive a folder containing the certificate, which includes four files as illustrated.
3. Copy these files and place them in the DvSum folder.
4. Once copied, open PowerShell and select option 2.
5. Provide the path to the keystore (obtained in step 1).
After successful execution, the results can be observed by browsing to the gateway.
Create a new keystore with your certificate
Choice: 3
Use this option if you have a certificate and private key. The script will create a new keystore and save your certificate in the keystore.
Use an existing keystore with certificate and private key
Choice: 4
Use this option if you already have a keystore fully configured with the appropriate SSL certificate. The script will update the DvSum gateway to use this keystore.
All the information of the keystore is displayed and the user is asked if it's the correct keystore, configuration is completed accordingly.
Uninstall the Gateway
Use the following command to uninstall the gateway:
# Uninstall the gateway
.\dvsum_gateway_setup.bat uninstall
Note:
After uninstalling the previous gateway, proceed with the new dvsum_gateway_setup.bat file in order to install the latest gateway.
Linux Installation
Download the installer
Use the link provided above. You can either download directly to the Linux server, or download to a desktop first and then upload to the Linux server. Your corporate firewalls and network policies will determine which is easier.
Download directly
The provided link redirects to the latest version of the download. As a best practice, it's good to determine what version of the installer you will download, and use that name when you save the file.
# Simple curl command to see the redirected URL
# Note the filename "DvSum_1.2.3.tar.gz" in the response
curl "
https://apis.dvsum.ai/admin/account/saws/download-gateway?env=linux&app=dc"
# response:
https://dvsum-app-data-prod.s3.amazonaws.com/connector/linux/DvSum_1.2.3.tar.gz?AWSAccessKeyId=ASI..
.
Now that we have the filename "DvSum_1.2.3.tar.gz" from the command above,
# Using the filename "DvSum_1.2.3.tar.gz" from the previous output
# Run curl using the same URL as above but with these additional parameters
# -L to follow the redirect
# -o to specify the downloaded filename
curl -L -o DvSum_1.2.3.tar.gz "
https://apis.dvsum.ai/admin/account/saws/download-gateway?env=linux&app=dc"
Download to desktop first
If you are unable to download directly to the Linux server, then you can download first to your desktop computer. From there upload to the Linux server using SCP or any other file transfer mechanism.
Unzip the installer
# Create a directory for the gateway
mkdir /home/my_user/dvsum_gateway

# Unzip the installer
# In this example these directories are used:
# /home/ubuntu/dvsum_installers
# /home/ubuntu/dvsum_gateway
cd /home/my_user/dvsum_gateway
tar -vxzf ../dvsum_installers/DvSum_2.0.0.tar.gz
Python
Python3 must be installed. The installer will install the required packages. Administrators may optionally install these packages before running the installer.
# Confirm Python version
$ python --version
Python 3.12.3

# Install required packages manually (optional)
cd /home/my_user/dvsum_gateway
python3 -m venv ./python_service/venv
./python_service/venv/bin/pip install --upgrade pip
./python_service/venv/bin/pip install -r ./python_service/requirements.txt
Non-interactive Installation
The non-interactive mode reads all relevant information from two properties files during the installation. This is the most common installation method.
Edit
configuration.properties
Set the value for
api.Key
Confirm the value for
port
Confirm the value for
python.service.port
Review all other properties â€” in most cases, no updates are needed.
Note:
For
Gateway version 1.3.8 and below
, use the property names as listed above (
api.Key
,
port
,
python.service.port
).
For
Gateway version 2.0.0 and above
, use the updated property names:
dvsum.apiKey
dvsum.port
dvsum.pythonServicePort
Edit
ssl_config.properties
The properties that you need to set in this file depend on the SSL option that you select when running the script.
Generate a cert (SSL Option 1): complete sections 1 and 2.
Add cert to existing keystore (SSL Option 2): complete sections 2 and 3.
Add cert to new keystore (SSL Option 3): complete sections 2 and 3.
Use existing keystore (SSL Option 4): complete section 2.
Comments on section 2:
Keystore Configuration
These settings define how the Java keystore is configured for SSL support.
# 2. Keystore Configuration
# ----------------------------------------
# Settings for the Java keystore that will hold the SSL certificates
dvsum.keystorePath= # Typically left blank. It will be named based on the domain_name.
dvsum.keystorePass= # Use a strong password.
dvsum.keyStoreType= # Typically left blank. Defaults to JKS if not specified.
dvsum.alias= # Any name can be used. A descriptive alias like dvsum-gateway-cert is recommended.
Launch the installer
Use the following command to execute the gateway setup script and proceed with the installation and configuration of the DvSum Gateway automatically, without requiring any user interaction.
# In this example, Generate a cert (SSL Option 1) is selected.
./dvsum_gateway_setup.sh --ssl-option=1 --non-interactive
Validate the Gateway
Follow these steps to verify if the installation succeeded.
. Verify that the Gateway is accessible at: https://<your-domain>:<port>
. Check the logs at ./logs/SAWS.log for any important messages
Interactive Installation
In an interactive installation the installer will prompt you for all options and all properties that you need to provide.
Launch the installer
Use the following command to access the gateway setup file and proceed with installing and configuring the DvSum Gateway.
./dvsum_gateway_setup.sh
Appendix
Using Port 443
Most customers prefer to open the standard port for https, 443, rather than opening port 8183. The simplest way to achieve this is to simply redirect traffic from port 443 to port 8183.
Ubuntu example
# Add the iptables rule.
sudo iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 8183

# Install iptables-persistent if not already present.
sudo apt-get update
sudo apt-get install iptables-persistent

# Save the current iptables rules
# and ensure they start on reboot.
sudo netfilter-persistent save
sudo systemctl enable netfilter-persistent

--------------------------------------------------------------------------------

================================================================================
TITLE: Gateway Downloads
URL: https://dvsum.zendesk.com/hc/en-us/articles/24882110844692-Gateway-Downloads
================================================================================

To connect to a data source, you use either the DvSum web service or an on-premises gateway. Refer to the article
DvSum Web Service vs On-Premises Gateway
for details.
If you're using an on-premises gateway, you'll need to download and install the gateway.
All Gateway Downloads
Data Catalog Linux
Data Catalog Windows
Data Quality Windows

--------------------------------------------------------------------------------

================================================================================
TITLE: Sample Data and On-Premises Gateway
URL: https://dvsum.zendesk.com/hc/en-us/articles/23896259032596-Sample-Data-and-On-Premises-Gateway
================================================================================

In this Article
Overview
Details about locally-stored sample data
Data in scan_output folder
Sample data unavailability
Gateway not running
Certificate validation required
Data Retention
Overview
When a local gateway is in use and a data source is scanned, a new file is generated and stored in a folder called "scan_output". Sample data for each table is stored in a corresponding file named *_sample_data.json.
Details about locally-stored sample data
Data in scan_output folder
The sample data file is available in an encrypted format. It's saved in the folder "scan_output".
The
AES/CBC/PKCS5Padding algorithm is used for symmetric data encryption.
Sample data unavailability
Sample data is normally available to DvSum users to view. Sample data becomes unavailable in the DvSum application in the following two situations.
Gateway not running
If the gateway is not in the running state, then it cannot respond to requests to display the sample data.
Solution: Restart the gateway
Certificate validation required
If the gateway is running, but it's using a self-signed certificate, then then the browser cannot establish a connection to the gateway. Certificate Validation is required in order to connect to the gateway.
Solution: Click "here" in the displayed message. Accept the self-signed certificate.
Afterwards, a manual browser refresh is required in order to view the sample data again.
Data Retention
The scan_output folder contains separate folders corresponding to all of the scans that took place. The retention policy for these folders can be configured as follows:
Edit the file configuration.properties.
Set the property "dataRetentionPolicyEnabled" to "true" or "false"
Set the property "dataRetentionPeriodInDays" as desired

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Azure Databricks (Service Principal Access)
URL: https://dvsum.zendesk.com/hc/en-us/articles/22643550730132-Configure-Azure-Databricks-Service-Principal-Access
================================================================================

Step 1: Configure an app in Azure portal
Register an application with the Microsoft Entra ID endpoint in the Azure portal. Alternatively, you can use a Microsoft Entra ID app that is already registered.
Sign in to the
Azure portal
.
If you have access to multiple tenants, subscriptions, or directories, click the
Directories + subscriptions
(directory with filter) icon in the top menu to switch to the directory in which you want to register the application.
Search for and select
Microsoft Entra ID.
Within
Manage, select
App registrations > New registration.
For
Name, enter a name for the application.
In the
Supported account types
section, select
Accounts in this organizational directory only (Single tenant).
In the
Redirect URL (optional)
leave empty
Click
Register.
On the application pageâ€™s
Overview
page, in the
Essentials
section, copy the following values:
Application (client) ID
Directory (tenant) ID
Add
AzureDatabricks
to the required permissions of the registered application. You must be an admin user to perform this step. If you encounter a permissions-related issue while you perform this action, contact your administrator for help.
On the application pageâ€™s
Overview
page, on the
Get Started
tab, click
View API permissions.
Click
Add a permission.
In the
Request API permissions
pane, click the
APIs my organization uses
tab, search for
AzureDatabricks, and then select it.
Enable the
user_impersonation
check box, and then click
Add permissions.
Click
Grant admin consent for ###
and then
Yes. To perform this action, you must be an admin user or have the privilege to grant consent to the application.
Step 2: Add the Microsoft Entra ID service principal to your Azure Databricks account
This steps works only if your target Azure Databricks workspace is enabled for
identity federation
. If your workspace is not enabled for identity federation, skip ahead to Step 3.
In your Azure Databricks workspace, click your username in the top bar and click
Manage account.
Alternatively, go directly to your Azure Databricks account console at
https://accounts.azuredatabricks.net
.
Sign in to your Azure Databricks account, if prompted.
On the sidebar, click
User management.
Click the
Service principals
tab.
Click
Add service principal.
Enter a
Name
for the Microsoft Entra ID service principal.
For
UUID, enter the
Application (client) ID
value from Step 1.
Click
Add. Your Microsoft Entra ID service principal is added as an Azure Databricks service principal in your Azure Databricks account.
Step 3: Add the Microsoft Entra ID service principal to your Azure Databricks workspace
If your workspace is enabled for
identity federation
:
In your Azure Databricks workspace, click your username in the top bar and click
Admin Settings.
Click on the
Identity and access
tab.
Next to
Service principals, click
Manage.
Click
Add service principal.
Select your Microsoft Entra ID service principal from Step 2 and click
Add. Your Microsoft Entra ID service principal is added as an Azure Databricks service principal in your Azure Databricks workspace.
Skip ahead to Step 4.
If your workspace is not enabled for identity federation:
In your Azure Databricks workspace, click your username in the top bar and click
Admin Settings.
Click on the
Identity and access
tab.
Next to
Service principals, click
Manage.
Click
Add service principal.
Click
Add new.
For
ApplicationId, enter the
Application (client) ID
for your Azure service principal from Step 1.
Enter some
Display Name
for the new service principal and click
Add. Your Microsoft Entra ID service principal is added as an Azure Databricks service principal in your Azure Databricks workspace
Step 4: Assign workspace-level permissions to the service principal
If the admin console for your workspace is not already opened, click your username in the top bar and click
Admin Settings.
Click on the
Identity and access
tab.
Next to
Service principals, click
Manage.
Click the name of your service principal to open its settings page.
On the
Configurations
tab, check the box next to each entitlement that you want your service principal to have for this workspace, and then click
Update. Check following check boxes
Active
Databricks SQL Access
Workspace access
On the
Permissions
tab, grant access to any Azure Databricks users, service principals, and groups that you want to manage and use this service principal.
Step 5: Enable Personal access token for Service principal
Log into your Databricks workspace
If the admin console for your workspace is not already opened, click your username in the top bar and click
Admin Settings.
Click on the
Advanced
tab.
In Access control
Enable personal access token
Click on permission settings
Search for your service principal name
Select permission "Can Use"
Click Add
Step 6: Enable Hive metadata access for Service principal
Log into your Databricks workspace
Click on the Catalog
tab in left menu.
Click on catalog that was set earlier.
Open permissions tab
Click on Grant
Search for your service principal name
Check the privileges you would like to grant
Select "All"
Click Grant
Step 7:
Enable cluster access for
Service principal
To configure the
Compute
cluster permissions:
Navigate to the
Permissions
section.
Select the relevant
Service Principal (SP) name
.
Set the permission to
Can Attach To
.
To configure the
SQL Warehouse
permissions:
Navigate to the
Permissions
Assign the permission level
Can Use
Additional References:
https://learn.microsoft.com/en-us/azure/databricks/dev-tools/app-aad-token
https://learn.microsoft.com/en-us/azure/databricks/dev-tools/service-principals

--------------------------------------------------------------------------------

================================================================================
TITLE: Scan Logs
URL: https://dvsum.zendesk.com/hc/en-us/articles/21582531523348-Scan-Logs
================================================================================

In this Article:
Introduction
Authentication & Test Connection Logs
Particular Scan Job Logs
Reading Logs on Scan Job Page
Cancelling the Scan
Aborting the Scan
Scheduled Scan Jobs
Introduction
Scanning is a very important part of the DvSum Application. It is bringing the Data from the Database to our application.
During the scan, a lot of things are going on in the back-end like how DvSum Webservice is connected to the Database and how tables are profiled one by one in our application. Most of the part is Technical but there are some areas where normal users can find useful insights by checking the logs that are being generated when the data is coming in our application.
We will further explore how users can abort the ongoing scans.
1- Authentication & Test Connection Logs
When the credentials of any data source are authenticated, the database and schemas are selected, and "Test Connection" is passed then there are some basic logs that are generated like the connection is established with the database and the schemas in it. The generated logs can be seen in the "Logs" tab.
All the logs related to the Source can be found in this tab and they are generated in the below cases:
Credentials are authenticated & Test Connection is passed
Tables are being scanned
Questions asked on Agent
For more information about adding a Data source here is the article
Adding Snowflake as Data Source.
The Logs are being continuously generated at the back-end so in order to fetch the latest logs user will be required to click on the "Refresh" button icon. The search bar can also be used for searching particular words in the Logs.
2- Particular Scan Job Logs
When a scan is started, a Scan job is created for that particular scan in the "Scan History" tab.
On the Scan job page, only the logs that are related to the scanning of the tables are present. This can be more useful instead of looking for the whole source-related logs.
Let us cover some flow how scans can be started, and cancelled, and how logs can be viewed. Since the Scans are of different types so for more information regarding different scans here is the article
Cataloging & Profiling Data Sources.
2.1- Reading Logs on Scan Job Page
As soon as a scan is started, logs are generated on the scan job page. The Logs may not just appear as soon as the scan is started and they might take 3-5 minutes to start. As the logs are continuously being generated they need to be refreshed to bring the latest logs.
Since the tables are being profiled during the scan so users can check the status that how many tables have been profiled. This can be done easily by looking at the logs or users can use the search bar to check that how many tables have been fetched into the application.
Once all the tables are profiled then the scan will be completed and the status will be changed from running to completed. When the scan is completed users can still see all the logs that were generated.
2.2- Cancelling the Scan
When a scan is started, before going to the running state, for a few seconds it goes scheduled. In this state, if any Scan Job page is opened and the user cancels the scan then that particular Scan job will be cancelled.
Once all the tables are profiled then the scan will be completed and the status will be changed from running to completed and the users can still see all the logs that were generated.
2.3- Aborting the Scan
When a scan is started it goes to the running state and it starts the catalog execution and tables are profiled one by one. During this state, if the running scan is cancelled then it will be aborted.
On the UI there will be an "Abort Scan" button visible and when it is clicked, the scan job will be aborted.
Please note that if a scan is running and some tables are refreshed/ profiled and the scan is now aborted, the tables which were already fetched into the application will not be affected.
3- Scheduled Scan Jobs
Users have the option of running scheduled scans in which the scan will start running at a particular time mentioned by the user in the settings of the source. The logs generated in scheduled scans are the same as the on-demand scans.

--------------------------------------------------------------------------------

================================================================================
TITLE: Catalog, Profile & Lineage Scan on Data Sources
URL: https://dvsum.zendesk.com/hc/en-us/articles/20310539391252-Catalog-Profile-Lineage-Scan-on-Data-Sources
================================================================================

In this Article:
Introduction
Catalog Scan
Profiling
Catalog & Profile Scans
Catalog, Profile & Lineage Scans
Scheduling a Job
Special Cases for Certain Sources
Introduction
To get the data in the DvSum Catalog, the Data source should be Scanned from the database. The scanning process is divided further into three steps which are catalog scan,Â  profile scan & lineage scan. This article will show how to perform these three types of scans. Before moving further make sure that you know how to add a data source and how a source is authenticated. Here is the
link
to the article that shows how to add an Oracle Data source.
Catalog Scan
Cataloging the data source means that during the authentication there are some databases and schemas added and schemas further contain tables that have the data. When the source is authenticated then "Scan Now" button can be seen. When the "Scan Now" button is clicked it further shows two options:
Profile
Lineage
When the user doesn't select any of the above options the "Catalog" scan will run automatically. Catalog scan will run in any case.
When any source is scanned using the scan type "Catalog" then a job will be scheduled with the scan type "Catalog".
The scan type can be will be shown in the job detail page.
After few minutes the catalog step will be completed and the status will be changed from "Running" to "Completed".
When the Scan is completed then scan information can be viewed by clicking on the "Job Execution ID". The scan results page shows us the total tables and columns that are scanned. The scan type is also mentioned on the Scan Results page.
Go to the
Dictionary
dropdown o
n the Database tables tab
, the data sources that are newly scanned can be seen. Since the data source was "cataloged" so the tables will come but there will be no records in the tables and it can be verified by looking at the record count which will be empty.
Profiling
Only the columns in the tables are not useful until the data is not there so in order to bring data of the tables in the catalog, the table must be profiled. When any table is selected then "Run Profiling" button can be seen above.
On clicking "Run Profiling" the tables will be profiled (all the data present in the tables will be fetched in the catalog) and a job will be scheduled.
When the Job will be completed then all the tables will be profiled. On the Database Tables tab there will be record count for the tables that were profiled indicating that the data is now present in the tables:
Note: Scan Results tab for the scan type "Profile" does not exist
Catalog & Profile Scans
When the source is authenticated then on the "Scan Now" button there is an option of "Profile" which is a combination of cataloging and profiling (full scan). It will bring the schemas and tables along with the data inside them in the catalog. Once the "Scan" button is clicked, a scan job will be created of the type "Catalog & Profile" will be created.
Once the scan is completed, the scan details information can be seen by clicking on the Job IDÂ  which will open up the job detail page which contains all the information related to the scan along with the scan type:
Go to the
Dictionaries
dropdown o
n the Database Tables tab, all the tables along with the data will be present in the catalog. Here we do not require profiling separately as it was done during the scan.
It is to be noted that scan type "Catalog & Profile" basically brings all the schemas and tables present inside it along with the data. If the data for some tables is required then the source must be cataloged first and then profiling can be applied to specific tables.
Catalog, Profile & Lineage Scans
On the Scan Now button the user has the option of running "Lineage" scans with "Catalog" or the user can also run "Catalog", "Profile" and "Lineage" scans by checking all the checkboxes. Now along with the "Catalog" and "Profile" scan, the Lineage will also be rendered into the application. Once the user clicks on the Scan button the job will be created for "Catalog", "Profile" & "Lineage".
Users can access the information related totheÂ  scan by clicking on the particular scan name.
Scheduling a Job
The above examples of scans are basically on-demand which means the moment the "Scan" button is clicked, a job will be created and start running but if there is a scenario in which scans are required to run daily or start from a specific date then scans can be scheduled. On the settings tab of the Data Source, the "General" tab contains scanning information, and on clicking "Edit", jobs can be scheduled and the user can mention the scan types:
It is to be noted that only scan types "Catalog", "Catalog & Profile", "Catalog & Lineage" & "Catalog, Profile & Lineage" can be scheduled. The scan type "Profile" can not be scheduled for a particular time. On selecting the right scan type, the scan frequency, start, and end time can be selected according to the requirement. Once the information is saved, a job will be scheduled with the scan type that was selected:
Special Cases for Certain Sources
In our application, there are some sources that don't have the option of separate "Catalog" & "Profile" and "Catalog" & "Lineage. For these sources on the "Scan Now" button on Data sources, there will be no drop-down appearing asking for selecting scan type. In these sources, the scan type will be "Catalog & Profile" or "Catalog" & "Lineage" by default. These 4 sources are:
Azure Data Lake Storage (Catalog & Profile scan available)
Power BI (Catalog & Lineage scan available)
Tableau (Catalog & Lineage scan available)
File Upload (Catalog & Profile scan available)

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Salesforce as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/19345773418388-Configure-Salesforce-as-a-Source
================================================================================

In this Article:
Overview
Step-by-Step Configuration
Configure a Connected App in Salesforce
Configure Permission Sets in Salesforce
Adding Salesforce as a Data Source in DvSum
Step 1: Add a New Source
Step 2: Configure Connection Settings
Step 3: Save the Configuration
Running a Data Scan
Step 1: Start a Scan
Step 2: Review Scan Results
Overview
Integrating Salesforce with DvSum allows users to analyze CRM data, generate reports, and ensure data governance. This guide outlines the steps to configure Salesforce as a source in DvSum.
Step-by-Step Configuration
1. Configure a Connected App in Salesforce
Open the browser, navigate to
salesforce.com
, and log in.
Navigate to Setup â†’ Apps â†’ AppManager to create an app.
Tip:
Use the Quick Find box.
In the Connected Apps section, click "New Connected App".
Enter a name to be displayed to users when they log in to grant permissions to your app, along with a contact email address.
Enable
OAuth Settings
, and enter the relevant value in the
Callback URL
box:
For Data Intelligence:
https://apis.dvsum.ai/data-sources/sources/salesforce/saveVerifierCode
Select the scope of permissions that your app should request from the user. Save the changes and Continue to the next screen.
Click your app name to open a page with information about your app. Note the OAuth client credentials. These properties are needed to add Salesforce as a source in DvSum DQ:
Consumer Key
Consumer Secret
2. Configure Permission Sets in Salesforce
To connect to Salesforce, the user must have the necessary permissions to call Salesforce APIs. This is achieved by configuring the appropriate properties within the relevant Permission Set.
The Permission Set can be accessed through one of the following paths (the interface layout may vary depending on whether Lightning Experience or Classic view is used, but the navigation remains the same):
ADMINISTRATION â†’ Users â†’ Permission Sets â†’ <permission_set>
ADMINISTRATION â†’ Users â†’ Users â†’ <user> â†’ Permission Set Assignments â†’ <permission_set>
Once there, enable
System Permissions â†’ API Enabled
to grant access to any Salesforce.com API.
Once the Connected App and Permission Set are configured, you can proceed with setting up DvSum.
Adding Salesforce as a Data Source in DvSum
Step 1:
Add a New Source
Navigate to the Data Sources tab.
Click Add Source.
Select Salesforce as the source type.
Provide a Source Name and click Save.
Step 2: Configure Connection Settings
Open the newly created source.
Enable On-Premise Web Service.
Select the appropriate SAWS (if applicable).
(Optional) Enable Sandbox Mode and enter the sandbox URL.
Enter the Client ID and Client Secret from the Salesforce Connected App.
Click Authenticate.
Log in to Salesforce when prompted.
Note
: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click
here
Step 3: Save the Configuration
Once authenticated, return to the source settings page.
Click Done (top-right corner).
Click Save.
Note
: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click
here
Step 3:
Once the Authenticate button is clicked, it will redirect to a new tab and open the Salesforce login page. Login to the salesforce account. Once the login step is complete, it will redirect back to the source detail page and there user will see that the source is authenticated successfully.
Step 4:
After the credentials are authenticated, we need to save the source. For that, scroll up to the top. From the top right corner click on the â€œDoneâ€ button.
After that click the â€œSaveâ€ button. The source will get saved successfully.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Running a Data Scan
Step 1: Start a Scan
Navigate to Scan History.
Click Scan Now.
A job will be created to fetch data from Salesforce.
After the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.
Step 2: Review Scan Results
Once the scan completes, click on the Scan Name.
The Scan Summary page will display:
New tables and columns were fetched.
Data insights.
After the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.
To explore table details, g
o to the
Dictionaries
dropdown and select the
Data Dictionary
â†’ Recently Refreshed.
Click on any table name to view details.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure MongoDB as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/17494822453524-Configure-MongoDB-as-a-Source
================================================================================

In this Article:
Overview
Step-by-Step Configuration
Add MongoDB as a Data Source
Configure Connection
Save the Connection
Running a Data Scan
Start a Scan
Review Scan Results
Browsing the Data Catalog
Additional Resources
Overview
Integrating
MongoDB
with
DvSum
allows users to query NoSQL data, perform analytics, and enforce data governance. This guide provides step-by-step instructions for configuring
MongoDB
as a data source in
DvSum
.
Step-by-Step Configuration
1. Add MongoDB as a Data Source
Navigate to
Administration â†’ Data Sources â†’ âŠ• Add Source
.
Select
MongoDB
from the list of available sources.
2. Configure Connection
Once the source is saved, you will be redirected to the connection settings detail page.
Most common
: Connect Using DvSum Web Service
Whitelist the DvSum application by IP address as indicated. Then enter the required connection information:
Host
Port
Instance Name
DB Login
DB Password
Alternative
: You may
install a DvSum Edge Gateway
behind your firewall. Then connect to your SQL Server instance from this web service installed on premises. Read more details in
DvSum Web Service vs On-Premises Edge Gateway
.
After entering the credentials, Authenticate the source.
Once the source is Authenticated, the Database section will appear.
3. Save the Connection
After credentials are authenticated and the database is selected, then you must save the source. For that, scroll up to the top. From the top right corner click the â€œDoneâ€ button.
After that click the â€œSaveâ€ button.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Running a Data Scan
1. Start a Scan
Navigate to
Scan History
.
Click
Scan Now
.
The scan will initiate, creating a job to fetch data from
MongoDB
.
2. Review Scan Results
Once the scan completes, the status will change to
Completed
.
Click on the
Scan Name
to open the
Scan Summary
page.
The
Scan Summary
displays:
Number of new tables and columns fetched.
Data insights and schema updates.
When the scan completes, the status will change to "Completed".
After the scan completion, click on Scan Name and it will open the Scan Summary page for this scan.
The Scan Summary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this scan.
Your MongoDB connection is now fully configured and functional.
Browsing the Data Catalog
Navigate to the Data Dictionary on the left sidebar.
Click on the Recently Refreshed tab to view tables discovered in the recent scan.
Click on table names to see detailed metadata and structure.
Watch this quick video tutorial of how to add and configure an Mongo DB source into DvSum app.
Additional Resources
Read more about
DvSum Web Service (Cloud SAWS)
Read more about
DvSum Edge Gateway (On-Premises SAWS)

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure PostgreSQL as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/12597290833172-Configure-PostgreSQL-as-a-Source
================================================================================

In this Article:
Overview
Adding PostgreSQL Source in DvSum
Prerequisite: Enabling General Query Log for PostgreSQL
Step-by-Step Configuration
Step 1: Add a New Postgres DataSource
Step 2: Configure Connection Settings
Step 3: Select Database and Schema(s)
Step 4: Save the Connection
Step 5: Run a Data Scan
Reviewing Scan Results
Video Tutorial
Overview:
This article outlines the process of configuring PostgreSQL as a data source in DvSum, enabling integration for data cataloging and profiling. The steps provided apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations based on the specific platform.
Adding PostgreSQL source in DvSum:
Prerequisite: Enabling General Query Log for PostgreSQL
Before configuring MySQL as a source, ensure that the general query log is enabled for your PostgreSQL server. This is essential for tracking data lineage and obtaining insights into usage patterns. For more information, refer to the
Enabling General Query Log for Data Sources
article.
You can follow the steps mentioned below to configure and authenticate a PostgreSQL source:
Step-by-Step Configuration
Step 1: Add a New Postgres DataSource
Navigate to
Data Sources
.
Click on
Add Source
.
Select
PostgreSQL
as the data source.
Enter a
source name
and click
Save
.
Step 2: Configure Connection Settings
After saving, you will be redirected to the
connection settings
page.
Enable the
On-premise Web Service
checkbox if applicable.
Select the
SAWS (Smart Adaptive Web Service)
that is set up and running.
Enter the following details:
Host
Port
Database Login
Password
Note
: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click
here
Step 3: Select Database and Schema(s)
Once authentication is successful, the
Database
section appears.
Select the
database
you wish to scan.
If scanning all schemas, leave the
Schema checkbox
unchecked.
To scan specific schemas:
Check the
Schema checkbox
.
Select the required schemas from the
Available Schemas
list.
Move them to the
Selected Schemas
tab.
Once the database is selected, we now have an option of limiting the scan to some specific schema(s) or we have to scan all of them. For the PostgreSQL source, we would have a Schema field as well which will contain a checkbox, If we want to scan all the schemas then we shouldn't check this checkbox and proceed with saving and scanning the source. But if we want to limit our scan to some specific schemas then check this checkbox.
Once it is checked then the list of available schemas will be displayed. Users can select single or multiple schemas from the Available Schemas list and move them to the Selected Schemas tab on the right.
Step 4: Save the Connection
Scroll to the top of the page.
Click Done.
Click Save.
The PostgreSQL source is now configured successfully.
After that click the â€œSaveâ€ button. The source will get saved successfully.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Step 5: Run a Data Scan
Navigate to the Scan History page.
Click Scan Now.
The scan will run, and a job will be created.
Once completed, the status will change to Completed.
Click on the Scan Name to view the Scan Summary.
Once the status of the job gets Completed, our new PostgreSQL source's scan will be completed successfully.
After the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.
Reviewing Scan Results
The
Scan Summary
page displays insights such as the number of tables and columns fetched.
Navigate to
Data Dictionary
to view the scanned tables.\
Click on the
Recently Refreshed
tab to see newly added tables.
Click on individual table names for more details.
Video Tutorial:
Watch this quick video tutorial on how to add and configure a PostgreSQL source into DvSum app.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Databricks as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/11918822204436-Configure-Databricks-as-a-Source
================================================================================

In this article:
Overview
Prerequisites
Step-by-Step Configuration
Step 1: Adding a Databricks Source in DvSum
Step 2: Configure Connection
Step 3: Select Database
Step 4: Save & Test Connection
Step 5: Scan the Data Source
Authentication Options
Scenario 1: Authentication via Access Token
Scenario 2: Authentication via Client Secret
Reviewing Scan Insights
Overview:
Azure Databricks is an optimized platform for Azure, offering tight integration with services like Azure Data Lake Storage, Azure Data Factory, Azure Synapse Analytics, and Power BI. It allows data storage in a unified, open lakehouse while consolidating analytics and AI workloads.
This article describes the process of configuring Databricks as a data source in DvSum, facilitating the integration of data cataloging and profiling. The steps outlined apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with minor platform-specific variations.
Adding Databricks source in DvSum:
Prerequisites:
Enabling Query History for Databricks
Before configuring Databricks as a source, ensure that query history is enabled for your Databricks account. This is crucial for tracking data lineage and gaining insights into usage patterns. For more information, refer to the
Enabling Query History for Data Sources
article.
Cluster and Account Setup:
For authentication of the Databricks Source, a user must have an account on the Azure Databricks portal on which a cluster is running attached to a database. On the Azure Databricks portal, go to the Compute tab and start your cluster if it is in the stop state.
Required Table Access for Catalog Scans
To support cataloging and schema discovery in Databricksâ€”especially for use cases involving the ability to pick and choose catalogsâ€”it is
important to have read-only access
to the following system tables:
system.information_schema.catalogs
system.information_schema.schemata
system.information_schema.tables
system.information_schema.columns
Without access to these tables,
cataloging scans will not function as expected
. Please ensure this access is in place before initiating scans.
Step-by-Step Configuration
Step 1: Adding a Databricks Source in DvSum
Navigate to
Data Sources
.
Click on
Add Source
.
In the modal, select
Databricks
.
Provide a
source name
and click
Save
.
Step 2: Configure Connection
Once the source is saved, you will be redirected to the connection settings detail page of the new Databricks source. First, enable the
On-premise Web Service
checkbox and select the appropriate
SAWS (Secure Access Web Service)
that is currently set up and running.
You can authenticate using either:
Access Token
Client Secret
Note
: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click
here
Authentication Options
You can authenticate using either:
Access Token
Client Secret
Scenario 1:
Authentication via Access Token
Enable the
On-Premise Web Service
checkbox.
Select the
SAWS
(Secure Access Web Service) that is set up and running.
Enter the following details:
Server Hostname
HTTP Path
Personal Access Token
Click
Authenticate
.
Scenario 2: Authentication using Client Secret
~Prerequisites for Configuring Azure Databricks (
Service Principal Service
):
Please refer to the article to configure
Azure Databricks (Service Principal Service).
Under the
Host Information
section, select
Client Secret
.
Enter the following details:
Server Hostname
HTTP Path
Azure Client Id
OAuth Secret
Click
Authenticate
.
Note:
To optimize job performance and memory usage with
OAuth Secret
(a confidential key used to securely authenticate and authorize applications when integrating with external services), select the checkbox and enter the OAuth Secret value. This will automatically initiate and connect to the clusters prior to execution.
The OAuth Secret can be generated by the admin from the Service Principal's secret tab.
Step 3: Select Database
Once authenticated, the
Database
section will appear.
Select the appropriate
Catalog Name
from the dropdown.
(Optional) To restrict scanning to specific schemas, enable the
Limit to specific schemas
checkbox and choose the required schemas.
Step 4: Save & Test Connection
Scroll to the top and click
Done
.
Click
Save
.
Click
Test Connection
to validate the setup.
After that click the â€œSaveâ€ button. The source will get saved successfully and after that click on the â€œTest Connectionâ€ button.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Step 5: Scan the Data Source
Navigate to Scan History.
Click Scan Now.
Wait for the job status to change to Completed.
Click on the Scan Name to view the Scan Summary
Reviewing Scan Insights
Go to the
Dictionaries
dropdown and select the
Data Dictionary
tab.
Click on
Recently Refreshed
to view newly discovered tables.
Click on table names to explore metadata details.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Tableau as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/10713006510868-Configure-Tableau-as-a-Source
================================================================================

In this Article:
Overview
Prerequisites
Step-by-Step Configuration
Step 1: Prepare Your Tableau Account
Step 2: Add Tableau as a Source in DvSum
Step 3: Configure the Connection Settings
Step 4: Save the Tableau Source
Step 5: Scan the Tableau Source
Step 6: Analyzing Tableau Reports and Datasets
Viewing Reports from Tableau Scan
Viewing Data Sets from the Tableau Scan
Video Tutorial
Overview:
Tableau is a leading data visualization tool used for data analysis and business intelligence. We use Tableau as a data source in our DvSum Data Intelligence app. By adding and scanning the tableau source in our app, customerâ€™s reports and tableau data sources will be fetched into our app to make the data more insightful to analyze. Letâ€™s get started with Tableau. Now letâ€™s get started with adding Tableau as a data source.
Prerequisites:
Before configuring Tableau as a data source, ensure you have:
Access to a
Tableau Server
or
Tableau Online
account.
Required connection details:
Server URL
Site ID
Authentication method
(Access Token)
Necessary API access and permissions for retrieving Tableau data.
User role with adequate permissions to access and fetch Tableau workbooks and datasets.
Step-by-Step Configuration
Step 1: Prepare Your Tableau Account
Log in to your
Tableau
account.
Ensure that some
datasets and workbooks
are available in your Tableau account.
Create new
workbooks
connected to the existing datasets.
Ensure that sheets are created within these workbooks.
The screenshot below shows multiple datasets and workbooks in the Tableau account:
Step 2: Add Tableau as a Source in DvSum
In the DvSum application, navigate to the Data Sources tab.
Click on the Add Source button.
Select Tableau from the list of available sources.
Provide a Source Name and click Save.
Step 3: Configure the Connection Settings
After creating the Tableau source, you will be redirected to the Connection Settings page.
In the Credentials section, enter the following details:
Server URL
Site Name
Access Token Name
Access Token Value
Click on Authenticate to verify the credentials.
Step 4: Save the Tableau Source
Once authentication is successful, scroll to the top of the page.
Click on the Done button in the top-right corner.
Click on Save to finalize the configuration.
After that click the â€œSaveâ€ button. The source will get saved successfully.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Step 5: Scan the Tableau Source
Navigate to the
Scan History
page.
Click the
Scan Now
button.
A scan job will be created.
Once the job status is
Completed
, the Tableau source scan will be successful.
Click on the
Scan Name
to open the
Scan Summary
page.
The
Scan Summary
page displays insights, including the number of
datasets and workbooks fetched
.
Step 6: Analyzing Tableau Reports and Datasets
Click on the Analytics
Dictionary
from the left navigation tab.
In the
Recently Refreshed
tab, view the fetched
Reports
and
Data Sets
.
Reports from Tableau scan:
From the listing select some recently scanned Report and the user will be redirected to the detail page of the Report. The screenshot below shows how the Reports detail page looks like.
On the detail page, a description of the report is fetched from the Tableau report. This means this report has the same description in Tableau and the same description is fetched and shows up here. The author of the report is basically the person who created this report in Tableau so this value is also basically fetched. Further, we have a lineage of the Report which actually shows us that the current Report is derived from which data set and which tables are connected to that data set.
Viewing Data Sets from the Tableau Scan
Select a recently scanned dataset from the listing.
You will be redirected to the
Dataset Detail page
.
The
Overview
tab displays the description and author fetched from Tableau.
The
Data
tab includes
Profiling Info
and
Field Summary.
The
Field View
section displays
Data Source
fields and
Data Connection
fields.
video tutorial:
Watch this quick video tutorial of how to add and configure an Oracle source into DvSum app.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure MySQL as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/10636157522708-Configure-MySQL-as-a-Source
================================================================================

In this Article
Overview
Prerequisites
Adding MySQL as a Source in DvSum
Step 1: Install and Set Up MySQL Locally
Step 2: Add MySQL as a Data Source in DvSum
Step 3: Configure Connection Settings
Step 4: Select a Database Schema
Step 5: Save and Scan the Source
Video tutorial
Overview
This article explains how to configure MySQL as a data source in DvSum for data cataloging and profiling. The steps apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations depending on the platform.
Prerequisites
Before setting up MySQL as a source in DvSum, ensure the following:
Required Access & Configuration:
A running MySQL database (on-premises, cloud, or managed service like AWS RDS, Azure Database for MySQL)
General Query Log enabled (for tracking data lineage and usage patterns). See
Enabling General Query Log for Data Sources
for details.
Adding MySQL as a Source in DvSum
Step 1: Install and Set Up MySQL Locally
Install MySQL and MySQL Workbench on the machine where SAWS (Self-Hosted Agent Web Service) is running.
Ensure MySQL Workbench is configured properly.
Installation Guide
Step 2: Add MySQL as a Data Source in DvSum
Open
DvSum DI
.
Navigate to
Data Sources
> Click
Add Source
.
Select
MySQL
as the data source.
4. Enter a
Source Name
and click
Save
.
Step 3: Configure Connection Settings
You will be redirected to the
Connection Settings
page.
Enable
On-premise Web Service
and select the SAWS instance.
Enter the following details:
Host
: MySQL server address
Port
: Default is 3306
DB Login
: MySQL username
Password
: MySQL password
Click
Authenticate
.
Note:
MySQL Workbench and SAWS must be on the same machine; otherwise, authentication will fail.
Note:
By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click
here
Step 4: Select a Database Schema
After successful authentication, a
Database Selection
field will appear.
Choose the schema to be scanned (single selection only).
Step 5: Save and Scan the Source
Scroll up and click
Done
.
Click
Save
to finalize the MySQL source setup.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Go to
Scan History
and click
Scan Now
.
Once the scan completes, click on the
Scan Name
to view the
Scan Summary
.
On the Scan Summary page, it will show all the insights of the scan i.e how many new tables and columns are fetched in this scan from the database that we selected earlier.
To have more insights into the tables' details:
Click on
"Data Dictionary"
from the sidebar.
The
Table Listing
view will appear.
Click on the
"Recently Refreshed"
tab.
This tab displays all the tables retrieved in the most recent scan.
Click on any
table name
to view more details about that table on the
detail page
.
Authentication Failed
Ensure correct username and password.
By following these steps, you can successfully integrate MySQL as a data source in DvSum and leverage its data insights and quality features.
video tutorial:
Watch this quick
video tutorial
of how to add and configure an Oracle source into DvSum app.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Azure SQL & Azure Synapse Analytics
URL: https://dvsum.zendesk.com/hc/en-us/articles/10636132296852-Configure-Azure-SQL-Azure-Synapse-Analytics
================================================================================

In this Article:
Overview
Prerequisite: Enable Query History via Query Store
Step-by-Step Configuration
Step 1: Adding a Databricks Source in DvSum
Step 2: Configure Source
Step 3: Scan Data Source
Step 4: Review Data Dictionary
Video Tutorial
Additional Resources
Overview:
This article outlines the process of configuring Azure SQL or Azure Synapse Analytics as a data source in DvSum, enabling integration for data cataloging and profiling. The steps provided apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations based on the specific platform.
Prerequisite: Enable Query History via Query Store
Before proceeding, ensure that
Query Store
is enabled on your Azure SQL Database or Azure Synapse Analytics instance. Query Store captures query history, which is required for lineage tracking and usage insights in DvSum. For detailed steps, refer to the article
Enabling Query History for Data Sources.
For step-by-step instructions on creating users, configuring Query Store, and setting up required permissions, refer to
Azure SQL & Azure Synapse Analytics User Setup and Query Store Configuration for Metadata Extraction
You can follow the steps mentioned below to configure and authenticate a Azure SQL or Azure Synapse Analytics source:
Step by Step Configuration
Step 1:Â Adding a Databricks Source in DvSum
Navigate to
Administration â†’ Data Sources
in DvSum.
Click
"âŠ• Add Source"
to open the wizard.
Select the appropriate
Data Source Type
.
Enter a
name
for the data source.
Click
Save
.
2. Configure Source
The
Connection
tab opens automatically after saving.
Alternatively, navigate to:
Administration â†’ Data Sources â†’ Settings â†’ Connection
.
Click
Edit
and configure the connection details:
Server Name
Database Name
Authentication
Click
Authenticate
to test the connection.
After authentication, select a
database
and optionally, specific
schemas
.
Click
Done
and
Save
to complete the process.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
3. Scan Data Source
Navigate to
Administration â†’ Data Sources â†’ Scan History
.
Click
"Scan Now"
to perform an initial scan.
The scan will progress from
Scheduled â†’ Running â†’ Completed
.
After completion, click the
Scan Name
to open the
Scan Summary
.
Review the scan insights
4. Review Data Dictionary
Navigate to
Data Dictionary
in the DvSum menu.
Click on the
"Recently Refreshed"
tab to find recently scanned tables.
Click on the table name for detailed information.
Video tutorial:
Watch this quick video tutorial on how to add and configure an Azure Data Source into DvSum app.
Additional Resources
Read more about
DvSum Web Service (Cloud SAWS)
Read more about
DvSum Edge Gateway (On-Premises SAWS)

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Snowflake as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/10627532986004-Configure-Snowflake-as-a-Source
================================================================================

In this Article:
Overview
Setting Up Snowflake for DvSum Integration
User and Role Configuration for Metadata Access
Lineage Access Configuration
Steps to Configure Snowflake as a Source
Step 1: Add Snowflake as a Source
Step 2: Configure Connection Settings
Step 3: Select Database & Schema
Step 4: Save the Source Configuration
Step 5: Run a Scan
Review Data Dictionary
Video Tutorial
Overview
This article outlines the process of configuring Snowflake as a data source in DvSum, enabling integration for data cataloging and profiling. The steps provided apply to both DvSum Data Insights (DI) and DvSum Data Quality (DQ), with only minor variations based on the specific platform.
Setting Up Snowflake for DvSum Integration
Before configuring Snowflake in DvSum, a dedicated user and role must be created with the appropriate privileges for metadata access. Additionally, specific account-level grants are required to enable access to
query history
and
data lineage
information.
Before proceeding with the source configuration in DvSum, follow the steps below to set up Snowflake with the required user, role, and privileges.
User and Role Configuration for Metadata Access
1. Metadata Access Configuration
To enable cataloging and profiling in DvSum, a Snowflake user must have access to metadata such as tables and columns. How this access is granted may vary depending on internal Snowflake policies.
The script below is a
sample only
and can be adapted as needed. Itâ€™s not required to run this script as-is â€” the key requirement is that the user should be able to execute the following test queries successfully.
Test Queries â€“ Required Metadata Access
These queries can be used to confirm that the user has sufficient access to metadata:
-- * Tables metadata (replace <DATABASE_NAME>)
SELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.TABLES ORDER BY created DESC;

-- * Columns metadata (replace <DATABASE_NAME>)
SELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.COLUMNS ORDER BY table_name, ordinal_position;
Sample Script â€“ User and Role Creation
If a suitable user and role are not already available, the following script can be used as a reference to create them with minimal, controlled access. This is one possible approach â€” feel free to adapt or implement based on your organizationâ€™s standards.
Before executing the script, update the placeholders (
<USERNAME>
,
<PASSWORD>
,
<ROLE_NAME>
,
<WAREHOUSE>
, etc.) with appropriate values for your Snowflake environment.
-- =====================================================
-- Snowflake Script: Create User
-- Purpose: Create user with minimal privileges for metadata access
-- =====================================================

-- Configuration placeholders - Update these values
-- SET username = '<USERNAME>';
-- SET password = '<PASSWORD>'; 
-- SET role_name = '<ROLE_NAME>';
-- SET warehouse = '<WAREHOUSE>';

-- =====================================================
-- SECTION 1: USER CREATION
-- =====================================================
-- * Create user with secure defaults
CREATE USER <USERNAME>
PASSWORD = '<PASSWORD>'
DEFAULT_ROLE = '<ROLE_NAME>'
DEFAULT_WAREHOUSE = '<WAREHOUSE>'
COMMENT = 'User for reading metadata and query history';

-- =====================================================
-- SECTION 2: ROLE CREATION AND ASSIGNMENT
-- =====================================================
-- * Create custom role with minimal privileges
CREATE ROLE <ROLE_NAME>
COMMENT = 'Role for accessing metadata and query history with minimal privileges';

-- * Assign role to user
GRANT ROLE <ROLE_NAME> TO USER <USERNAME>;

-- =====================================================
-- SECTION 3: WAREHOUSE ACCESS
-- =====================================================
-- * Grant warehouse usage for metadata queries
GRANT USAGE ON WAREHOUSE <WAREHOUSE> TO ROLE <ROLE_NAME>;

-- =====================================================
-- SECTION 4: DATABASE AND SCHEMA ACCESS
-- =====================================================
-- * Grant USAGE on all databases for INFORMATION_SCHEMA access
GRANT USAGE ON ALL DATABASES IN ACCOUNT TO ROLE <ROLE_NAME>;
GRANT USAGE ON ALL SCHEMAS IN ACCOUNT TO ROLE <ROLE_NAME>;

-- * Grant USAGE on future databases and schemas
GRANT USAGE ON FUTURE DATABASES IN ACCOUNT TO ROLE <ROLE_NAME>;
GRANT USAGE ON FUTURE SCHEMAS IN ACCOUNT TO ROLE <ROLE_NAME>;

-- * Tables metadata (replace <DATABASE_NAME>)
/*
SELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.TABLES ORDER BY created DESC;
*/

-- * Columns metadata (replace <DATABASE_NAME>)
/*
SELECT * FROM <DATABASE_NAME>.INFORMATION_SCHEMA.COLUMNS ORDER BY table_name, ordinal_position;
*/
2. Lineage Access Configuration
To enable query history and lineage extraction in DvSum, the user must have access to Snowflakeâ€™s
ACCOUNT_USAGE
views and the appropriate privileges.
As with the previous section, the
goal
is for the user to be able to successfully run the test queries below. If that works, lineage-related access is correctly configured. The grants script provided afterward is a
reference
, not a requirement to follow exactly.
Test Queries â€“ Required for Lineage Extraction
-- * Query history (all users, last 30 days)
SELECT *
FROM snowflake.account_usage.query_history
WHERE start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY start_time DESC;

-- * Views metadata
SELECT * FROM snowflake.account_usage.views WHERE deleted IS NULL;

-- * Functions metadata
SELECT * FROM snowflake.account_usage.functions WHERE deleted IS NULL;

-- * Stored procedures metadata
SELECT * FROM snowflake.account_usage.procedures WHERE deleted IS NULL;
Sample Grants Script â€“ Lineage Access
The following script is intended as a reference to help grant the minimum required privileges for enabling query history and lineage features in DvSum.
If your existing setup already supports the test queries mentioned earlier, you may skip this step.
Otherwise, use this script as a baseline and adapt it to align with your internal roles, naming conventions, and security policies.
-- * Switch to ACCOUNTADMIN for account-level privileges
USE ROLE ACCOUNTADMIN;

-- ===========================================================
-- SECTION 5: ACCOUNT-LEVEL PRIVILEGES FOR LINEAGE METADATA
-- ===========================================================
-- * Grant MONITOR privilege for query history access across all users
GRANT MONITOR ON ACCOUNT TO ROLE <ROLE_NAME>;

-- * Grant access to SNOWFLAKE.ACCOUNT_USAGE views
GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE <ROLE_NAME>;

-- =====================================================
-- SECTION 6: VERIFICATION QUERIES
-- =====================================================
-- * Query history (all users, last 30 days)
SELECT *
FROM snowflake.account_usage.query_history 
WHERE start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY start_time DESC;

-- * Views metadata 
SELECT * FROM snowflake.account_usage.views WHERE deleted IS NULL;

-- * Functions metadata
SELECT * FROM snowflake.account_usage.functions WHERE deleted IS NULL;

-- * Stored procedures metadata
SELECT * FROM snowflake.account_usage.procedures WHERE deleted IS NULL;
Steps to Configure Snowflake as a Source
Step 1: Add Snowflake as a Source
Go to the Administration dropdown and select the
Data Sources
tab.
Click the
Add Source
button.
In the modal that appears:
Select
Snowflake
as the data source.
Provide a meaningful
Source Name
.
Click
Save
.
Step 2: Configure Connection Settings
After saving, you will be redirected to the
Connection Settings
page.
Enable the checkbox for
On-premise Web Service
(if applicable).
Select the
SAWS
(Secure Agent Web Service) that is set up and currently running.
Enter the required connection details:
URL
Warehouse
Database Login
Password
Click
Authenticate
to validate the connection.
Note
: By Default the SAWS type will be cloud. For more information regarding Cloud SAWS, click
here
Step 3: Select Database & Schema
Once authenticated, the
Database Selection
section appears.
Choose the database you need to scan (only one database can be selected).
Choose one of the following options:
Scan
all schemas
(leave the schema checkbox unchecked).
Scan
specific schemas
(check the schema checkbox and select schemas from the list).
Once it is checked then the list of available schemas will be displayed. User can select single or multiple schemas from the Available Schemas list and move them to the Selected Schemas tab on the right.
Step 4: Save the Source Configuration
Scroll to the top of the page.
Click the Done button in the top-right corner.
Click Save to complete the setup.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
Step 5: Run a Scan
Navigate to the
Scan History
page.
Click the
Scan Now
button.
A job will be created, and once the status shows
Completed
, the scan will be successful.
After the scan completion, click on Scan Name and it will open the Scan Summary page of this scan.
On the Scan Summary page, it will show all the insights of the scan i.e how many new tables and columns are fetched in this scan from the schemas we selected earlier.
Review Database Tables
Navigate to the Dictionary tab from the sidebar.
Click on the Recently Refreshed tab.
This tab displays all tables fetched in the most recent scan.
Click on the table name to access more detailed metadata and structure information.
By following these steps, you can successfully integrate Snowflake as a source in DvSum, enabling advanced data profiling and cataloging capabilities.
Video tutorial:
Watch this quick video tutorial of how to add and configure an Snowflake source into DvSum app.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Microsoft SQL Server as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/10618954602004-Configure-Microsoft-SQL-Server-as-a-Source
================================================================================

In this Article:
Overview
Step-by-Step Configuration Guide
Add a New Data Source
Configure Connection
Option 1: Connect Using DvSum Web Service (Recommended)
Option 2: Connect Using DvSum Edge Gateway (For On-Premises SQL Server)
Select Database & Schemas
Scan the Data Source
Browsing the Data Catalog
Video Tutorial
Additional Resources
Overview
This article describes the steps needed to configure Microsoft SQL Server as a source in DvSum Data Catalog (DC). The same steps apply to configure a source in DvSum Data Quality (DQ) with only a slight variation.
Step-by-Step Configuration Guide
1. Add a New Data Source
Navigate to
Administration
â†’
Data Sources
.
Click
âŠ• Add Source
.
Select
Microsoft SQL Server
.
Provide a name for the source and click
Save
.
2. Configure Connection
Once the source is saved, the connection settings detail page appears.
Option 1: Connect Using DvSum Web Service (Recommended)
Whitelist the DvSum application
by IP address as indicated in the settings.
Enter the required connection details:
Host
: SQL Server hostname or IP.
Port
: Default is 1433 unless changed.
Instance Name
: (if applicable).
DB Login & Password
: Use SQL Authentication credentials.
Click
Authenticate
to verify credentials.
Option 2: Connect Using DvSum Edge Gateway (For On-Premises SQL Server)
Install
install a DvSum Edge Gateway
behind the firewall.
Connect to SQL Server using the installed web service.
Authenticate the source.
3. Select Database & Schemas
Once authenticated:
Choose the
Database
from the available list.
(Optional) Limit the scan to specific schemas by selecting them.
Click
Done
(top-right corner) and then
Save
.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
4. Scan the Data Source
Navigate to
Scan History Page
.
Click
Scan Now
.
Once the scan completes, the status will change to
Completed
.
Click on the
Scan Name
to view the Scan Summary.
The Scan Summary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this scan.
Your SQL Server connection is now fully configured and functional.
Browsing the Data Catalog
Navigate to
Data Dictionary
.
Click the
Recently Refreshed
tab.
View newly discovered tables and their details.
Video Tutorial
Watch this quick video tutorial of how to add and configure an Microsoft SQL Server source into DvSum app.
Additional Resources
Read more about
DvSum Web Service (Cloud SAWS)
Read more about
DvSum Edge Gateway (On-Premises SAWS)

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Oracle as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/10583264674836-Configure-Oracle-as-a-Source
================================================================================

In this Article:
Overview
Prerequisites
Step by Step configuration
Add Source
Configure Connection
Method 1: Connect Using DvSum Web Service
Method 2: Connect Using DvSum Edge Gateway
Save the Connection
Scan the data source
Browsing the Database Columns
Lineage Support for Oracle
Query History in Oracle
Additional Resources
Overview
This article describes the steps needed to configure Oracle as a source in DvSum Data Intelligence (DI). The same steps apply to configure a source in DvSum Data Quality (DQ Legacy) with only a slight variation.
Prerequisites
Oracle configuration
User
: Create a user to be used in DvSum, or identify an existing user.
Table permissions
: Grant read-only access to the user for schemas and tables that you would like to catalog and profile.
System table permissions:
Grant read-only access to the user for the following system tables:
V$SQL
DBA_USERS
ALL_USERS
ALL_OBJECTS
ALL_TABLES
ALL_SYNONYMS
ALL_TAB_COMMENTS
ALL_EXTERNAL_TABLES
ALL_TAB_COLS
ALL_COL_COMMENTS
ALL_CONSTRAINTS
ALL_CONS_COLUMNS
In order to grant access the below statement can be used:
GRANT SELECT ON [TABLE_NAME] TO [USER]
Oracle GRANT documentation
Step by Step configuration
1. Add Source
To create a data source, navigate to Administration â†’ Data Sources â†’ âŠ•Add Source.
Select Oracle.
Give the source a name, and save it.
2. Configure Connection
Once the source is saved, you will be redirected to the connection settings page.
Method 1: Connect Using DvSum Web Service
Whitelist the DvSum application by IP address as indicated.
Enter the required connection details:
Host
Port
SID or Service Name
Database Login
Database Password
Click
Authenticate
to verify the connection.
Method 2: Connect Using DvSum Edge Gateway
If your Oracle database is behind a firewall,
install a DvSum Edge Gateway
to enable a secure connection.
Learn more about the differences in
DvSum Web Service vs On-Premises Edge Gateway
.
Once authenticated, the
Database section
will appear.
Select the schemas to catalog.
3. Save the Connection
After entering credentials and selecting schemas, scroll to the top of the page.
Click
Done
in the top-right corner.
Click
Save
to finalize the connection setup.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
4. Scan the data source
Navigate to the
Scan History
page.
Click
Scan Now
to initiate a scan.
Once the scan is complete, the status will update to
Completed
.
Click on the
Scan Name (e.g., SCN-000286)
to view the scan summary.
The
Scan Summary Page
provides insights into the scan, including:
The number of new tables and columns fetched.
Your Oracle connection is now fully configured and functional.
Browsing the Database Columns
After a successful scan, explore the discovered tables and columns:
Go to the Dictionaries dropdown and select the
Database Columns
tab.
Click on the
Recently Refreshed
tab to see the latest scanned tables.
Click on a table name to view detailed metadata.
Lineage Support for Oracle
DvSum supports lineage extraction for Oracle sources using query history and stored procedure or package code analysis.
To enable this:
Ensure the Oracle user has
read-only access
to the
ALL_SOURCE
view to allow access to stored procedure or package definitions.
All relevant schemas containing the procedure or package code must be included in the source configuration.
For detailed instructions on how to set this up, refer to the article:
Lineage in DvSum
Query History in Oracle
Query history is enabled by default for Oracle, so no additional configuration is required. DvSum automatically scans query history for lineage analysis.
To verify that query history is being captured, you can run the following SQL query:
SELECT v.*
FROM v$sql v
LEFT JOIN all_users du ON v.PARSING_USER_ID = du.user_id
WHERE command_type IN (1, 2, 6, 7, 189)
AND ROWNUM < 10;
Note
: This query returns recent DML operations using Oracle's
v$sql
view and can be used to confirm that query history is available to DvSum.
Additional Resources
Read more about
DvSum Web Service vs On-Premises Gateway
Read more about
Gateway Installation

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Microsoft Power BI as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/8960509886868-Configure-Microsoft-Power-BI-as-a-Source
================================================================================

In this Article:
Overview
Prerequisites
Step-by-Step Configuration
Confirm Availability of Datasets and Reports
Add Power BI as a Source in DvSum
Configure Connection Settings
Save the Connection
Scan the Data Source
Browsing the Data Catalog
Troubleshooting Tips
Video Tutorial
Next Steps
Additional Resources
Overview
This guide walks you through setting up Microsoft Power BI as a data source in DvSum, enabling seamless data integration, governance, and analysis. By adding Power BI as a source, you can scan reports and datasets, making them available for documentation, curation, and tracking lineage from their originating data sources.
Prerequisites
Before proceeding, ensure you have the following:
A valid Power BI account with appropriate permissions.
The Client ID, Client Secret, and Tenant ID for your Power BI application.
Access to the Power BI API for seamless integration.
A DvSum account with access to Data Catalog (DC) or Data Quality (DQ).
Step-by-Step Configuration
1. Confirm Availability of Datasets and Reports
Log in to your Power BI account.
Ensure that reports and datasets are available and accessible based on your user permissions.
2. Add Power BI as a Source in DvSum
Log into DvSum Data Catalog (DC).
Navigate to
Administration
â†’
Data Sources
â†’
Add Source
.
In the pop-up window:
Select
Power BI
as the source.
Provide a
Source Name
.
Click
Save
.
3. Configure Connection Settings
Once a new Power BI source is created, you will be redirected to the connection settings detail page of the new source. In the Credentials section in Connection settings, enter these three properties:
Client ID
Client Secret
Tenant ID
After entering the credentials, authenticate the source.
Once the source is authenticated, a
"Workspaces"
section will appear below the Authenticate button. By default, all workspaces will be scanned.
To limit the scan to specific workspaces, check the
"Limit to specific workspaces"
option. This will enable two tabs:
"Available Workspaces"
and
"Selected Workspaces"
, allowing you to choose which workspaces to include in the scan. You can also choose whether to
include personal workspaces
by selecting the corresponding checkbox.
If "Limit to specific workspaces" is checked, the workspaces will be listed in the â€œAvailable Workspacesâ€ tab. Select workspaces that are to be scanned, and move them to the â€œSelected Schemasâ€ tab.
4. Save the Connection
To save, scroll up to the top. From the top right corner click on the â€œDoneâ€ button.
After that click the â€œSaveâ€ button. The source will get saved successfully.
Note:
When adding or editing a Data Source, if incorrect details or invalid credentials are entered, it will still allows to save the Data Source. However, authentication will fail, and the Data Source will be marked as unusable with a red icon. The Data Source will not be usable until the correct credentials are provided.
The connection is saved, but it cannot be used until valid authentication details are updated.
5. Scan the Data Source
Navigate to the
Scan History Page
.
Click
Scan Now
to initiate the scan.
Once completed, the status will change to
Completed
.
Click on the
Scan Name
to open the Scan Summary page.
The summary will show the number of reports and datasets fetched.
Navigate to Scan History Page and click the "Scan Now" button. When the scan is complete, the status will change to "Completed".
After the scan completes, click on Scan Name and it will open the Scan Summary page of this scan.
The Scan Summary page displays all the insights of the scan. It indicates how many new workbooks and datasets were found in the scan. In the screenshot below it shows that 9 Reports and 5 Datasets were found.
Your Power BI connection is now fully configured and functional.
Browsing the Data Catalog
Semantic Models
Now, letâ€™s analyze the semantic models that we got from this scan. Navigate to the Assets Dictionary on the left sidebar and apply the asset type filter "BI Semantic Model". You will be able to see all the semantic models. From there, go to the Â "Recently Refreshed" tab and select the semantic models that we have got in the recent scan.x
The Semantic Model can further have BI Tables if they were created in it, and the list of BI tables can be seen on the detail page of the Semantic Model:
The BI table detail page has an "Overview" tab, which contains all the properties of the BI tables, and a "Data" tab, which will contain the BI fields of the Table. Users can also see the lineage on BI Table if it exists:
Reports
From the BI Reports listing, select a recently scanned Report, and the user will be redirected to the detail page of the Report. The screenshot below shows what the Reports detail page looks like.
Please note that only BI Reports have a separate listing page; for other BI assets, users will have to go to the Assets dictionary and apply the asset type filter
On the detail page, a description of the report is fetched from the Power BI report. This means this report has the same description in Power BI, and the same description is fetched and shows up here. The author of the report is basically the person who created this report in Power BI, so this value is also basically fetched. Further, we have a lineage of the Report, which actually shows us that the current Report is derived from which data set and which tables are connected to that data set.
Dashboards
From the Assets dictionary listing, users can apply the asset type filter "BI Dashboard," and all the dashboards (if there were any) can be seen on the listing page. The screenshot below shows what the dashboard detail page looks like:
Troubleshooting Tips
If the test connection fails, confirm that prerequisites are complete and API validation has been performed (see
[Configure Microsoft Power BI Permissions]
).
Ensure the Client Secret is valid and not expired.
Verify that metadata scanning is enabled in Power BI.
Video Tutorial
Watch this quick video tutorial on how to add and configure a Power BI source into DvSum app.
Next Steps
For setting up permissions in Power BI:
[
Configure Microsoft Power BI Permissions
]
Additional Resources
Read more about
DvSum Web Service (Cloud SAWS)
Read more about
DvSum Edge Gateway (On-Premises SAWS)

--------------------------------------------------------------------------------

================================================================================
TITLE: Supported Connectors/Data Sources
URL: https://dvsum.zendesk.com/hc/en-us/articles/6067542488724-Supported-Connectors-Data-Sources
================================================================================

Introduction
DvSum supports a wide range of connectors to help users integrate their data seamlessly. This article provides a comprehensive list of supported sources and key details for each. Whether you are working with databases, cloud data warehouses, or enterprise applications, DvSum ensures smooth connectivity to enhance data governance, quality, and analytics.
What is a DvSum Connector?
A DvSum Connector is a unified service that encapsulates native data connection drivers (e.g., JDBC, ODBC, SAP JCo, etc.), a scheduler, connection routing, security, and a server to handle requests. A single connector can service multiple data sources and users, providing a flexible and scalable solution for data integration.
Supported Data Sources
DvSum can connect to
150+ data sources
, including:
Enterprise Applications
(SAP, NetSuite, Salesforce)
SaaS Applications
(Google Analytics, HubSpot, Zendesk)
On-Premises Databases
(Oracle, SQL Server, PostgreSQL, MySQL)
Cloud Databases
(Snowflake, BigQuery, Amazon Redshift, Azure SQL)
Data Lakes
(AWS S3, Azure Data Lake, Google Cloud Storage)
File-Based Sources
(CSV, Excel, Google Sheets, Shared Drives)
If your application is not listed here, DvSum can build a connector to your application as long as it supports
external application access through JDBC or ODBC connectivity
.

--------------------------------------------------------------------------------

================================================================================
TITLE: DvSum Web Service vs On-Premises Gateway
URL: https://dvsum.zendesk.com/hc/en-us/articles/16448743421716-DvSum-Web-Service-vs-On-Premises-Gateway
================================================================================

In this Article
What is DvSum Web Service?
IP Whitelisting
Before IP whitelisting
After IP whitelisting
Advantages Over Local On-Premises SAWS
What is DvSum Web Service?
DvSum Web Service (sometimes referred to as Cloud SAWS) is an online connector designed to facilitate seamless connectivity to sources available online. It provides analogous functionality to the on-premises gateway (sometimes referred to as SAWS), but it eliminates the need for users to install the connector locally on their machines. The DvSum Web Service enables users to establish connections with various online sources, such as APIs, databases, or services, without the burden of setting up and managing local installations.
By leveraging the DvSum Web Service, users can access and interact with online sources directly, streamlining the process of retrieving data or integrating with external systems. The connector is hosted in the cloud, offering convenience, scalability, and ease of use for users. They can utilize Cloud SAWS to securely connect to authorized online sources, leveraging its capabilities to fetch data, make requests, or perform desired actions without the need for local installation.
DvSum connects to data sources using one of these:
DvSum Web Service
On-Premises Gateway
In this article, we will be covering the process through which a source can be connected using the DvSum Web Service and how this compares to using an on-premises gateway.
By default when any source is added, it will be connected using the DvSum Web Service, so there is no extra step required for connection. But if the user chooses to authenticate using the on-premises gateway then they will need to enable the checkbox of On-premises Web Service.
For Configuring of Data Source (Snowflake) using On-Premises Local Saws, click
here
IP Whitelisting:
For the Sources in which any Inbound Connection is not allowed or requires VPN for the connection to be made will be required to whitelist the given IP address in order to authenticate the credentials for that source. Currently, there are two sources for which IP whitelisting will be required:
Microsoft SQL Server
Oracle (Host selected as "qasaws.dvsum.com")
Once the IP address is whitelisted the network firewall will allow the particular connector to be able to connect with that particular source.
Before IP whitelisting
After IP whitelisting
Note
: For whitelisting, users will be required to whitelist the DvSum IP where the organization's data source has been hosted. Admin of that source will be authorized to do that.
After Successful Authentication, the user can choose any database and save the changes and scan that particular source to bring in the tables:
Advantages Over Local On-Premises SAWS
No Prior Installations Required:
In the case of Local On-premises SAWS, it should be locally installed and running but in the case of Cloud SAWS, no installation is required.
Much Faster than Local On-Premises SAWS:
Scans that run on Cloud SAWS take less time as compared to the scan when it is run using the local on-premises SAWS.

--------------------------------------------------------------------------------

================================================================================
TITLE: DvSum Edge Gateway Installation (SAWS) for Data Catalog and CADDI on Linux
URL: https://dvsum.zendesk.com/hc/en-us/articles/14609957062164-DvSum-Edge-Gateway-Installation-SAWS-for-Data-Catalog-and-CADDI-on-Linux
================================================================================

In this Article
Overview
Detailed Steps
Network Configuration
Linux Server Prerequisites
Additional Information
Installation Video
Overview
This document describes the installation process for the DvSum Edge Gateway for DvSum Data Catalog and CADDI (DC). It covers all aspects of the installation including middleware needed and how to deploy the Gateway on a Linux machine.
For Windows, refer to
DvSum Edge Gateway Installation (SAWS) for Data Catalog and CADDI
.
Note:
Gateway, Connector, SAWS (Stand-Alone Web Service) and webservice are used interchangeably referring to the DvSum Edge Gateway.
Detailed Steps
Network Configuration
If the DvSum Gateway is installed on a server with restricted outside access to specific URLs, you'll need to whitelist the following addresses on the network. This will enable the DvSum Gateway to communicate with the DvSum SaaS application and ensure the service functions properly and securely connects to the necessary resources on the network. Please configure the network settings accordingly to allow for this communication.
URLs that must be whitelisted for outbound access on port 443:
https://apis.dvsum.ai
wss://3k7tif2dyi.execute-api.us-west-2.amazonaws.com/prod
https://dvsum-app-data-prod.s3.amazonaws.com
Ports that must be open for inbound access from end user computers:
Default: 8183
Common alternative: 443
Linux Server Prerequisites
Ensure that your Linux distribution has systemd support and x86_64 server architecture.
# Confirm Architecture
lscpu
# Expected output includes:
# Architecture: x86_64
The minimum memory requirement is 2GB (with 4GB recommended).
The minimum disk space requirement is 1GB (with 5GB recommended).
Download Link for
tar.gz
file
Step 1:
Download the DvSum service
tar.gz
file from the provided link by DvSum Support.
Step 2:
Copy the downloaded tar.gz file to your Linux server.
Create a new directory, e.g., DvSum_Service, and copy the downloaded tar.gz into that directory.
Step 3:
Navigate to the directory where you have copied the tar.gz file and unzip it.
Use the following command: "tar -xzf DvSum_1.1.11.tar.gz"
Step 4:
In the DvSum application, create the DvSum gateway and copy the communication key.
Step 5
: Update the configuration.properties file with the copied communication key.
The default port will be set as 8183.
Step 6:
To install the DvSum service, execute the shell script "install_dvsum_service.sh".
Additional Information
Installation Video
Video tutorial link
Video tutorial on how to install the DvSum Gateway on Linux.
Linux Configuration
It can be useful/desirable to connect to the gateway on the standard SSL port 443. There are multiple ways to achieve this. One common configuration technique is to redirect incoming traffic on port 443 to the DvSum Gateway listening on port 8183 using iptables.
sudo iptables -t nat -I PREROUTING -p tcp --dport 443 -j REDIRECT --to-ports 8183
Troubleshooting
Confirm that your gateway has started successfully and is now listening on port 8183.
netstat -at
# If the machine has many services, it can be useful to filter the output of netstat
netstat -at | grep 8183
Confirm that the machine is able to reach the DvSum SaaS application (confirm that it's not blocked by a firewall).
# Sample commands to confirm connectivity
curl -v --tlsv1.2 https://apis.dvsum.ai/catalog/scan/sockets-url
curl -v --tlsv1.3 https://dvsum-app-data-prod.s3.amazonaws.com

--------------------------------------------------------------------------------

================================================================================
TITLE: DvSum API Access and Authentication Management
URL: https://dvsum.zendesk.com/hc/en-us/articles/41740390031764-DvSum-API-Access-and-Authentication-Management
================================================================================

In This Article:
Overview
Authentication Methods
Generate and Store Credentials
Rotate API Credentials
Download New Credentials
Delete API Clients
View Assigned APIs
User Roleâ€“Based API Access
Using API Documentation & Authentication Flow
Overview
DvSum provides a robust set of APIs that allow external systems to interact with the platform. These APIs can be used to:
Manage data elements (tables, fields, terms, etc.)
Trigger approval workflows
Automate repetitive tasks
Authentication is handled through
AWS Cognito client credentials
, ensuring secure machine-to-machine communication. This allows seamless integration with enterprise processes while maintaining high security standards.
Refer to
DvSum API Documentation
for comprehensive API documentation.
Authentication Methods
DvSum supports
two OAuth2-based authentication methods
for accessing its APIs.
Both methods use
AWS Cognitoâ€™s
client_credentials
flow
to obtain an
access_token
, and both provide access to the
same set of APIs.
The key difference lies in
who owns the credentials
and
how permissions (RBAC) are assigned.
Method 1: Service Authentication (Machine-to-Machine)
Uses
OAuth2 Client Credentials flow
via AWS Cognito.
Credentials (
client_id
and
client_secret
) are created under
Administration â†’ Account â†’ Application Security â†’ Manage OAuth2 Clients
.
Represents a
service principal
with its own identity.
Permissions are
assigned directly to the auth client. No roles involved.
Ideal for
automated workflows, background jobs, or system integrations
without user involvement.
Method 2: User-Based Authentication
Also uses
OAuth2 Client Credentials flow
via AWS Cognito.
Requires updating the
role
to allow
API Access
and associating
scopes
with the role.
Credentials (
client_id
and
client_secret
) are created under
My Settings â†’ API Credentials
for individual users.
Tokens inherit
user-specific roles and permissions
via RBAC.
Suitable for scenarios requiring
user context, auditing, or personalized access control.
Add a Client
Navigate to
Administration â†’ Account â†’ Application Security â†’ Manage OAuth2 Clients
and click
Add
.
Definition
Provide a
Name
for the OAuth client and an optional
Description
.
Set the
Secret Rotation Interval
to define the duration after which the credentials expire. The interval can be set from
7 days to 1 year
.
Alternatively, select
Never Expires
. A warning message highlights that indefinite credentials increase security risks, and regular rotation is recommended.
These are machine-to-machine credentials, that are not tied to any user account. They act as independent identities for scripts or background processes accessing DvSum APIs.
Scope
Select which APIs the client should have access to:
All Scopes
â€“ grants access to all APIs.
Specific Scopes
â€“ grant access only to selected APIs.
Access Level
:
Read
access is enabled by default.
Write
access can be enabled where applicable (e.g., Asset Listing).
Configure Alerts
In the
Alert
tab, you can configure notifications to be sent when credentials are about to expire.
Internal Recipients
:
Select from available
Groups
or
Users
and move them to the
Selected
list.
The OAuth client creator is automatically added by default.
External Recipients
:
Enter comma-separated emails (e.g.,
example@email.com
).
Only emails from
whitelisted domains
are allowed.
Expiry Alerts
: Expiration alerts are sent
3 days before credential expiry
, then repeated daily until credentials are refreshed or rotated.
Generate and Store Credentials
Review all settings.
Click
Save
to generate the
Client ID
and
Client Secret
for authentication.
Important:
Credentials can only be viewed once. After closing the modal, they cannot be retrieved again.
Download and store them securely (e.g., password manager or encrypted vault).
The downloaded CSV file contains both the Client ID and Client Secret, as shown below.
Rotate API Credentials
If credentials are compromised or nearing expiry, you can
rotate
them.
Click the
More Options (â‹®)
menu for the client.
Select
Rotate
.
Confirm Rotation
A message will confirm that new credentials will be generated.
Old credentials remain valid for
24 hours
, giving time to update integrations.
After 24 hours, old keys are automatically disabled.
Download New Credentials
Once rotated, new credentials are issued.
Copy or download them immediately â€” they cannot be viewed again after closing.
Note:
Remember to update all applications and services with the
new Client ID/Secret
before the old ones expire.
Delete API Clients
You can delete one or multiple OAuth2 clients if they are no longer required. Deletion immediately revokes access for the associated Client ID and Secret.
Delete an Individual Client
Click the
More Options (â‹®)
menu next to the client.
Select
Delete
.
Confirm the deletion.
Bulk Delete Multiple Clients
Use the
main checkbox
at the top of the OAuth2 client list to select all displayed clients, or select individual checkboxes for multiple clients.
Once selected, the
Delete
option will appear at the top of the list.
Click
Delete
and confirm to remove all selected clients.
Note:
Deleting an OAuth2 client is irreversible. Any applications or integrations using the deleted credentials will immediately lose access. If access is still required, create a new client and update your systems with the new credentials.
View Assigned APIs
On the
Manage OAuth2 Clients
page, the
Resources
column shows how many APIs are assigned to each client.
Clicking the API count (e.g.,
â€œ6 APIsâ€
) opens a detailed view of the assigned scopes.
The detailed view lists all APIs granted to the client, along with their
access levels
(Read, Write, Delete).
This helps administrators quickly verify which endpoints each client can access and adjust scope if needed.
Summary of Service-Based Authentication
Use the generated credentials to authenticate via
OAuth2 Client Credentials flow
.
Refer to the
API documentation
for details on endpoints, request formats, and sample payloads.
User Roleâ€“Based API Access
In addition to service-based authentication clients, DvSum also supports
user roleâ€“based credentials
. This allows administrators to enable API access tied directly to a userâ€™s role.
Enable API Access for a Role
Navigate to
Administration â†’ User Management â†’ User Roles
.
Either add a new role or edit an existing one.
Go to the
API Access
tab.
Select
Enable API Access
.
Configure Role-Based API Access
Set a
Secret Rotation Interval
(e.g., 180 days) or choose
Never Expires
(not recommended).
Select the
APIs
this role should have access to.
By default, any APIs already available to the role in the UI are pre-selected.
Additional APIs can be enabled as needed.
Save the configuration.
Generate and Manage User Credentials
Once API access is enabled for a role:
Users assigned to that role will see a new
API Credentials
tab under
My Settings
.
From there, each user can:
Generate
their own Client ID and Secret.
Rotate
their credentials when needed.
Delete
their credentials if they are no longer required.
Notes:
The
expiry interval
is set at the role level by administrators. All users in that role inherit the same expiry.
Users
cannot change expiry
, but they can generate, rotate, and delete their own credentials.
The API access scope always reflects the permissions granted to the user role. If role permissions change, the user access changes automatically.
Once generated, users authenticate using the same flow as service-based clients: generate an
access token
via the Authentication API and use it in subsequent API calls.
API Credentials Tab Availability
By default, the
API Credentials
option under
My Settings
will appear
greyed out
for users.
To activate this tab, the user must be part of a
User Group
that is assigned to a
User Role with API Access enabled
.
API Credentials disabled before Role Assignment:
API Credentials Enabled after Role Assignment:
Manage Generated Credentials
Once the
API Credentials
tab is active and the user generates their credentials:
Download
: Just like with service-based clients, credentials must be
downloaded immediately
. They cannot be viewed again once the modal is closed.
Rotate
: Users can rotate their own credentials to issue a new secret. The old secret remains valid for 24 hours, after which it is automatically disabled.
Delete
: Users can delete their credentials if they are no longer required. A new set can be generated at any time as long as their role has API access enabled.
Note:
Rotation and deletion are managed at the
user level
, but
expiry
is controlled by the
role configuration
set by administrators.
Using API Documentation & Authentication Flow
All available DvSum APIs are listed in the official API documentation:
DvSum API Documentation
You can download the collection from this site and import it into
Postman
or another API client.
Before You Begin (Pre-requisites)
Before running the authentication request, make sure you have the following set up in
Postman
:
Create an Environment (if not already set):
Go to the
Environments
tab in Postman and create a new environment.
Add a variable named
authUrl
with the value:
auth.dvsum.ai
Click
Set active
to activate the environment.
Set up Headers for Authentication:
In the
Headers
tab of your authentication request (
/oauth2/token
), ensure the following keys and values are added:
Content-Type: application/x-www-form-urlencoded 
Accept: application/json 
Authorization: Basic <base64_encoded_clientid_secret>
Once these pre-requisites are complete, proceed with
Step 1: Authenticate with Client Credentials
as described below.
Step 1: Authenticate with Client Credentials
Use the
Client ID
and
Client Secret
you generated earlier to obtain an
access token
.
Open the
Authentication API
(first in the documentation).
Combine your Client ID and Secret in the format:
client_id:client_secret
Convert this string into
Base64 encoding
Once generated,
copy the encoded value
and
paste it into the Authorization header
in Postman as shown below:
Authorization: Basic <base64_encoded_clientid_secret>
When you send the request, the API will return an
access token
.
Step 2: Use the Access Token
The access token is a
Bearer token
valid for
1 hour
.
Copy this token and include it in the header for all subsequent API requests:
Authorization: Bearer <your_access_token>
Step 3: Run an API Example
Once you have an access token, you can use it to call APIs that your client/role has access to.
Example request:
GET {{baseUrl}}/node-listing/gov-view?node_type=TBL&node_id=40837560
Headers:
Accept
:
application/json
Authorization
:
Bearer <your_access_token>
Running
Get Governance Views
API in Postman:
A successful call returns
200 OK
with the response payload.
Important Notes
Access is restricted to the APIs that were explicitly granted in the
Scope
tab when creating the client.
Tokens must be refreshed every hour using the same authentication API.
Tip:
Import the full Postman collection from the
DvSum API Documentation
for comprehensive examples of all available APIs.

--------------------------------------------------------------------------------

================================================================================
TITLE: Logical View
URL: https://dvsum.zendesk.com/hc/en-us/articles/44352096354964-Logical-View
================================================================================

In This Article
Overview
Logical View as an Asset
Creating a Logical View
Role of SQL Definition
Viewing a Logical View
When to Use a Logical View
Summary
Overview
A
Logical View
is an asset in DvSum that represents a
logical dataset defined using a SQL query
, independent of physical database tables.
While tables in DvSum are created directly from database metadata, Logical Views allow users to define datasets at a
logical or schema level
. This makes them useful for modeling filtered, derived, or analytical datasets that do not exist as standalone physical tables.
Logical Views behave like datasets within DvSum and can be viewed, managed, and used similarly to other assets.
Logical View as an Asset
Logical View is available as a
system asset type
and can be created from
Asset Management
.
These are visible in the
Asset Dictionary
and can be browsed and managed like other assets.
Creating a Logical View
To create a Logical View:
Navigate to
Asset Management
Click
Add Asset
Select
Logical View
as the Asset Type
Select the
Schema
Provide a
Name
and optional
Description
Save the asset
After saving, the Logical View opens in its dataset view.
Role of SQL Definition
The
SQL Definition
defines the query used to construct the Logical View.
Users can enter a SQL query that selects and filters data from one or more underlying tables or views. The result of this query represents the dataset exposed by the Logical View.
After entering the SQL query, click
Validate
to verify that the query is valid and can be executed successfully.
If validation is successful, a confirmation message is displayed.
A SQL definition can be used to:
Select specific columns
Filter records based on conditions
Apply business logic to shape the dataset
Once validated, the SQL definition can be saved and used as the Logical Viewâ€™s data source.
Viewing a Logical View
After saving and validating the SQL definition, the Logical View behaves like a dataset within DvSum.
The
Overview
tab displays asset-level details
The
Data
tab shows the dataset defined by the SQL
The
Data Quality
tab allows data quality rules to be applied
When to Use a Logical View
Logical Views are useful when:
Data needs to be represented logically rather than physically
A dataset is derived using filters or transformations
Physical tables do not align with analytical or validation needs
You want to apply data quality rules on a SQL-defined dataset
They provide flexibility without requiring changes to the underlying database.
Summary
Logical Views extend DvSumâ€™s asset model by allowing users to define and manage
SQL-based logical datasets
independently of physical database tables.
They enable flexible data modeling, support analytical use cases, and integrate seamlessly with existing dataset and data quality workflows.

--------------------------------------------------------------------------------

================================================================================
TITLE: Getting Started with Custom Assets and Attributes in DvSum DI
URL: https://dvsum.zendesk.com/hc/en-us/articles/38888864440340-Getting-Started-with-Custom-Assets-and-Attributes-in-DvSum-DI
================================================================================

This article provides quick access to documentation related to custom asset and attribute configuration in the DvSum DI Application. Use the links below to navigate to the full articles.
Creating and Managing Custom Asset Types
Learn how to define custom asset types, configure relationships, and enrich assets within the application.
Defining and Configuring Custom Attributes
Understand how to create, configure, and control visibility of custom attributes across asset types.
These resources will help you tailor the metadata experience to your organizationâ€™s governance and data management needs.

--------------------------------------------------------------------------------

================================================================================
TITLE: Managing Custom Assets in DvSum
URL: https://dvsum.zendesk.com/hc/en-us/articles/38663390098964-Managing-Custom-Assets-in-DvSum
================================================================================

In This Article:
Overview
Defining a Custom Asset Type
Customizing the Asset Type
Configuring Relationships for an Asset Type
Adding a Custom Asset
Editing and Enriching the Asset
Overview
The DvSum DI Application supports
Custom Assets
, allowing organizations to define and manage asset types that align with their unique data landscape. This capability enables teams to create asset categories tailored to specific governance frameworks, business processes, and domain requirements.
Custom Assets are fully integrated into the DvSum platformâ€”they appear in the asset feed, support Custom Attributes, and participate in workflows, lineage, and search. This ensures a flexible and consistent metadata experience across the catalog.
Defining a Custom Asset Type
Before creating a Custom Asset, the asset type must be defined. To do this:
Navigate to the
Asset Management
section from the left-side menu.
Click on
Asset Types
.
Click the
Add
button at the top right.
On the listing page, a predefined set of default asset types is displayed. These system-provided assets are visible even if no custom assets have been added. Any newly created custom assets will appear alongside these default entries.
This opens the
Add Asset Type
pop-up.
Enter the required details for the new asset type.
Click
Add
to save.
Customizing the Asset Type
Once the asset type is created, the
Asset Type Detail
page is displayed. This page acts as a
template
for all assets created using this type. Any configurations made hereâ€”such as custom attributes, tags, or relationshipsâ€”will automatically apply to all assets of this type. Changes made to the asset type template will reflect across all associated assets.
Add or edit
Custom Attributes
Define
Tags
Set up
Relationships
with other asset types
These configurations allow you to enrich your custom asset with relevant metadata, improving its discoverability, traceability, and integration within workflows.
Configuring Relationships for an Asset Type
As part of configuring a custom asset type, you can define how it relates to other assets in the system using the
Relationships
section.
Navigate to the
Relationships
tab on the asset type detail page.
You can
add multiple relationship types
, each with a:
Relationship Type Name
(e.g.,Â Contains,Â Governed By)
Target Asset Type
(e.g.,Â BI Model and Report,Â Data Domain)
Each relationship type includes a checkbox that allows you to
enable or disable multi-asset linking
.
Enable this option to define relationships that support linking multiple assets of the selected type. For example, allowing an asset to be related to several database tables.
You can also remove a relationship type using the
trash can icon
under the Actions column.
Adding a Custom Asset
Once your custom asset type is configured, you can begin adding assets to the system.
Steps to Add a Custom Asset:
Navigate to the Assets Dictionary
From the
left-side navigation menu
, click on
Assets Dictionary
.
Click on "Add Asset"
In the
All Assets
section, click the green
Add Asset
button at the top right.
This will open a form where you can enter details for the new asset, including selecting the appropriate
Asset Type
, filling in standard and custom attributes, and establishing relationships.
Editing and Enriching the Asset
After adding a new asset, youâ€™ll be directed to the
Asset Detail Page
, where you can view and manage all the metadata associated with the asset.
The configurations made on the
Asset Type Detail Page
â€”such as
Custom Attributes
,
Tags
, and
Relationships
â€”are automatically applied to the asset. From here, you can:
Edit or update custom attributes
(e.g., URLs, rich text fields)
Add additional relationships
to link the asset with other entities
Delete existing relationships
if they are no longer relevant
Assign or update ownership
via the
Managed By
section (e.g., Data Owner, Data Steward)
Add or modify tags
like Data Classification, Certification, or Dataset Quality
Include additional information
such as descriptions, data domains, and other contextual metadata

--------------------------------------------------------------------------------

================================================================================
TITLE: Creating and Managing Custom Attributes
URL: https://dvsum.zendesk.com/hc/en-us/articles/38568362581908-Creating-and-Managing-Custom-Attributes
================================================================================

In This Article
Overview
Accessing Custom Attributes
Adding Custom Attributes
Attribute Detail Page
Updating Custom Attribute Values via import
Attribute Configuration Options
Input Patterns for Validation
Attribute Visibility Control
Overview
The DvSum DI Application now supports
Custom Attributes
, allowing users to tailor the asset metadata to their organizationâ€™s specific needs. Previously, asset attributes were limited to a predefined set of fields. With this enhancement, users can define and manage their own attributes, improving flexibility, governance, and contextual relevance across all asset types.
Custom Attributes can be created for various asset categories such as databases, schemas, reports, models, and more. These attributes appear alongside standard fields in the asset detail pages and can be used to enrich asset metadata, support workflows, and enhance discoverability.
Accessing Custom Attributes
To manage Custom Attributes in the DvSum DI Application:
Navigate to the
Administration
from the left-side menu
Click on
Asset Management -> Custom Attributes
.
This page displays a list of all defined attributes with details such as:
Attribute Label
Type (e.g., Date, Checkbox, URL, Pick List)
Apply To Asset Types
Usage Count
Modified By
Modified On
Enabled (On/Off)
Adding Custom Attributes
To add a new Custom Attribute:
On the
Custom Attributes
page, click the green
Add
button at the top right.
A pop-up window titled
Add Attribute
will appear.
Fill in the following fields:
Type
(Required)
â€“ Choose from the dropdown list:
Checkbox
Date
Date Time
Number
Pick List
Rich Text
Mark Down
Single Line Text
Upload
URL
Label
(Required)
â€“ Enter a name for the attribute.
Description
(Optional)
â€“ Add any notes or context for the attribute.
Click
Add
to save the attribute or
Cancel
to discard.
Custom Attributes are
reusable
and can be applied across
multiple asset types
, enabling consistent metadata enrichment and governance.
Attribute Detail Page
After the attribute is added, a detail page is displayed to complete the setup. This page is divided into two sections:
Basic Configuration
This section displays the core details of the attribute, including type, label, and description. An option is available to enable a tooltip, which will appear on asset detail pages to provide additional context.
Attribute Configuration
This section allows further customization of the attribute. Configuration options include validation rules, default values, mandatory status, and multi-value support. A checklist is provided to specify which asset types the attribute should apply to, ensuring relevance and consistency across the catalog.
The
Show Tooltip
checkbox enables a tooltip to be displayed on asset detail pages. When hovered, the attributeâ€™s description will appear as a tooltip, providing users with quick contextual information without needing to open the full attribute details.
Attribute Configuration Options
The Attribute Configuration screen includes several checkboxes that define how the custom attribute behaves across assets:
Mandatory
When enabled, this setting ensures that a value must be provided for the attribute. It enforces data completeness by preventing asset creation or updates without a valid entry.
Allow Multiple Values
This option allows the attribute to accept more than one value. It is useful for cases where an asset may be associated with multiple tags, categories, or other multi-select metadata.
Fine-Grained Control (Apply To Asset Types)
Enabling this checkbox allows attribute behaviorâ€”such as mandatory status and default valuesâ€”to be customized per asset type. This overrides global settings and provides more granular governance across different asset categories.
Updating Custom Attribute Values via import
Custom attribute values can be added or updated for the assets to which the attributes are applied. To update custom attribute values in bulk, first add the custom attribute column to a custom view for the relevant asset type and then download that view. The downloaded Excel template will automatically include all custom attributes applied to the assets.
Note:
Data Import feature can be used to add or update values for custom attributes, the custom attributes themselves cannot be imported.
For more information on using the Data Import feature, refer
Data Import
article.
Input Patterns for Validation
For attribute types such as
URL
,
Number
, and
Date
, input patterns can be defined to enforce structured data entry. The
Attribute Validation
section provides a dropdown with predefined patterns, and also includes an option to create a custom pattern.
Clicking on
Create a Custom Pattern
opens a configuration form where a new pattern can be defined using regular expressions. The form includes fields for pattern name, regex, description, and sample values to illustrate expected input formats.
Note
:
The regular expressions used in custom attribute validation follow the
ECMAScript (JavaScript) regex standard
, which is natively supported by modern browsers. This allows you to use familiar JavaScript-style regex patterns for creating flexible and intuitive validations.
This flexibility allows organizations to tailor validation rules to match specific formatting requirements and ensure consistent data quality across assets.
Attribute Visibility Control
The
Custom Attributes
page includes options to manage attribute visibility and lifecycle. Attributes can be toggled between
Active
and
Inactive
using the
Attribute Visibility
dropdown. This allows administrators to control which attributes are available for use without permanently deleting them.
Visibility can be managed in multiple ways:
Mass Update
â€“ Multiple attributes can be selected and updated in bulk using the visibility dropdown.
Single Attribute Toggle
â€“ Individual attributes can be activated or deactivated directly from the listing.
Within Attribute Detail Page
â€“ Visibility settings can also be adjusted from within the attributeâ€™s configuration screen.

--------------------------------------------------------------------------------

================================================================================
TITLE: Assets Dictionary
URL: https://dvsum.zendesk.com/hc/en-us/articles/35793716144532-Assets-Dictionary
================================================================================

In This Article
Overview
Accessing the Assets Dictionary
Adding an Asset
Downloading Assets from the Asset Dictionary
Mass Update
Steps to Perform Mass Update
Managing Tags in Assets Dictionary
Adding Tags
Removing Tags
Deleting Assets in Assets Dictionary
Importing Assets in the Assets Dictionary
Expanded Asset Coverage
Viewing and Navigating Assets
Asset Detail Pages
Relationships Section in the Overview Tab
Overview
The Assets Dictionary in the DvSum DI Application provides a centralized and structured view of all assets within the system, enhancing discoverability, governance, and consistency. By standardizing asset management across various categories, including databases, schemas, reports, and models. It ensures seamless integration with workflows while giving users greater control over their data.
Accessing the Assets Dictionary
On the left-side menu of the DvSum application, click on
Assets Dictionary
to navigate to its detailed page.
This page displays the consolidated list of all the assets in the DvSum application.
Adding an Asset
The Add Asset feature in the Assets Dictionary allow users to add the custom Assets to the system.
Steps to Add a Custom Asset
Navigate to the Assets Dictionary
From the left-side navigation menu, click on
Assets Dictionary
.
Click on â€œAdd Assetâ€
In the
All-Assets
section, click the green
Add Asset
button at the top right.
A form will open where you can enter details for the new asset. Only custom or user-defined assets can be added manually using the Add Asset option.
Click
Add
to save.
After adding a new asset, youâ€™ll be directed to the
Asset Detail Page
, where you can view and manage all the metadata associated with the asset.
The configurations made on the
Asset Type Detail Page
â€”such as
Custom Attributes
,
Tags
, and
Relationships
â€”are automatically applied to the asset. From here, you can:
Edit or update custom attributes
(e.g., URLs, rich text fields)
Add additional relationships
to link the asset with other entities
Delete existing relationships
if they are no longer relevant
Assign or update ownership
via the
Managed By
section (e.g., Data Owner, Data Steward)
Add or modify tags
like Data Classification, Certification, or Dataset Quality
Include additional information
such as descriptions, data domains, and other contextual metadata
Search Asset from Asset Dictionary
You can also search governance assets directly using the
search icon
in the
Governance View
and across listing pages.
This provides seamless access to governed terms, policies, and related metadata without switching contexts, improving discoverability and navigation.
Downloading Assets from the Asset Dictionary
DvSum enables users to export data from the Asset Dictionary into an Excel file. This allows you to store, analyze, or share data offline as needed.
Click
Download
.
An Excel file containing the asset data will be generated and automatically downloaded.
Mass Update:
The
Mass Update
feature in the
Assets Dictionary
allows users to update multiple asset records simultaneously.
Steps to Perform Mass Update:
Select the assets you want to update.
Click on
Mass Update
from the top menu.
In the
Mass Update
pop-up, choose a field to update from the
Field
dropdown. Available options include:
Status
Data Domain
Select the appropriate value for the chosen field.
Click
Apply
to confirm the changes or
cancel
to discard them.
Managing Tags in Assets Dictionary
Adding Tags
Select the assets to which you want to add tags.
Click on the
Tags
dropdown and select
Add Tags
.
In the
Add Tags
dialog, select the desired tag(s) from the dropdown.
(Optional) Check the
Overwrite Tags
option if you want to replace existing tags.
Click
Add
to apply the tags.
Removing Tags
Select the assets from which you want to remove tags.
Click on the
Tags
dropdown and select
Remove Tags
.
In the
Remove Tags
dialog, select the tag(s) to be removed.
Click
Remove
to finalize the changes.
Deleting Assets in Assets Dictionary
Select the assets you want to delete.
Click on
Delete Asset(s)
.
A confirmation dialog will appear stating:
"This action will remove the asset(s). This action cannot be undone. Are you sure?"
Click
Delete
to proceed or
Cancel
to abort the action.
Note:
Asset deletion is irreversible. Verify your selection carefully before proceeding. Only Deprecated assets can be deleted.
Importing Assets in the Assets Dictionary
When selecting a specific asset in the Assets Dictionary, users will also see an
Import
option. This allows users to bring in updated or additional asset information directly from supported sources.
Navigate to the asset you want to update.
Click on the
Import
option from the top menu.
Follow the prompts to upload or sync data.
Note:
The Import feature functions the same way as described in the Data Import article.
For more information on using the Data Import feature, refer
Data Import
article.
Expanded Asset Coverage
The Assets Dictionary now includes detail pages for additional asset types beyond tables, columns, reports, and models. The newly supported assets include:
Databases
Schemas
Business Intelligence (BI) sources
Source fields
ETL programs
Each of these asset types now has a dedicated detail page, providing structured information for better governance and traceability.
Creating and Managing Views
The
Asset Dictionary
also supports the creation of
custom views
, allowing users to filter and organize datasets based on their preferences. These views can be accessed from the dropdown menu at the top of the asset list, which includes both system-generated and user-created views.
Selecting a view updates the display to show only the relevant assets and opens a
governance-focused view
for the selected asset type. From here, users can perform key actions such as:
Downloading or importing datasets
Saving and sharing views
Filtering by criteria like "Recently Refreshed" or "Assigned to Me"
This functionality enhances user productivity by enabling focused asset management and streamlined governance workflows.
Viewing and Navigating Assets
The Assets Dictionary consolidates all assets into a single view, allowing users to:
Search for specific assets easily
Click on any of the Names to display the detail page
Access detail pages for a comprehensive view of asset properties
Assets Detail Pages
Each asset type features a standardized detail page, which includes an
Overview
tab displaying key properties such as:
Description
â€“ A brief summary of the asset
Data Domains
â€“ Classification of the asset based on its data context
Governance Metadata
â€“ Information related to data ownership, compliance, and usage
Additional Info
â€“ Supplementary details related to the asset
Similar Assets
â€“ Suggestions of assets with shared characteristics
Relationships
â€“ Links to related assets within the system
Relationships Section in the Overview Tab
The
Relationships
section provides a structured view of how an asset is connected to other assets within the system. It displays all associated assets, helping users understand dependencies and data flow.
Note:
The domain assigned to an asset is automatically inherited by all its dependent assets.
More information on
[How to Link Terms to Columns]
and
[How to Manage Relationships between Assets]

--------------------------------------------------------------------------------

================================================================================
TITLE: Executing Jobs via API
URL: https://dvsum.zendesk.com/hc/en-us/articles/30417646570132-Executing-Jobs-via-API
================================================================================

In this article:
Overview
Prerequisites & API Authentication
API Endpoints for Job Execution
Request Parameters
Job Setup
Examples of Job Execution via API
Manual Execution
Using Postman
Script Automation
Overview
DvSum allows users to execute jobs programmatically using APIs. This guide provides step-by-step instructions to trigger jobs, check execution status, and handle API responses efficiently.
Each job can run in one of two distinct modes: it can either be scheduled to run at predetermined times or triggered via an API for on-demand execution. However, only one method can be active at a time; jobs cannot operate as both scheduled and API-triggered simultaneously. The mode of execution should be chosen based on specific requirements to ensure consistent operation under a single, exclusive mode.
For more details on how to create a job, refer to the
Creating
a
New
Job
article.
Prerequisites
Before executing a job via API, ensure you have the following:
A Job:
The job must be configured for API execution (it cannot be scheduled).
OAuth Client Credentials:
You can create an OAuth2 Client from the Application Security tab and use the Client ID and Secret or enable API Access at the User Role level.
Access Token:
Using the above OAuth credentials, generate an access token through the Authentication API mentioned in the
API Documentation.
Refer to the article
â€˜
DvSum API Access and Authentication Management
â€™
for details on creating an authentication client and generating access tokens.
API Endpoints for Job Execution
1. Execute Job (POST Request)
This endpoint triggers the execution of a job.
Endpoint:
POST https://apis.dvsum.ai/integration/jobs/[job_id]/execute
2. Get Job Execution Details (GET Request)
This endpoint retrieves the status and details of the job execution.
Endpoint:
GET https://apis.dvsum.ai/integration/jobs/[execution_id]/details
Request Parameters
Include the following headers for both POST and GET requests:
Authorization:
Bearer <access_token>
Content-Type:
application/json
Note:
The access token is valid for 1 hour. Generate a new token using your Client ID and Secret when it expires.
Step 1: Configure the Job for API Execution
Navigate to the
Schedule
tab in the DvSum platform.
Select the schedule type as
Execute via API
.
For detailed instructions, refer to the
Creating
a
Job
article.
Step 2: Generate API URLs
Click on
Generate API URL
. This will generate both the POST and GET request URLs needed for API execution.
Examples of Job Execution via API
To facilitate job execution via API in DvSum, three main approaches are available: manual execution, using Postman, and script automation. Each method provides options for managing job execution and monitoring through the API, with step-by-step instructions provided for each option below. The approach can be selected based on workflow and technical requirements.
Manual Execution
This section covers the process for manually executing jobs via API. If you prefer direct control over job execution, follow this step-by-step guide.
Step-by-Step Guide to Executing Jobs via API
Step 1: Execute the Job
curl -X POST
https://apis.dvsum.ai/integration/jobs/[job_id/execute
-H "Authorization: Bearer <access_token>" \
-H "Content-Type: application/json" \
Copy the POST request URL and paste it into your command prompt or API client (e.g., Postman).
Run the command to execute the job:
POST
"https://apis.dvsum.ai/integration/jobs/[job_id]/execute"
Step 2: Retrieve Execution Details
curl -X GET
https://apis.dvsum.ai/integration/jobs/[execution_id]/details
-H "Authorization: Bearer <access_token>"
-H "Content-Type: application/json"\
After executing the job, copy the GET request URL and paste it into your command prompt or API client.
Replace the following placeholders:
[execution_id]
with the execution ID from the POST request.
[<access_token>]
with your valid access token obtained using OAuth Client credentials or User Role based credentials
Run the GET request to retrieve job execution details:
GET
"https://apis.dvsum.ai/integration/jobs/[execution_id]/details"
Step 3: View Execution Details
The GET request will display the status and other details of the job execution once completed.
Step 4: Fetch Exception Records
Case1:
On-prem Gateway
For an on-prem gateway, exception records can be downloaded, and the requested URL can be accessed using the provided
x-api-key
in the response.
Case 2:
Cloud Gateway
For a cloud gateway, exception records can be downloaded using the
pre-signed URL
provided in the response.
Note:
Exception records can only be downloaded up to a limit of
10,000 records
.
Using Postman
Enhance the process by importing the API requests into Postman for more structured testing. This method allows you to interact with the API and automate requests efficiently visually.
Calling API Endpoints for Job Execution
1. Execute Job (POST Request)
This endpoint triggers the execution of a job.
Endpoint:
POST
https://apis.dvsum.ai/integration/jobs/[job_id]/execute
2. Retrieve Execution Details (GET Request)
This endpoint retrieves the status and details of the job execution.
Endpoint:
GET
https://apis.dvsum.ai/integration/jobs/[execution_id]/details
3. View Execution Details
The provided download request URL in the execution details can be used to view the execution details.
Script Automation
Automate job execution by using a script. This method is ideal for users who want to integrate the API into their workflow. Sample scripts can be provided for reference to help automate repetitive tasks.

--------------------------------------------------------------------------------

================================================================================
TITLE: Creating a new Job
URL: https://dvsum.zendesk.com/hc/en-us/articles/27001229007380-Creating-a-new-Job
================================================================================

In this article:
Overview
Step-by-Step Guide
Step 1: Navigate to the Job Creation Page
Step 2: Choose a Job
Step 3: Configure Job Parameters
Step 4: Configure Notification Settings
Step 5: Set Job Scheduling Options
Scheduling Information
One-Time Execution
Recurring Execution
End Conditions
Add a Job from Job Detail Page
Overview:
A job in DvSum automates data validation, transformation, and integration tasks by executing a specified number of Data Quality (DQ) rules and profile tables. Jobs can be scheduled or triggered via API, but not both simultaneously. This guide walks you through creating a new job, configuring its parameters, and scheduling it for execution.
Step-by-Step Guide
Step 1: Navigate to the Job Creation Page
Log in to DvSum and navigate to the
DQ Rules
tab.
Select any rule, click on
More Actions
, and then
Add to Jobs
.
Step 2: Choose a Job
From the dropdown, select an already created job or click on
Create a New Job
.
If creating a new job, a dialog box will appear. Click
OK
.
Step 3: Configure Job Parameters
In the
Definition
tab, add a job description.
Choose the selection type:
Fixed
Dynamic
Applying Filters When Using Dynamic Selection
When configuring the
Selection Type
, if you choose
Dynamic
, filters must be applied to define which
rules or tables
will be included in the job at runtime.
To apply filters:
Select the
Dynamic
option under
Selection Type
.
Under the
Add Criteria
section, click
Add Condition Group
to define one or more filter conditions.
Once the conditions are set, click
Apply Filter
.
After applying the filter:
The
Selected Rules
or
Selected Tables
section will automatically populate based on the applied criteria.
These selections are
not editable
â€” you cannot manually add or remove individual items.
Note: If no filter conditions are applied, no rules or tables will be included dynamically in the job.
Step 4: Configure Notification Settings
In the
Notification
tab, you can define how and when email notifications are sent, and specify who should receive them.
Email Notification Options
When you first open the Notification tab, you'll see two options:
Send schedule completion email
Send email on job delay
Once
Send schedule completion email
is selected, an additional option appears:
Send email on alerts
All three options can be selected together. Here's what each one does:
Notification Option
Purpose
Send schedule completion email
Notifies recipients when the job finishes its scheduled execution.
Send email on alerts
Sends notifications when alert conditions are triggered during a job.
Send email on job delay
Alerts recipients if the job exceeds its expected duration.
Job Delay Alert Configuration
If
Send email on job delay
is selected, a configuration panel appears with the following settings:
Duration Threshold
Automatically Determined Duration
(default): Uses Interquartile Range (IQR) outlier detection on job run times to identify delays. Manual edits are allowed until four executions, after which thresholds auto-adjust.
Set Custom Duration
: Manually specify the maximum expected duration. An alert is triggered if the job exceeds this time.
Input format:
HH:MM:SS
Alert Frequency
Define how often reminder emails should be sent after a delay is detected.
Input format:
HH:MM:SS
Email Recipients
You can specify both internal and external recipients:
Internal Recipients
Select users or groups from the
Available
list.
Move selections to the
Selected
list using the arrow buttons.
At least one internal recipient is required.
External Recipients
Enter comma-separated email addresses.
Only emails from whitelisted domains are allowed.
Example:
example@email.com, another@email.com
Step 5: Set Job Scheduling Options
Navigate to the
Schedule
tab.
Choose a schedule type:
Scheduled
(For scheduled execution)
Execute via API
(Refer to the
Executing Job via API article
)
Scheduling Information
One-Time Execution
Choose a
start date
and
time
.
Set an
end date
, if applicable.
Recurring Execution
Daily
Choose the number of days between executions.
Weekly
Select the number of weeks and the days for execution.
Monthly
Choose the number of months.
Select
repeat on days of the month
or
week of the month
.
End Conditions
Never Ends
: The job runs indefinitely.
Ends After
: Set the number of occurrences.
Ends On
: Set a specific end date.
Additional Way to Add a Job: From the Job Detail Page
Apart from adding jobs via the DQ Dictionary tab, you can also add a job directly from the
Job Detail page
. This provides a convenient way to manage jobs and quickly add related jobs without navigating back to the dictionary.
How to add a job from the Job Detail page:
1-From administration Tab navigate to the
Jobs
drop down and select
Definitions
in the
DI tool
.
2- Click the
Add Job
button located at the top.
3- Follow the same steps to select rules, configure parameters, notifications, and scheduling as described earlier.
Cloning Jobs:
From the Job Detail page, you can also use the
More Actions
menu to quickly clone an existing job. This allows you to duplicate the job configuration and make adjustments without creating it from scratch.
Job can also be cloned from the
Job Detail page
by navigating to
Edit > Clone Job
.
Profile Tables
A
Profile Table
job is used to analyze the structure and quality of your data by generating profiling statistics on selected tables. Profiling helps you understand the characteristics of your data before applying rules or transformations.
Purpose of Profile Jobs
Identify data distribution and patterns.
Detect data quality issues such as missing values, duplicates, or anomalies.
Gain insights into column-level statistics (null counts, distinct values, min/max, etc.).
How to Create a Profile Job
1. Job Category
Select
Profile Tables
as the job category. This specifies that the job will profile data instead of executing rules.
2. Follow the same steps to select tables, configure parameters, notifications, and scheduling as described earlier.
Scan Source
A
Scan Source
job is used to discover metadata from a data source, helping build a catalog of data assets. Optionally, the scan can include profiling and lineage for deeper insights.
Purpose of Scan Source Jobs
Discover and catalog datasets, tables, columns, and schemas
Generate metadata inventory for governance and data discovery
Optionally include profiling statistics and data lineage mapping
How to Create a Scan Source Job
Job Category
Select
Scan Source
as the job category. This indicates that the job will scan a data source for metadata.
Data Source and Scan Catalog Options
Choose the appropriate
Data Source
from the dropdown.
Under
Scan Catalog
, you can optionally select:
Profile
â€“ to include profiling statistics in the scan
Lineage
â€“ to analyze data relationships and flow
If neither option is selected, the job will schedule a
catalog-only scan
, capturing only metadata like table names, columns, and schema details.
Follow the same steps to configure parameters, notifications, and scheduling as described earlier.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Exclusion Policy
URL: https://dvsum.zendesk.com/hc/en-us/articles/39824964543764-Data-Exclusion-Policy
================================================================================

In this Article:
Overview
Accessing and Applying the Data Exclusion Policy
Managing Exclusion Policies
Visual Examples: How Exclusion Affects Views
Data Dictionary â€“ Column View
Data Quality (DQ) Dictionary â€“ Profiling Tab View
Restricting the Data View by Data Domain
Restricting the Data View by Source
Restricting the Data View by Databases
Overview
The Data Exclusion Policy feature empowers administrators to control data visibility based on user roles. This policy ensures that sensitive table data is hidden from users who should not have access to it. Once access to a table is revoked, associated column data and rule-level exception data are also automatically restricted.
This functionality helps safeguard sensitive data and streamlines the user experience by ensuring that each user role only interacts with data they are permitted to view.
Accessing and Applying the Data Exclusion Policy
To configure a new Data Exclusion Policy:
Navigate to:
Administration â†’ Account â†’ User Security
Scroll to the
Sample Data Exclusion Policy
section.
Click
Add Exclusion Policy
.
Fill in the Required Information:
Policy Name:
Specify a unique and identifiable name for the policy.
Role:
Select the role to which this policy will apply.
Users can then configure criteria by selecting specific columns. Multiple condition types are available, allowing flexible and targeted policy setup based on the selected column values.
Note:
The data will be hidden only for users assigned to the role(s) selected while creating the policy.
Users with all other roles will continue to view the data without any restriction.
Managing Exclusion Policies
Administrators can manage existing exclusion policies through the following actions:
Bulk Actions
Select multiple policies to update visibility settings or delete them in one go.
Edit/Delete
Click the
ellipsis (â‹¯)
next to a policy to make changes or remove it.
Activate/Deactivate
Use the
toggle switch
to enable or disable a policy without permanently deleting it.
Visual Examples: How Exclusion Affects Views
1. Data Dictionary â€“ Column View
Before Adding the Role in the Exclusion Policy:
All users can view every column and its metadata in the Grid View. Showing all the metadata inside the Column.
After Adding the Role in the Exclusion Policy:
Column data will be completely hidden from users assigned to restricted roles.
If you select the Dictionary view and click on any column name to view the
Column Reference
page, the visual distribution will no longer be visible. Instead, a message will be displayed stating that the information is restricted due to the applied policy.
2. Data Quality (DQ) Dictionary â€“ Profiling Tab View
Before Adding the Role in the Exclusion Policy:
On the
Rules Detail
page, Exception data from all rules are visible to the users.
After Adding the Role in the Exclusion Policy:
All the exception data of the rules will be hidden from users with restricted roles.
Important:
Even if a user is part of a role covered by the exclusion policy, they can still access exception data for rules they own.
Example:
In this case, the user
Amulya
belongs to a role restricted by the exclusion policy. However, because she is the owner of the rule, she retains access to its exception data.
The rule that Amulya owns:
The rule that Amulya does not own:
3. Restricting the data view by Data Domain.
When a criteria condition is applied to a Data Domain to limit view access, all datasets within that domain will have their column-level details hidden in the Data tab, restricting users from viewing individual columns.
4. Restricting the data viewÂ by Source
Source-level restrictions limit access to column details in the Data tab. This control applies uniformly across all datasets linked to that source, supporting consistent data governance and preventing unauthorized access to sensitive information.
5. Restricting the data view by Databases:
When a criteria condition is applied to a database, profiling and exception details are restricted across all related databases as per the exclusion policy. This cascading restriction plays a key role in maintaining data governance and security.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Classification
URL: https://dvsum.zendesk.com/hc/en-us/articles/39292360682900-Data-Classification
================================================================================

In This Article
Overview
Tagset
Applying Classification Tags
Roles and Permissions: Data Security Admin vs General Users
Linking Columns to Tags
Tag Behavior
Customization
Data Masking in Rule Execution & Export
Overview
Data Classification is a foundational capability in DvSum, enabling organizations to manage data sensitivity and enforce privacy policies. Classification tags can trigger privacy actions such as masking and column hiding, all of which are configurable by Data Security Admins.
This article explains how data classification tags are created, applied, and managed within DvSum. It covers who can assign or modify tags, how roles affect permissions, and how classification levels drive masking and other privacy-related actions.
Tagset
Classification tags are centrally managed in DvSum by administrative users. These tags are customizable and typically include:
Public
Internal
Sensitive
Restricted
Accessing the Tagsets:
Navigate to
Administration.
Go to
Asset Management.
Click on
Tags.
Select the
Data Classification
tagset.
Within the Data Classification tagset, users have the ability to create new tags. By default, these tags are assigned a low severity level and cannot be linked to columns or terms that carry a higher sensitivity classification (e.g., Sensitive or Restricted).
These classification tags can be applied to both
columns
and
terms
.
Applying Classification Tags
For Columns (Field Dictionary)
Navigate to the
Dictionaries >
Database Columns.
Select the desired column(s).
Open the
More Actions
dropdown.
Click on
Data Classification
.
For Terms (Business Glossary)
Go to the
Dictionaries >
Glossary Terms.
Select the desired term(s).
Open the
More Actions
dropdown.
Click on
Data Classification
.
Roles and Permissions: Data Security Admin vs General Users
Data Security Admin
Full access to assign, elevate, or downgrade the sensitivity of classification tags.
Can configure masking, hiding, and other privacy actions for each tag.
General Users
Can increase the sensitivity level of a tag (e.g., from
Internal
to
Sensitive
).
Cannot downgrade the classification level.
If they attempt to lower the sensitivity, a warning message is displayed, as shown in the screenshot below.
Upgrade and Downgrade Rules
Data Security Admin
Can
upgrade
classification tags in all scenarios.
Can
downgrade a column
only if
all linked terms have equal or lower severity
.
For example: If a column is tagged as Sensitive, and all its linked terms are Internal, the column can be downgraded to Internal.
Can
downgrade a term
only if
all linked columns have equal or lower severity
.
For example: If a term is tagged as Restricted, and all its linked columns are Internal, the term can be downgraded to Internal.
General Users
Can
upgrade
classification levels freely.
Cannot
downgrade
classification tags under any condition.
If the severity remains the same (e.g., Sensitive to Sensitive), they are
not allowed to reassign
the tag
Linking Columns to Tags
Users can associate columns with terms that have the same or
lower
severity level.
Example:
If a column is classified as
Sensitive
or
Public
, and a term is marked as
Sensitive
, the column can be linked to that term.
Note:
If a user tries to link a column to a term with
higher
severity, DvSum will display a warning message.
Severity-Based Linking Rules
The ability to link columns and terms depends on their classification severity:
From a Term
:
A user can link a column only if the
columnâ€™s severity is equal to or less than
the termâ€™s severity.
From a Column
:
A user can link a term only if the
termâ€™s severity is equal to or greater than
the columnâ€™s severity.
This keeps the logic aligned with the data model and avoids inappropriate tag associations.
Tag Behavior:
Each classification tag in DvSum can be configured to trigger specific privacy-related actions. These behaviors are determined by the
severity level
of the tag and are enforced consistently throughout the platform.
Default Tag Behaviors:
Internal / Public
: Columns are marked as default; no masking is applied.
Sensitive
: Data is masked as "XXX" throughout the application.
Restricted
: Columns are both hidden and masked as "XXX" throughout the application. This behavior can be modified by a Data Security Admin.
Each classification tag can be configured to perform specific privacy actions, such as:
Masking
: Replacing sensitive data with masked values like "XXX".
Hiding
: Making columns invisible to users without appropriate access.
Default Behavior When Tag or Access Is Not Specified
In cases where classification or access settings are not explicitly provided:
If
no classification tag is assigned
, the column or term
defaults to Public
.
If
â€œMaskedâ€
access is set, the behavior will follow that of the
Sensitive
tag.
If
â€œNo Accessâ€
is set, the behavior will follow that of the
Restricted
tag.
Default Configuration Examples:
Restricted
: Columns are masked and hidden across the application.
Sensitive
: Columns are masked, and a sensitivity icon appears next to the column name.
Customization:
Data Classification Tagset
The â€œClassificationâ€ tagset is a system-defined tagset created by default in DvSum to enforce data privacy and masking policies.
Default Configuration:
Exists by default in the system.
Applies to assets such as columns, tables, and glossary terms.
Supports one or multiple classifications per asset, depending on the
â€œOnly Single Value Allowedâ€
setting.
Default tags: Public, Internal, Sensitive, Restricted.
Note:
This customization can only be performed by users with Data Security Admin privileges.
What Can Be Modified (by Data Security Admin):
Users can tailor the tagset to match their organizationâ€™s sensitivity framework. They can:
Rename tags (e.g., change â€œSensitiveâ€ to â€œConfidentialâ€)
Change the associated asset types (e.g., apply to Columns, Tables, Terms)
Add new tags (new tags are created with low severity by default)
Adjust the
â€œOnly Single Value Allowedâ€
setting:
Check
it if only one classification tag should be assigned per asset.
Uncheck
it to allow multiple classification tags per asset.
What Cannot Be Modified:
Tagset Type is Fixed:
This tagsetâ€™s type is always set to "Classification" and cannot be changed to another type.
Data Masking in Rule Execution & Export
When a column is marked as
Sensitive
, it is masked as "XXX" throughout the UI.
The same masking applies during
rule execution
.
If the rule runs on a table containing masked columns, the resulting
exported Excel file
will also display masked values for those columns.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Security Admin
URL: https://dvsum.zendesk.com/hc/en-us/articles/22680186312468-Data-Security-Admin
================================================================================

In this Article
Introduction
Data Security Admins
Hiding Tables and Columns
Deleting Assets
Adding "Data Classification" Tags
Introduction
This article explains how a user can be added as a Data Security Admin and describes the exact authorizations for a Data Security Admin.
1. Data Security Admins
The Account Admins can add Data Security admins from the User Security tab which is present in the Account Settings page.
In the dropdown, all the active users in the application will be shown. The users who have "Admin" access to the catalog can only be added as Data Security Admins. The users who do not have "Admin" access to the catalog can be added to the list of Data Security Admins but there will be a warning sign shown to update its permissions.
In this case, the user "Hassan Ghafoor" is in the list of Data Security Admins but actually not a Data Security admin until its permissions are not set to Catalog Admin. On clicking "Update User Role", the user settings will open from where permissions can be changed:
Now when the user is logged in from the Data Security Admin account there will be some extra options that will be available on the Data Dictionary and Field Dictionary:
2. Hiding Tables and Columns
Data Security Admins see additional options on the Database Tables page that other users cannot see which allow them to hide and show objects. When any tables are selected from the Database Tables, the "More Actions" button will be visible. The "More Actions" button is a drop-down with the following 3 options:
Hide Table(s)
Unhide Table(s)
Delete Table(s)
When any tables are selected and the user selects the option "Hide Table(s)" then all the selected tables will be hidden. The hidden tables can be seen when the "Show hidden tables" checkbox is checked.
Users can mark any table as hidden. The hidden tables won't show at:
Table Dictionary
Enterprise Search
Exported Excel File
Hiding columns:
On the Database Columns when any column or columns are selected from the Database Columns, the "More Actions" button will be visible. The "More Actions" button is a drop-down with the following 4 options:
Hide Columns
Unhide Columns
Data Classification
Delete Columns
When any columns are selected and the user selects the option "Hide Column(s)" then all the selected unhidden tables will be hidden. The hidden columns can be seen when the "Show hidden columns" checkbox is checked. The hidden columns will be greyed out.
When a column is marked as hidden, The hidden column won't show at:
Field Dictionary
On the Rules Creation form
Rule Detail Page - Data Tab
Rule Detail Page - Definition Tab
Exported Excel File
3. Deleting Assets(Tables, Columns and Rules):
Deleting Table and Columns:
The selected tables and columns can be deleted and they will be removed from everywhere on the application:
Note:
To delete tables or columns, users must either be the owners of those assets with Catalog Admin permissions and have the Data Security Admin role.
Deleting rules:
The selected rules can be deleted by clicking the
More Actions
dropdown. Once deleted, they will be removed from everywhere on the application.
Note:
To delete rules, users must either be the ruleâ€™s creator with
Catalog Admin
permissions or have the
Data Security Admin
role.
4. Adding "Data Classification" Tags
Users can add the "Data Classification" tags which can be added only by Data Catalog Admins:
The following 4 tags can be seen on the "Data Classification":
Internal
Public
Restricted
Sensitive
The description of every tag can be seen on hover. For more information about the tags separately there is the link of the "
Data Masking
" article.
Once any tag is assigned to column(s), they can be seen on the listing view:
The selected columns can also be deleted and they will be removed from everywhere on the application:
Note: For the deletion of columns, users should have Catalog Admin permissions but they don't have to be necessarily Data Security Admin
One More thing to note here is that only the tables or columns that will have the status "Deleted" can be deleted from the Catalog.
Managing Data Classification Tags
As a
Data Security Admin
, you are responsible for applying classification tags (such as
Confidential
,
PII
, etc.) to data assets like columns, tables, and glossary terms.
Tagging behavior depends on how the
Custom Tagset
is configured in the system.
Single vs Multiple Tags
Each tagset has a setting:
â€œOnly Single Value Allowedâ€
. This determines whether you can assign
only one
tag or
multiple
tags from that tagset to a single asset.
If
checked
, you can apply
only one
classification tag per asset.
If
unchecked
, you can apply
multiple classification tags
to the same asset.
This allows more flexibility in labeling data that falls under more than one classification (e.g., both
Internal
and
Confidential
).

--------------------------------------------------------------------------------

================================================================================
TITLE: Differentiating Empty & Nulls Strings
URL: https://dvsum.zendesk.com/hc/en-us/articles/36505019344532-Differentiating-Empty-Nulls-Strings
================================================================================

In this Article
Overview
Understanding the concept of Empty Strings
Understanding the concept of Nulls
Additional Resources
Overview
In databases,
empty
and
null
values may look similar but have different meanings. An
empty
value means an empty string (
''
) is stored in a column, especially in string-type fields. Understanding this difference is crucial, as the concept is used in
Blanks
and
Value Range Rule
checks. A
null
means no value has been storedâ€”you'll see it
NULL
explicitly written in the database.
1. Understanding the concept of Empty Strings
Before jumping into the application, let's try to understand the concept directly from the database. Here, we have a column named
"capital"
, and we can see that empty strings are stored in itâ€”these are empty values:
Now, if we check the same column in the application, we can see that the
"capital"
column has some empty values.
If we apply the
Blanks Rule
with the
"Empty"
checkbox selected, 4 exceptions are expected. Navigate to the
Blanks Rule
and apply this check.
Note:
Selecting at least one of the checkboxes is necessary to save the Blanks Rule.
After saving and running the rule, the number of exceptions will be
4
, which is the expected result.
This same check for
empty strings
can also be used in the
Value Range Rule
, and the concept remains the same as explained above.
2. Understanding the concept of Nulls
Now, letâ€™s consider the same
"capital"
column. In the database, we can see entries where it
[NULL]
is mentionedâ€”these are the
null
values.
From the profiling information in the application, we can see that the
"capital"
column also has some null values.
If we apply the
Blanks Rule
with the
"Null"
checkbox selected, the expected number of exceptions will be
284
.
As shown earlier, the same check is available in the
Value Range Rule
, and the concept is identical. In the
Ruleset Rule
, both
Blanks
and
Value Range
checks are available, and the logic remains the same.
Additional Resources
To better understand the context of this article, itâ€™s essential to be familiar with the following rules, as they are used when applying blank and null checks:
Foundational DQ - Blanks Rule
Foundational DQ - Value Range Rule
Foundational DQ - Ruleset Rule

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Quality Overview
URL: https://dvsum.zendesk.com/hc/en-us/articles/23520582795924-Data-Quality-Overview
================================================================================

In this article:
Edit Table
Column Sequence
Rules
Introduction
Data quality refers to the accuracy, completeness, consistency, and reliability of data within an organization. High-quality data ensures better decision-making, compliance, and operational efficiency. The Data Quality tab in DvSum contains all the settings related to rules. Users can access the Recommended & Available Rules created for the table opened and create new rules. This article provides an overview of all the major options available in the Data Quality tab.
Edit Table
Edit the table details by clicking on the
Gear Icon
button
The following fields can be edited:
Load Profile
All Data
: Select this load profile if data is fully refreshed regularly.
Incremental Data
: Select this if data is normally loaded incrementally (appended).
Metric Time
: If the load profile is set to incremental, then select from the Data and Timestamp fields in the table. This field will be used in relevant DQ rules, such as calculating daily load volumes.
Exception Record Limit
Users can define a threshold for how many exceptions a rule can generate. For example, if the user does not want more than 500 exceptions for any rule created on the table, they can set this limit accordingly.
Filter Condition
The
Rich Filter Condition
allows users to specify specific conditions to apply to columns before executing rules. Users can add a
Filter Condition
or a
Custom Expression
. Multiple conditions and condition groups can be set as needed.
On the Rich Filters, users can add multiple conditions and condition groups according to the requirement
Note: Condition group joins various conditions (the ones in the silver box)
Column Sequence
The sequence of the columns can be opened from the
Gear icon
of settings and the user can change the sequence of the columns. This change of sequence will be observed once the rules are executed.
Rules
Available Rules
Create new rules and review existing rules.
Select an existing rule to run it online or offline. For details, refer to the article
Run Online/Offline.
Mass Update:
The Mass update option allows the users to set the:
Workflow Status (Acknowledge or Resolve)
Priority (High, Medium, Normal)
Tags:
Users can add or remove the tags from the Rules. Once any rule or rules are selected the option is visible above:
More Actions:
Add to Job -
update the existing schedules or add a new schedule.
Remove from Job -
This option allows the user to remove any existing schedule of the rule(s)
Clone rule
- This option allows the user to clone the current rule
Delete rule
- This option allows the user to Remove the rule permanently
Rules - Recommended rules
Recommended Rules
These are the rules that are recommended based on the profiling of the source's tables
If the User wants to add a recommended rule they can simply click on the
ADD
button
The following rule types are
Recommended
:
Blank
Value Range
Count
Metric

--------------------------------------------------------------------------------

================================================================================
TITLE: Pivot View Configuration in Data Quality Rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/44369988407572-Pivot-View-Configuration-in-Data-Quality-Rules
================================================================================

In This Article
Overview
Accessing Pivot View
Configuring Pivot View
Overview
The Pivot View in Data Quality Rules provides an interactive way to analyze exception data by grouping and aggregating records directly within the application. It allows users to summarize large volumes of exception data, identify patterns, and gain insights.
This article is part of the
Rule Detail Page
documentation. For information on the
Data tab
, including how exception data is generated and displayed, see
Rule Detail Page â€“ Data Tab
.
Accessing Pivot View
The Pivot View can be accessed from within a Data Quality rule.
Open the
DQ Rule
.
Go to
Data Quality â†’ Data
.
Click on the
Pivot View
icon.
Select
Configure Pivot View
.
Define the required grouping and summarization parameters.
Save the configuration to render the Pivot View results.
Note:
If the Pivot View has not been configured previously, the system displays a message indicating that the Pivot View is not yet configured.
Configuring Pivot View
To configure the Pivot View, users must select
Configure Pivot View
from the Pivot View panel within the Data Quality rule. All available columns from the
Data
tab are displayed for configuration.
Users can configure it by:
Defining Row Groups
Select aggregation options such as Maximum and Minimum
Select Column
Save the configuration settings.
These configurations determine how the exception data is grouped and summarized in the Pivot View.
Note:
After saving the Pivot View configuration, only a preview of up to 200 records is displayed. To populate the Pivot View with the full dataset, the rule must be re-run. users can configure the maximum number of exception records displayed in the data grid, up to 75,000 records.
Pivot View data can be exported to Excel, similar to the Data tab. When exported in Grid format, grouped columns are downloaded as individual columns in the Excel file.
Pivot View is intended for interactive, UI-based analysis. Pivoted data can be manually exported from the UI but is not included in automated exception outputs.

--------------------------------------------------------------------------------

================================================================================
TITLE: Cross-System DQ - Reconciliation Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/44350372876308-Cross-System-DQ-Reconciliation-Rule
================================================================================

In This Article
Overview
Creating a Reconciliation Rule
Rule Configuration
Reconciliation Type
Aggregate-Level Reconciliation Types
Data Source and Table Selection
Key Field Mapping
Metric Fields
Additional Fields
Sort Order
Data Comparison
What You Can Achieve with the Reconciliation Rule
Overview
The
Reconciliation rule
is a Cross-System Data Quality (DQ) rule used to compare data between two tables, either at the
record level
or the
aggregate level
, within a single source or across different sources.
This rule allows users to:
Match records between tables using one or more key fields
Compare metric values for matched records
Validate record counts between datasets
Compare aggregated metric values
Creating a Reconciliation Rule
To create a Reconciliation rule:
Navigate to a table
Click
Add Rule
Select
Cross System DQ
Choose
Reconciliation
Rule Configuration
After selecting the Reconciliation rule, configure the rule using the sections described below.
Reconciliation Type
The
Reconciliation Type
defines how the comparison between the source and reference tables is performed.
DvSum provides
four reconciliation types
, grouped into
record-level
and
aggregate-level
checks.
Record-Level Reconciliation Types
Each reconciliation type exposes a specific set of configuration sections based on the comparison being performed.
Record Level Metric Check
The
Record Level Metric Check
compares
numeric metric values for matched records
between the source and reference tables.
Records are first matched using key fields, and metric values are then compared for each matched record.
Available configuration sections:
Data Source & Table Selection
Choose the source and reference datasets for comparison.
Key Fields Mapping
Select one or more fields that uniquely identify records. The same fields must be selected on both source and reference.
Metric Fields
Select numeric fields whose values will be compared between source and reference.
Additional Fields
Select additional context or identification fields to include in the comparison results.
Record Level Check
The
Record Level Check
compares records between the source and reference tables based on defined key fields, without comparing metric values.
This check focuses on
record presence and alignment
between datasets.
Available configuration sections:
Data Source & Table Selection
Choose the source and reference datasets for comparison.
Key Fields Mapping
Select one or more fields that uniquely identify records.
Additional Fields
Select additional context or identification fields to include in the results.
Aggregate-Level Reconciliation Types
Aggregate Count Check
The
Aggregate Count Check
compares the
total number of records
between the source and reference tables.
This check evaluates whether record counts are aligned across datasets.
Available configuration sections:
Data Source & Table Selection
Choose the source and reference datasets for comparison.
Key Fields Mapping
Select one or more fields to group records for comparison (if applicable).
Absolute Delta
When enabled, the system displays the absolute difference between source and reference counts without considering the sign.
Aggregate Metric Check
The
Aggregate Metric Check
compares
aggregated metric values
between the source and reference tables.
Metric values are aggregated before comparison using a selected aggregation function.
Available configuration sections:
Data Source & Table Selection
Choose the source and reference datasets for comparison.
Key Fields Mapping
Select one or more fields to group records for aggregation.
Metric Fields
Select numeric fields to aggregate and compare. Aggregation functions include
SUM, MIN, MAX, and AVG
.
Data Source and Table Selection
For all reconciliation types, users must define:
A
Source
connection and table
A
Reference
connection and table
Filters can be applied independently to both tables to control which records participate in the reconciliation.
Key Field Mapping
Key fields define how records are matched between the source and reference tables.
By default, records are matched using the same field or set of fields selected on both the source and reference tables.
When record uniqueness depends on more than one field, multiple fields can be selected to form a composite key.
If
Asymmetric Keys
is enabled, the selected fields are concatenated to form a single key value that is used for comparison.
This allows reconciliation of datasets where no single field uniquely identifies a record.
Metric Fields
Metric fields define the
numeric values
that are compared between the source and reference tables.
For each selected metric field:
The system automatically calculates the
difference (Delta)
between the source and reference values.
When grouping is enabled, an aggregation function (
SUM, MIN, MAX, AVG
) must be selected for each metric.
Absolute Delta
When
Absolute Delta
is enabled, the system displays the
absolute value of the calculated difference
, without considering whether the source value is higher or lower than the reference value.
This is useful when the focus is on the
size of the difference
, rather than the direction of the difference.
Use
Add Field
to compare additional metric fields.
Additional Fields
Additional fields are used to include
extra identifying or contextual columns
in the reconciliation results.
These fields do not participate in key matching or metric comparison. Instead, they help identify
which specific records
are being compared when reviewing reconciliation results, especially for
record-level checks
.
These fields help users interpret reconciliation results more effectively.
Sort Order
The
Sort Order
section controls how reconciliation results are sorted when the results view is loaded.
Users can define one or more columns to sort the results and specify the sort direction for each column.
Configuration
Sort By
Select a column by which the reconciliation results should be sorted.
Ascending
Sorts results from lowest to highest (Aâ€“Z, smallest to largest).
Descending
Sorts results from highest to lowest (Zâ€“A, largest to smallest).
Add Sort Column
Allows multiple sort columns to be added to define a hierarchical sort order.
If no sort criteria are defined, the results are displayed using the default system order.
Data Comparison
The
Data Comparison
section defines how differences between the source and reference data are evaluated.
Variance Percent
Variance Percent
defines the maximum allowed percentage difference between the
source metric value
and the
reference metric value
before the comparison is marked as a mismatch.
A value of
0
means the source and reference values must match exactly.
A non-zero value allows small differences within the specified percentage range.
This is useful when minor variations are expected due to rounding, timing differences, or calculation behavior between systems.
Comparison Type
The
Comparison Type
determines how records from the source and reference tables are matched during reconciliation.
Full
Includes all records from both source and reference tables, whether matched or unmatched.
Inner
Includes only records that exist in both source and reference tables.
Left
Includes all records from the source table and matching records from the reference table.
Right
Includes all records from the reference table and matching records from the source table.
The selected comparison type controls which records appear in the reconciliation results and how unmatched records are handled.
What You Can Achieve with the Reconciliation Rule
Using the Reconciliation rule, users can:
Compare records and metrics across systems
Validate data completeness and accuracy
Identify mismatches at both record and aggregate levels
Reconcile transactional or metric-based datasets using flexible matching logic

--------------------------------------------------------------------------------

================================================================================
TITLE: Jira Integration for DQ Rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/41467238935700-Jira-Integration-for-DQ-Rules
================================================================================

Overview
In this article:
Overview
Jira Integration Setup
Configuring User Settings in JIRA
Steps to Configure Jira Integration
How to Create a Jira API Token
Configuring Jira in DQ Rules
Enable Integration
Ticket Creation Modes
Manual Ticket Creation
Automatic Ticket Creation
Ticket Workflow & Status Sync
Ticket ID Column
Non-Exception Rule Ticket Creation
Mass Update
Summary
This article explains how to create Jira tickets directly from Data Quality (DQ) rules in DvSum.
By integrating Jira workflows into the platform, teams can:
Manage rule exceptions more efficiently.
Eliminate manual exception tracking.
Ensure systematic assignment, monitoring, and resolution of issues.
Jira Integration Setup:
Before creating tickets, users must configure Jira integration within the application. The integration ensures that Jira tickets can be created and assigned properly.
Prerequisite:
Configuring User Settings in JIRA
The Admin of the JIRA account first needs to add users to the account, who will create API tokens and create JIRA integration in DvSum.
Once the user is added, the role must be assigned to the user, and the minimal role that must be assigned is "User."
After the user is added to the organization, the user must be added to the project on which the Jira integration will be created. On Jira, select your project and go to its settings, where users can be added:
While adding the user, the user must have a "Member" level role at a minimum to be able to create tickets in Jira.
Once the user is added to the project, they will receive an invite, and this user can now create an API token from this account, and the user will be able to create Jira integration in DvSum.
Steps to Configure Jira Integration:
Navigate to the
Administration
tab >
Account
.
Click on the
Integrations
tab
.
Click
Add
Integration and choose
Jira
.
4. Fill in the required fields:
Name:
Provide a recognizable name for the integration.
Service URL:
Enter the URL of your Jira instance.
Project Key:
Enter the project key (The key you provided while creating the project in Jira for ticket creation).
Username/Email:
Enter the Jira account username or email.
API Token:
Add the Jira API token if you already have one.
If you want to generate a new one, you can click on the link button, which will navigate you to the Atlassian/Jira account.
How to create a Jira API token:
Log in to your Atlassian/Jira account in a browser.
Open Account Settings â†’ Security â†’ API tokens (or navigate to https://id.atlassian.com/manage-profile/security/api-tokens).
Click Create API token and provide a label (for example: Token Name, like (Jira Integration), and token expiry date).
Given the token name
Set the expiry date and click on create.
Once done, copy the generated token.
Paste this token into the integrationâ€™s API Token field within the DvSum app.
Jira Cloud API tokens inherit the permissions of the user who created them. Ensure that the user has appropriate access to the project.
Note:
At present, the Create API token with scopes option is not supported. Only the standard Create API token functionality is available.
Once the Jira account details are added and tested successfully, at the end of the connection details, you will see the project details, e.g., Issue Type, Priority, and Assignee.
Note:
When adding or editing an Integration, if incorrect details or invalid credentials are entered, the system will still allow the test connection and to save it. However, the Integration will not be usable until the credentials are verified.
Configuring Jira in DQ Rules
Enable Integration
Once Jira integration is complete, rules can be configured to create tickets when exceptions are detected.
Navigate to DQ Rules.
Select a rule where you want Jira integration to be enabled.
Go to Edit Settings and navigate to External Ticketing.
Turn on the option Enable Jira Ticket Creation.
Select the integration value from the dropdown.
Define how exceptions should trigger ticket creation:
Manual creation.
Automatic creation (based on exception detection only)
Click on save.
Ticket Creation Modes
Manual Ticket Creation
Navigate to a DQ rule with exceptions.
Click the Create Ticket button.
Fill in ticket details
Records to include
(You can select any one option based on what exceptions you want to see as an attachment in the ticket).
Select Issue Type (the dropdown will show the values defined in your Jira project).
Title (By default, it will show the rule description, but you can add your own defined title as well).
The
description
field has the pre-defined format, but you can also add a description or the details as per what you want to see in the ticket.
Select the
assignee
of the ticket from the dropdown (these are the users who are added in your Jira project).
Click on the create button to finish.
As soon as you click the Create button, a Jira ticket will be created that is linked to the DQ rule.
Note:
Clicking on the ticket ID will redirect you to the Jira account for a detailed view of the ticket. You can see the ticket was created.
Records To Include Options:
All Exception:
It will record all the exceptions in the ticket.
Exception records since last ticket creation:
This includes only the exceptions that have appeared since the last time a ticket was created.
Exception records since last ticket creation from latest run only:
It captures exceptions from the most recent run of the rule execution that also havenâ€™t been ticketed yet.
Note: Since both options are related to Exception Tracing, they will only be enabled when Exception Tracing is turned on.
For more details on how to configure exception tracing on DQ rules, you can go through this article on
Configuring Exception Settings for Rules in DvSum
.
No Exception ticket:
These are tickets that are created manually or for reasons other than automatic exceptions.
If the ticket is newly created, it will capture only the exceptions that have occurred since the last ticket creation. This ensures that previously logged exceptions are not duplicated, and the ticket reflects only the latest set of issues that need attention.
Automatic Ticket Creation
Navigate to a DQ rule with exceptions.
Go to Edit Settings and navigate to External Ticketing.
Change creation mode to Automatic
Choose the exception option under the Records to Include.
Select the Issue type and assignee
Note:
For automatically generated tickets, fields such as the title and description cannot be manually controlled. These are automatically populated and updated based on the rule configuration.
After saving the settings, the Create Ticket button will be disabled, and a ticket will be created automatically during the next execution of the rule.
Ticket Workflow & Status Sync
To Do:
When a new ticket is created, the ticket action workflow is enabled, and the initial status is set to To Do.
In Progress:
When a ticketâ€™s status is updated from To Do to In Progress in Jira, the change automatically syncs with the DvSum rule detail page. This triggers the workflow, and the rule status is updated to In Progress.
Jira Ticket
DvSum Rule
Resolved/Done:
When the assignee updates the ticket status to Resolved or Done, the workflow syncs with the rule, and the rule status is updated to Resolved.
Note:
If the DQ workflow is enabled for this rule, its execution will be managed through the associated Jira ticket. The workflow status will mirror the Jira ticket status, meaning the rule is governed via Jira instead of the action workflow.
Ticket ID Column
A new column called Ticket ID has been added in the DQ Dictionary governance view available column and in the data tab of the rule detail page as well, which displays only the most recently created ticket for the rule.
Non-Exception Rule Ticket Creation
For non-exception rule types such as Unique Values, Count, Metric (Sum), and Freshness, Jira tickets can be created either manually or through automatic ticket creation.
The overall process for creating a Jira ticket is a little simpler for these rules, as they don't have any exception dat,a so a simple ticket with title and description is created
Behavior Based on Rule Status
If the rule status is Healthy, then a manual ticket can bee created but an automatic ticket can not be created.
If the rule status is Alerting, then both manual and automatic tickets can be created.
Mass Update
Users can now update DQ rules to integrate with Jira using the mass update functionality from the DQ Dictionary listing view.
Navigate to the DQ Dictionary listing view.
Select one or more DQ rules, then click Mass Update.
From the drop-down, choose Ticketing Integration.
Select the desired action (Update or Make Empty) and then pick the appropriate Jira integration value.
Click Apply to save the changes.
Summary
The Jira integration with DvSum streamlines exception management for DQ rules by:
Allowing
direct Jira ticket creation
(manual or automatic) from within the platform.
Ensuring tickets are linked to specific rules, making it easy to track health and resolution.
Providing a
single view in DvSum
to monitor rule exceptions and related tickets.
Leveraging Jiraâ€™s workflows, notifications, and project tracking features for efficient issue resolution.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configuring Exception Settings for Rules in DvSum
URL: https://dvsum.zendesk.com/hc/en-us/articles/39028423072916-Configuring-Exception-Settings-for-Rules-in-DvSum
================================================================================

In this article:
Accessing the Exception Configuration Panel
Exception Configuration Options
Alerting Exception Files
Exceptions to Include
Email Recipients
Enable Exception Tracing
New Metadata Columns in the Data Tab
The
Exception Configuration
panel provides greater control over how exceptions are managed in DvSum. By customizing these settings, users can streamline exception handling, reduce alert fatigue, and improve traceability.
This article explains the purpose and functionality of the
Exception Configuration
panel and how to use it effectively.
Accessing the Exception Configuration Panel
To access the
Exception Configuration
panel:
To configure exception settings for a rule:
Navigate to the
Dictionaries
>
DQ Rules
.
Choose a rule from the
listing page
.
Go to the
Data Quality
tab.
Click on
Edit
to open the rule settings.
Select the
Notifications
section from the left panel.
Scroll down to locate the
Exception Configuration
panel.
Exception Configuration Options
The
Exception Configuration
panel includes the following options:
Alerting Exception Files
When enabled, this option generates exception files during rule execution.
These files can be used for auditing or further analysis.
Exceptions to Include
All Exceptions
: Includes all exceptions identified during the rule run.
New Exceptions Only
: Includes only exceptions that are new since the last execution.
Email Recipients
Allows users to define who should receive exception alerts.
You can select from default email groups or specify custom recipients.
These recipients will receive the exception notification email when the rule is run.
Enable Exception Tracing
Enabling
Exception Tracking
is required to identify and differentiate between
new
and
existing
exceptions during rule execution.
This feature uses a
key field
(e.g.,
customer_id
) to trace recurring exceptions across multiple runs. Once enabled, users can easily spot
new exceptions
, which are typically the primary focus during data quality reviews, along with the
existing exceptions
.
Note:
After enabling Exception Tracking and executing the rule, additional metadata columns will appear in the
Data
tab to support exception analysis.
New Metadata Columns in the Data Tab
When Exception Tracing is enabled, the following columns are added to the
Exception Deep Dive
view in the
Data
tab:
DV First Observed
â€“ Timestamp of when the exception was first detected.
DV Status
â€“ Indicates whether the exception is
New
or
Existing
.
DV Age
â€“ Shows how long the exception has existed since it was first observed.
These columns help users monitor exception trends and prioritize remediation efforts more effectively.
Status
filter has been added, allowing users to quickly switch between viewing
All
,
New
, or
Existing
exceptions.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational Data Quality - Uniqueness Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/35889122920468-Foundational-Data-Quality-Uniqueness-Rule
================================================================================

In this Article:
Overview
Steps to Configure the Uniqueness Rule
Step 1: Access Data Dictionary
Step 2: Select Data Quality Rules
Step 3: Add a New Rule
Step 4: Configure Basic Inputs
Step 5: Validate and Execute the Rule
Overview
The "UNIQUENESS" rule ensures that values within a specific data element are unique across the dataset. This rule is crucial for maintaining data integrity by detecting and preventing duplicate entries.
For instance, if a customer table contains duplicate records for the same customer, the Uniqueness rule will identify these duplicates and display the duplicate keys along with their count. This helps users pinpoint and resolve data duplication issues efficiently.
For more details about the ruleâ€™s scope, threshold, notifications, and other configurations, refer to the
Rule Detail page.
Steps to Configure the Uniqueness Rule
Step 1: Access Data Dictionary
Log in to
DvSum
.
Go to the
Dictionary
dropdown and select the
Database Tables
tab.
Choose the relevant
Data Source
and
Table Name
.
Step 2: Select Data Quality Rules
Click on the
Data Quality
tab.
Choose
Available Rules
.
Step 3: Add a New Rule
Click on the
âŠ• Add Rule
button.
Select the
Foundational DQ
category.
From the available options, choose
UNIQUENESS
.
Step 4: Configure Basic Inputs
In the
Rule Wizardâ€™s Basic Input
section:
Enter the
Rule Description
.
Select the
Column Name
for which uniqueness needs to be validated.
Step 5: Validate and Execute the Rule
Save the rule configuration.
Click
Run
to execute the rule.
Review the results to identify and address any duplicate records.

--------------------------------------------------------------------------------

================================================================================
TITLE: Access DQ Rule information via REST API
URL: https://dvsum.zendesk.com/hc/en-us/articles/33018039015828-Access-DQ-Rule-information-via-REST-API
================================================================================

In this Article
Overview
Detailed Steps
API Key
Making an API Call to Retrieve DQ Rules
Key Information
Example API Call
Defining and Viewing DQ Rules in DvSum
Custom Views and API Extraction
Testing with Postman
Overview
All information about Data Quality (DQ) rules is accessible through the DvSum web user interface and is also available via API. Using the API provides users with greater flexibility and control over their DQ results. For example, you can integrate the data into a corporate BI tool or dashboard, or archive the results for future review.
The data retrieved via API corresponds to what is displayed on the
DQ Dictionary
page and is filtered based on the specific views of your rules that you define on this page.
Detailed Steps
Authentication Update
DvSum APIs now use
OAuth2 Client Credentials
for secure authentication instead of direct API keys.
To access APIs:
Navigate to Administration â†’ Account â†’ Application Security â†’ Manage OAuth2 Clients.
Create an
Authentication Client
and download the generated
Client ID
and
Client Secret
.
or
Alternatively, enable API access at the user role level, and then generate a user roleâ€“based credential from
My Settings â†’ API Credentials
tab.
Use these credentials to generate an
Access Token
using the Authentication API mentioned in the
API Documentation.
Refer to the article
â€˜API Access and Authentication Managementâ€™
for details on creating an authentication client and generating access tokens.
DvSum API Access and Authentication Management
Making an API Call to Retrieve DQ Rules
To fetch a list of Data Quality (DQ) Rules, follow the steps below:
Key Information
Base URL
:
https://apis.dvsum.ai/node-listing/listing/rules?view-name=
Parameter
:
Name
:
view-name
Value
: Specify the name of the DQ Rules view you have defined in DvSum.
Example: Use
"DQ Rules"
for the pre-defined view available to all users.
Headers
:
Value
: Use the API key obtained in the previous step.
Authorization
: Bearer
<access_token>
.
Example API Call
https://apis.dvsum.ai/node-listing/listing/rules?view-name=DQ Rules
Ensure to replace
view-name
with your desired DQ Rules view name as required.
Defining and Viewing DQ Rules in DvSum
To access and define views in DvSum, follow these steps:
Navigate to the DQ Dictionary
:
Go to the
Dictionary
dropdown and select the
DQ Rules
in the DvSum platform.
Select a View
:
Choose any available view to retrieve the rules and fields relevant to your needs.
Default View
:
The default view is
"DQ Rules"
, which is accessible to all users.
You can also define a custom view to specify the rules and fields you want to retrieve.
These views can be used as input parameters for API calls to fetch specific Data Quality rules and related fields.
Custom Views and API Extraction
When using
custom views
for API extraction, keep the following in mind:
Owner vs. Non-Owner Accounts:
If the view is created by the
owner account
, it is directly available for API calls.
If the view is created by a
non-owner account
, it must first be
shared with the owner account
before it can be used in the API.
Sharing a Custom View:
Navigate to the DQ Dictionary and locate the custom view you created.
Use the
share option
to share the view with the owner account.
Once shared, the owner can access the view, and it becomes available for API extraction.
Note:
If a custom view from a non-owner account is not shared with the owner, attempting to use it in an API call will result in no data being retrieved.
Testing with Postman
Below is an example where the selected view is
â€˜Analyze Rulesâ€™
.
The headers and the successful JSON response to the GET request are displayed.

--------------------------------------------------------------------------------

================================================================================
TITLE: Reference Dictionary with Value Range/Ruleset Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/28703505711636-Reference-Dictionary-with-Value-Range-Ruleset-Rule
================================================================================

In this Article
Overview
Detailed Steps
Ruleset Rule
Overview
Reference dictionaries can be added to value range and ruleset rules. This allows you to map the columns defined in the reference dictionary to the columns in your dataset, ensuring that data is validated according to the data defined in the reference dictionary. For more information, please see this detailed article on
reference dictionary
.
Detailed Steps:
Step 1:
Log in to DvSum, navigate to the Dictionary dropdown, select the Database Tables tab, and then choose the relevant data source and table name.
Step 2:
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3:
Select the "âŠ• Add Rule" button, then choose the "Foundational DQ" category. From the list of options, click on "Value Range".
Step 4:
Basic Input
In the Rule Wizard's Basic Input section, Select "Ref Dictionary" from dropdown in Comparison type.
Step 5:
Select the Dictionary and add relevant source fields.
Also, we can cancel the fields and map the fields only we want to.
Step 6:
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.
Ruleset Rule
Step 1:
Select the "âŠ• Add Rule" button, then choose the "Foundational DQ" category. From the list of options, click on "Ruleset".
Step 2:
In the Rule Wizard's Basic Input section, Select "Ref Dictionary" from dropdown in Value range check.
NOTE:
Reference dictionaries with any defined hierarchical levels will not be shown here.
Step 3:
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Reference Dictionary
URL: https://dvsum.zendesk.com/hc/en-us/articles/28688102882196-Reference-Dictionary
================================================================================

In this Article
Overview
Detailed Steps
Step 1: Access Reference Dictionary
Step 2: Add a New Dictionary
Step 3: Add Code/Value Pairs
Step 4: Import File
Step 5: Verify Data
Overview
Reference Dictionary is like a pre-organized reference book where users can create their own dictionaries, associating useful information with sets of values. This is helpful for DvSum validation audits, allowing users to choose a reference dictionary with a set of data values instead of listing individual ones.
Detailed Steps
Step 1: Access Reference Dictionary
Log in to DvSum, proceed to Administration tab > Organizations > Reference Dictionary.
Step 2: Add a New Dictionary
Select
Add Dictionary
and add Name, Description and select one of the below:
Key/value pair:
A key/value pair is a way to store and organize information where each piece of data (value) is labeled with a unique identifier (key).
is Hierarchical:
A dictionary that arranges attributes in a parent-child hierarchy.
Step 3: Add Code/Value Pairs
Choose how to add code value pairs either by entering values manually or import file.
Step 4: Import File
Click on Import File and choose the file to be uploaded, click OK and then click Done.
Also there is an option to Enter data manually
Step 5: Verify Data
Fields are populated from upload file in values section.

--------------------------------------------------------------------------------

================================================================================
TITLE: Dashboard and Widgets
URL: https://dvsum.zendesk.com/hc/en-us/articles/30481164045588-Dashboard-and-Widgets
================================================================================

In this Article
Overview
Default Dashboards
Dashboard Creation
Pick from Gallery
Create your Own
Sharing the Dashboard
Delete a Dashboard
Clone a Dashboard
Drill-down Functionality
Overview
A dashboard is a centralized interface that visually organizes and presents key data points, enabling users to monitor, analyze, and make informed decisions. In the DvSum application, dashboards provide an efficient way to manage and track data across various modules, offering customizable views and functionality. This article covers the types of dashboards available in DvSum, how to create and customize your own dashboards, and the options provided for sharing, downloading, and managing dashboard data. Whether using default dashboards or building your own, DvSum's
dashboard features help streamline data presentation and enhance decision-making.
Default Dashboards
Four default dashboards are provided:
CADDI Dashboard
Catalog Dashboard
DQ Dashboard
Platform Dashboard
Note:
The default dashboards can not be shared with any other user.
Created by Me: Displays all dashboards created by the user.
Shared with Me: Lists all the dashboard shared with the user by someone else.
Dashboard Creation
Click on Create Dashboard
Specify a name and description. Click on "Create" button.
An empty Dashboard is displayed.
Click on Add Widget, choose either "Pick from Gallery" or "Create your Own".
Pick from Gallery
Includes all available widgets from every module within the DvSum application.
After selecting a widget, the detail page is displayed These template widgets automatically populate the title, module, preferred
chart type
for
slicers
(Y-axis) and
metrics
(X-axis). The data can be
sorted
for display on the chart, and
Data Labels
(legends) can be enabled on the charts.
Create your Own
A new widget can be created and the module can be targeted, own filter criteria and chart type can be selected.
Module Filter Criteria
Module filter criteria are predefined conditions that allow users to narrow down data within a module based on specific parameters, helping to focus on relevant information. These filters streamline user interaction by providing a customized view that aligns with their needs and objectives.
New filter criteria can also be added.
This newly created criteria will then be populated in Selected Filter:
After saving the widget, it
will start appearing on Dashboard.
Enabling Data Labels
To enable data labels on widgets, users can do so from the create/edit widget form, as shown below.
When data labels are enabled for the widget, it will be displayed as shown in the image below.
Disabling Data Labels
When data labels are disabled for the widget, it will be displayed as shown in the image below.
You can add more widget by clicking on "
Add Widget
" Button.
The widget can be expanded by clicking the Expand Icon:
Upon clicking the three dots, below options are displayed:
Add to Home:
The selected widget will start appearing on the Home page as shown below:
Download Widget
The widget can be downloaded as a CSV file.
Sharing the Dashboard
It is possible to either keep a Dashboard private or share it with others. When sharing, two options are available: granting Editor or Viewer rights. If shared as a Viewer, the recipient will have three options: expanding and viewing widgets in full screen, downloading, and adding the dashboard to their home screen.
Viewer Access
Editor Access
Delete a Dashboard
When deleting a dashboard, a prompt appears indicating the number of people the dashboard is shared with and asking if the user would like to proceed.
Clone a Dashboard
Dashboards that are set as default can be cloned but cannot be deleted. However, dashboards created by the user can be both deleted and cloned.
Drill-down Functionality
Drill-down functionality is available in Dvsum's dashboard widgets, allowing users to click on charts, pies, or bars to access more detailed data. This feature enables exploration of granular insights directly from the visual elements of the dashboard.

--------------------------------------------------------------------------------

================================================================================
TITLE: Table Detail Page
URL: https://dvsum.zendesk.com/hc/en-us/articles/42041709770772-Table-Detail-Page
================================================================================

In this Article
Overview
Accessing the Table Detail Page
Overview Tab
Definition Section
Asset-Specific Details
Managed By
Tags
Custom Attributes
Data Quality Tabs
Usability Score
Lineage
Relationships
Additional Info
Actions Available
Edit
Delete Asset
4. Data Tab
1. Profiling Info
2. Column Summary
3. Column View
5. Data Model
6. Data Quality
Overview
The Table Detail Page provides a comprehensive view of a data table and its associated metadata, relationships, and quality metrics. It helps users understand the tableâ€™s context, ownership, and attributes.
1. Accessing the Table Detail Page
Navigate to
Dictionaries
>
Database Tables.
Click on the
Table Name
to open the Table Detail Page.
2.Overview Tab
The
Overview
tab summarizes key information about the selected table, including general metadata, relationships, and custom attributes.
1.Definition Section
This section provides a high-level description of the table, explaining its purpose and contents.
Description:
A brief summary of what the table contains or represents.
Data Domain:
The subject area the table belongs to (e.g., Finance, HR, Education).
Relationships:
How the table connects to other entities, such as:
Belongs To Database Schema
â€“ identifies the schema or database.
Contains Database Columns
â€“ shows the number of columns in the table.
Validated By Data Quality Rules
â€“ lists any rules linked to the table.
2. Asset-Specific Details
Displays system-generated metadata about the table.
Record Count:
Number of records in the table.
Last Scanned On:
The last time the table was profiled or scanned.
Last Updated On:
When metadata was last updated.
3. Managed By
Shows ownership and stewardship information.
Data Owner:
Individual or team accountable for the dataset.
System Owner:
Person or application responsible for technical maintenance.
Data Steward:
Responsible for governance, quality, and documentation.
Audit Trail:
Records of when and by whom the table was created or updated. By clicking on the three dots on audit trail shows the activities done on that table.
4. Tags
Tags categorize and enhance discoverability of the table. Tags can be used for search, filtering, and reporting.
Data Classification:
e.g., Confidential, Public.
Certification Tag:
Indicates whether the dataset has been validated.
Custom Tags:
User-defined labels for grouping or filtering.
Data Quality Tags:
Highlight data quality aspects or issues.
Note: For more information on Tags, refer the article
How to create Tag
.
5. Custom Attributes
Displays organization-specific metadata fields that enrich the table definition.
Examples include:
Authentication Type:
Specifies the access mechanism.
Validation Check:
Boolean or validation field.
Numeric or Text Attributes:
Custom fields like credit card numbers or notes.
Custom attributes vary based on organizational configuration.
For more information on the custom attribute, refer the article
Creating and Managing Custom Attribute
.
6. Data Quality Tabs
Clicking the arrow mark will redirect to the Data Quality page and provides insights into profiling results, rule validations, and quality metrics.
For more details on the DQ rules, refer the article
Rule Detail Page
.
7. Usability Score
Clicking on the eye icon in the overview page, user can see the details of the usability score of the table. The
Usability Score
reflects the completeness and quality of metadata. A higher score indicates that more detailsâ€”like definitions, tags, and ownershipâ€”are filled in.
For more information on understanding and setting up the usability score refer to the article
Usability Score
.
8. Lineage
Lineage
refers to the traceability of data through various stages in its lifecycle. It describes how data moves, transforms, and is used throughout an organization or system. Essentially, data lineage provides a "map" of the data's journey, showing where it originates, how it is processed, and where it is ultimately consumed.
For more information on lineage, refer to the article
Lineage in DvSum
.
9. Relationships
Relationships describe how different assets are connected to each other. In a well-structured data asset management system, relationships help link tables, datasets, and data elements to ensure data is organized logically and can be traced back to its origin.
For more information on adding relationship to asset, refer to the article
How to Manage Relationships Between Assets
.
10. Additional Info
The Additional Info tab allows users to add, edit, or remove supplementary information related to a table. It provides a text editor with formatting options (such as font style, size, color, and numbering) to include detailed notes or context as needed. Once the information is entered, users can save it as an item, modify or delete it, and choose to publish or discard the changes. Published additional information becomes part of the tableâ€™s details.
For more information on Additional info tab, refer to the article "
Rich Text Additional Information
"
3.Actions Available
Edit:
Update table metadata, description, or tags.
When we click the
Edit
button on the Overview page, it switches to editable mode, allowing the user to modify all the metadata information of the table mentioned above.
Clicking on the pencil icon allows the user to edit the required metadata.
The user then needs to click the
checkmark
, select
Done
, and finally click the
Publish
button to save and apply the changes.
Delete Asset:
Tables marked as
Deleted
will have the option to be permanently removed from the system.
4. Data Tab
The
Data Tab
provides a detailed view of the dataset's content, allowing users to examine individual records and column-level statistics.
Note: For more information on the Run Profiling, refer to the article
Run Online & Run Offline
.3.
1. Profiling Info
The
Profiling Info
section provides essential metadata about the dataset, helping users understand the scope and last interactions with the data:
Record Count
: Displays the total number of records in the dataset. This helps assess the size of the data and understand its scope.
Last Scanned On
: Shows the last time the dataset was scanned or updated. This is crucial for determining the freshness of the data.
Last Updated On
: Indicates when the data was last modified or updated, helping track data changes over time.
2. Column Summary
The
Column Summary
section provides an overview of the tableâ€™s structure, including key details about each column:
Total Columns
: The total number of columns in the dataset.
Granularity
: Specifies the level of detail at which the data is recorded. For instance, granularity could be at the level of individual transactions, customer records, or events.
Attributes
: Lists the attributes in the dataset that are key to understanding the dataset's structure. For example, a numerical attribute like 'Weight' or a categorical one like 'Action'.
Measures
: This would include any calculated or aggregated values in the dataset, such as totals or averages.
Time Series
: If the dataset includes time-based data (such as timestamps or dates), this section would highlight that, enabling users to track trends over time.
3. Column View
The
Column View
provides detailed insights into each column in the dataset, allowing users to visualize the distribution and patterns within the data.
In column view there are two types of view,
Grid view
Dictionary view
Grid View:
The
Grid View
provides a visual representation of the record counts across columns, as shown below.
Dictionary view:
The Dictionary View provides the list of columns in the table.
Users can edit the columns in the
Dictionary View
. After making the necessary changes, click
Save
to apply and update the data.
Users can use the
Filter
option in the
Dictionary View
to easily access the required columns.
5. Data Model
An
Entity Relationship (ER) Diagram
visually represents how datasets and their columns are connected within the system. It helps users understand data relationships and gain a clearer view of the overall data structure.
For detailed information the Entity Relationship (ER) Diagram, refer to the article
Entity Relationship Diagram - ERD
.
6. Data Quality
The
Data Quality
tab in DvSum manages all rule-related settings, allowing users to view recommended and available rules or create new ones to ensure accurate, complete, and reliable data.
The
Data Quality
tab in the Database Tables contains the following sections:
Statistics
: Displays key metrics such as DQ Score, total rules, rules with alerts, total exceptions, total records scanned, and last rule execution status.
Rules
: Lists all data quality rules applied to the column, including details such as rule ID, description, run status, alert status, and exception count.
Note: To learn more about the Rules section in Data Quality tab, refer the article
Data Quality Overview
.

--------------------------------------------------------------------------------

================================================================================
TITLE: Enabling Query History for Data Sources
URL: https://dvsum.zendesk.com/hc/en-us/articles/33861396141076-Enabling-Query-History-for-Data-Sources
================================================================================

In This Article
1. Enable Query History on Databricks
2. Enable Query History in SQL Server, Azure SQL, Azure Synapse
3. Enable Query History in PostgreSQL
4. Enable Query History in Snowflake
5. Enable Query History in MySQL
6. Enable Query History in Netezza
7. Query History in Oracle
Introduction
Query history plays a vital role in data lineage formation within applications. By capturing the details of query execution it allows for precise tracking of how data flows and evolves. This article provides a detailed guide to enabling query history across major data platforms such as Databricks, SQL Server, Azure SQL, Azure Synapse, PostgreSQL, and Snowflake. It also touches on additional considerations for databases where query history is enabled by default, such as Oracle and Netezza.
Whether working with on-premises databases or cloud-based solutions, this guide will help you configure query history to ensure seamless lineage formation and support effective data analysis and governance.
1. Enable Query History on Databricks
To activate
Query History
in Databricks for lineage tracking, follow these essential steps:
Authenticate
using one of the supported methods:
Personal Access Token (PAT)
OAuth Client Credentials (Client ID & Secret)
Assign necessary permissions
at multiple levels:
Catalog
Cluster
SQL Warehouse
(Optional)
Verify access
by executing a validation query or making an API call to confirm that historical query data is being captured successfully.
Choose an Authentication Method
You can authenticate using either a
Personal Access Token
or
Client Secret credentials
for the service principal.
Personal Access Token
Open your workspaceâ€™s
Admin Settings
(click your username in the top bar > Admin Settings).
Go to the
Advanced
tab.
Under
Access Control
:
Enable
Personal Access Token
.
Click on
Permission Settings
.
Search for your user.
Assign the
"Can Use"
permission.
Click
Add
.
Once enabled, run the Lineage scans on the Databricks source to begin bringing in Lineage data into the application.
Client Secret Credentials (Alternative to Personal Access Token)
If using Client Secret credentials, ensure the following permissions are in place:
Navigate to
Admin Settings > Access Control
.
Assign the following minimum required permissions:
Workspace permissions
: Can Use
Catalog access
: Required permissions at the catalog level (see next section).
Compute access
: Can Attach To for the cluster or SQL Warehouse permissions.
2. Enable Catalog access for Users
Log into your Databricks workspace.
Click on the
Catalog
tab in the left menu.
Select the
catalog
configured for Lineage.
Open the
Permissions
tab.
Click
Grant
.
Search for your
user or service principal
.
Grant the necessary privileges; at a minimum, SELECT and READ_METADATA are required.
Click
Grant
.
Enable cluster access for the User
To configure the
Compute
cluster permissions:
Navigate to the
Permissions
section.
Select the relevant
user
or
service principal.
Set the permission to
Can Attach To
.
To configure theÂ SQL WarehouseÂ permissions:
Navigate to theÂ Permissions
Assign the permission levelÂ Can Use
Grant Access to
system.access
Schema for Lineage Queries
Databricks now provides access to fine-grained query history and user actions via the
system.access
schema.
This schema is required to access detailed historical logs used for lineage tracking.
Steps:
Navigate to
the Catalog Explorer in your Databricks workspace.
Expand the
system
catalog and select the
access
schema.
Click the
Permissions
tab.
Grant your user or service principal the following
minimum permissions
:
SELECT
Use_Schema
Example SQL (if using Unity Catalog SQL):
GRANT
SELECT
, Use_Schema
ON
SCHEMA system.access
TO
`your
-
user
-
or
-
service
-
principal`;
This enables your application or service principal to query audit tables like
query_history
,
query_text
, and
query_execution
.
Databricks Validation Check: Did it work?
You can test access by calling the query history endpoint using
curl
:
curl --location
'https://<DATABRICKS_HOST>/api/2.0/sql/history/queries'
\
--header
'Authorization: Bearer <PAT/ OAUTH token>'
Replace
<DATABRICKS_HOST>
with your workspace URL and
<PAT/ OAUTH token>
with your personal access or OAuth token.
2. Enable Query History in SQL Server, A
zure SQL, Azure Synapse
Use the
Query Store
.
The Query Store is enabled by default for new Azure SQL Database and Azure SQL Managed Instance databases.
Query Store is not enabled by default for SQL Server 2016 (13.x), SQL Server 2017 (14.x), SQL Server 2019 (15.x). It is enabled by default in the
READ_WRITE
mode for new databases starting with SQL Server 2022 (16.x). To enable features to better track performance history, troubleshoot query plan related issues, and enable new capabilities in SQL Server 2022 (16.x), we recommend enabling Query Store on all databases.
Query Store is not enabled by default for new Azure Synapse Analytics databases.
Enabling the Query Store can be done in multiple ways.
Enable Query Store using the Query Store page in SQL Server Management Studio
In Object Explorer, right-click a database, and then select
Properties
.
Note: Requires version 16 or later of Management Studio
In the
Database Properties
dialog box, select the
Query Store
page.
In the
Operation Mode (Requested)
box, select
Read Write
.
Enable Query Store using Transact-SQL statements
Use the
ALTER DATABASE
statement to enable the query store for a given database. For example:
ALTER DATABASE <Your Database name>
SET QUERY_STORE (QUERY_CAPTURE_MODE = ALL);
In Azure Synapse Analytics, enable the Query Store without additional options, for example:
ALTER DATABASE <database_name>
SET QUERY_STORE = ON;
SQL Server, Azure SQL, Azure Synapse Validation Check: Did it work?
After completing the above steps, verify that query logging is properly enabled by running the following SQL query:
SELECT TOP 100 qsr.*
FROM sys.query_store_query qsr
LEFT JOIN sys.query_store_query_text qsrt
ON qsr.query_text_id = qsrt.query_text_id
LEFT JOIN sys.query_context_settings qcs
ON qcs.context_settings_id = qsr.context_settings_id
LEFT JOIN sys.schemas s
ON s.schema_id = qcs.default_schema_id;
3. Enable Query History in PostgreSQL
To enable query history in PostgreSQL, you can modify the configuration file (postgresql.conf) and use SQL statements for more fine-grained control.
Modify postgresql.conf
You need superuser privileges to edit this file.
You can locate
postgresql.conf
in your PostgreSQL data directory.
Common locations include:
/etc/postgresql/<version>/main
(Linux)
C:\Program Files\PostgreSQL\<version>\data
(Windows)
Edit
postgresql.conf
Set
logging_collector
to '
on
'
Set
log_statement
to 'all' to track SQL queries. For reference, these are the valid values:
none
(default): Log no statements.
ddl
: Log data definition language (DDL) statements like CREATE, ALTER, and DROP.
mod
: Log moderate-level statements, including DDL and most of the data manipulation language (DML) statements.
all
: Log all statements, including SELECT, INSERT, UPDATE, DELETE, and more.
Use SQL Statements for Fine-Grained Control
If you want to enable or disable query history for specific databases or sessions, you can use SQL statements. This can be especially useful for debugging or auditing purposes. You can change logging settings on a per-session or per-database basis using the following SQL commands:
To enable query history for the current session only:
SET log_statement = 'all';
To enable query logging for a specific database (replace your_database with the actual database name):
ALTER DATABASE your_database SET log_statement = 'all';
To disable query logging for a specific database:
ALTER DATABASE your_database SET log_statement = 'none';
These SQL statements will take effect immediately for the current session or database.
Remember to be cautious with query history in production environments, as it can generate large log files and potentially impact performance. Always monitor your log files and adjust the log level as needed to balance the need for more information and the need for better performance.
PostgreSQL Validation Check: Did it work?
After completing the above steps, verify that query logging is properly enabled by running the following SQL query:
S
ELECT
pst.
*
FROM
pg_stat_statements pst
LEFT
JOIN
pg_database pd
ON
pst.dbid
=
pd."oid"
LEFT
JOIN
pg_catalog.pg_user pu
ON
pu.usesysid
=
pst.userid
LIMIT
10
;
This query will return the recent queries logged in PostgreSQL. If it returns results, query logging is successfully enabled.
4. Enable Query History in Snowflake
Query history is always enabled in Snowflake. So the only task required is to make sure that the relevant user has access to the history.
The configured user should have access to SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
By default, the SNOWFLAKE database is visible to all users; however, access to schemas in this database can be granted by a user with the ACCOUNTADMIN role using either of the following approaches:
Grant IMPORTED PRIVILEGES on the SNOWFLAKE database.
USE ROLE ACCOUNTADMIN;
GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE SYSADMIN;
GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE customrole1;
For more information regarding snowflake, this article
Account Usage in Snowflake
can be referred
Snowflake Validation Check: Did it work?
After completing the above steps, verify that query logging is properly enabled by running the following SQL query:
SELECT
QUERY_ID,
QUERY_TEXT,
DATABASE_NAME,
SCHEMA_NAME,
QUERY_TYPE,
USER_NAME,
EXECUTION_STATUS,
START_TIME,
END_TIME,
TOTAL_ELAPSED_TIME,
SESSION_ID
FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
WHERE START_TIME > (CURRENT_TIMESTAMP - INTERVAL '30 DAY')
LIMIT 10;
5. Enable Query History in MySQL
MySQL does not provide a built-in query history like some other databases, but query history can be tracked using the following methods:
Using the
MySQL General Query Log
The General Query Log records all queries executed on the server.
Steps to Enable:
Enable the general log:
SET
GLOBAL
general_log
=
'ON'
;
SET
GLOBAL
log_output
=
'TABLE'
;
-- or 'FILE'
If using the
TABLE
option, view the logs:
SELECT
*
FROM
mysql.general_log;
If using the
FILE
option, check the log file location:
SHOW
VARIABLES
LIKE
'general_log_file'
;
Considerations:
Enabling the General Query Log may impact performance.
Logs should be cleared regularly to avoid storage issues.
Using the
MySQL Performance Schema
The Performance Schema tracks recent queries with performance-related details.
Steps:
Enable the
performance_schema
in the MySQL configuration file (
my.cnf
or
my.ini
):
[mysqld]
performance_schema
=
ON
Restart the MySQL server.
Query the
events_statements_history
table for recent queries:
SELECT
*
FROM
performance_schema.events_statements_history;
For a summary of all queries:
SELECT
DIGEST_TEXT, COUNT_STAR
FROM
performance_schema.events_statements_summary_by_digest
ORDER
BY
COUNT_STAR
DESC
;
Additional Notes on Performance Schema Query History:
Default Behavior:
The
events_statements_history
table stores only a limited number of entries, depending on the
performance_schema
configuration.
Check the current size limit:
SHOW
VARIABLES
LIKE
'performance_schema_events_statements_history_size'
;
Increasing History Size:
To retain more query history, adjust the
events_statements_history_size
parameter:
Edit the
my.cnf
or
my.ini
file:
[mysqld]
performance_schema_events_statements_history_size
=
10000
Restart MySQL to apply changes.
Alternatively, change the size dynamically for the current session:
SET
GLOBAL
performance_schema_events_statements_history_size
=
10000
;
After increasing the history size, more queries will be stored in
events_statements_history
.
MySQL Validation Check: Did it work?
After completing the above steps, verify that query logging is properly enabled by running the following SQL query:
SELECT ess.*
FROM performance_schema.events_statements_summary_by_digest ess
LEFT JOIN performance_schema.events_statements_history esh
ON ess.DIGEST = esh.DIGEST
LIMIT 10;
6. Enable Query History in Netezza
In Netezza, query history can be accessed using the
HISTDB.HISTDBOWNER.NZ_QUERY_HISTORY
table, which is part of the History Database (HISTDB). This advanced feature provides detailed historical information about queries, sessions, and system activity.
7. Query History in Oracle
For Oracle-specific lineage and query history setup, refer to
Configure Oracle as a Source
.

--------------------------------------------------------------------------------

================================================================================
TITLE: Lineage in DvSum
URL: https://dvsum.zendesk.com/hc/en-us/articles/33694180953108-Lineage-in-DvSum
================================================================================

Overview
Data lineage in the DvSum Application provides a comprehensive view of the data's journey, helping users understand how data moves, transforms, and interacts across systems. It offers a visual map of the data flow, from the source to the destination, showing all stages of transformation and any dependencies involved. By utilizing data lineage, it is possible to trace the movement of data in real time, detect issues quickly, and assess the impact of changes to processes. This feature is essential for improving data transparency, ensuring data quality, and maintaining compliance with governance standards. This article covers the following topics:
Prerequisites for Data Lineage
Performing a Lineage Scan
Step 1: Navigate to Data Sources
Step 2: Initiate a Scan
Step 3: Select Scan Options
Step 4: View Scan Details
Understanding Lineage Visualization
Lineage View
Footer Options
Header Options
Placeholder Tables
Data Quality (DQ) Observability and Data Classification
DQ Observability
Data Classification
Exploring Functional and Technical Lineage
Functional Lineage
Technical Lineage
ETL Nodes
Managing Lineage Permissions in User Roles
Permission Levels
Entity Level vs. Entity-Attribute Level
Current Functionality
Demonstration
Adding an Upstream Node
Supported Data Sources
Prerequisites for Data Lineage
To enable effective data lineage in the DvSum application, the following prerequisites must be met:
Complete Database and Schema Scan:
Ensure that all relevant databases, schemas, and their associated tables/views are scanned in the DvSum application.
Query History Configuration:
Query history must be enabled for your data sources to derive both functional and technical lineage. For detailed instructions, refer to the
Enable
Query
History
for
Data
Sources
article.
Comprehensive Query Tracking:
Ensure all query types relevant to lineage (e.g.,
INSERT
,
MERGE
,
UPDATE
, and
SELECT
) are captured and made available for lineage analysis.
Lineage Extraction from Procedures, Functions, and Notebooks:
To successfully capture lineage from Stored Procedures, Functions, and Databricks Notebooks, users must have the necessary access permissions based on the source system:
Databricks:
The user must authenticate using a
Personal Access Token
.
The user must have access to
Databricks Notebooks
.
Snowflake:
The
user cataloging the Snowflake source must be the owner of the stored procedure
being used. Without this ownership, DvSum will not be able to extract lineage information.
For instructions on how to grant ownership, refer to the
Configure Snowflake as a Source
article.
All Other Sources:
The user must have access to the relevant stored procedures to retrieve lineage information.
Performing a Lineage Scan
Step 1: Navigate to Data Sources
Access the
Data Sources
section in the application.
Select the desired data source.
Step 2: Initiate a Scan
Navigate to the
Scan History
tab.
Click the
Scan Now
button, which provides the following options:
Profile
: Profiles data assets.
Lineage
: Tracks and visualizes the data flow.
The catalog scan runs by default. Users can use checkboxes to select additional scan types.
Step 3: Select Scan Options
Selecting the
Lineage
checkbox triggers a lineage scan.
Selecting both
Profile
and
Lineage
options triggers all three scans (Catalog, Profile, and Lineage) simultaneously.
This setup allows for the customization of scans based on specific requirements.
Step 4: View Scan Details
Locate the scan name and click on it to access detailed insights.
The detailed Insights are displayed as such:
Navigate to the
Table Detail
page for further exploration.
Understanding Lineage Visualization
Lineage View
The lineage visualization opens in a new tab, displaying:
Reference Node
(center): The selected table or view.
Upstream Nodes
: Sources influencing the reference node.
Downstream Nodes
: Entities impacted by the reference node.
Footer Options
Expand All Upstream Nodes
: Displays all upstream relationships.
Reference View
: Tables reference view ID displayed
Set Context
: Focuses the lineage on a specific node or column.
In the example, lineage is already set for a node, but any other node can be selected as Context:
Resetting to the original node is still an option if another node is set:
Expand Columns: All the columns can be expanded and have the same footer options as tables:
Placeholder Tables in Lineage
Placeholder tables are tables that exist in the database but are not recognized within the application. These tables are visually indicated by a dotted line around their boundary and placeholder text will be written on the node.
Key behaviors of placeholder tables:
They will
not be accessible elsewhere
in the application except in the lineage view.
The
Reference View
for placeholder tables
will not open
.
The
Reference View for their columns will also not open
.
Other than the above two options, the rest of the options on the placeholder node will be the same as that of any other table node.
This ensures that users can now identify and track placeholder tables within the lineage flow while maintaining standard lineage functionalities.
Header Options
Search:
Search for specific tables, nodes, or columns within the lineage view. Searches highlight all relevant elements, providing a quick way to locate specific data flows.
Reset View
: Resets the visualization to the default reference node view.
Share Lineage:
Shares a link to this lineage view with people or groups
Layers:
This displays two new options when clicked
Data Quality (DQ) Observability and Data Classification
DQ Observability
: Highlights upstream nodes in the lineage view with active alerting rules applied.
Data Classification
: Sensitive data tags and classifications (e.g., PII, PCI) are prominently displayed within the lineage visualization.
Exploring Functional and Technical Lineage
Functional Lineage
Functional lineage illustrates the data flow at an abstract level, showing tables, columns, and relationships (Everything discussed above was Functional Lineage)
Technical Lineage
Technical lineage includes:
Source and target code blocks derived from query history.
Attribute-level insights are not available for precise analysis.
If the angle brackets are clicked the code is displayed:
ETL Nodes in Technical Lineage
Technical Lineage also includes
ETL Nodes
, representing
Stored Procedures (SPs)
in the database. Stored Procedures are pre-written SQL scripts that execute complex data transformations and manipulations within the database. In our application, we refer to them as
ETL Nodes
.
Key Features of ETL Nodes:
Transformations on Columns
: ETL Nodes display the transformations applied to tables and columns within the lineage.
Stored Procedure Code Visibility
: Users can now
view the full SQL code of the stored procedure
from the ETL node.
Column-Level Transformations
: The lineage details now include transformations applied at the column level, providing deeper insights into data movement.
Just like any other node, the ETL node can also be set as context for viewing lineage from its perspective. The same thing can be done on transformations on ETL nodes.
Managing Lineage Permissions in User Roles
The
Lineage
section in user roles configuration allows managing access to lineage data. Below are the available permissions:
Permission Levels
User access to lineage features depends on assigned roles:
No Access
: Restricts lineage visibility.
Functional Lineage
: Allows entity and attribute-level exploration.
Functional and Technical Lineage
: Grants full access, including technical insights.
Entity Level
: Displays relationships between entities like tables or datasets.
Entity-Attribute Level
: Shows relationships at both entity and attribute levels.
Current Functionality
Admin Role
: Only admins can upload lineage data through the import process.
Upstream Nodes
: Users can add upstream nodes at the
table level
but not at the
column level
.
Demonstration
To add an upstream node:
Go to the
Table Detail Page
.
Click
Edit
to add upstream nodes at the table level.
Supported Data Sources
Lineage functionality is currently supported for:
Relational Databases
: Oracle, SQL Server, MySQL, PostgreSQL, IBM Db2, Azure SQL
Data Lakes and Warehouses
: Snowflake, Databricks, Azure Synapse, Netezza, Amazon S3
BI Tools
: Tableau, Power BI
Not Supported
: MongoDB, Salesforce, Azure Data Lake, and file uploads.
Enabling Lineage in Oracle
To enable lineage tracking from Oracle in DvSum, especially when stored procedures or packages are involved, the following setup is required.
Accessing Stored Procedure/Package Code
To extract lineage from stored procedures or packages, DvSum needs access to their source code. This source code is stored in Oracleâ€™s
ALL_SOURCE
system view. The Oracle user should have
read-only access
to this view to allow DvSum to parse the code and identify lineage relationships.
You can use the following SQL query to retrieve the procedure or package code:
SELECT
OWNER,
NAME,
TYPE,
TEXT,
LINE
FROM
ALL_SOURCE
WHERE
OWNER IN ('{schema_names}')
Note
: Replace
{schema_names}
with the actual schema names where the procedures or packages are defined.
Source Configuration Requirement
Make sure that all relevant schemas (where the procedure or package code resides) are included in the source configuration in DvSum. If any required schemas are missing, lineage extraction may not work as expected.

--------------------------------------------------------------------------------

================================================================================
TITLE: Run Online & Run Offline
URL: https://dvsum.zendesk.com/hc/en-us/articles/27004996852500-Run-Online-Run-Offline
================================================================================

In this Article
Overview
Details
Table Profiling
Rule Execution
Overview
Users can execute many different tasks in DvSum Data Catalog (DC). Testing a rule might take just a few seconds. But some rules could take several minutes or even longer. Therefore DC offers the ability to "
Run Online
" and "
Run Offline
" to give users the flexibility to run jobs as required.
Executing a rule synchronously and waiting to see the results is often the most efficient way to test a new rule. Executing a long-running job in the background and reviewing the results later is sometimes preferred.
Details
The option to "
Execute Now
" or "
Run Offline"
applies in several places.
Database Tables
DQ Rules
Table detail page
Rule detail page
The options are complementary and give developers the flexibility to work as they prefer.
Run
Online
- Will immediately execute the action. Note that if the user navigates to another page while the task is executing, then the task will be aborted.
Run Offline
- Will schedule the action to run soon. Typically the task will begin execution within a few seconds. The job will be visible in the "
Jobs"
tab.
Table Profiling
Table profiling can be executed from multiple places.
Go to the
Dictionary
dropdown and select the
Database Tables
tab.
Rule Execution
DQ Rules:
Rule Detail Page:
Note: Only the first 300 exceptions are displayed in the Grid on the Data tab. if needs to view more exceptions, must enable the â€œInclude all Exceptionsâ€ checkbox in the Run Online dropdown before running the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Entity Relationship Diagram - ERD
URL: https://dvsum.zendesk.com/hc/en-us/articles/19960718695700-Entity-Relationship-Diagram-ERD
================================================================================

In this Article
Introduction
Data Model Tab
Add/Edit Relationships
Setting Tables as Context
Grid View
Levels in ERD
Introduction
An Entity Relationship (ER) Diagram is a visual map that acts like a flowchart, revealing the connections between various elements, such as people, objects, or ideas, within a system. In our application, the ER Diagram serves as a helpful tool to show how different datasets and their respective columns are related. It provides a clear, organized view, allowing users to grasp the connections between datasets and gain a deeper understanding of the overall data structure. This article explores the role of an ER diagram in improving data comprehension.
Data Model Tab
Navigate to Dictionaries >Database Tables from the side bar
Click on a table name
The ER diagram can be found on the detail page of any table within the "Data Model" tab.
In the ER diagram, you'll notice a main table that stands out, and the other tables connected to it are linked with arrows to show their relationships.
By default, you can see the name of each table.
For more detailed information, you have the option to expand each table to see the specific columns it contains.
To the left of the ER diagram there are some interactions such as:
Zoom In
Zoom Out
Fit View
Toggle Interactivity
Users can change the zoom level of the diagram based on their preferences. This can be done by either using the mouse scroll along with the CTRL button or through zoom controls. To move around the diagram, users can right-click and drag their mouse to pan the view in different directions.
The "Fit View" option instantly resets the ER diagram to its default view on the screen.
This "Toggle Interactivity" function serves to enable or disable various interactions within the ER diagram. For instance, when the toggle interactivity is in the locked state, users cannot add relations or access reference views of columns in the ER diagram.
Add/Edit Relationships
Before we proceed to add or edit relationships, let's first define the three fundamental relations within the application:
Relationship added from Database
Relationship recommended by DvSum
Relationship added by User
Every relationship has a reference view and it can be opened by simply clicking on the line that connects the two tables
The Relationship that is added to the database can neither be edited nor deleted.
The second type of relationship is the one that is recommended by DvSum. These relationships can be edited or deleted. There is another option of "Confirm" along with the "Edit". Once the user confirms the relationship this relationship is changed to the "User added" relationship type.
The "User added" relationship is the third type of relationship. One way of making the relationship "User added" is just confirming the relationship which is recommended by DvSum:
Another method to create a "User-added" relationship is by manually adding and defining the relationship.
To add a relationship, click on the "+" icon located in the top-right corner of the source table. This icon allows users to initiate the process of adding a new relationship.
Once the "+" icon is clicked the "Add Relationship" reference view is opened. Here the Source table is mentioned, Users can add the Reference table from the drop-down menu and define any type of relationship
After adding the Reference table and selecting the Relationship type the user will be required to add source fields and reference fields for which the relationship is being added. Users can add single or multiple columns for the relationship.
Note
: It is to be noted that once any column is added to a relationship then that column can not be added again, the column will be greyed out when a user tries to add it to the relationship:
Once the relevant fields are added user can click on "Save Changes" and the relationship will be created. In this way, the user can add multiple relationships to the source table. The relationship that is added by the user can be edited again by clicking on the line connecting the tables. If the relationship is not required then it can be removed also:
Setting Tables as Context
On the ER diagram other than the source table there is an arrow icon on the other tables on the top right.
When this button is clicked the detail page of the respective table is opened and the Data Model Tab is displayed:
Grid View
By default, the view which is opened is the Model view of the "Data Model" tab. Along with the model view, there is another view "Grid View" which basically shows all the relationships defined in the ER diagram and some other information. The relationships that are recommended by DvSum or user-added can also be edited/deleted from here.
Levels in ERD
There are levels defined in the ER diagram.
On
Level 1
the tables that are directly related to the source table are shown:
Moving to
Level 2
, you'll discover the tables associated with tables on Level 1.
In the same way, the tables that are further related to the highlighted tables will be shown in the 3rd+ level thus showing the ER diagram in detail.

--------------------------------------------------------------------------------

================================================================================
TITLE: Rich Text Additional Information
URL: https://dvsum.zendesk.com/hc/en-us/articles/19188186862996-Rich-Text-Additional-Information
================================================================================

In this Article
Introduction
Accessing Additional Info tab for Tables
Accessing Additional Info tab for Terms
Introduction
DvSum allows its users to add additional information to the Table Detail page and Glossary Detail page. Although this feature already existed, in order to increase the capability of the feature, enhancements have been made, which are discussed in this article.
Accessing Additional Info tab for Tables
The Table Detail Page can be accessible through the Database Tables from the left menu, and it can be opened by clicking on the Table name.
Once the Table Detail Page is opened, the user can perform different tasks like "
Edit
" the changes in the Table.
Once the "
Edit"
button is clicked, the Table detail page changes to edit mode. Additional Information can be found by scrolling downwards.
Once the user clicks the Edit icon on Additional Info, the text editor will open up, and just like any text editor, it has options like selecting different font families and font sizes, changing font colors, adding numbering to the text, etc.
Any necessary additional information regarding the table can be added here according to the requirement of the user.
Once the information is added, the user can click on the blue arrow icon on the bottom right to add the additional information.
After incorporating the information, it will be included as an item. At this point, the user can choose to edit the newly added information or remove it. Additionally, the user has the option to append another additional info item if desired
. If the user has added the additional info item that is/are required, then the user can click on the "tick" button on the top right:
If the user chooses to publish the changes, the added information becomes part of the table. Conversely, if the user discards the changes, the additional information will not be included in the table.
Note:
For more information on Table detalied page, refer to the article "
Table Detail Page"
Accessing Additional Info tab for Terms
Just like the Table detail page, the term detail page can be opened from the Business Glossary tab from the left menu:
Once the detail page is opened, the user can enter the edit mode by clicking the "
Edit
" button, and additional info can be found by scrolling downwards. Users can add the required additional info:
Once the user has added the information, they can publish the changes:
If workflow is enabled for a term, instead of "Publish," the user will see "Submit for Approval." So, to finalize the changes, approval from the designated approver is necessary before publishing the term.
For more information about the workflow of terms, go through the article
here.

--------------------------------------------------------------------------------

================================================================================
TITLE: Entity Field Format in the Field Dictionary Template
URL: https://dvsum.zendesk.com/hc/en-us/articles/36407279883156-Entity-Field-Format-in-the-Field-Dictionary-Template
================================================================================

In this Article
Overview
Entity Field Format
Field Dictionary Download
Field Dictionary Import
Overview
When working with the Field Dictionary in DvSumâ€”either downloading or importingâ€”you may notice that the
Entity
field contains dot-separated text. This format provides the full path of the entity, including its associated domain and subdomain.
Note:
The Entity field represents a glossary term in DvSum. While it also displays the full hierarchy including domain and subdomain (<DataDomain>.<SubDomain>.<EntityName>), its primary purpose is to map the field or column to the corresponding glossary term.
Entity Field Format
The
Entity
field uses the following structure:
<DataDomain>.<SubDomain>.<EntityName>
Example
Default.Default.US_PASSPORT
This structure ensures clarity when identifying entities that may exist under different domains or subdomains.
Field Dictionary Download
When downloading the Field Dictionary:
The
Entity
column will display the full hierarchy as described above.
The
Data Domain
column reflects the domain assigned to the column itself, which may be different from the domain shown in the Entity field.
Field Dictionary Import
When importing the Field Dictionary:
The
Entity
field must follow the format:
<DataDomain>.<SubDomain>.<EntityName>
This format is also reflected in the
sample file
that can be downloaded from the import screen. Use it as a reference when preparing your import.
Ensure any edits made to the Entity field maintain the correct format to avoid errors during import.

--------------------------------------------------------------------------------

================================================================================
TITLE: Database Columns
URL: https://dvsum.zendesk.com/hc/en-us/articles/34934067464084-Database-Columns
================================================================================

In this Article
Overview
Database Columns
Accessing the Field Dictionary
Database Columns Details
Column Detail Page
Overview Tab
Profiling Tab
Data Quality Tab
Overview
The Database ColumnsÂ in DvSum provides users with a comprehensive view of all data columns across different sources and datasets. It consolidates metadata, profiling details, and data quality insights into a structured interface, enabling users to explore and manage their data efficiently.
Accessing the Database Columns
On the left-side menu of the DvSum application, Go to the
Dictionary
dropdown and select the
Database Columns
to navigate to its detailed page.
This page displays all column-specific details, provided that the sources are
connected and cataloged
.
Database Columns Details
The Field Dictionary provides an organized view of column-related metadata, including:
Source
Table
Column Name
Business Name
Column Description
Data Type
Profiling Info
Range Value
DQ Score
Column Sub-type
Column Detail Page
Clicking on a
Column Name
opens the
Column Detail Page
, which contains three tabs:
Overview
â€“ Displays general metadata and descriptions.
Profiling
â€“ Shows profiling statistics and insights.
Data Quality
â€“ Highlights rule violations and data quality metrics.
Overview Tab
The
Overview
tab in the
Field Dictionary
comprises several key sections that provide a structured summary of column attributes and configurations. These sections typically include:
Column Name & Description
: Displays the name of the column along with a brief description of its purpose.
Data Type
: Specifies the type of data the column holds (e.g., String, Integer, Boolean).
Source & Lineage
: Identifies the origin of the column and traces its transformation across datasets.
Usage & Dependencies
: Indicates where and how the column is utilized within the system.
Constraints & Validation
: Lists any applied rules, such as mandatory columns, unique constraints, or default values.
Tags & Classification
: Allows categorization of columns for better organization and governance.
Additional Info
: Displays any extra details relevant to the column.
Similar Columns
: Suggests columns with similar characteristics to aid in analysis and comparison.
Editing and Deleting a Column
On the
Overview
page, users have the option to either
Edit
or
Delete
a column.
Edit
: Clicking the
Edit
button (represented by a blue button with a pencil icon) allows users to modify the columnâ€™s properties, such as its name, tags, or other metadata.
Delete
: The
Delete
button (represented by a red trash bin icon) enables users to remove the column permanently. Users should exercise caution while deleting, as this action may not be reversible.
Profiling Tab
The
Profiling
tab in the Field Dictionary contains the following sections:
Statistics
: Displays key metrics such as record count, number of empty/null values, unique values, and completeness percentage.
Information
: Provides details including the last profiled date, min-max values, whether the column is a primary key or nullable, technical data type, column position, size, and the number of fractional digits.
Note:
The Min-Max values depend on the column's data type. For string data types, the minimum and maximum values are determined based on lexicographical order.
Visualization
: Includes graphical representations of data distributions, with options to view distribution and pattern insights.
Histogram time intervals automatically determined based on the span of date values:
0â€“1 day â†’ Hourly
2â€“30 days â†’ Daily
31â€“60 days â†’ Weekly
61â€“730 days â†’ Monthly
731â€“1460 days â†’ Quarterly
Over 1460 days â†’ Yearly
Run Profiling
: An option is available to initiate online profiling for updating column statistics and insights.
Data Quality Tab
The
Data Quality
tab in the Field Dictionary contains the following sections:
Statistics
: Displays key metrics such as DQ Score, total rules, rules with alerts, total exceptions, total records scanned, and last rule execution status.
Rules
: Lists all data quality rules applied to the column, including details such as rule ID, description, run status, alert status, and exception count.
Add Rule
: Provides an option to add new data quality rules for monitoring and validation. Only the following three rules can be applied:
Blanks
: Checks for missing or empty values in the column.
Value Range
: Ensures that data falls within a specified range.
Data Format
: Validates the format of the data based on predefined patterns.
DQ Score History
Clicking on the arrow icon history is displayed.
Learn more about the Rules section in Data Quality tab
here
Field Reference Page
The Field Reference page appears when a user clicks on a column name from the homepage.
It slides out from the right side of the screen, displaying relevant information about the selected column.

--------------------------------------------------------------------------------

================================================================================
TITLE: Glossary Term
URL: https://dvsum.zendesk.com/hc/en-us/articles/35943193733396-Glossary-Term
================================================================================

In this Article:
Introduction
Accessing the Glossary Term
Adding Terms in Glossary Term
Key Feature: Auto-Update of Linked Columns
Introduction:
Glossary Term is a structured repository of business terms and definitions used to ensure consistent terminology across data assets. It supports data governance by enabling categorization, improved data searchability. This guide outlines the step-by-step process to add and manage terms efficiently.
Accessing the Glossary Term
Log in to DvSum.
Navigate to the Glossary Term tab.
Adding terms in Glossary Term
Adding single term
Refer to the article
Adding New Term
for detailed steps for adding new term in the Glossary term.
Adding multiple terms
Steps to Bulk Upload Terms:
Click on the
"Download"
option.
An
Excel sheet
containing all existing terms and their details will be downloaded.
Open the Excel sheet and
add multiple new terms
along with their details.
Save the updated Excel sheet.
Click on the
"Import"
option.
For more information on importing, refer
How to import files
.
Key Feature of Glossary Terms
One of the powerful features of the
Glossary terms
in
DvSum
is its ability to automatically update linked columns when a change is made to a term.
How It Works:
If a
term
in the Business Glossary is linked to multiple
tables and columns
in different databases, any changes made to that term are
automatically reflected
across all linked columns.
For Example:
Edit the term
"First Name"
in the
Glossary Terms
.
Modify relevant details such as definition.
If a tag is
added
,
removed
, or
modified
on the term, all linked columns will immediately reflect the change.
Click
"Save"
to apply the changes.
Result:
The update is
automatically reflected
in all linked columns.
No manual updates are required in individual columns.
Before:
The term
"First Name"
has no tags assigned except "Sensitive tag".
A tag
"Test"
is added to the term
"First Name"
in the Glossary Terms.
After:
All columns linked to the term
"First Name"
now show the
"Test"
tag automatically along with the existing tag.
This eliminates the need to manually update individual columns, ensuring
consistency and accuracy
.
Auto Linking: Based on Name + Data Type
DvSum supports
auto-linking of glossary terms to dataset columns
based on:
Term Name
matching the
column name
Matching data type
This means that if a column name and data type match an existing term in the Glossary terms,
linking happens automatically
, reducing the need for manual mapping.
For example, a glossary term "Customer ID" of type
String
will auto-link to columns named "Customer ID" with type
String
in datasets.
Before and After Example
Before:
Glossary term "First Name" has only the "Sensitive tag"
Several columns are linked to this term but do not have the "Test" tag
Action:
The "Test" tag is added to the glossary term "First Name"
After:
All columns linked to "First Name" automatically inherit the "Test" tag
No changes were needed at the column level
This automated propagation maintains alignment across metadata and ensures ongoing data quality.
Linking Terms to Columns
To manually or programmatically link glossary terms to columns, refer to the article:
How to link glossary Terms

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Sign Up for DvSum Data Catalog
URL: https://dvsum.zendesk.com/hc/en-us/articles/15389497274260-How-to-Sign-Up-for-DvSum-Data-Catalog
================================================================================

Overview
Join the thousands of satisfied users who have already unlocked the full potential of our Data Catalog. Sign up now and experience the difference for yourself! Follow these simple signup steps below and get started using DvSum Data Catalog in less than 3 minutes.
Step 1:
Browse to DvSum's
Agile Data Catalog
page and click on "
Start for Free
".
Step 2:
On the next screen enter an email you want to register for the trial account. A verification link will be sent to the provided email.
Note:
This user will be the
Account Owner
.
Step 3:
In the received email click on the "
Verify Email
" button. You will be redirected to the Signup form. Fill this out to create your account.
Step 4:
On the
DvSum Data Catalog Sign-In page
, enter your login credentials to sign in and access all the amazing features and functionalities our Data Catalog has to offer.
Next steps:
Follow the product tour wizard to get started.
Happy Cataloging!

--------------------------------------------------------------------------------

================================================================================
TITLE: An Overview of Usage Analytics
URL: https://dvsum.zendesk.com/hc/en-us/articles/8596413680020-An-Overview-of-Usage-Analytics
================================================================================

In this Article
Overview
Usage Dashboard Sections
Catalog Inventory
Data Catalog
Analytics Catalog
Business Glossary
Tags
Users
Overview
Usage analytics refers to the use of interactive visualizations and metrics displayed on a dashboard to monitor and analyze data
Usage Analytics can be accessed from
Administration Tab -> Usage
Analytics.
Usage Dashboard Sections
Usage Dashboard consists of multiple sections:
The catalog inventory section shows the data count of items.
Other sections are the Data Catalog, Analytics Catalog, Business Glossary, Tags, and Users which show data in the form of tables with summary pages.
Catalog Inventory
The catalog inventory section shows the data count of the following items:
Sources
Datasets
Fields
Domains
Categories
Entities
Tagset
Tags
User Groups
Users
The dashboard shows this information on runtime if the count is updated for any items. It will be reflected in the Catalog inventory as well.
Below the Catalog inventory section, there is a clickable navigation button that shows data of a clicked button.
For example: Click on the Data Catalog button, section will show data from the Data Dictionary in the form of tables with summary pages.
1. Data Catalog
This section shows information about sources, Datasets, and fields.
1.1
Data Sources
Usability
It shows the average usability score of sources, which includes Datasets and columns.
Usability score of source out of 10.
Quality
It shows the average quality of sources, including Datasets and columns.
But the Quality score will be calculated for Fields only. The quality score shows as a percentage.
1.2
Datasets
Count
It shows the total count of all Datasets in the source.
Usability
It shows the average usability score of Datasets in the source.
Usability score of Dataset out of 10.
1.3
Fields
Count
It shows total count of all Fields in the source.
Usability
It shows the average Meta score of Fields in the source.
Usability score of Fields out of 10.
Quality
It shows the average Data Quality score of Fields in the source.
The quality score shows as a percentage.
Curation Tracking
The curation Tracking section shows the results of all sources from the Data Dictionary Table.
Every Source row in the below Data dictionary section is clickable, when a user clicks on any source row, detailed information will be shown in the curation tracking section for the selected source.
The curation tracking section shows information as Count & percentage for Datasets, fields & All items ( Combined count of Datasets and fields).
Description
It shows how many Datasets & fields have descriptions available.
Ownership
It shows Data owners/System owners are assigned to how many Datasets.
Note
: Ownership is not applicable to Fields.
Linked To Entity
It shows Functional area is linked to how many Datasets & fields.
Profiling
It shows the Profiling count for Fields.
Note
: Profiling is not applicable to Datasets.
2. Analytics Catalog
This section shows information about sources, Datasets, and fields from Analytics Dictionary.
1.1
Data Sources
Usability
It shows the average usability score of sources, which includes Datasets and columns.
Usability score of source out of 10.
1.2
Datasets
Count
It shows the total count of all Datasets in the source.
Usability
It shows the average usability score of Datasets in the source.
Usability score of Dataset out of 10.
1.3Â  Â Reports
Count
It shows the total count of all Reports in the source.
Usability
It shows the average usability score of Reports in the source.
Usability score of Dataset out of 10.
1.4
Metrics
Count
It shows total count of all Metrics in the source.
Usability
It shows the average Meta score of Metrics in the source.
Usability score of Fields out of 10.
Curation Tracking
The curation Tracking section shows the results of all sources from the Analytics Dictionary.
Every Source row in the below Analytics Catalog section is clickable, when a user clicks on any source row, detailed information will be shown in the curation tracking section for the selected source.
The curation tracking section shows information as Description, Lineage and Duplicates.
3. Business Glossary
This section shows information about Entities:
Domain
The domain Column shows a list of all domains.
Categories
It shows the count of categories linked to the Domain
Entities
It shows the count of entities linked to the category and out of total entities how many entities are linked.
Count of Linked Data Assets
It shows the total count of linked Data assets for Entities
Top Entities
The top entities section shows the top 5 entities of all domains that are linked to Data dictionary items. Every Domain row in the above Glossary section is clickable, when the user clicks on any domain, detailed information of the top 5 entities will be shown against the selected domain.
This table shows:
Entity
The Entity column shows names of entities.
Description
This column shows a description of entities.
Linked Data dictionary Items
It shows the count of Entities linked to Data dictionary items.
Sample
This column showsÂ  1 or 2Â  columns linked-to the entity.
4.Tags
This section shows information about Tagset:
Tagset
This column shows the list of all tags
Tags
This column shows the count of tags available in the respective tagset and the Percentage shows how many tags are used in Dataset / Fields/ Terms
Tagged Assets
It shows the total count of used tags in Datasets/fields/terms
Top Tags
The top Tags section shows the top 5 most linked tags of all the tags. Every Tagset row in the above Tags section is clickable, when the user clicks on any tagset, detailed information of the top 5 entities will be shown against the selected domain.
This table shows:
Tags
This column shows the top tags
Linked Count
This column shows the count of tags that are linked to Dataset / Fields/ Terms
Sample
This column showsÂ  1 or 2Â  Datasets/ Terms/ Entity linked to Tags.
5. Users
Catalog Usage:
This section shows information about the user and their activities:
Users
The first column shows the list of users with badges assigned on the basis of activities:
i)Â  Master Badge
ii) Star Badge
iii) Explorer Badge
Activity
This column shows the total count of activity for the user with streak count.
Notes
This column shows the number of searches the user has performed, Contribution made by users, and streak count.
Search activity includes performing the search, create a governance view, opening a ref page & open a governance view.
Contribution activity includes Making a suggestion for Datasets/ terms, Endorsing and publishing a Dataset/term.
Streak contains the number of consecutive days the user is active on the application.
Steward Activity
This section shows information about steward user and their activities:
Users
The first column shows the list of steward users with their roles.
Assigned Assets
This column shows the count of assigned assets to the respective steward user
Usability Score
This column shows the usability score of assetsÂ  assigned to steward user
Seward Activity
It shows a count of activities (searches & contributions) performed by the steward with a streak.

--------------------------------------------------------------------------------

================================================================================
TITLE: Asset and Glossary Status Lifecycle
URL: https://dvsum.zendesk.com/hc/en-us/articles/43341691915668-Asset-and-Glossary-Status-Lifecycle
================================================================================

In this Article
Overview
Asset Statuses
Glossary Statuses
Overview
This article explains the purpose, definition, and lifecycle flow of each status used in the
Asset Dictionary
,
Data Dictionary
,
Field Dictionary
,
Rules
, and the
Business Glossary
.
Asset Statuses
Applicable to: Assets, Tables, Columns, and Rules.
New
The
New
status indicates that an asset has been scanned and detected for the very first time. It is assigned automatically during the initial scan when the asset enters the system. No review or action is required from data stewards or owners at this stage, and the asset is visible to all users.
Purpose:
To clearly identify assets that are newly introduced into the system and have not yet been compared against any previous scan.
Default
The
Default
status indicates that an asset has remained unchanged since the previous scan.
From the second scan onward, if the scanner finds the same asset with no metadata differences, the status automatically becomes
Default
without steward or owner intervention, and visibility remains open to all users.
Purpose:
To show that the asset has been validated by at least one re-scan and is in a consistent, unchanged state compared to its previously scanned version.
Note:
If an asset disappears or its schema changes during the second scan, it moves directly from
New â†’ Deleted
, not Default.
Modified
The
Modified
status indicates that the assetâ€™s metadata has been updated by the user compared to the previous version, such as when descriptions, domains, or business metadata are updated. The status will change from
New/Default
to
Modified
when the user manually updates any metadata and saves the asset without publishing it.
These assets remain visible to all users but indicate that internal updates have been made and are pending review or publication.
Published
The
Published
status means that the asset has been fully reviewed, validated, and approved. This status is set manually by a user after completing validation.
Workflow behavior:
If the asset belongs to a
workflow-enabled domain
, the transition from
Modified â†’ Published
requires approval from the assigned data steward or data owner.
If the asset does
not
belong to a workflow-enabled domain, it can be published directly without approval.
Deleted
The
Deleted
status Â is assigned automatically when the asset is no longer detected in the source system, such as when a table is dropped or a column is removed or when schema is changed in the source.
No steward or owner involvement is needed as this status will be automatically updated after the scan, and deleted items remain visible to all users.
Note:
Users cannot manually change an active assetâ€™s status to Deleted. Deleted assets remain in the system for audit and historical reference.
Please refer to this article for more details on how to mark an asset as deleted:
How to delete asse
ts
Glossary Statuses
These are the other statuses applicable specifically to Glossary Terms.
Note:
When creating Views for Glossary Terms, the
Default
option in filters refers to the
Draft
status.
Draft
The term is created but not yet reviewed or approved; it may still be undergoing edits.
Modified
Indicates that the termâ€™s metadata has been updated from its previously published state and requires review before re-publishing.
Published
The term has been reviewed, approved, and validated.
For more details on approving and publishing the Glossary Trems, refer to the article:
Approving Glossary Terms
.
Note:
A status will appear in the filter only if it has been applied to at least one asset. For example, if the
Draft
status is not applied to any asset, it will not be shown in the filter when the user clicks on the funnel.

--------------------------------------------------------------------------------

================================================================================
TITLE: Visualizing Agent Questions on Dashboard
URL: https://dvsum.zendesk.com/hc/en-us/articles/43031984347924-Visualizing-Agent-Questions-on-Dashboard
================================================================================

In this Article:
Overview
Capturing API-based Properties & Parameters
Automatic Reflection on the Dashboard
Widget Creation & Customization
Overview
This feature introduces a dynamic integration between
agent questions
and the
dashboard
. When a user interacts with an AI agent through an API and provides
custom properties and parameters
, those parameters are automatically exposed on the dashboard for further analysis. This enables users to visualize and monitor agent interactions using customizable widgets, filters, and charts all based on the dynamically generated fields.
Capturing API-based Properties & Parameters
When a user asks a question through the API (for example):
â€œAnalyze customer IDs 12345658 for FTTHâ€
The request may include additional
properties
and
parameters
such as location, market code etc.
Automatic Reflection on the Dashboard
Automatic Reflection on the Dashboard
The captured parameters are dynamically displayed in the
Agent Question Analytics
of the dashboard.
Whenever a user adds new properties or parameters through an API query, these are instantly reflected as additional columns in the dashboard dataset.
For example, after the query above, the following columns might appear automatically:
Customer ID, Location, Market Code, etc.
Note:
Anything that is sent as a
keyâ€“value pair under the â€œpropertiesâ€ attribute
in the API request will automatically be captured and shown on the dashboard as a separate field.
This means every new key introduced in the API payload appears as its own column in the dataset â€” no manual configuration or schema update is needed.
All parameters or properties captured from the API automatically appear under the
Additional Attributes
section when creating or editing a widget.
This ensures that all newly introduced fields are readily available for selection and analysis.
Example view:
The below image shows how these dynamically generated fields appear within the
Agent Question Analytics
widget.
Example 2:
This chart shows how many records exist for each market code.
Widget Creation & Customization
Users can then navigate to the Dashboard and create a Widget of type
â€œAgent Question Analytics.â€
If youâ€™re not familiar with creating a widget, see the detailed guide here:
Dashboard and Widgets
Within this widget:
Users can apply slicers (filters) on any of the dynamic parameters such as location, market code, or service type.
Users can select chart types (e.g., bar, pie, line, table) to visualize data according to their needs.
All the dynamically created columns from the API (e.g., location, market code, tool ID) will be available for data slicing and analysis.
This allows flexible visualization of agent query data with no need for predefined schema updates.

--------------------------------------------------------------------------------

================================================================================
TITLE: Enterprise Search
URL: https://dvsum.zendesk.com/hc/en-us/articles/42822153397908-Enterprise-Search
================================================================================

In this Article:
Overview
Searching for Assets
Search Suggestions
Search by Asset Name
Search by Table and Column Name
Search by Unique Asset ID
Search by Custom Attribute Values
Using Filters
Filtering Options
Filters Available for All Assets
Available Filters for Tables
Available Filters for Columns
Available Filters for Terms
Filters for Rules
Search Results
Consistent Result Display
Example Search Scenarios
Summary
Overview
The
Enterprise Search
feature provides a unified way to explore and discover assets across your organization including datasets, reports, tables, rules, schemas, and more. It enables users to locate information quickly without needing to navigate through multiple modules.
Searching for Assets
Enterprise Search supports flexible search capabilities to help you find exactly what you are looking for.
Search Suggestions
As you type a query, Enterprise Search provides real-time suggestions to help refine your search.
If the system detects alternate or closely related terms, it displays a
â€œDid you meanâ€¦â€
line above the suggestion list.
Users can click one of the suggested phrases to update the search term or press
Go
to execute the current query.
Fuzzy Search
Typos trigger â€œDid you meanâ€¦?â€ suggestions, while results are based on exact or prefix matches.
Expanded Search
Enable
Expanded Search
to include similar and partial matches for broader, more relevant results.
For example, if you search for
â€œClaim Bâ€
with this option checked, results will also include assets containing the word
â€œClaimâ€
â€”not just exact matches.
This helps surface related or similar assets that might otherwise be missed.
Example Behavior
The â€œDid you meanâ€¦â€ line shows suggested alternative keywords.
The suggestions list shows recent or recommended searches and matching assets with basic metadata.
Clicking a suggestion or the â€œDid you meanâ€¦â€ phrase runs a new search with that term.
Search by Asset Name
Enter the full or partial name of any dataset, table, report, or rule to view all related results.
For example, typing
Sales
will bring up all assets whose names contain that keyword.
Note:
Exact matches to your entered keyword appear at the
top of the results
, allowing quick access to the most relevant tables, columns, or reports.
Search by Table and Column Name
You can search directly using a table name or column name.
If you add a period (
.
) and the column name after the table name (for example,
Orders.OrderDate
), the search will return all matching assets containing that column.
This also supports
full-path search
such as
DB.Schema.Table.Column
, allowing more precise targeting across complex data models.
Search by Unique Asset ID
Enterprise Search also allows searching through a specific
unique ID
of an asset.
This ID can be found in the URL of the asset (for example, a Power BI report or dataset).
Entering this ID in the search bar will return that particular asset directly.
You can also paste a
Power BI report URL
to locate that exact report in Enterprise Search.
Search by Custom Attribute Values
Assets with custom attributes such as picklist values can be located by searching those values.
For example, if a picklist attribute contains values like
Single
or
Double
, typing those terms in the search bar will display all assets assigned with that particular value.
In addition to standard assets, Enterprise Search supports a wider range of technical and custom objects including
databases, schemas, stored procedures, and custom assets
, ensuring broader visibility across your organizationâ€™s data catalog.
Using Filters
In addition to free-text search, users can apply filters to refine search results based on different asset characteristics.
Filtering Options
Filters in the left panel enable refinement of search results across asset types such as tables, columns, rules, terms, and agents. Each asset type provides its own set of filter categories, allowing more targeted exploration of data assets.
Filters available for All Assets
Data Owners -
Filter by the assigned data owner responsible for the Table.
Data Steward
- Filter by the assigned data steward responsible for the Table.
Data Domain
â€“ Filter by assigned domain.
Last Updated On
- Filter based on the last updated date.
Status
â€“ Filter based on Deleted, Default, New, Modified, and Â Published.
Tags
â€“ Filter by applied metadata tags.
Available Filters for Tables
You can refine your search using the filters on the left panel:
Format
â€“ Filter by format of the data i.e. Files or Tables.
Last Refreshed On
â€“ Filter based on the last scan or refresh date.
Record Count
â€“ Filter by data size or volume.
Source
â€“ Filter by data source name.
The available filter list changes dynamically based on the selected asset type.
Note:
Selecting the Plus (+) icon beside a filter reveals available options for more precise asset selection, such as tables, columns, rules, or agents.
Selecting Clear at the top of the filter section resets all applied filters.
Available Filters (for Columns)
You can refine your search using the filters on the left panel:
Data Type
â€“ Filters by the columnâ€™s data type (e.g., String, Integer, Date).
Empty or Null
- Filters based on the number of empty or null values. Users can set the minimum and maximum limits for empty or null values to retrieve the desired columns.
Entity
â€“ Filters by the related entity or object.
Table -
Filters based on the table names to which the columns belong.
Unique Count
â€“ Filters by the distinct value count of a column.
Available Filters for Terms
You can refine your search using the filters on the left panel:
Sub-domain
â€“ Filters by the sub-domain under which the term is classified.
Type
â€“ Filters by term type (e.g., Business Term, Technical Term).
Filters for Rules
You can refine your search using the filters on the left panel:
Alert Status
â€“ Filter by the alert level or current notification state.
Last Executed On
- Filters by the latest execution date.
Rule Type
â€“ Filter by rule type such as Orphan Records, Missing Values, or Data Consistency.
Run Status
â€“ Filter by the latest execution status (e.g., Valid, Invalid, Failed).
Search Results
Selecting a column from the search results opens a detailed panel on the right side, displaying key metadata and attributes
Selecting the arrow icon in the detail panel redirects to the Dictionary tabâ€”specifically to the Database Column pageâ€”where the column can be viewed in full context along with related governance information.
Consistent Result Display
The presentation of search results has been standardized to ensure consistent metadata visibility and layout across all asset types.
This makes it easier to interpret and compare results within the Enterprise Search interface.
Example Search Scenarios
Searching
Orders
returns all tables and reports containing that table name.
Searching
Orders.OrderDate
lists all assets containing that column.
Entering a unique
Power BI asset ID
(from its URL) displays that specific report.
Searching
Lahore
returns all assets with that picklist value assigned in their custom attributes.
Pasting a
Power BI report URL
shows the matching report directly.
Applying filters for
Asset Type = â€œData Quality Ruleâ€
and
Status = â€œPublishedâ€
displays only published data quality rules.
Summary
Enterprise Search centralizes discovery across all connected data assets.
It helps you find, filter, and access the information you need faster and more efficiently.
Whether you are searching by name, column, ID, or URL, or refining results through filters and tags, Enterprise Search ensures that all relevant assets are easily within reach.
Note:
AI Agents are not included as part of Enterprise Search.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Domains & Data Sub-Domains
URL: https://dvsum.zendesk.com/hc/en-us/articles/41940755917204-Data-Domains-Data-Sub-Domains
================================================================================

In this Article
Overview
Accessing Data Domains
Adding a New Data Domain
Mass Update Data Domain for Tables and Columns
Mass Update Data Domain for DQ Rules
Data Sub-Domain
Accessing Data Sub-Domains
Adding a New Data Sub-Domain
Mass Update Data Sub-Domain
Overview
Data Domains help organize and manage data assets by grouping them into logical categories based on business or functional areas. Defining Domains and Sub-Domains makes it easier to manage ownership, improve governance, and maintain better visibility across all data.
Accessing Data Domains
Navigate to
Administration > Organization
.
Click on
Data Domains
.
The
Data Domains
page displays a list of all existing domains configured in the system.
Adding a New Data Domain
To create a new domain:
Click the
Add
button.
2. In the
Add Domain
form, provide:
Domain Name
Description
Data Steward
3. Select
Enable Metadata Workflow
to activate metadata management options.
Metadata Workflow: Enabled
The 'Workflow' dropdown under the 'Enable Metadata Workflow' section provides two available options:
1. Anyone can approve â€“ Any assigned approver can approve the metadata update.
2. All need to approve â€“ Every assigned approver must approve before publishing updates.
Note:
For complete details on governance roles and responsibilities, refer to the
Data Governance Roles and Responsibilities
article.
Sub-Domain Selection
The
â€œChoose Sub-Domainsâ€
section allows the user to assign sub-domains to the new Data Domain.
Click the
Save
button to save the domain. Then the newly added domain will appear at the top of the listing. An icon will indicate that the domain has the workflow enabled, as shown below.
Steward that is selected while creating the Domain will be the one who will be editing the term and making the changes. Then changes will be sent for approval to the approver(s). Approver's job will be to review the changes made by the Steward and approve or reject them.
For more information on the approving glossary terms, refer to the article
Approving Business Glossary Terms
.
Mass Update Data Domain for Tables and Columns
The
Mass Update
feature allows updating the Data Domain for multiple assets simultaneously across different modules, including:
Asset Dictionary
Database Tables
Database Columns
Step 1: Navigate to the Module
Go to the module where the update needs to be applied:
Asset Dictionary
Database Tables
Database Columns
Step 2: Select the Records
In the asset list, select the checkboxes next to the items to be updated.
From the toolbar at the top, click
Mass Update
. A
Mass Update
dialog box will appear.
Step 3: Configure the Update
Make the necessary changes in the dialog box.
Click
Apply
to confirm the update. A confirmation message will appear once the update is successful.
The updated Data Domain will now be visible under the
Data Domain
column for all selected records.
Note:
Data Domains can also be assigned to multiple assets through
relationships
, similar to how it is done for rules, as described in the next section.
Mass Update Data Domain for DQ Rules
To update the data domains for multiple rules at once,
Navigate to
DQ Rules
, select the rules wanted to update.
Select the Relationship to add the data domains.
3. Select the data domain that needs to added/updated.
4. Click
Apply
.
Note:
Data Domains and Data Sub-Domains can be updated for multiple assets simultaneously using the
Import
feature. For more information, refer to the article
Data Import
.
Data Sub-Domain
The Data Sub-Domains section allows users to view, manage, and organize sub-domains associated with various data domains.
Sub-Domains are logical subdivisions within Data Domain, used to organize datasets at a more granular level. They enable structured classification, enhance metadata traceability, and support precise governance across the data catalog
Accessing Data Sub-Domains:
To access the
Data Sub-Domains
page:
Go to
Administration
>
Organization.
Click on
Data Sub-Domains
.
Adding a New Data Sub-Domain
To create a new Data Sub-Domain:
Click on the
Add
button
In the form, define:
Classification
Ownership
Governance
Choose the
Priority
level of importance from the dropdown options:
â€¢
Normal
â€“ Default priority level.
â€¢
High
â€“ Marks sub-domains requiring critical attention.
â€¢
Low
â€“ Used for less significant domains.
Once a domain is selected, the sub-domain will inherit the workflow settings from the domain. This means the sub-domain will have the same Workflow Type, Stewards, and Approvers as the parent domain. Click Save to save the sub-domain.
Click
Save
.
Note:
If the workflow is enabled in Domain then its associated sub-domain will inherit all the domain's workflow settings. And if the workflow is disabled in Domain, then workflow can be enabled separately for the associated sub-domain.
Approval & Access Behavior
Domains can be assigned to user groups. When a domain is linked to a group, approval is considered complete as soon as
any one
member of that group approves it, even though the approval request is issued to all members.
Note:
Group-based approval is
not supported
when working with Import Data sources. For file imports, only
individual users
can be assigned and must approve.
Mass Update Data Sub-Domain
The
Mass Update
feature allows users to update
Data Sub-Domain
for Glossary Terms assets modules:
Note:
Sub-Domains can only be assigned to
Glossary Terms
. For
Tables
and
Columns
, Sub-Domains cannot be added separately. In the case of Columns, Sub-Domains are automatically inherited when a Glossary Term is associated with the column.
Step 1: Go to Glossary Term and select the records for which Data Sub-domain needs update.
Step 2: A
Mass Update
dialog box will appear.
Step 3: Configure the Update.
Step 4: Click
Apply
to confirm the update. A confirmation message will appear once the update is successful.
Step 5: The updated
Data Domain
will now appear under the
Data Domain
column for all selected records.
For more information on the approving glossary terms, refer to the article
Approving Glossary Terms
.
Note: Users can edit the default Data Domain and Sub-Domain; however, they cannot delete them as these are system defaults.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Apply Filters in Governance Views
URL: https://dvsum.zendesk.com/hc/en-us/articles/40953420948884-How-to-Apply-Filters-in-Governance-Views
================================================================================

In this Article:
Equals
Does Not Equal
Includes
Excludes
Is Empty
Not Empty
Wildcard Match
Note
DvSum enables users to apply different filter conditions while creating or customizing views in the Data Dictionary, Field Dictionary, Analytics Dictionary, and Business Glossary.
For detailed steps on creating Governance Views,Â refer to the
How to Create Governance Views article
. Once a view is created, filters can be applied using the specify criteria as described below.
Equals
What it does:
Returns rows where the column value exactly matches the specified value.
Example:
Source Name = Databricks â†’ only rows where Source Name is "Adventure Works."
Does Not Equal
What it does:
Returns rows where the column value is not equal to the specified value.
Example:
Source Name â‰  Databricks â†’ excludes rows where Source Name is "Adventure Works."
Includes
When using the
Includes
filter under
Specify Criteria
, the behavior varies depending on the selected field:
For Table Name:
When â€œTable Nameâ€ is selected, users need to
type a keyword
to search for available values from the database.
A message appears below the field saying:
â€œType to search for values from database. You can select up to 100 values only.â€
This means users must start typing to fetch matching table names, and they can
select a maximum of 100 values
.
For Other Fields:
For other filter fields (e.g., Columns, Tags, Source, etc.), the list of available values will automatically appear
when you click on the text box
, without the need to type a keyword.
Excludes
What it does:
Returns rows where the column does not contain the specified value.
Example:
Status excludes "Published" â†’ omits rows where the Status field contains "Published."
Is Empty
What it does:
Returns rows where the column has no value (blank/null).
Example:
Glossary Term is empty â†’ fetches rows with no glossary term assigned.
Not Empty
What it does:
Returns rows where the column contains any value (not blank/null).
Example:
Glossary Term not empty â†’ fetches rows that have a glossary term assigned.
Wildcard Match
What it does:
Allows partial or pattern-based matching using wildcards (* or % depending on syntax).
Example:
Table Name wildcard match "order" â†’ matches names like "order," "order history," or "vw order."
If * or % is used in the filter with *
"Add
"**, it will fetch all rows that start with
"Add."
Note:
Users can add multiple criteria within a single view.

--------------------------------------------------------------------------------

================================================================================
TITLE: Exception Storage Integration for Rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/39857375831956-Exception-Storage-Integration-for-Rules
================================================================================

In this Article
Overview
1. Creating an Integration
1.1 Access Integration Settings
1.2 Configure the Integration
2. DDL Script Generation & Execution
3. Test Connection and Validation Steps
3.1 Schema Match Failures
3.2 Write-Back Access Errors
4. Log All Rule Executions (Optional)
4.1 Configuration Option: "Log All Rule Executions"
4.2 What Happens When Enabled?
5. Enabling Exception Storage on Rules
5.1 Navigate to Rule Configuration
5.2 Enable Exception Storage in the Workflow Section
6. Running Rules with Exception Storage
6.1 Run Rule â†’ Online Execution
6.2 Run Rule â†’ Offline Execution
6.3 Gateway Compatibility Scenarios
7. Unsupported Integration Types
7.1 Unsupported Databases
7.2 Unsupported Rule Types
Overview
After a DQ rule is run, the exception records are stored in encrypted files local to the DvSum gateway when an on-premises gateway is used. The records are stored in an encrypted S3 bucket by default when the cloud gateway is used. In both cases the records are visible to users within the DvSum GUI and via API.
Exception Storage Integration allows exception records to be stored in external database tables. This enables easier traceability, auditability, and integration with reporting systems or downstream analytics.
1. Creating an Integration
1.1 Access Integration Settings
Navigate to:
Administration â†’ Account â†’ Integrations (Tab)
1.2 Configure the Integration
Select the Data Source Type
Choose a supported connection (currently limited to databases).
Note:
The Integrations tab is designed to support various types of external connections. As of now, it only supports
Databases
.
Specify the Target Table Name
This is where rule execution results will be stored.
2. DDL Script Generation & Execution
Once the source is configured, the system auto-generates a
DDL script
containing predefined columns (e.g., execution ID, rule ID, rule name, rule type, rule description, etc.). These fields are required for writing execution results to the target table.
Dynamic Table Name:
If you change the target table name, the table name in the DDL script will auto-update.
Test Connection:
If the user clicks on
Test Connection
before executing the script in the database, the table will not exist or may not be accessible. As a result, schema validation will be skipped, and the write-back operation will not be performed.
Copy and Execute the DDL Script
After the DDL is generated,
copy it
using the
â€œCopy Codeâ€
button or from the dialog window.
Execute the script in the
same target database
selected during integration setup. This creates the required table structure with predefined columns (e.g., execution ID, rule ID, rule name, rule type, etc.)
3. Test Connection and Validation Steps
After executing the DDL script in the selected database, return to the Integration setup and click
Test Connection
. The system performs the following three validation checks:
Table Existence:
The table must exist in the selected database.
Schema Match:
The table structure must match the expected schema (all required columns must be present).
Write-Back Access:
The system must be able to insert and delete a test record to verify permissions.
3.1 Schema Match Failures
If the table already exists but wasnâ€™t created using the system-generated DDL script, it may lack one or more required columns.
The
Test Connection
may fail during the schema validation step.
Click
â€œShow Error Detailsâ€
to view the list of missing or mismatched columns.
3.2 Write-Back Access Errors
Scenario 1: Missing Insert Permission
Even if the table structure is correct, the system will show:
â€œWrite-back access is not allowed â€“ insert permission test failedâ€
if the database user does not have insert rights.
During validation, the platform attempts to insert dummy JSON data into the external table and immediately deletes it to confirm access.
Note:
This happens because, during Test Connection, the system tries to insert dummy JSON data into the external table.
After the insertion, it deletes the dummy data. This helps validate whether write-back access is allowed.
In this case, the insert test fails due to missing permissions.
Scenario 2: Missing Delete Permission
If insert permissions are present but delete permissions are missing, you can see:
â€œWrite-back access is not allowed â€“ delete permission test failedâ€
4. Log All Rule Executions (Optional)
This option determines whether you want to store execution results
only when exceptions are found
, or
for every run,
even when no exceptions are triggered.
4.1 Configuration Option: "Log All Rule Executions"
During integration setup, you can see a checkbox labelled
â€œLog all rule executionsâ€
.
Enabled:
Every rule execution is logged in the storage table, regardless of whether exceptions were found.
Disabled (default):
Only executions that generate exceptions are logged.
4.2 What Happens When Enabled?
On enabling this setting and clicking
Save
, all future executions of rules linked to this integration will be logged.
Each run will insert a record in the target table, even when the result is
healthy
(i.e., no exceptions).
Now store exceptions externally in the specified table.
Best practice: Enable this option if organization requires
full execution history
for reporting or compliance purposes.
5. Enabling Exception Storage on Rules
To store exception data from rule executions into the configured external storage, Exception Storage must be explicitly enabled at the
rule level
.
5.1 Navigate to Rule Configuration
Go to the
Dictionaries
dropdown
,
Click on
Database Tables
.
Select the desired table.
Open the
Data Quality
tab and choose the rule you want to configure.
Click
Edit
on the rule.
5.2 Enable Exception Storage in the Workflow Section
In the rule edit screen, scroll to the
Workflow
section.
Locate the
Exception Configuration
tab.
Enable the
Exception Storage
toggle.
From the dropdown, select one or more configured integrations where exceptions should be stored.
Click
Save
.
Note:
User can configure
multiple integrations
for a single rule if needed.
Clicking on an
integration
name will redirect to the full
Integration configuration page
, allowing to review or modify its setup.
6. Running Rules with Exception Storage
Once Exception Storage is configured and enabled on a rule, exceptions can be logged to the external table during both
Run Online
and
Run Offline
executions. The mechanism of storing differs slightly based on how the rule is run.
6.1 Run Rule â†’ Online Execution
When a rule is run manually via
Run Online
, a
Configuration Preferences
dialog appears with the following options:
Include All Exceptions
â€“ Check this option to include all exceptions during the rule execution.
Add Exceptions to Storage
â€“ Check this box if you want the exceptions to be stored in the external tables when the rule is run.
After execution:
A
success message
appears confirming that exceptions were loaded into the configured table.
The same message also appears in the ruleâ€™s
Activity section
as a system comment.
The user can confirm this by reviewing the corresponding table in their external database.
Note:
To convert the raw exception data into a valid JSON format, users can run the following SQL query on Microsoft SQL server.
USE [your_database_name];

SELECT
 execution_id,
 exception_data AS RawExceptionData,
 TRY_CAST(exception_data AS NVARCHAR(MAX)) AS ExceptionData_Text,
 JSON_QUERY(TRY_CAST(exception_data AS NVARCHAR(MAX))) AS FormattedJson
FROM
 [your_schema_name].[your_table_name]
WHERE
 execution_id = [your_execution_id];
6.2 Run Rule â†’ Offline Execution
When a rule runs via
Run Offline
(through workflows or schedules), exception storage happens automatically based on the rule's configuration; no manual input is required.
Notification Email Outcome:
After execution, a notification email is sent to the rule owner and relevant users. The
Workflow Action
section shows one of the following:
If exceptions were stored successfully:
â€œExceptions have been loaded to storage location.â€
If no integration is configured on the rule:
â€œNot Configured.â€
6.3 Gateway Compatibility Scenarios
When the rule and the integration are hosted on different gateway types (On-Prem vs. Cloud), the system behavior varies:
Rule Location
Integration Location
System Behavior
On-Prem
Cloud
Warning:
"Selected integration uses a different gateway. This integration requires exception data to exit your network temporarily before being stored."
Cloud
On-Prem
Info:
"Selected integration uses a different gateway type." (Not blocking)
Note: These messages help clarify when exception data might leave your internal network, allowing teams to address any compliance or security implication
7. Unsupported Integration Types
7.1 Unsupported Databases
The following integration targets are
not supported
:
Azure Data Lake Storage (ADLS)
Power BI
MongoDB
Amazon S3
(Rules can read from S3, but cannot write exceptions to S3 as external storage)
7.2 Unsupported Rule Types
As of now, the following rule types are
not supported
for Exception Storage:
Freshness Rules
Metric Rules
Count Rules
Unique Value Rules
These rules are not yet compatible with Exception Storage because they do not produce row-level exceptions that can be stored in external systems.
All other rule types that generate exception samples are supported.

--------------------------------------------------------------------------------

================================================================================
TITLE: Rule Detail page
URL: https://dvsum.zendesk.com/hc/en-us/articles/26973748320532-Rule-Detail-page
================================================================================

In this Article:
Overview
Initial State of a Newly Created Rule
Key Functionalities Overview
Scheduling Status
Alert Status
Healthy
Alerting
Run Result
Passed
Failed
Run Options (Online & Offline)
Edit Functionality
Overview
Scope
Threshold Configuration
Workflow
Workflow Actions
Tabs on Rule Detail Page
Data Tab
History Tab
Chart View
Grid View
Instructions Tab
Column Sequence Tab
Overview:
Many features are added on the Rule detail page like offline and online execution capabilities, providing users with greater flexibility. The alert status of each rule is displayed as either "Healthy" or "Alerting" based on specified thresholds. Users can now easily edit rule definitions, descriptions, priorities, and scope, including window types and lookback days. Data aggregation into buckets enables detailed data quality checks. Various threshold types allow tracking and alerting based on exceptions. Rule notifications, action statuses, and scheduling options have been enhanced for improved rule management. The history tab provides execution charts and grids for comprehensive performance analysis. Overall, all these features enhance rule customization, monitoring, and analysis which will be explained in this article.
Initial State of a Newly Created Rule:
Before moving to functionalities let's go through when any rule is created. The "Data" tab and "History"
tab is disabled initially when the rule is created.
Here on the "Run Rule", the user can either run the rule online or offline. For more information regarding
Run Online & Run Offline, please visit the article "
Run Online/Run Offline
". Once the Rule is run, the "Data"
tab (if there are exceptions) & History tab gets enabled:
Now we will go through the major functionalities that have been implemented in the User Interface are
highlighted in the screenshot below for a quick review.
Key Functionalities:
The Rule Detail page has a significant number of features Let's explore these enhancements in detail:
Not Scheduled
A notification at the top of the page is shown which indicates whether the rule is scheduled. If the rule is not scheduled it will not run automatically.
Alert status
Alert status for the rule will be shown as
Healthy
- if the metric value is within specified threshold limits, then the alert status will be Healthy
Alerting
- If the metric value is not within specified threshold limits, then the status will be Alerting
Run Result
Passed
-
If the metric value has zero exceptions then the run result will be passed and the alert status will be healthy.
Failed
-
If the metric value is not within the defined threshold then the run result will fail and the alert status will be alerting applicable to Unique values, Freshness, Metric, and Count rule.
Run:
Online & Offline rules can be run directly from the Rule detail page
For Details please check the article
Run Online/Offline
Edit Functionality:
This button will allow the user to update the following:
Overview
Rule Description
: Users can update the description for the rule
Priority
: Users can update the priority for the rule
Open Rule Definition:
Users can directly open the rule definition page from here
Attributes
Users can add any tags to the rule
Scope
Metric Time
If the metric time field is selected on the table level. Selected field names will be inherited from the rule as well
Otherwise, the user can also select/update the metric time field at the rule level as well
Users can set the window type: - Used to define what is the scope of data to be selected.
All Data - By Default all the rules will window type as All data, which will consider all the data in the table during theÂ  execution of the rule
Data Max Time - In the case of incremental data, we can choose this option to run validation only on newly added data based on the timestamp available on the table
Clock Time -Â  In the case of incremental data, we can choose this option to run validation only on newly added data based on the current timestamp available on the table
Example:
There is a rule scheduled to run on 2024-01-06 at 11:00:00. This rule is applied to the "UPDATE_DT" column. Please review the sample data below, which produces different outputs based on the selected window type:
Users can set the
Lookback days
for Data max time & Clock time
Users can optionally aggregate data into
Buckets
and DQ checks will be performed on each bucket.
No bucket
1 Day
1 Hour
Users can select available
Slicer
options
Threshold
For the Metric type, we can choose whether we want to track and alert based on the number of exceptions or percentage of exceptions.
No Threshold
- Metric will not alert.
Constant
- Metric will be compared against constant thresholds.
Users can set the Upper bound and Lower Bound
Relative
- Percentage change (increase or decrease) in metric compared to the previous bucket or execution. For DQ exception checks, the decrease will not alert.
Users can set the percentage for the relative threshold type
Adaptive
- Thresholds auto-adjust based on observations using outlier detection techniques. It uses the Interquartile range technique to detect if the metric is an outlier.
Users can choose the threshold bounds from the 3 available options:
Upper and Lower
Upper
Lower
Workflow
Assign a Rule to a User
Assign the rule to a specific user for ownership and accountability.
Add/Update the Schedule for a Rule
Select or create a scheduled job to automate rule execution.
Exception Configuration
(New)
Attach Exception File
: Includes exception details in the notification email.
Enable Exception Tracking
: Allows tracking of new and existing exceptions across runs.
For a detailed explanation of Exception Configuration and how it impacts the Data tab, refer to the article:
Configuring Exception Settings for Rules in DvSum.
Work Flow Actions:
Before moving to Workflow Actions, we need to add a Data Quality workflow enabledÂ  Data domain to our table. On the "Data Domain" tab create a new domain or use an already existing one:
The "Data Quality Workflow" checkbox should be enabled on creating the new Data Domain:
Now on the "Overview" tab on the Table detail page, the above Data domain needs to be added:
Once we have added the Data Quality workflow Data Domain to our table we can then add any scheduled job from the drop-down
Workflow actions are going to appear at the top right
Actions dropdown will contain In Progress and Resolve
When the user marks the rule as In Progress, the status will be changed to "In Progress".
When the user resolves the rule, a Pop-up will be displayed with the Reason Code and Description
Workflow status is changed to resolved
One thing to note here is that the "Actions" button and its options on the top right show if the Rule has met the following conditions:
The rule must have exceptions
The rule must be executed
The Table on which the Rule is created must have the Data domain added for which the Data Quality workflow is enabled
The scheduler must be attached to the Rule
Just so you know, workflow actions will also show when ticketing is enabled on the rule. For more details on Ticketing, please take a look at the article
JIRA Integration on DQ Rules.
The workflow status is Activity bar. Clicking on the comments icon opens the detailed activities.
On the Activity bar, clicking on the comments icon will open the detailed activities.
Tabs on Rule Detail Page:
On the Lower Section of the Rule detail page, the user will be able to see only the 3 tabs by default:
Users can click on
Show More
to view the other tabs available:
After clicking on the
Show More
tab, the user will see the following tabs and if the user wants to hide them they simply need to click on
Hide Others
Data Tab:
Data View
The "Data" tab is available after the rule is executed, regardless of whether exceptions are generated. By default, exceptions are highlighted in
red
for the column(s) where the rule is applied. All exceptions, along with the corresponding data in the table, are displayed.
Users can filter the exceptions using the drop-down menu with the following options:
Current:
Displays exceptions from the most recent execution.
30 Days:
Shows exceptions from the past 30 days.
90 Days:
Shows exceptions from the past 90 days.
All:
Displays the metric value for all available data, up to a maximum of the last 120 days (selectable via the drop-down).
Note: Only the first 300 exceptions are displayed in the Grid on the Data tab. if needs to view more exceptions, must enable the â€œInclude all Exceptionsâ€ checkbox in the Run Online dropdown before running the rule.
One thing is to be noted here that the "Data" tab will be shown for the following rules only:
Data Format
Blanks
Value Range
Uniqueness
Ruleset
Custom Query
Orphan Keys
Orphan Records
Compare Schema
Compare Metric
Note:
The Data tab supports column-level filtering and allows copying specific rows or column values directly from the exception grid for easier analysis.
Pivot View
.
The
Data
tab provides a detailed, interactive view of rule execution results, with a focus on exception data through the
Pivot View
.
The Pivot View in Data Quality Rules enables users to analyze exceptions by grouping and aggregating records directly within the application, helping identify patterns and derive insights.
For more information, refer to the sub-article
â€œ
Pivot View Configuration in Data Quality Rules.
â€
History:
The history tab will show the execution history for the rule in the form of a chart
The History tab has 2 different views which are:
Chart View
-
In this view, the data is shown to the user in the form of a chart
When the slicer is selected, the data in the History Chart will get distributed based on the Slicer's Field
Grid View -
In this view, the same data is shown to the user in a tabular form
Users can view execution history for:
Current
: It will show the metric value for the current execution
30 Days
: It will show metric value for 30 days
90 Days:
It will show the metric value for 90 days
All:
Displays the metric value for all available data, up to a maximum of the last 120 days.
Instructions:
Users can add any instructions.
Column Sequence:
On the "Column Sequence" tab, the users can set 3 different sequences that can be applied to the Rule:
Suggested Sequence
Table Specific Sequence
Rule Specific Sequence
For more details on Column Sequence, there is a separate article written for "
Column Sequence
".

--------------------------------------------------------------------------------

================================================================================
TITLE: Adaptive Thresholds
URL: https://dvsum.zendesk.com/hc/en-us/articles/18139200580116-Adaptive-Thresholds
================================================================================

In this Article:
When to use Adaptive Thresholds
How Adaptive Thresholds are Calculated
Example
Why IQR * 1.5?
How to set Adaptive Thresholds
When to use Adaptive Thresholds
Classic data quality (DQ) rules define a test that must be met for data to be considered valid. This could be a simple not-null test or a cross-reference integrity check or a more complex calculation. Typically the acceptable maximum threshold for this test would be zero. Invalid values are not acceptable.
But it turns out there are many real-world reasons to allow more flexible thresholds. One reason is that data is known to have a baseline number of invalid records. This is not
desired
, but it's expected. And in practice the team monitoring data quality doesn't want to receive a notification every day that there are invalid records--since that might well be every single day. But they would certainly be interested to get a notification if the number of invalid records spikes one day.
An even more common use case for wanting adaptive thresholds is for
data observability
tests. In many situations the number of records arriving each day should be roughly the same. A small variation is expected, but a large variation could indicate a problem. But how can we quantify "small variation" and "large variation"? This is where adaptive thresholds can help.
How Adaptive Thresholds are Calculated
Adaptive thresholds in DvSum DQ are set using the common statistical technique called Interquartile Range (IQR). In this method, we first calculate the 25th percentile and 75th percentile for past values. These values are referred to as the 1st quartile (Q1) and 3rd quartile (Q3). The IQR is simply the difference between Q3 and Q1. We define outliers as values which are more than 1.5 IQRs below Q1 or more than 1.5 IQRs above Q3.
Example
An example of the adaptive thresholds in action makes them clear. Consider the following values for the number of records loaded daily by a data pipeline.
You can also toggle to display the values in a grid. Here are those values:
1009, 990, 840, 825, 1115, 586, 899, 1029, 773, 1151, 1394, 1327, 870, 1128, 1089, 1097, 982, 741, 1105, 992, 737, 1240, 1014.
The first 4 data points have no thresholds displayed. This is because the IQR threshold calculation requires a minimum of 4 previous data points to be meaningful. The thresholds for the 5th data point are calculated as follows.
The 4 previous data points: 1009, 990, 840, 825
Q1: the median of 825 and 840 = 832.5
Q3: the median of 990 and 1009 = 999.5
IQR: Q3 - Q1 = 167
IQR * 1.5 = 250.5
Upper Threshold: 999.5 + 250.5 = 1250
Lower Threshold: 832.5 - 250.5 = 582
The same logic applies in identifying the data on Aug 17 as an outlier. The previous data points (1009, 990, 840, 825, 1115, 586, 899, 1029, 773, 1151) yield a Q1 value of 825 and a Q3 value of 1029. Those correspond to threshold values of 519 and 1,335.
Why IQR * 1.5?
The intuitive answer is that if the data is truly following a normal distribution and we're only seeing random noise in the results, then setting thresholds like this are expected to generate alerts only very rarely. The data quality team won't get too many notifications. But how many?
The complete details of the mathematical calculation are beyond the scope of this article. But setting the bounds at Q1 - 1.5*IQR and Q3 + 1.5*IQR will correspond to setting the bounds at
Î¼ Â± 2.7Ïƒ. With these bounds slightly fewer than 1% of measurements from a normal distribution will generate alerts.
Stated in a less-precise but more business-oriented manner, if you get an alert then it's
quite likely
that something went wrong. Maybe a file got loaded twice, or the underlying data has truly changed. You should investigate. But of course it's
possible
(about a 0.7% chance) that this is just a random fluctuation in the data.
How to set Adaptive Thresholds
Prerequisite: you have already created a DQ rule. Pipeline checks like COUNT and METRIC are the most common types to use adaptive thresholds. For more details about how to define DQ rules, refer to the
DQ Rule Details
page.
Edit the rule, then navigate to the tab "Thresholds".
Thresholds for DQ rules are set in this dialog in DvSum Data Quality:

--------------------------------------------------------------------------------

================================================================================
TITLE: Clone Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/27010646948884-Clone-Rule
================================================================================

In this Article
Cloning From DQ Dictionary Page
Multiple Rules Cloning From Different Sources
Multiple Rules With the Same Source
Cloning From Rule Detail Page
Users can now easily duplicate existing rules with minor adjustments and assign them to other users. This eliminates the need for manual rule creation, saving time and allowing users to create rules quickly using the new clone functionality. For more information on this feature, follow the steps outlined below.
Cloning From DQ Dictionary Page
Single Rule Cloning
Cloning From DQ Dictionary Page
Steps 1:
Navigate to the "DQ Dictionary" page and choose a specific rule. Click on "More Actions" at the top, then select "Clone Rule" from the drop-down menu, as illustrated below:
Step 2:
Upon selecting "Clone Rule," the user will be taken to the "Clone Rule" modal. At the top, there will be a prefix. The user will be required to mention the target data source on which the rule is to be cloned:
Step 3:
After cloning the rule, the user will be taken to the Clone Summary detail page. In the grid below, you will find the Original Rule ID, the cloned Rule ID (which remains blank in case of failed cloning), the default source of the rule, the source on which the rule has been cloned, the status indicating whether the cloning was successful or failed, any remarks in case of unsuccessful cloning, and finally, a description. In the top right corner, there is a "Download Summary" button to download a report of this screen, as illustrated below:
Multiple Rules Cloning From Different Sources
Multiple Rules Cloning From Different Sources
Step 4:
If the user selects multiple rules belonging to different sources, the same step 1 will be executed. Upon redirecting to the "Clone Rule" modal, you will observe the prefix, and below that, a target sources section. In the case of multiple rule sources, there will be both "From Source" and "To Source" options.
Step 5:
On the Clone Summary detail page, you will be able to see the number of rules that encountered errors and those that were successfully cloned, as illustrated below:
Multiple Rules With the Same Source
Multiple Rules With the Same Source
Step 6:
Choose rules that belong to the same source. Opt for the "Clone Rule" option, as demonstrated below:
You'll encounter a familiar screen as before, but this time, as the rules belong to the same source, there's only one target source displayed. Users can modify the target source if necessary. Simply click the "Clone" button, as indicated below:
Upon successful cloning, you will observe that the two rules have been duplicated, and their details will be displayed as shown below:
Note:
A rule that has been cloned can be treated independently from the original (parent) rule on which the cloning was performed. This implies that any configurations made in the cloned rule will not affect the parent rule.
Cloning From Rule Detail Page
Cloning From Rule Detail Page
Step 7:
Choose any rule and navigate to its detail page and click on "3 dots" where you will find a "Clone Rule" button on the top right side, as illustrated below:
Step 8:
After clicking the "Clone" button, you will be directed to the "Clone Rule" modal. Here Prefix of the Cloned Rule is mentioned and the user will have to mention the target data source on which the rule will be cloned.
S
tep 9:
After cloning the rule, the user will be redirected to the Clone Summary detail page where the user can see whether the rule is successfully cloned or not. Under the heading "Clone Rule" there will be the link of the Cloned rule
Rules can also be cloned from the "Data Quality" tab on the Rule detail page:

--------------------------------------------------------------------------------

================================================================================
TITLE: Column Sequencing
URL: https://dvsum.zendesk.com/hc/en-us/articles/26980095608596-Column-Sequencing
================================================================================

In this Article
Setting Column Sequence
Suggested Column Sequence
Table Specific Column Sequence
Rule Specific Column Sequence
Rules Where Column Sequence is Available
Note:
The Column Sequence tab will be shown for the following rules:
Users have 3 different options for Column Sequencing on the Rule detail page. In this article, we will go through how users can set Column sequences and what are the 3 different column sequences defined on the Rule detail page.
Setting Column Sequence
On the Data Quality tab of the Table Detail page, the column sequence can be opened by clicking on the Gear settings icon
Once Column Sequence is opened then the user can change the sequence for any column of the table:
Now on the Rule detail page, on the "Column Sequence" tab user can select one of the three options that are:
Suggested
Table Specific
Rule Specific
Suggested Column Sequence
By default, there is a sequence of the table that is defined and that is the "Suggested" column sequence.
After running the rule the suggested column sequence that was defined can be seen on the "Data" tab.
Table Specific Column Sequence
The "Table Specific" option is disabled if the user does not change the Column sequence from the "Data Quality" tab on the Table detail page. Once the user changes the Column sequence from there then this option is enabled. The Column sequence which was defined the the Table level can be seen here.
Once the user saves this option and runs the rule then the sequence on the "Data" tab will be the same as what was defined on the "Data Quality" tab.
Rule Specific Column Sequence
If the user changes the "Suggested" column sequence or the "Table specific" column sequence then the new sequence will be called "Rule Specific" specific. Users can make any changes in the column sequencing and save the option.
After running the rule, the user can see the same sequence on the "Data" tab that was defined in the "Column Sequence" tab:
Note:
The Column Sequence tab will be shown for the following rules on the Rule detail page:
Data Format
Blanks
Value Range
Orphan Records
Ruleset

--------------------------------------------------------------------------------

================================================================================
TITLE: Cross-System DQ - Compare Schema Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25404616533524-Cross-System-DQ-Compare-Schema-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Add a New Rule
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
Introduction
The "Compare Schema" rule involves a comprehensive examination of data from two distinct schemas to verify that they are consistent with each other.
Consider there is a scenario in which consistency of two schemas is to be checked. The Compare Schema Rule will compare the schema structure between two sources, highlight differences in tables and columns, include disparities in data types where applicable.
In essence, the Compare Schema rule ensures that data across different sources aligns accurately, providing insights into potential discrepancies or variations.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Configure the Rule:
Step 1: Access Data Dictionary
Log in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.
Step 2: Select Table & Available Rules
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3: Add a New Rule
Select the "âŠ• Add Rule" button, then choose the "Cross-System DQ" category. From the list of options, click on "COMPARE Schema"
Step 4: Basic Input Setup
In the Rule Wizard's
Basic Input
section, provide the
Source Schema
and
Target Schema
for the comparison. You can also add filters to refine the scope of comparison. Additionally, use the
Ignore Prefixes and Suffixes
option to remove common naming patterns (e.g., 'vw_' or '_temp') from table or column names, improving matching accuracy between the source and target schemas.
Step 5: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Cross-System DQ - Compare Metric Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25404439140372-Cross-System-DQ-Compare-Metric-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Add a New Rule (Cross-System DQ â†’ Compare Metric)
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
Introduction
The "Compare Metric" rule involves assessing aggregated quantities or metrics from two distinct data sources to verify their consistency and detect any differences or discrepancies.
It denotes the process of examining aggregated quantities or metrics from two separate data sources to ensure alignment and identify any discrepancies or differences.
For instance, one might compare the total shipment volume for the last three months recorded in the Enterprise Resource Planning (ERP) system with the aggregated shipment volume data stored in the data warehouse. This comparison aims to validate that both sources provide consistent information regarding shipment volumes and to pinpoint any discrepancies between them.
In summary, the Compare Metric rule helps ensure data accuracy and reliability by examining combined metrics from various sources.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
Detailed Steps:
Step 1: Access Data Dictionary
Log in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.
Step 2: Select Table & Available Rules
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3: Add a New Rule (Cross-System DQ â†’ Compare Metric)
Select the "âŠ• Add Rule" button, then choose the "Cross-System DQ" category. From the list of options, click on "COMPARE METRIC"
Step 4: Basic Input Setup
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Metric Type, Column Name and Reference source, reference table and column name corresponding to the table in which you want to compare for the metric.
Step 5: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Cross-System DQ - Orphan Keys Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25403824473364-Cross-System-DQ-Orphan-Keys-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Add a New Rule (Cross-System DQ â†’ Orphan Keys)
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Add a New Rule (Cross-System DQ â†’ Orphan Keys)
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
Introduction
The "Orphan Keys" rule identifies a situation where a group of records of a particular type exists in one system but is absent in another system where it should also be present, categorized by their shared attributes or types.
For instance, shipments designated for Customer X might be recorded in one system, but not included in another system's list of shipments organized by customer. Also, Product Category B could be listed among shipments, yet it doesn't appear in the list of products. This discrepancy points to a mismatch in data synchronization or categorization between the systems.
In summary, the Orphan Keys rule reveals differences between datasets in various systems, underlining the importance of organizing data consistently across different platforms.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
Detailed Steps:
Step 1: Access Data Dictionary
Log in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.
Step 2: Select Table & Available Rules
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3: Add a New Rule (Cross-System DQ â†’ Orphan Keys)
Select the "âŠ• Add Rule" button, then choose the "Cross-System DQ" category. From the list of options, click on "Orphan Keys"
Step 4: Basic Input Setup
<></>
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Column name, Reference source, Reference table and reference column name corresponding to the table in which you want to search for the orphan records.
Step 5: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Cross-System DQ - Orphan Records Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25403704015636-Cross-System-DQ-Orphan-Records-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Add a New Rule (Cross-System DQ â†’ Orphan Records)
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
Introduction
The "Orphan Record" rule identifies instances where a data entry exists in the records of one system but is absent from another system where it should be present based on expected data consistency between the two systems.
For example, if "Order A" is recorded in the list of orders within the Enterprise Resource Planning (ERP) system but is missing from the Transportation System's list of planned orders, it indicates an orphan record situation. This discrepancy suggests that there may be inconsistencies between the two systems regarding the management or synchronization of order data.
Addressing orphan records typically involves aligning data between systems, ensuring data integrity, and establishing mechanisms to prevent such discrepancies in the future.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
Detailed Steps:
Step 1: Access Data Dictionary
Log in to DvSum, proceed to the Data Dictionary tab, and select the relevant Data source and Table Name.
Step 2: Select Table & Available Rules
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3: Add a New Rule (Cross-System DQ â†’ Orphan Records)
Select the "âŠ• Add Rule" button, then choose the "Cross-System DQ" category. From the list of options, click on "ORPHAN RECORDS"
Step 4: Basic Input Setup
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description, column name, reference source , reference table and reference column name corresponding to the table in which you want to search for the orphan records.
Step 5: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Pipeline Checks - Metric Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25247261939604-Pipeline-Checks-Metric-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Add a New Rule (Pipeline Checks â†’ Metric)
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
Introduction
The "Metric" rule is a kind of Pipeline Check used to ensure data pipelines are working correctly. It focuses on verifying data quantities, checking data quality and performance, and keeping track of data timeliness in processing workflows. By doing so, it improves the flow, quality, and efficiency of data.
In simple terms, the Metric rule checks if the total amount of data falls within an expected range. For example, it might ensure that the sum of open order quantities is between 75 million and 100 million units. This rule is essential for maintaining accurate and efficient data processing.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Configure the Rule:
Step 1: Access Data Dictionary
Log in to DvSum and proceed to the Data Dictionary tab under the Dictionaries and select the relevant Data source and Table Name.
Step 2: Select Table & Available Rules
Step 3: Add a New Rule (Pipeline Checks â†’ Metric)
Select the "âŠ• Add Rule" button, then choose the "Pipeline Checks" category. From the list of options, click on "Metric"
Step 4: Basic Input Setup
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Metric Column, and Metric Type corresponding to the table in which you want to search if the aggregated quantity is within the expected range.
Step 5: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Pipeline Checks - Freshness rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25246892783380-Pipeline-Checks-Freshness-rule
================================================================================

In this Article:
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Available Rules
Step 3: Configure Settings Reference
Step 4: Add a New Rule (Pipeline Checks â†’ Freshness)
Step 5: Basic Input Setup
Step 6: Validate & Run the Rule
Introduction
The "Freshness" rule is a type of Pipeline Check that ensures the timeliness and reliability of data within data processing workflows. It verifies data quantities, assesses data quality and performance, and monitors if data is present or updated within a specified time frame.
In simple terms, the Freshness Check monitors whether data is available or updated within a certain timeframe. For instance, in a weather forecasting app, if data was initially fetched at "2023-09-30 08:00:00" and the next execution timestamp is "2023-09-30 14:30:00," resulting in a time difference of 6 hours, but the defined threshold is 3 hours, then the Freshness Check would alert because the difference exceeds the defined threshold.
This rule enhances the flow, quality, and efficiency of data processing by ensuring that data remains current and relevant, thus improving the reliability of the overall system.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Configure the Rule:
Step 1: Access Data Dictionary
Log in to DvSum, proceed to the Data Dictionary tab under the Dictionaries, and select the relevant Data source and Table Name.
Step 2: Select Table & Available Rules
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3: Configure Settings Reference (Load Profile & Metric Time)
Open the Settings Reference page and enter the Load Profile & Metric Time
Note: The Load Profile should be set to Incremental Data
Step 4: Add a New Rule (Pipeline Checks â†’ Freshness)
Select the "âŠ• Add Rule" button, then choose the "Pipeline Checks" category. From the list of options, click on "Freshness"
Step 5: Basic Input Setup
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description and Threshold Hours corresponding to the table in which you want to search that data is not present or updated by a certain time.
Step 6: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Pipeline Checks - Count Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25246624213396-Pipeline-Checks-Count-Rule
================================================================================

In this article:
Introduction
How to Configure the COUNT Rule
Step 1: Log in to DvSum
Step 2: Access the Data Quality Tab
Step 3: Add a New Rule
Step 4: Basic Input Configuration
Step 5: Validate & Execute
Introduction
The
COUNT
rule is a type of Pipeline Check designed to ensure the accuracy and efficiency of data pipelines. It verifies data quantities, assesses data quality and performance, and monitors data timeliness within data processing workflows.
In essence, the
COUNT
rule checks whether the number of records falls within the expected range. For example, in an open order extract, the COUNT rule would verify if the count of sales orders is between
800,000 and 1 million records
.
This rule enhances data flow, quality, and efficiency by ensuring that the expected number of records is processed, maintaining consistency and reliability in data pipelines.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Configure the COUNT Rule
Step 1: Log in to DvSum
Navigate to the
Dictionaries
>
Data Dictionary
tab.
Select the relevant
Data Source
and
Table Name
.
Step 2: Access the Data Quality Tab
Select the
Table Name
.
Click on the
Data Quality
tab.
Choose
Available Rules
.
Step 3: Add a New Rule
Click the
âŠ• Add Rule
button.
Select the
Pipeline Checks
category.
From the list of options, choose
COUNT
.
Step 4: Basic Input Configuration
In the Rule Wizardâ€™s
Basic Input
section, fill in the following details:
Rule Description
: Provide a meaningful name for the rule.
Threshold Count
: Define the expected range for the number of records.
Step 5: Validate & Execute
After saving the rule, review its definition.
Click
Run
to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational DQ - Ruleset
URL: https://dvsum.zendesk.com/hc/en-us/articles/25246448647828-Foundational-DQ-Ruleset
================================================================================

In this Article:
Introduction
Why is it Important?
How to Configure the Ruleset Rule
Step 1: Log in to DvSum
Step 2: Access the Data Quality Tab
Step 3: Add a New Rule
Step 4: Basic Input Configuration
Step 5: Validate & Execute
Introduction
The
Ruleset Rule
is a foundational Data Quality (DQ) check that combines three key validation rules:
Blanks Check
â€“ Ensures that critical fields are not left empty.
Value Range Check
â€“ Restricts values within a defined range.
Data Format Check
â€“ Verifies that data follows a specified format (e.g., numerical, string, date).
By applying these checks, the
Ruleset Rule
helps maintain data accuracy, consistency, and completeness.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
Why is it Important?
The
Ruleset Rule
plays a crucial role in preventing data quality issues, such as:
Missing Data
â€“ Ensuring key fields like Customer_ID are always populated.
Incorrect Values
â€“ Restricting inputs to valid predefined ranges.
Inconsistent Formatting
â€“ Standardizing data representation for consistency across datasets.
For example:
Customer_ID cannot be blank
, ensuring no missing records.
The â€œCountryâ€ column must contain only two-character country codes (e.g., US, UK).
Date fields should follow the â€œYYYY-MM-DDâ€ format.
By enforcing these rules, organizations can avoid incomplete or inaccurate records that may impact reporting and decision-making.
How to Configure the Ruleset Rule
Step 1: Log in to DvSum
Navigate to the
Dictionaries
>
Database Tables
tab.
Select the relevant
Data Source
and
Table Name
.
Step 2: Access the Data Quality Tab
Select the
Table Name
.
Click on the
Data Quality
tab.
Choose
Available Rules
.
Step 3: Add a New Rule
Click the
âŠ• Add Rule
button.
Select the
Foundational DQ
category.
From the list of options, choose
RULESET
.
Step 4: Basic Input Configuration
In the Rule Wizardâ€™s
Basic Input
section, fill in the following details:
Rule Description
: Provide a meaningful name for the rule.
Ruleset Checks
: Choose the specific checks you want to apply:
Blanks Check
â€“ Ensures no empty values in selected columns.
Value Range Check
â€“ Restricts values to a specified range.
Data Format Check
â€“ Verifies consistency in data format.
S
tep 5: Validate & Execute
After saving the rule, review its definition.
Click
Run
to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational DQ - Custom Query
URL: https://dvsum.zendesk.com/hc/en-us/articles/25211951751060-Foundational-DQ-Custom-Query
================================================================================

In this Article:
Overview
Why is it Important?
How to Configure the Custom Query Rule
Step 1: Log in to DvSum
Step 2: Access the Data Quality Tab
Step 3: Add a New Rule
Step 4: Configure the Custom Query
Step 5: Validate & Execute
Overview
The
Custom Query Rule
allows users to create and execute
custom SQL queries
for advanced data validation, analysis, and reporting. It provides flexibility to define unique data quality checks beyond predefined rules, enabling users to tailor queries to their specific needs.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
Why is it Important?
The
Custom Query Rule
is essential for:
Complex Data Validations
â€“ Ensuring data integrity using custom conditions.
Advanced Data Analytics
â€“ Running complex aggregations and transformations.
Tailored Reporting
â€“ Extracting and monitoring specific insights from data.
For example, users can create a query to:
Identify duplicate customer records by checking for repeated Customer_IDs.
Verify if order totals match invoice amounts across multiple tables.
Detect missing transactions in a financial dataset by checking sequential order numbers.
How to Configure the Custom Query Rule
Step 1: Log in to DvSum
Navigate to the
Dictionaries
>
Database Tables
tab.
Select the relevant
Data Source
and
Table Name
.
Step 2: Access the Data Quality Tab
Select the
Table Name
.
Click on the
Data Quality
tab.
Choose
Available Rules
.
Step 3: Add a New Rule
Click the
âŠ• Add Rule
button.
Select the
Foundational DQ
category.
From the list of options, choose
Custom Query
.
Step 4: Configure the Custom Query
In the Rule Wizardâ€™s
Basic Input
section, provide:
Rule Description
â€“ A clear description of the ruleâ€™s purpose.
Custom Query
â€“ The SQL query you want to execute.
Example SQL Query:
SELECT Customer_ID, COUNT(*)
FROM Orders
GROUP BY Customer_ID
HAVING COUNT(*) > 1;
This query checks for duplicate
Customer_IDs
in the
Orders
table.
Using Stored Procedures (for Oracle Data Sources Only)
When creating a
Custom Query rule
for an
Oracle data source
, you can optionally enable the
Stored Procedure
checkbox.
This allows the rule to execute a database stored procedure instead of a regular SQL
SELECT
query.
Stored Procedure Overview
When the
Stored Procedure
option is enabled, the Data Quality rule will execute a stored procedure that returns its result as a
REFCURSOR
.
This means the procedure must output a result set (similar to a query output) that DvSum can read and use for Data Quality checks.
How it Works
Select the
Stored Procedure
checkbox under
Basic Input
.
Enter the procedure name and its parameters in the
Custom Query
field.
Example:
PROC_NAME(param1, param2, ..., resultparam
OUT
SYS_REFCURSOR);
The platform executes this procedure on the connected Oracle database.
The result returned via the
SYS_REFCURSOR
will be used as the dataset for the rule.
Note:
The Stored Procedure option is available
only for Oracle data sources
. For other data sources, use a standard SQL query instead.
Step 5: Validate & Execute
After saving the rule, review its definition.
Click
Run
to execute and test the rule.
View the results and refine the query if needed.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational DQ - Unique Values Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25211808095892-Foundational-DQ-Unique-Values-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Select Table & Data Quality Rules
Step 3: Add a New Rule (Foundational DQ â†’ Unique Values)
Step 4: Basic Input Setup
Step 5: Validate & Run the Rule
Introduction
The "UNIQUE VALUES" rule evaluates whether the count of unique values within a particular dataset falls within an acceptable range or matches a specified number.
For instance, if there are 100 ship-from locations expected, the rule anticipates the count of unique locations to range between 95 and 100. An occurrence exceeding 100 suggests an error, typically due to duplicated or erroneous entries. Conversely, if the count falls below 95, it signifies a potential error, often attributable to missing or incomplete data.
By applying this rule, data analysts can ensure that the number of unique values conforms to expectations, thereby upholding data quality and reliability.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Configure the Rule:
Step 1: Access Database Tables
Log in to DvSum and proceed to the Data Dictionary tab under the Dictionaries and select the relevant Data source and Table Name.
Step 2: Select Table & Data Quality Rules
Select the table name then select the Data Quality tab and choose Available Rules.
Step 3: Add a New Rule (Foundational DQ â†’ Unique Values)
Select the "âŠ• Add Rule" button, then choose the "Foundational DQ" category. From the list of options, click on "Value Range".
Step 4: Basic Input Setup
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Column Name, corresponding to the table in which you want to search for Unique records.
Step 5: Validate & Run the Rule
After saving the rule, you'll see its definition. Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational DQ - Value Range Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25211047471124-Foundational-DQ-Value-Range-Rule
================================================================================

In this article:
Introduction
How to Set Up the Value Range Rule
Step 1: Navigate to Data Quality Rules
Step 2: Add a New Rule
Step 3: Configure Rule Parameters
Single Value
Range Value
Reference Dictionary
Step 4: Validate the Rule
Introduction
The Foundational DQ Value Range Rule ensures that data values fall within an acceptable range, preventing incorrect or outlier values. This rule is crucial for maintaining data accuracy in fields such as transaction amounts, dates, and numerical identifiers.
For example, in manufacturing, the rule might enforce that the yield percentage should always be between 0.01 and 1. Similarly, in sales orders, the order type should be limited to specific values like ZOST, ZOCO, or ZOFR. In another scenario, for shipments, the item_category should only include values listed in the item_category reference.
By applying this rule, data integrity is maintained, ensuring that values are within acceptable boundaries or options, thus enhancing the reliability of analyses and processes relying on this data.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Set Up the Value Range Rule
Step 1: Navigate to Data Quality Rules
Log in to DvSum.
Go to Dictionaries > Database Tables.
Select the relevant Data Source and Table Name.
Step 2: Add a New Rule
Select the table name and go to the Data Quality tab.
Choose Available Rules.
Click on the âŠ• Add Rule button.
Select Foundational DQ and choose Value Range.
Step 3: Configure Rule Parameters
In the Rule Wizard's Basic Input section:
Rule Description: Enter a meaningful description.
Column Name: Select the column to apply the rule.
Comparison Type: Choose one of the three options:
Single Value
Range Value
Reference Dictionary
Single Value
As the name suggests, for a single value there is a single threshold. From the list of operators, you can define the threshold on a column name. When comparison type is mentioned then the user can further select 3 more options from Value which are:
Absolute Value
Expression
Reference Column
The above different Value types are explained below with examples for better understanding.
Consider an example, where we want to create a VR rule on "Population" which should be greater than equal to the "Relative Budget". Specifying this as a "Valid" range, this rule will show the records where "Population" is less than "Relative Budget" as exceptions.
Another example is that we can select the option "Absolute Rule Value" and in this case, only the value mentioned will be considered as valid value
Another example is that here we have defined an expression " > 100 " which means that the data will be valid where the column "Population" has values greater than 100:
One more example is that here we have defined an expression ">DATEADD(day, 7, CURRENT_DATE()) which means that the data will be valid where the column "Population" has values greater than 7 days from the current date.
Range Value
For the Range values, you need to provide the minimum and maximum threshold. Values which will be not in range will be marked as exceptions.
Reference Dictionary
In the Rule Wizard's Basic Input section, Select "Ref Dictionary" from dropdown in Comparison type.
Select the Dictionary and add relevant source fields.
If user selects,
Treat Empty Strings as Valid Values
: Empty strings ("") will be considered
valid
and allowed by the rule.
Treat Null as Valid Values:
NULL values will be considered
valid
. Theyâ€™ll be treated as acceptable data
and allowed by the rule.
Ignore Case:
The rule will
not differentiate between uppercase and lowercase
characters.
For example, "Shipping" and "shipping" would be treated as
equal
.
Note:
To know more about Empty and Null values for Value rage rule, refer the article
Null and Empty Values
.
Step 4: Validate the Rule
Save the rule and check its definition.
Click
Run
to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational DQ - Data Format Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25210951110420-Foundational-DQ-Data-Format-Rule
================================================================================

In this Article
Introduction
Use Cases & When to Apply It
How to Configure the Rule
Step 1: Access Data Dictionary
Step 2: Create a New Rule
Step 3: Basic Input
Step 4: Configure Validation Conditions
Step 5: Save & Run the Rule
Introduction
The Foundational DQ Data Format Rule ensures that data adheres to predefined formats, such as date formats, phone numbers, or email structures. This rule helps maintain consistency and prevents data entry errors that can lead to integration issues and failed validations.
For example, if Social Security Numbers (SSN) should follow the pattern XXXX-XXX-XXX, any deviation would be flagged as an error.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
Use Cases & When to Apply It
The rule is particularly useful when:
Validating email addresses, phone numbers, dates, and other structured data.
Ensuring standardization across datasets before data integration.
Avoiding processing errors due to incorrect formatting.
Maintaining compliance with regulatory or business standards
How to Configure the Rule
Step 1: Access Data Dictionary
Log in to DvSum
Navigate to
Dictionaries
â†’
Database Tables.
Select the relevant
Data Source
and
Table Name
Data Quality
â†’ Choose
Available Rules
.
Step 2: Create a New Rule
Click
"
âŠ•
Add Rule"
.
Select the
"Foundational DQ"
category.
Choose
"DATA FORMAT"
.
Step 3: Basic Input
In the Rule Wizard's Basic Input section, you need to fill in the Rule Description, Column Name, Pattern, and Filter condition corresponding to the table in which you want to search for Data Format records.
Step 4: Configure Validation Conditions
Define the expected format using a valid regular expression. The actual regex syntax supported depends on the underlying database technology connected to DvSum.
Here are some commonly used patterns:
Email
â†’
^[\w\.-]+@[\w\.-]+\.\w{2,}$
Matches
example@domain.com
Phone Number
â†’
^\(\d{3}\)\s\d{3}-\d{4}$
Matches
(123) 456-7890
Date (YYYY-MM-DD)
â†’
^\d{4}-\d{2}-\d{2}$
Matches
2025-07-02
Postal Code (ZIP)
â†’
^\d{5}(-\d{4})?$
Matches
12345
or
12345-6789
Note: Since regex is evaluated at the database level, pattern support may vary by database (e.g., Snowflake, PostgreSQL). Always validate patterns against your specific data source.
Step 5: Save & Run the Rule
Click
"Save Rule"
.
Click "
Run
" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Foundational DQ - Blanks Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/25210423692564-Foundational-DQ-Blanks-Rule
================================================================================

In this Article
Introduction
How to Configure the Rule
Step 1: Access Data Quality Rules
Step 2: Create a New Rule
Step 3: Define the Rule Parameters
Step 4: Save & Run the Rule
Introduction
The "BLANKS" rule addresses the availability and completeness of data within a dataset. It focuses on identifying whether there are any missing values present. This rule is particularly important for ensuring data integrity and accuracy.
For example, let's consider a dataset containing customer information, including ship-to addresses. The BLANKS rule might be applied to check if there are any records where the address field is empty or missing in the customer ship-to information. By enforcing this rule, one can ensure that all necessary data fields are populated, thus maintaining the quality and reliability of the dataset.
For further information about the rule detail page including scope, threshold, notifications, etc., please see the detailed article
Rule Detail page.
How to Configure the Rule
Step 1: Access Data Quality Ruless
Log in to DvSum and
Navigate to
Dictionaries
â†’
Database Tables.
Select the relevant
Data Source
and
Table Name
.
Data Quality
â†’ Choose
Available Rules
.
Step 2: Create a New Rule
Click
"
âŠ•
Add Rule"
.
Select the
"Foundational DQ"
category.
Choose
"BLANKS"
.
Step 3: Define the Rule Parameters
Select the
Table and Column
where blank values should be restricted.
Fill in the Rule Description, and Column name.
If user selects:
Treat Empty Strings as Blanks:
Any field that contains an empty string ("") will be considered as a blank value.
Treat Null Values as Blanks
: Database-level NULL values will be treated the same as blank/missing values.
Note:
To know more about the Null and Empty values for Blank Rule, refer the article
Null and Empty Values.
Step 4: Save & Run the Rule
Click
"Save Rule"
.
Click "Run" to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: How is DQ Score calculated for rules?
URL: https://dvsum.zendesk.com/hc/en-us/articles/27005589675796-How-is-DQ-Score-calculated-for-rules
================================================================================

Introduction
The DQ Score helps assess how well a data quality rule is configured and ready for execution. A higher score indicates that the rule is properly set up, while a lower score suggests areas that may need improvement. It provides a precise overview of data quality and assists users in making relevant decisions.
DQ Score Calculation
The DQ Score is calculated slightly differently for different types of rules. The table below describes how the score is determined for each type of DvSum rule.
Rule Type
DQ Score Calculation
Count
Metric
If the threshold minimum is 0 and the result is less than or equal to 0, then the DQ score is 100%.
If the threshold minimum is not 0 and the result is less than or equal to the threshold minimum, then the DQ score is:
result/threshold minimum
If the result is greater than or equal to 2 times the threshold maximum, then the DQ score is zero.
If the result is greater than or equal to the threshold maximum but less than 2 times the threshold maximum, then the DQ score is:
(result - threshold maximum) / threshold maximum
Blank Check
Value Range
Ruleset
Orphan Records
Cross Dataset
Orphan Keys
Data Format
If there are no exceptions, then the DQ Score is 100%.
If there are exceptions, then the DQ Score is calculated as:
(total records - exception records) / total records
CUSTOM QUERY
If there are no exceptions, then the DQ score is 100%.
If there are exceptions, then the DQ score is:
(total records - exception records) / total records
If the total record count is less than the custom query count, then the DQ score is 0%.
Compare Schema
Compare Metric
If the calculated result is less than or equal to the reference result, then the DQ score is:
calculated result/reference result
If the calculated result is greater than or equal to 2 times the reference result, then the DQ score is 0%.
If the calculated result is greater than or equal to the reference result, then the DQ score is:
(calculated result - reference result) / reference result

--------------------------------------------------------------------------------

================================================================================
TITLE: Post-Import Steps for Workflow-Enabled Domains
URL: https://dvsum.zendesk.com/hc/en-us/articles/39826074221332-Post-Import-Steps-for-Workflow-Enabled-Domains
================================================================================

In this Article:
Workflow Behavior After Import
Steps to Manually Submit Imported Records for Approval
Navigate to the Asset Type
Find Recently Imported Records
Select Records to Submit
Click Mass Update
Submit for Approval
Confirm Submission
Approve the Records (For Approver)
Import File Behavior for the Status Field
Known Limitation
Workflow Behavior After Import:
After completing the import,
records are not automatically submitted for approval
.
In workflow-enabled domains:
Newly imported records
will have
Draft
status
Updated records
will have
Modified
status â€” regardless of their previous status or the value in the import file
To move records through the approval workflow, a
manual submission
is required after import.
Steps:
1. Navigate to the Asset Type
Open the domain where the records were imported (e.g., Business Glossary, Data Dictionary, Field Dictionary).
2. Find Recently Imported Records
Apply a filter to a column
Last Updated On,
like:
Last Updated On â†’ Greater than â†’ Yesterday
3. Select Records to Submit
Select the relevant records from the grid.
By default, clicking the checkbox on the top-left corner selects all records currently visible on the page.
2. To select all records in the current view (not just the visible page), click the
"Select all records in this view"
option that appears after selecting the first checkbox (as shown above).
Tip: There is
no hard limit on records
. The system allows bulk selection beyond 1000 records, provided you've filtered or configured the view accordingly.
4. Click
Mass Update
.
5.
Submit for Approval
In the Mass Edit modal:
Field
: Status
Value
:
Submit for Approval
Click
Submit
.
6.
Confirm
Records will now enter the
workflow process
and await approval.
7. Approve the Records (For Approver)
1. Go to the
Assigned to Me
section in the listing page of the relevant asset type.
2. Select the records â†’ Click
Mass Update
â†’ Set
Status
to
Approved
â†’ Click
Submit
.
Import File Behavior for the Status Field
The system ignores the Status value during import. All new records are set to
Draft
, and all updated records are set to
Modified
, regardless of whether the column is included or omitted.
Regardless of the import type,
manual submission is always required
to trigger the approval workflow.
Known Limitation
Automatic publishing or submission via import is not supported.
Records
cannot be auto-submitted
into the workflow during import, even if the
Status
is set to
Published
in the import file.
Manual action is required post-import
to submit both
newly added
and
updated
records for approval through the UI (e.g., via Mass Update).

--------------------------------------------------------------------------------

================================================================================
TITLE: Post-Import Steps for Non-Workflow Domains
URL: https://dvsum.zendesk.com/hc/en-us/articles/39824926810644-Post-Import-Steps-for-Non-Workflow-Domains
================================================================================

In this Article:
Workflow Behavior After Import
Steps to Manually Publish Imported Records
Navigate to the Asset Type
Find Recently Imported Records
Select Records to Update
Click Mass Update
Update Status
Import File Behavior for the Status Field
Known Limitation
Workflow Behavior After Import:
After completing the import,
records are not automatically published
.
In non-workflow domains:
Newly imported records will have
Draft
status
Updated records will have
Modified
status, this happens regardless of their previous status or the value provided in the import file
Steps:
1. Navigate to the Asset Type
Open the domain where the records were imported (e.g., Business Glossary, Data Dictionary, Field Dictionary).
2. Find Recently Imported Records
Apply a filter to a column
Last Updated On,
like:
Last Updated On â†’ Greater than â†’ Yesterday
3. Select Records to Update
Select the relevant records from the grid.
By default, clicking the checkbox on the top-left corner selects all records currently visible on the page.
To select all records in the current view (not just the visible page), click the
"Select all records in this view"
option that appears after selecting the first checkbox (as shown above).
Tip: There is
no hard limit on records
. The system allows bulk selection beyond 1000 records, provided you've filtered or configured the view accordingly.
4. Click Mass update.
5. Update Status
In the Mass Update modal:
Field
: Status
Value
: Published
Click
Submit
.
Confirm the status is updated to
Published
. Once the status is published, then you can verify the changes in the respective section where you modified.
Tip: Save a custom view with filters to repeat bulk updates easily.
Import File Behavior for the Status Field
The system ignores the Status value during import. All new records are set to
Draft
, and all updated records are set to
Modified
, regardless of whether the column is included or omitted.
Known Limitation
Automatic publishing or submission via import is not supported.
Manual action is required post-import
to submit both
newly added
and
updated
records for approval through the UI (e.g., via Mass Update).

--------------------------------------------------------------------------------

================================================================================
TITLE: Usability Score
URL: https://dvsum.zendesk.com/hc/en-us/articles/37654166387860-Usability-Score
================================================================================

In this Article:
Introduction
Improving Usability Score
Accessing the Overview Page
Understanding the Usability Score
Improving the Usability Score
Completing the Required Fields
Finalizing the Updates
Result
Required Metadata Fields for Full Score
Introduction
In DvSum, the
Usability Score
measures the completeness of a tableâ€™s metadata. A full score of
10
indicates that all essential metadata fields have been properly filled in, promoting better governance, discoverability, and trust in the data asset.
Note:
Full Score = 10
: Achieved when
all required metadata fields
are populated.
Partial Score
: If some fields are missing, the score is
calculated as the average
of the available metadata inputs.
If the Usability score is not 10/10 then the asset will considered as
Incomplete.
To mark an asset as Complete, the usability score must be updated to 10/10.
Improving Usability Score
1. Accessing the Overview Page:
Click on any
table name
,
field name
, or
term name
.
You will be redirected to the
Overview Page
.
The
Usability Score
is prominently displayed at the top of this page.
2. Understanding the Usability Score:
A
partial usability score
indicates that
not all required metadata fields
have been populated.
3. Improving the Usability Score:
Click on the
Edit
button.
A list of
required fields
that need to be filled will be displayed.
4. Completing the Required Fields:
Navigate to the
respective tab(s)
where the required fields are located.
Enter the
necessary information
in each required field.
5. Finalizing the Updates:
After filling in all required fields, click
Save
.
Then click
Done
and
Publish
.
6. Result:
Once all steps are completed, the
Usability Score
will be updated to
10/10
.
Once an asset achieves a 10/10 Usability Score, its status automatically transitions from
Incomplete
to
Complete
. For assets with workflow enabled, the asset must be approved and published after these updates. If any required fields are not filled in, the asset will continue to remain in
Incomplete
until all required information is provided.
Required Metadata Fields for Full Score
To reach a usability score of
10/10
, the following fields must be filled in:
Description
Ownership
Tags
Domains
Additional Info
Lineage
Additional info.
Note:
Missing even one of these fields will reduce the score.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Amazon S3 permissions: Use Existing Resources (Limited Permissions)
URL: https://dvsum.zendesk.com/hc/en-us/articles/37053578369684-Configure-Amazon-S3-permissions-Use-Existing-Resources-Limited-Permissions
================================================================================

In This Article:
Overview
Prerequisites
Create an AWS IAM User
Assign Custom Inline Policy
Connect Your S3 Data Source in DvSum
Overview:
This guide walks you through configuring Amazon S3 as a data source in DvSum. It is intended for users who already have their S3 buckets created and populated and prefer to manage their AWS resources directly with limited IAM permissions.
If you'd rather have DvSum manage the setup using AWS Glue (including crawlers and databases), please refer to
Configure S3 Data Source in DvSum (Elevated Permissions Required)
.
1. Prerequisites
Ensure the following before proceeding:
You have access to an AWS account with appropriate permissions.
Your Amazon S3 buckets are already created and populated with data.
You are familiar with AWS IAM policies and can assign roles or managed policies.
2. Create an AWS IAM User
Create a new IAM user in your AWS account that DvSum will use to connect to S3.
3. Assign Custom Inline Policy
To fine-tune access, you can use a JSON policy like the following example. This ensures only the required buckets and services are accessible:
Note:
Please Ensure all comments are removed from the final policy JSON before deployment
{
"
Version
"
:
"
2012-10-17
"
,
"
Statement
"
: [
{
"
Sid
"
:
"
AthenaQueryExecution
"
,
"
Effect
"
:
"
Allow
"
,
"
Action
"
: [
"
athena:StartQueryExecution
"
,
"
athena:GetQueryExecution
"
,
"
athena:GetQueryResults
"
,
"
athena:StopQueryExecution
"
,
"
athena:GetWorkGroup
"
,
"
athena:ListDataCatalogs
"
],
"
Resource
"
:
"
*
"
/*
-- Â Please Ensure all comments are removed from the final policy JSONÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â before deployment ----
Note:
- To restrict Athena to a specific WorkGroup:
Use Resource ARN like:
"Resource": "arn:aws:athena:<REGION>:<YOUR-AWS-ACCOUNT-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ID>:workgroup/<YOUR-WORKGROUP-NAME>"
- Example:
"Resource": "arn:aws:athena:us-west-2:123123123123:workgroup/dvsum-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  workgroup"
*/
},
{
"
Sid
"
:
"
GlueCatalogAndCrawlerAccess
"
,
"
Effect
"
:
"
Allow
"
,
"
Action
"
: [
"
glue:GetDatabase
"
,
"
glue:GetDatabases
"
,
"
glue:GetTable
"
,
"
glue:GetTables
"
,
"
glue:UpdateTable
"
,
"
glue:GetPartition
"
,
"
glue:GetPartitions
"
,
"
glue:GetCatalogImportStatus
"
],
"
Resource
"
:
"
*
"
/*
---- Please Ensure all comments are removed from the final policy JSONÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  before deployment ---
Note: (remove these comments in actual policy JSON)
- To restrict Glue access to specific Catalog/Databases/Tables:
Glue Catalog ARN:
"Resource":
"arn:aws:glue:<region>:<account-id>:catalog"
Glue Database ARN:
"Resource":
"arn:aws:glue:<region>:<account-id>:database/<database-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name>"
Glue Table ARN:
"Resource":
"arn:aws:glue:<region>:<account-id>:table/<database-Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â name>"
*/
},
{
"
Sid
"
:
"
S3DataSourceBucketsAccess
"
,
"
Effect
"
:
"
Allow
"
,
"
Action
"
: [
"
s3:GetObject
"
,
"
s3:ListBucket
"
,
"
s3:GetBucketLocation
"
],
"
Resource
"
: [
/*
Important:
This policy limits access to specific S3 buckets only.
As a result, only AWS Glue databases that catalog data stored inÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  these allowed S3 buckets can be queried successfully. Make sure thatÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â any Glue Data Catalog resources (databases and tables) you useÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  correspond to these permitted S3 buckets.
*/
"
arn:aws:s3:::<YOUR-DATA-SOURCE-BUCKET-1>
"
,
"
ARN:AWS:S3:::<YOUR-DATA-SOURCE-BUCKET-1>/*
"
,
"
arn:aws:s3:::<YOUR-DATA-SOURCE-BUCKET-2>
"
,
"
arn:aws:s3:::<YOUR-DATA-SOURCE-BUCKET-2>/*
"
// Add more data buckets as needed
]
},
{
"
Sid
"
:
"
AllowGetUserSelf
"
,
"
Effect
"
:
"
Allow
"
,
"
Action
"
:
"
iam:ListAttachedUserPolicies
"
,
"
Resource
"
:
"
arn:aws:iam::<YOUR-AWS-ACCOUNT-ID>:user/<YOUR-IAM-USER-NAME>
"
},
{
"
Sid
"
:
"
S3AthenaQueryResultBucketAccess
"
,
"
Effect
"
:
"
Allow
"
,
"
Action
"
: [
"
s3:GetObject
"
,
"
s3:PutObject
"
,
"
s3:ListBucket
"
,
"
s3:GetBucketLocation
"
],
"
Resource
"
: [
"
arn:aws:s3:::<YOUR-ATHENA-STAGING-BUCKET>
"
,
"
arn:aws:s3:::<YOUR-ATHENA-STAGING-BUCKET>/*
"
]
}
]
}
Important:
Please make sure to update the following placeholders in the policy JSON before deploying it:
<REGION>
: The AWS region where your resources are located (e.g., us-west-2).
<YOUR-AWS-ACCOUNT-ID>
: Your AWS account ID.
<YOUR-WORKGROUP-NAME>
: The name of your Athena workgroup.
<YOUR-GLUEDB-NAME>
: The name of your Glue database. It will be used for scanning S3 data sources.
<YOUR-DATA-SOURCE-BUCKET-1>
,
<YOUR-DATA-SOURCE-BUCKET-2>
: These are the names of your S3 data source buckets.
Itâ€™s important to include the buckets associated with GlueDB in orderÂ to grant access to them. You may add more buckets as needed.
<YOUR-IAM-USER-NAME>
: The name of the IAM user for whom you want to allow self-service access.
<YOUR-ATHENA-STAGING-BUCKET>
: The name of your S3 bucket for Athena query results.
Note: Ensure that all comments are removed from the final policy JSON before deployment.
4. Connect Your S3 Data Source in DvSum
After successfully creating the IAM user with the necessary permissions, follow these steps within DvSum:
Navigate to Add Source:
Go to the "Sources" section in the DvSum application and click on "Add Source."
Select Amazon S3:
Choose "Amazon S3" as the source type.
Enter Credentials:
Provide the "Access Key" and "Secret Key" associated with the IAM user you created.
Verify Authentication:
Click the "Check Authentication" button to ensure DvSum can successfully connect to your AWS environment using the provided credentials.
Specify Glue Database and S3 Bucket(s):
In the designated field for "Glue Database" enter the name of the AWS Glue database you wish to connect to.
In the designated field for "S3 Staging Bucket" enter the name of the S3 bucket(s) containing used for athena metadata storage.
Save Configuration:
Click the "Save" button to finalize the Amazon S3 data source connection within DvSum.
For step-by-step instructions and screenshots of this process, refer to
this guide
.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Configure External Recipients & Whitelist Domains
URL: https://dvsum.zendesk.com/hc/en-us/articles/37016421057428-How-to-Configure-External-Recipients-Whitelist-Domains
================================================================================

In this article:
Overview
Add External Email Addresses in the Job Notification Tab
Handle Domain Whitelisting
Verify the Email Address
Adding Multiple Recipients
Overview:
In DvSum's DI (Data Intelligence) tool, you can configure job notifications to be sent to users who are not part of your DvSum organization (external users). This guide walks you through the steps required to successfully add external email addresses.
For more details onÂ  jobs, refer to the article
Creating new Job
.
Step 1: Add External Email Addresses in the Job Notification Tab
From the
Administration
Tab navigate to
Jobs
and select
Definition
.
Click on the Job ID for which you want to add an external recipient.
Click
Edit
on the selected job.
Go to the
Notification
tab.
Enter the external email address in the external recipients field.
Press Enter
after typing the email address.
This step is important:
Only after pressing Enter
will the email address be properly saved.
Once saved, the email will appear enclosed in a box.
Step 2: Handle Domain Whitelisting
If you encounter an warning saying that the domain (e.g.,
gmail.com
,
yahoo.com
) is
not whitelisted
, perform the following:
Go to
Administration
>
Account
.
Navigate to
User Security
>
External Domains
.
Click on
Add Domain
.
Enter the domain name (e.g.,
gmail.com
) and
save
it.
Step 3: Verify the Email Address
After whitelisting the domain, return to the Job Notification tab.
Re-enter the external email address.
Press Enter
to save it properly.
The email should now be accepted without any errors and displayed in a box format.
Adding Multiple Recipients:
Users can enter multiple external email addresses, provided each address belongs to a whitelisted domain.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to set the exception record limit for rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/37016023734036-How-to-set-the-exception-record-limit-for-rules
================================================================================

In this article:
Navigate to the Data Dictionary Module
Access the Data Quality Section
Adjust the Exceptions Record Limit
Save Your Changes
Additional Reference
To configure the number of exception records for a specific table, follow the steps outlined below:
Step 1: Navigate to the Data Dictionary Module
Open the
Dictionaries
>
Database Tables
module from your system.
Locate the relevant table associated with the rule for which you want to adjust the exception record count.
Step 2: Access the Data Quality Section
Within the table settings, locate the
Data Quality
section.
Steps 3:
Adjust the Exceptions Record Limit
Under the
Data Quality
section, click on
Settings
.
In the
Exceptions Record Limit
field, enter the desired number. The maximum allowable limit is
75,000
records. You can set the limit anywhere between 300 and 75,000 based on your needs.
Steps 4: Save Your Changes
After entering the desired record limit, click
Save
to confirm and apply the changes.
Note:
Minimum Limit: The minimum number of exception records you can set is 300.
Maximum Limit: The maximum number of exception records you can set is 75,000.
Additional Reference:
Refer the article
Rule detail page
to know more about exporting exception records.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Reset Password on the DI Platform
URL: https://dvsum.zendesk.com/hc/en-us/articles/36793238343060-How-to-Reset-Password-on-the-DI-Platform
================================================================================

In this article:
Scope and When to Use This Guide
Non-SSO Flow: Resetting Your Password
Initiate Password Reset
Verify Your Identity
Retrieve Reset Code
Create a New Password
Complete the Process
SSO Flow: Resetting Password After Disabling SSO
Overview
Disabling SSO (Admin Action)
Temporary Password Delivery
Log In & Reset Password
Scope and When to Use This Guide
This guide walks users through the process of resetting their password in both standard (email/password) and SSO (Single Sign-On) environments.
You should use this guide if:
You've forgotten your password and can't log into your DvSum account.
You're using email/password-based authentication.
Your organization has disabled SSO and you need to access your account.
The guide is divided into two main sections:
Non-SSO Flow
â€“ For standard email/password users.
SSO Flow
â€“ For users switching from SSO to password-based login.
Non-SSO Flow: Resetting Your Password
For users with email/password authentication, follow the steps below to reset your password securely.
1. Initiate Password Reset
Navigate to the DvSum login page:
https://app.dvsum.ai
Click on the
"Forgot Password?"
link located below the password input field.
2. Verify Your Identity
Enter the
email address
associated with your DvSum account.
Click
"Reset my password"
.
Tip:
Ensure the email address is an exact match with what's registered in DvSum.
3. Retrieve Reset Code
Open your inbox and find the email from
DI Support
.
Locate the
6-digit verification code
in the email body.
If you donâ€™t see the email:
Check your spam or junk folder.
If still not found, click on the â€œResend Verification Emailâ€ button to receive a new code.
4. Create a New Password
Return to the DvSum
Password Reset
page.
Enter the 6-digit
verification code
.
Set a
new password
that meets the following criteria:
8â€“12 characters in length
At least
one uppercase
letter
At least
one number or special character
(
!@#$%
)
Cannot be the same as any of your
last 3 passwords
Confirm the new password by entering it again.
5. Complete the Process
Click
"Change Password"
to save changes.
Youâ€™ll be
automatically redirected
to the login page.
Sign in using your
email address and new password
.
SSO Flow: Resetting Password After Disabling SSO
If your organization has disabled Single Sign-On (SSO), users must log in using standard password authentication. The steps below outline how to reset your password after this transition.
1. Overview
SSO allows users to log in via their organizationâ€™s authentication provider. However, if SSO is disabled, users need to reset their password manually to regain access.
2. Disabling SSO (Admin Action)
Note:
This step is typically performed by an administrator.
Step 1: Navigate to SSO Settings
Log in to the
DvSum Application
.
Go to
Administration â†’ Account Settings â†’ SSO.
Step 2: Disable SSO Live Mode
Locate the
SSO Live Mode
toggle switch.
Click the toggle to
disable
SSO.
Step 3: Confirm the Action
A confirmation pop-up will appear.
Click
OK
to proceed.
Once disabled, all users will be required to log in using passwords instead of SSO.
3. Temporary Password Delivery
After SSO is disabled, a system-generated email with a
temporary password
is sent to each registered user's email address.
Didnâ€™t receive the email?
Check your
spam/junk
folder.
Verify that the correct email address is used.
4. Log In & Reset Password
Use the
temporary password
from the email to log in at
https://app.dvsum.ai
.
Upon login, youâ€™ll be prompted to set a
new password
.
Follow the same password rules mentioned earlier.
Click
"Save"
to update your credentials and complete the login process.
SSO Flow: Resetting Password when SSO is Live
Once SSO is enabled in
Live Mode
, DvSum no longer stores user passwords. All authentication is handled by your organizationâ€™s Identity Provider (IdP), such as Microsoft Entra (Azure AD), Okta, or Ping.
Password reset flow:
The
â€œForgot Passwordâ€
link on the DvSum login page will no longer work.
Users must reset their password
directly in the Identity Provider (IdP) system
.
After resetting the password at the IdP, users can log in to DvSum using their updated SSO credentials.
DvSum support cannot reset passwords while SSO is Live.
If SSO is later disabled:
DvSum reverts to the standard login mechanism.
Users will need to set up new DvSum passwords using the regular password reset process described
How to Reset Password on the DI Platform
.
References:
Okta Support:
Password reset for users created via external IdP must be performed on the IdP.
Okta Support Article
Microsoft Entra (Azure AD) Support:
Users must reset their work or school passwords through Azure AD security info or mobile device if locked out.
Microsoft Support
AWS SSO with External Identity Provider:
Password reset is not allowed via AWS SSO when using an external identity source.
AWS Support
These references confirm that when SSO is active, password management is handled at the Identity Provider, not within the DvSum application itself. The linked DI Platform article provides step-by-step instructions for standard password resets when SSO is not active.
Note:
For more details on enabling SSO refer
Enabling SAML-Based Single Sign-On (SSO)
aticle.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Link Terms to Columns
URL: https://dvsum.zendesk.com/hc/en-us/articles/36043768656020-How-to-Link-Terms-to-Columns
================================================================================

In this Article:
Overview
Add a Term to a Column
Overview
Associating terms with columns enhances data governance and traceability by ensuring consistency in definitions. The Business Glossary allows users to link terms to relevant columns, making it easier to maintain standardized terminology. Follow the steps below to add a term to a column.
Add a Term to a Column
Go to the
Dictionary
dropdown and select the
Glossary Terms
tab.
Locate and click on the term you want to associate with column(s).
Open the Term Overview Page
On the Overview page, select
Edit
.
Scroll down to the
Relationships
section and click the
pencil
icon to add a relationship.
Click on
Add
button
After clicking on
Add
, choose the following options:
Relationship Type
:
Represented By
Asset Type
:
Database Column
Asset
: Search and select the relevant columns (e.g., Address, AddressLine1, AddressLine2, etc.).
Once selected, multiple columns can be associated with the term.
Click on Add
The chosen terms are displayed in a tabular form
Scroll to the top of the page and click Done
By following these steps, you can efficiently manage term-column relationships.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Manage Relationships Between Assets
URL: https://dvsum.zendesk.com/hc/en-us/articles/35968933714196-How-to-Manage-Relationships-Between-Assets
================================================================================

Relationships help define associations between different assets, ensuring better traceability and governance. The Assets Dictionary allows users to establish these connections, making it easier to navigate related data elements. Follow the steps below to add a new relationship to an asset.
Add a Relationship:
On the Overview page, select
Edit
.
Scroll down to the
Relationships
section and click the
pencil
icon to add a relationship.
Click on
Add
button
Enter the required details and add the relationship.
The relationship type may vary based on the Asset Type, such as "Governed by" or "Represented by."
Note:
Only column assets support adding a Reference Dictionary as a relationship.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Execute Selective Rules in DvSum
URL: https://dvsum.zendesk.com/hc/en-us/articles/35301308681876-How-to-Execute-Selective-Rules-in-DvSum
================================================================================

In this Article:
Overview
Step 1: Navigate...
Step 2: Apply Filters...
Step 3: Select Rules
Step 4: Execute Rules
Overview
In
Data Intelligence (DI)
, rules help validate, cleanse, and monitor data quality. Sometimes, instead of running all rules at once, you may need to execute specific rules explicitly based on certain conditions.
By applying filters in the DQ Dictionary, you can isolate and execute only the necessary rules.
Steps to Run Rules Explicitly Using Filters
Step 1: Navigate to the DQ Dictionary Page
Go to the
Dictionaries
>
DQ Dictionary
section within your DI environment.
This section contains all configured
Data Quality (DQ) rules
in DI.
Step 2: Apply Filters to Isolate Specific Rules
Use the
search bar or filtering options
to locate the specific rules you need to run.
You can filter rules based on various attributes like
Rule ID, Rule Name, Rule Type,
or
Status.
Example: If you need to run
DQ-0001234 and DQ-0005678
, apply a filter to display only these rules.
After filtering, only the relevant rules will be listed.
Step 3: Select the Rules
Once the desired rules are displayed,
select them
by clicking the checkbox next to each rule.
Ensure that only the required rules are selected before proceeding.
Step 4: Execute the Rules Using 'Run Rule'
After selecting the rules, click the
"Run Rule"
button located at the
top right
of the page.
A dropdown will appear with two options:
Run Online
:
Executes the rule instantly and provides real-time results.
Note:
If you change the tab while the rule is running, the process will get
aborted
.
Run Offline
:
Runs the rule in the background (via a scheduler).
Results will be available after execution is complete.
Execution status can be checked later in the
Job section
Note:
This option is Recommended for
large datasets
or when running multiple rules simultaneously.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to re-set password after disabling SSO?
URL: https://dvsum.zendesk.com/hc/en-us/articles/35213710872468-How-to-re-set-password-after-disabling-SSO
================================================================================

To disable SSO, the user should switch off SSO Live mode.
A confirmation pop-up will be displayed, Click OK.
After SSO is disabled, an email notification with a temporary password will be sent to the user. The temporary password can then be reset to a new one.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to delete a table in Data Dictionary?
URL: https://dvsum.zendesk.com/hc/en-us/articles/35166590464404-How-to-delete-a-table-in-Data-Dictionary
================================================================================

In This Article:
Overview
Steps to Enable Table Deletion
Overview:
Tables can only be deleted if their status is set to 'Deleted'. If an attempt is made to delete tables with any other status, as shown in the screenshot below, the system will display a pop-up message:
"No table(s) has been deleted."
To delete a table successfully, follow the steps below to change its status to 'Deleted' before proceeding with the deletion.
Steps to Enable Table Deletion
Navigate to the Administration >Data Source
Open the
Data Source
option from the left-hand menu.
Select the Tableâ€™s Source
Locate and select the source that contains the table you want to delete.
Go to
Settings
and select the
Connection
option.
Modify Schema Selection
Click the
Edit
button to open the
Schemas
page.
Identify the schema that contains the table.
Move the schema from
'Selected'
to
'Available'
to remove it from the data source.
Perform a Scan
Initiate a
scan
to update the system.
Once the scan is complete, the tableâ€™s status will be updated to
'Deleted'
.
Delete the Table
Once the status is set to
'Deleted'
, proceed with the table deletion.
Note:
For the Amazon S3 data source, updating the S3 buckets is required instead of moving the schema from 'Selected' to 'Available.'

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Create View in Jobs and Job Executions
URL: https://dvsum.zendesk.com/hc/en-us/articles/35132550978452-How-to-Create-View-in-Jobs-and-Job-Executions
================================================================================

In this Article:
Navigating to Definition and Job Executions
Creating a Custom View in Jobs
Clone View
Share View
Delete View
Export View
Overview
DvSum enables users to create custom views for Jobs and Job Executions, allowing better tracking, management, and analysis. Views help users focus on relevant data by filtering and organizing job records efficiently.
Navigating to Definition and Job Executions
Open the
Administration
tab on the left and choose
Jobs
menu.
From the drop down, click on
Definition
tab to access the Job Listing page.
Creating a Custom View in Jobs
Click on
Definition
in the left menu to open the Job Listing page.
Select
Create View
.
Enter a
View Name
.
Customize the view by:
Adding or removing columns.
Specifying criteria for filtering records.
Click
Save
to store the custom view.
For details on job creation, refer to the
Creating a New Job
article.
Note:
The user will not be able to Edit/ Share and Save the default view. These buttons will remain disabled.
The columns available in the current view will be automatically pre-selected.
Clone View:
To create a clone of any view, users can click on the Edit pencil icon next to the view name. This action will reveal the clone button, allowing users to duplicate the selected view.
Share View:
The view can be shared with other users by granting "Editor" or "Viewer" access.
Users with "Editor" access have the capability to Edit, Clone, Delete, Share, Save, and Export operations with a view.
Users with "Viewer" access can perform Share, Save, and Export operations with a view.
Delete View:
To delete a particular view, click on the Edit button, and you will find a delete button available.
Export View:
Simply click on the download button, and users can download the records available in the grid as an Excel file.
Note:
Users can also create their own view in Job Executions, following the same procedure as creating a view in Jobs.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to create a Tag
URL: https://dvsum.zendesk.com/hc/en-us/articles/10673319844884-How-to-create-a-Tag
================================================================================

In this Article:
Overview
Step 1: Access Tags
Step 2: Create a New Tagset
Step 3: Configure Tagset Information
Overview
User can classify data assets ( data dictionary, field dictionary, analytics dictionary and business glossary) using tags. User can link single and multiple tags to data assets. Lets see how tags can be created and link to data assets:
Step 1: Access Tags
Tags can be accessed by navigating to
Administration > Asset Management > Tags
.
By default following tags are available. You can also use the already-created tags.
PII classification
Sensitivity
Custome Tags
Dataset Type
Certification Tag
Dataset Quality tags
NOTE
: Certification tags and Dataset quality tags are non-editable by default.
Step 2: Create a New Tagset
Provide Tagset name, and description and click on Add button. The user will be redirected to the tagset configuration page.
The tagset detail page would have two section:
Information
Tag
Step 3: Configure Tagset Information
Now let's see each option in the information section for tagset:
- Name & Description:
Tagset name and definition would be pre-populated as this information was already provided while creating a tagset.
- Applicable To
Using the Applicable To option, the User can enable/disable tagset for the Dataset, Fields, terms , Model & Report. If applicable To is unchecked for any Data asset, then the tag will not be shown for unchecked data assets.
- Only Single value Allowed
Users can select only one tag for tagset. Multiple selections of tags is not allowed if this option is enabled.
- Allow New Values During Tagging
User can also add his own custom tag in the tagset from Data asset detail page.
Require Note when Tagging:
While linking a tag to any data asset user has to provide a note. Note will be shown when user hover over the tag.
- Make Tags Prominent:
Prominent tags with selected colors will be shown with the Data assets name on top of the detail page and Enterprise search.
Watch this quick video tutorial on how creating a tag works:

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Amazon S3 permissions: manage data source in DvSum (Elevated Permissions Required)
URL: https://dvsum.zendesk.com/hc/en-us/articles/360055209472-Configure-Amazon-S3-permissions-manage-data-source-in-DvSum-Elevated-Permissions-Required
================================================================================

In This Article:
Overview
Detailed Steps
Step 1: Add a New User
Step 2:Â Add Role
Step 3: Generate Access Key
Next Steps
Overview
DvSum Data Quality can perform data quality checks on data stored in Amazon Simple Storage Service (S3). DvSum uses Amazon Athena to provide access to the files, and it uses AWS Glue Data Catalog to crawl the files and to gather metadata.
In order to define a data source in DvSum you must first configure an AWS user with appropriate permissions for
S3
,
Athena
, and
Glue
. Follow the steps below to create a new user with the required permissions or to verify that an existing user has the permissions needed.
The detailed steps below explain the process to create a new user in the AWS Console. A command-line version of the same steps is provided as well. Users should follow either the Detailed Steps or the the Command-Line Instructions.
Detailed Steps
Step 1: Add a New User
Typically a new user is created to be used with DvSum. Using an existing user is of course valid as well, but it's important to validate that the user has all permissions documented below.
To add a new user, open the AWS Console, navigate to
IAM â†’ Access Management â†’ User
, and click on the 'Create user' button.
1.1 Set User Details
Set the user name to any valid name, and click on the Next Button.
1.2 Set Permissions
Select "Attach policies directly"
1.3 Create a Policy named "dvsum-s3-source-policy"
Click on the "Create policy" button and you will be redirected to the Create Policy page.
Then click on the JSON button, and paste the JSON provided below. Click the "Next" button.
{
"Version": "2012-10-17",
"Statement": [
{
"Sid": "ConfirmPoliciesAndPassRole",
"Effect": "Allow",
"Action": [
"iam:PassRole",
"iam:GetUser",
"iam:ListAttachedUserPolicies"
],
"Resource": "*"
}
]
}
1.4 Review and create the Policy
Important: the policy name must be exactly the name provide here. The description is optional.
Policy name:
dvsum-s3-source-policy
Policy Description: Policy used by DvSum to confirm permissions
Click the "Create policy" button.
1.5 Add Permissions Policies
1.5 Add Permissions policies
Return to the Create user wizard already in progress from earlier. Select the following permission policies:
AmazonS3FullAccess
(AWS managed)
AmazonAthenaFullAccess
(AWS managed)
AWSGlueServiceRole
(AWS managed)
dvsum-s3-source-policy
(customer managed)
Click the "
Next
" button.
1.6 Review and Create User
Review User details and Permissions Summary. Then click the "Create user" button.
Step 2:Â Add Role
AWS Glue requires a role with appropriate permissions which will be passed to the crawler when it runs.
2.1 Create role
Navigate to
IAM â†’ Access management â†’ Roles
and click the "Create role" button.
2.2 Define trust policy
In the first step of the create role wizard, select "
Custom trust policy".
Paste the code provided below. Click the "Next" button.
Copy from here and paste in the Custom trust policy:
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Service": "glue.amazonaws.com"
},
"Action": "sts:AssumeRole"
}
]
}
2.3 Add permissions
Add the following permission policies to the role.
AWSGlueServiceRole
AmazonS3FullAccess
Click the "Next" button.
2.4 Name, review, and create
Important: the role name must be exactly the name provide here. The description is optional.
Role name:
dvsum-glue-service-role
Role Description: Role used by DvSum to grant the Glue Crawler permission to access files
Click the "Create role" button.
Step 3: Generate Access Key
3.1 Navigate to Security Credentials
Navigate to
IAM â†’ Access Management â†’ Users
. Select the user you just created, and click the "Security credentials" tab.
3.2 Create Access Key
Click the "Create access key" button.
Select "Command Line Interface(CLI)". Click the "Next" button.
3.3 Add a description Tag
A description for the access key is optional, but setting it is a best practice.
Description tag value: Access key used by DvSum to access S3, Athena, and Glue services
Click the "Create access key" button.
3.3 Retrieve access keys
Save the Access key and Secret access keys. You will use these values when configuring the S3 data source in DvSum.
Next Steps
Now that you have an AWS Access Key associated with a user that has all required permissions, the next step is to follow the instructions on how to
Configure Amazon S3 as a Data Source in DvSum.
Command Line Instructions
The steps below achieve the same results as the detailed steps above using the AWS CLI.
# Create a new user (use any valid name for the user)
aws iam create-user --user-name dvsum-user
# Attach these 3 AWS managed policies
aws iam attach-user-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonAthenaFullAccess \
--user-name dvsum-user
aws iam attach-user-policy \
--policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \
--user-name dvsum-user
aws iam attach-user-policy \
--policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole \
--user-name dvsum-user
# Create dvsum-s3-source-policy (Use this exact name)
aws iam create-policy \
--policy-name dvsum-s3-source-policy \
--policy-document \
'{
"Version": "2012-10-17",
"Statement": [
{
"Sid": "GetUserInfo",
"Effect": "Allow",
"Action": [
"iam:GetUser",
"iam:ListAttachedUserPolicies",
"iam:PassRole"
],
"Resource": [
"*"
]
}
]
}'
# Attach the policy (your policy-arn will be different from this example)
aws iam attach-user-policy \
--policy-arn arn:aws:iam::<account-id>:policy/dvsum-s3-source-policy \
--user-name dvsum-user
# Create role for crawler
aws iam create-role \
--role-name dvsum-glue-service-role \
--assume-role-policy-document \
'{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Service": "glue.amazonaws.com"
},
"Action": "sts:AssumeRole"
}
]
}'
# Attach AWSGlueServiceRole policy to role
aws iam attach-role-policy \
--role-name dvsum-glue-service-role \
--policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
# Attach AmazonS3FullAccess policy to role
aws iam attach-role-policy \
--role-name dvsum-glue-service-role \
--policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
# Create an access key
aws iam create-access-key --user-name dvsum-user

--------------------------------------------------------------------------------

================================================================================
TITLE: Tracking Data Set Usage in Data Catalog
URL: https://dvsum.zendesk.com/hc/en-us/articles/11795434280212-Tracking-Data-Set-Usage-in-Data-Catalog
================================================================================

In This Article:
Overview
Data Usage Statistics in Data Catalog
Usage Classifications
Capturing Data Usage Statistics
Enable Query Logging (SQL Server / Azure)
Enable Query Logging (PostgreSQL)
Query Logging in Other Sources
Overview
DvSum Data Catalog (DC) manages lots of information about data sets--from metadata like column definitions to governance information like the data owner and data steward to data classifications like 'restricted' vs 'public' and much more. Data sets include tables and views in relational data sources, reports in business intelligence platforms, and objects in other applications like SAP and Salesforce.
One piece of information that DC tracks about relational data sets (tables and views) is data usage metrics documenting how often the data set is accessed by users via SQL queries. Query logs are maintained by the underlying database, and these statistics are summarized in DC to help users to understand which data sets are used most frequently.
These data set usage statistics are tracked in a property called "Popularity". This article explains where popularity is available in DC and how to configure databases to allow DC to gather this information.
Data Usage Statistics in Data Catalog
The data popularity is displayed in several locations in DC:
Data Dictionary table view
Data Set popup window
Data Set details page
Search results for tables
Examples of data popularity available in DC:
Usage Classifications
3 Stars - Heavily used
Table is used daily over the last 30 days
2 Stars - Moderately used
Table is used at least weekly over the last 30 days
1 Star - Lightly used
Table is used less than once per week over the last 30 days
If no stars are displayed it means either DvSum is unable to read query logs or the table has not been accessed in the past 30 days.
Note:
The popularity score is typically calculated for the last 30 days. The data retention policy for Snowflake tracks user query history for only 7 days. Keep in mind that retention policies and query logging details can be set differently by your corporate database administrators.
Examples in DvSum Data Catalog (DC)
Data Dictionary table view
Note: The field "Popularity" is an optional field, so add it to your view if it is not visible.
Data Set popup window
Data Set details page
Search results for tables
Note: If search results have multiple tables with the same name, the table with the higher popularity score will be shown first.
Capturing Data Usage Statistics in Data Sources
Enable Query logging in SQL Server, A
zure SQL, Azure Synapse
Use the
Query Store
.
The Query Store is enabled by default for new Azure SQL Database and Azure SQL Managed Instance databases.
Query Store is not enabled by default for SQL Server 2016 (13.x), SQL Server 2017 (14.x), SQL Server 2019 (15.x). It is enabled by default in the
READ_WRITE
mode for new databases starting with SQL Server 2022 (16.x). To enable features to better track performance history, troubleshoot query plan related issues, and enable new capabilities in SQL Server 2022 (16.x), we recommend enabling Query Store on all databases.
Query Store is not enabled by default for new Azure Synapse Analytics databases.
Enabling the Query Store can be done in multiple ways.
Enable Query Store using the Query Store page in SQL Server Management Studio
In Object Explorer, right-click a database, and then select
Properties
.
Note: Requires version 16 or later of Management Studio
In the
Database Properties
dialog box, select the
Query Store
page.
In the
Operation Mode (Requested)
box, select
Read Write
.
Enable Query Store usingÂ Transact-SQL statements
Use the
ALTER DATABASE
statement to enable the query store for a given database. For example:
ALTER DATABASE <database_name>
SET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE);
In Azure Synapse Analytics, enable the Query Store without additional options, for example:
ALTER DATABASE <database_name>
SET QUERY_STORE = ON;
Enable Query logging in PostgreSQL
To enable query logging in PostgreSQL, you can modify the configuration file (postgresql.conf) and use SQL statements for more fine-grained control.
Modify postgresql.conf
You need superuser privileges to edit this file.
You can locate
postgresql.conf
in your PostgreSQL data directory.
Common locations include:
/etc/postgresql/<version>/main
(Linux)
C:\Program Files\PostgreSQL\<version>\data
(Windows)
Edit
postgresql.conf
Set
logging_collector
to '
on
'
Set
log_statement
to 'all' to track SQL queries. For reference, these are the valid values:
none
(default): Log no statements.
ddl
: Log data definition language (DDL) statements like CREATE, ALTER, and DROP.
mod
: Log moderate-level statements, including DDL and most of the data manipulation language (DML) statements.
all
: Log all statements, including SELECT, INSERT, UPDATE, DELETE, and more.
Use SQL Statements for Fine-Grained Control
If you want to enable or disable query logging for specific databases or sessions, you can use SQL statements. This can be especially useful for debugging or auditing purposes. You can change logging settings on a per-session or per-database basis using the following SQL commands:
To enable query logging for the current session only:
SET log_statement = 'all';
To enable query logging for a specific database (replace your_database with the actual database name):
ALTER DATABASE your_database SET log_statement = 'all';
To disable query logging for a specific database:
ALTER DATABASE your_database SET log_statement = 'none';
These SQL statements will take effect immediately for the current session or database.
Remember to be cautious with query logging in production environments, as it can generate large log files and potentially impact performance. Always monitor your log files and adjust the log level as needed to balance the need for more information and the need for better performance.
Query logging in other sources
Query logging is enabled by default for these databases, so DC will typically gather popularity statistics with no additional configuration needed.
Oracle
Snowflake

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Use Global Filters
URL: https://dvsum.zendesk.com/hc/en-us/articles/10534035730836-How-to-Use-Global-Filters
================================================================================

In This Article:
Overview
How to apply Global Filters
Enterprise Search
Listing Pages
Home Page
Turn Off Global Filter (Note)
Overview
In the Data Catalog application, users have the catalog of all the sources, but when users want to look into the details of certain sources and data domains, t
hey have to apply filters again and again on different sections in the application to view the desired data so in order to save the time and effort we have
Global filter where users can choose the sources and data domains as user settings at the global level to view the data in the application.
How to apply Global Filters:
To apply global filters navigate to the My settings Page, Select the Data sources and Data Domain that you want to add as Filter, and click on the save button.
Global filter will be applied on Enterprise search, Listing pages ( Data Dictionary, Analytics Dictionary, Field Dictionary & Business glossary), and Home page. Let's see each one by one:
Enterprise Search
When users search any keyword using enterprise search, a global filter would be applied to selected Data sources and domains. Global filters will applicable on all tabs ( Tables, columns Terms, Model,s and reports) of enterprise search.
NOTE
: The user can also apply other filters available in enterprise search.
Listing Pages:
On Listing pages ( Data Dictionary, Analytics Dictionary, Field Dictionary & Business glossary) global filters gets applied and results will be shown on the basis of filters. Users can remove the Filter using the
Clear All
button.
Home Page:
When global filters settings are turned on, Feed on the home page will be shown on the basis of global filters.
Note:
Users can turn off the global filter anytime by un-checking the global filter flag for Data sources and data domains on the My settings page. Filter will not get applied on Enterprise Search, Listing pages and Home page
Watch this quick video tutorial on how to apply and use Global filters in the DvSum app.

--------------------------------------------------------------------------------

================================================================================
TITLE: How To: Mass Update
URL: https://dvsum.zendesk.com/hc/en-us/articles/10582893963924-How-To-Mass-Update
================================================================================



--------------------------------------------------------------------------------

================================================================================
TITLE: How to Schedule a Job for Data Sources
URL: https://dvsum.zendesk.com/hc/en-us/articles/18727787422228-How-to-Schedule-a-Job-for-Data-Sources
================================================================================

In this Article:
Overview
Scheduling a Scan Job
Configure Scheduler Settings
Scan Frequency Options
Daily Scans
Weekly Scans
Monthly Scans
End Conditions
Scan Summary
Video Tutorial
Overview:
Job schedulers automate and streamline the execution of tasks within web applications, ensuring timely and efficient processing of background jobs, thus enhancing overall system performance and user experience. In the Dvsum Data Catalog application, the scheduler plays a pivotal role in automating the data source scanning process. By setting predefined schedules, the scheduler ensures that scans occur without manual intervention, maintaining data accuracy and relevance. This automation optimizes resource utilization by enabling scans during off-peak hours, enhancing overall system efficiency. The automated scans also provide timely insights into data changes, allowing prompt identification of trends or anomalies.
Within the Data Catalog application, users are presented with two methods to initiate a scan. The first involves an immediate on-demand scan achieved by clicking the "Scan now" button on the data source's Scan History page.
The second option grants users the ability to schedule scans using a scheduler, a process that we will explore in more detail below.
Scheduling a Scan Job:
Navigate to the Data Source general settings from the detail page. There, the user will see the Scanning Information section which has the scheduling details. By default, it is set to empty as shown below.
Configure Scheduler Settings:
Navigate to Data Sources > Any Source > Settings > General
In order to configure scheduler settings, open the edit mode by clicking the "
Edit
" button. Within this mode, users will find and be able to adjust the desired scheduler settings as shown below.
Scan Frequency Options
First of all, the user has to set the Scan Frequency. Click on the Scan Frequency field dropdown and it will show the following options:
One Time:
This will create and execute just one job on a selected date and time.
Daily:
This will result in the creation and execution of jobs on a daily basis at a chosen time.
Weekly:
This will create and execute jobs on a weekly basis at a chosen day, date, and time.
Monthly:
This will create and execute jobs on a monthly basis at a chosen date, and time.
Now from the "
Starts On
" field, select the Date and Time at which job(s) will be executed.
Daily Scans:
Once the Daily scan frequency is set and the user selects the date and time of the frequency of the scan, the user can also set the day frequency at which job(s) will be scheduled and executed from the days dropdown as shown below.
By default, it is set as 1, which means jobs will be executed daily at the selected time. If the user selects it to 2 then jobs will be executed after every 2 days. If I start the scheduling on Monday and set the day frequency to 2, then jobs will be executed on Monday, Wednesday, Friday, and so on every second day.
In addition to this, users have the option to repeat the task multiple times within a single day. To enable this feature, users need to click on the checkbox labeled "Repeat tasks every." This action triggers additional text fields to appear, prompting the user to input the frequency in minutes and the total duration in hours for the task repetition. For instance, if a user enters 20 minutes for 1 hour and schedules it for 9:00 PM, a new task will be executed every 20 minutes. This means the initial task will start at 9:00 PM, followed by subsequent tasks at 9:20 PM, 9:40 PM, and concluding at 10:00 PM for the day.
Weekly Scans:
Once the Weekly scan frequency is set and the user selects the date and time of the frequency of the scan, the user can also set the days on which that scan will be repeated. Days checkboxes will appear in the "
Repeat on
" field and whichever days the user selects, a new scan will be executed on those days at the desired time.
Monthly Scans:
Once the Monthly scan frequency is set and the user selects the date and time of the frequency of the scan, the user can also choose to select the repetition of the scan. Once the Monthly scan option is selected, the "
Repeat by
" field will populate which will have two options "
Day of the month
" and "
Day of the week
". If "
Day of the month
" is selected, then a new job will be scheduled and executed on the selected date and time of every month.
End Conditions:
Now closing out the scheduling settings, select the end date/time of the scheduler from the "
Ends
" field. It has three options in the form of radio buttons which have the following purpose:
Never:
This means that the jobs will always be running on the desired date and time which is set and it will never stop.
After:
This means that after certain occurrences, jobs will stop getting auto-created and executed. The user has to enter a certain number of occurrences.
On:
This means that on a particular date that the user has selected, jobs will stop getting auto-created and executed.
Scan Summary:
Once all the settings are done, the Scan Summary will appear at the bottom as shown below. Save the General settings to start the execution of the scheduler.
Video Tutorial:
Watch this quick video tutorial of how to s
chedule a Job for Data Sources:

--------------------------------------------------------------------------------

================================================================================
TITLE: How to find help using our in-product Help Center in Dvsum App
URL: https://dvsum.zendesk.com/hc/en-us/articles/11525283492500-How-to-find-help-using-our-in-product-Help-Center-in-Dvsum-App
================================================================================

In this Article:
Overview
How to Interact with Message Support
How to Give Feedback to Different Guides
Overview:
Learning about DvSum has always been easy. But we have made onboarding on the Dvsum Data Intelligence app a lot easier and simpler. Users can find help quickly using our in-product help center.
Clicking on "Help" within Dvsum App (at the top-right-hand corner) will give you access to interactive guides. There are some detailed guides from adding a new source and scanning it, to searching tables, columns, models & reports, and more. Guides are in the form of Quick Tips, Tutorials, Videos, and Support Documentation (like the article that you're reading right now). All these bits of help can be reached directly within the application by searching a keyword inside the Help center as shown above.
How to interact with Message Support?
In order to interact with Dvsum Message Support, open the application and click on the HELP menu in the upper right-hand corner of the page. Then select "Message support" from the Additional Resources section. Use the Message support widget that pops up to tell us how we can help. Say "HI", submit your name & email, and our intake tool will attempt to find a self-service answer from our Help Center to assist you.
How to give feedback to different guides?
Dvsum support values your user experience and feedback. In order to give feedback on any of the Quick tips or videos, click on it. A modal will open which will guide you. If the guidance was helpful then select the "Yes, thanks" button and submit it. Otherwise, if you have any suggestions or concerns regarding the guidance, click on the "Not really" button, type your feedback in the comment box and submit it.
In case of giving feedback on a tutorial, select the tutorial and follow all the step-by-step guides. At the end when the tutorial finishes, a feedback section will appear from there you can follow the same steps as shown above to give feedback.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Import
URL: https://dvsum.zendesk.com/hc/en-us/articles/11197801100820-Data-Import
================================================================================

In this Article:
Overview
Steps to Data Import
Step 1: Starting the Import
Step 2: Preparing the Import File
Note on Subdomain Handling
Additional Notes for Sample File
Note on Status
Supported Statuses
Important Notes
Image: Existing Import Status Table Example
Example: Summary Report for Invalid Status
Example: Email Notification of Import Failure
Importing/Updating Values for Custom Attributes
Step 2.1: Download and Reuse Data for Import
Step 3: Field Mapping
Step 4: Import Completion
Step 5: Import Summary Email
Step 6: Post-Import Actions (Domain-Specific)
Overview
The Data Import option is available to users with Admin role and to users with Edit access. While the Administration menu option is visible only to Admins, Editors. Users without Editor privileges cannot import.
Whether you're working with system asset types (e.g., Glossary Terms, Data Dictionary, Analytics Dictionary, Fields & Lineage) or custom asset types, the Data Import functionality enables efficient population of the platform using structured Excel files.
Admins and Editors can import up to 50,000 records at a time using
.xls
,
.xlsx
, or
.csv
file formats. This capability significantly reduces manual effort and accelerates the onboarding and management of metadata assets.
Letâ€™s explore how the Data Import process works in detail.
Steps to Data import
Important:
Ensure that the Domain and Sub-domain are created in the application before starting the import process. The import will use these as a reference when uploading data.
Step 1: Starting the Import
You can start an import in two ways:
1. From the Administration menu â€“ Go to Administration â†’ Data Import, select the Asset Type from the dropdown, and click Next.
2. Directly from an asset view â€“ Open the listing page for the asset you want to import (for example, Glossary Terms) and click the Import button at the top right. This method works for any supported asset type.
Step 2: Preparing the Import File
To begin, download the sample file and fill in the data as per the instructions.
Important:
Field Dictionary Imports
:
The Entity field must now include both the
Data Domain
and
Subdomain
in the format:
DD.SD.EntityName
For example:
Default.Default.US_PASSPORT
.
This structure ensures each entity is uniquely identified during the import.
More information on above, refer to
this article
.
Note on Subdomain Handling
The Field Dictionary import template does not include a separate Subdomain column. Subdomain is not a required field in the Field Dictionary and is automatically inherited. A separate Subdomain column is available only in the Business Glossary import, where it is mandatory.
Additional Notes for Sample File:
Tags
:
The
Tags
column in the sample file allows multiple tags to be added using the format
{Tagset Name}={Tag}
. Multiple tags can be separated by a comma. For example:
Data Classification=Public, Custom Tags=PKR
.
Data Steward and Data Owner
:
These fields in the sample file must contain valid email addresses of users already added in the DvSum tool.
For Glossary Terms file imports only
The
Term Type
column must contain one of the
eight predefined term types
.
The applicability of
Business Rules
â€”such as Category Data Rule, Pattern Rule, Regex Pattern Rule, Value Range Rule, and Manualâ€”depends on the selected
Term Type
.
If the Term Type is Numerical
:
Only
Value Range Rule
and
Manual
are applicable.
For
Value Range Rule
, use the format:
{Min Value} - {Max Value}
Example
:
1 - 20
If the Term Type is Categorical Attribute
:
Use
Category Data Rule
in the format:
{Categories.Value 1}={Categories.Description}, {Categories.Value 2}={Categories.Description}
If the Term Type is Computed Measure
:
The
Term Formula
must be provided in the same file.
Once the file is filled correctly,
upload the sample file
through the Data Import screen.
By default, the
"Update Existing Records"
option is enabled for
Data Dictionary
imports, allowing you to update existing records during the import process.
Note on Status
If you include a
Status
column in your import file, the import process validates the value to ensure it is recognized by the platform.
Status Values Recognized and Processed During Import
If you include a
Status
column in your import file, note the following:
The following Status values are
recognized and processed
during Data Import:
Published
Draft
Modified
When these supported values appear in the import file:
The import
does not fail due to Status validation
The Status value is
accepted and processed
according to the domainâ€™s configuration (workflow or non-workflow)
If your file includes
unsupported or invalid status values
, such as:
Deleted
New
Suggested
Default
Any custom status
Then:
The import will
fail for those records
The import view will
not accept those rows
The summary report will display the remark:
Status has Invalid data
This validation helps ensure only valid and supported Status values are processed during import.
Important Notes
You
do not need to include
a Status column in the import template. The system will assign the correct status automatically.
If included, the Status column should only contain values recognized by the platform. Any other value will cause the row to fail validation.
Image: Existing Import Status Table Example
Caption: Import view showing records with different status outcomes.
Example: Summary Report for Invalid Status
Caption: Summary report showing failed record due to unsupported Status value ("Deleted") with remark "Status has Invalid data".
Example: Email Notification of Import Failure
Caption: Email notification from DvSum indicating that the import failed due to invalid data.
Importing/Updating values for Custom Attributes
The Data Import process also supports importing values for custom attributes. Any custom attributes applied to an asset type should be added as a column in the downloadable import templates.
Move the required custom attribute column from
Available
to
Assigned
so that it appears in the view, allowing users to download the template and populate values for those attributes.
To import custom attribute values:
Download the import template for the asset type.
Locate the custom attribute columns (these are added dynamically based on the attributes applied to that asset).
Enter the values in the respective custom attribute columns.
Upload the completed template through the Data Import tool.
Example:
If a custom attribute called â€œCredit Card Numberâ€ is applied to your assets, the template will include a â€œCredit Card Numberâ€ column. Filling in this column with values that match the defined format (e.g., 16-digit numbers like
1234-5678-9012-3456
) will assign the values to the correct assets once the import is processed.
After entering the values, follow the same steps to import.
Note:
Values that do not match the required format (defined when creating the custom attribute) will not be accepted.
For more details on creating and managing custom attributes, refer
Creating and Managing Custom Attributes.
Excel (.xlsx) File Requirements
When importing data using an Excel (
.xlsx
) file, ensure the file is clean before uploading.
The Excel file must contain
only one worksheet
.
Any formatting applied to rows beyond the actual data records must be removed.
Rows that appear empty but contain formatting are treated as records during import.
Note:
Failure to meet these requirements may result in additional or incorrect records being imported.
Step 2.1: Download and Reuse Data for Import
Users can also download data from the listing page and import the downloaded file using Data import:
Step 3: Field Mapping
Fields from the uploaded file will get mapped with Field in dvsum. Mapped Tab will show all fields which are mapped. Once All Required Fields are mapped. Users can Click on
Import
button and start import
If a field is unmapped, the user can map it individually by selecting the appropriate option from the dropdown available in the Fields section under the DvSum column. Similarly, to remove a single field, the user can click the remove icon provided for that field.
For bulk actions, the user can utilize the
Reset Field Mapping
and
Apply Auto Mapping
buttons.
Step 4: Import Completion
A success screen will be shown to the user when the file is imported successfully.
I'm Done
Button will redirect the user to the Data import step1.
Go to Assets Dictionary
button will redirect the user to the Assets dictionary listing page with respective Asset Type.
Step 5: Import Summary Email
After 3-5 minutes the user will receive an email with a complete summary of imported Data. Users can also download the summary of uploaded data using the Download button.
Step 6: Post-Import Actions (Domain-Specific)
After completing the import,
records are not automatically published
.
Please follow these additional steps to publish your records. The post-import steps vary depending on whether your domain has a workflow enabled or not.
Post-Import Actions â€“ Workflow-Enabled Domains
Post-Import Actions â€“ Non-Workflow Domains
Please refer to the relevant guide to complete your import process appropriately.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to do Target Scan of Data Sources
URL: https://dvsum.zendesk.com/hc/en-us/articles/10860307282580-How-to-do-Target-Scan-of-Data-Sources
================================================================================

How do you limit your scan to specific schemas?
Initially, you must add and configure a data source in Dvsum. Once successfully configured and authenticated, a Database field will be visible, containing a list of all available databases. From this list, you'll then proceed to select the specific database intended for the scanning process.
After selecting the database, the next step involves targeting specific schemas for scanning. To achieve this, you can focus your scan on particular schemas to retrieve metadata from the selected ones.
If the goal is to scan all available schemas within the chosen database:
Navigate to the connection settings tab of a source.
Ensure that the checkbox for "Limit to specific schemas" remains unchecked.
Save the settings.
Initiate the scan for the source.
The scan will comprehensively cover all schemas in the selected database.
If the goal is to scan specific schemas:
Check the "Limit to specific schemas" checkbox.
Select the particular schemas intended for scanning.
It is possible to choose either a single schema or multiple schemas for the scanning process.
DvSum supports different Data Sources including RDBMS sources such as
Oracle
,
Snowflake
,
My SQL
,
SQL Server
,
Azure SQL
, &
Azure Synapse.
Additionally, it includes different Reports-related sources like
Tableau
&
Power BI
. Comprehensive articles are available for each of these sources, providing step-by-step guides on how to add and configure them within the application. To access the detailed instructions for a specific source, simply click on the corresponding link mentioned above, and it will direct you to the relevant article.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to create Governance Views
URL: https://dvsum.zendesk.com/hc/en-us/articles/10027321551380-How-to-create-Governance-Views
================================================================================

In this Article:
Overview
Create View
Clone View
Share View
Saved View
Delete View
Export View
Video Tutorial
Overview:
DvSum enables users to create views on the Data Dictionary, Field Dictionary, Analytics Dictionary, and Business Glossary. Let's begin exploring this functionality.
Create View:
Navigate to the Dictionaries > Data Dictionary option in the left menu. This will take the user to the Data Dictionary listing page, where the Default Table view is initially shown. Choose the option to create a view of your preference.
On the "Create View" page:
Provide view name
Add/remove column for the view
Specify criteria
Note:
The user will not be able to Edit/ Share and Save the default view. These buttons will remain disabled.
The columns available in the current view will be automatically pre-selected.
Multi-Sort
You can now sort Governance Views using multiple columns. For example, first by
Table Name
and then by
Assigned To
.
The
Default Sort Order
can be configured directly in the UI, as shown below:
To apply multi-sort, hold the
Ctrl
key (or
Cmd
on Mac) and click on the columns you want to sort. Users will then see the rows ordered based on the hierarchy of columns selected, first by the primary column, then by the secondary, and so on.
Example:
Ctrl+Click on the Business Name column header now includes it in the sorting:
Clone View:
To create a clone of any view, users can click on the Edit pencil icon next to the view name. This action will reveal the clone button, allowing users to duplicate the selected view.
Share View:
The view can be shared with other users by granting "Editor" or "Viewer" access.
Users with "Editor" access have the capability to Edit, Clone, Delete, Share, Save, and Export operations with a view.
Users with "Viewer" access can perform Share, Save, and Export operations with a view.
Saved View:
The saved view will appear in the "Favourites" section of the views drop-down.
Delete View:
To delete a particular view, click on the Edit button, and you will find a delete button available.
Export View:
Simply click on the download button, and users can download the records available in the grid as an Excel file.
Note
:
For details on refining views using filters, refer to the article:
How to Apply Filters in Governance Views.
Video Tutorial:
Watch this quick video tutorial of how to create Governance Views:

--------------------------------------------------------------------------------

================================================================================
TITLE: How to use Certification and Quality Tagset?
URL: https://dvsum.zendesk.com/hc/en-us/articles/8946385941140-How-to-use-Certification-and-Quality-Tagset
================================================================================

In this article:
Overview
Require Note When Applying Tag
Make Tags Prominent
Certification Tagset
Dataset Quality Tagset
Overview
Certification and Dataset quality tags have been introduced for Datasets. By default, users are unable to alter these tags, and the checkboxes for 'Require Note When Applying Tag' and 'Make Tags Prominent' come pre-selected. The Certification tag is indicated by the green color, and the Data Quality Tag is represented by the red color. Tags for both tagsets have been pre-configured.
Require Note When Applying Tag:
Choosing this option means users must include a Tag note when adding a tag, and adding a tag without a note is not allowed.
Make Tags Prominent:
When this option is chosen, tags will appear at the top of the page alongside Dataset/Term/Field names, highlighted in the specified color (Red, Green, Blue). The default color for the new tagset is Blue, but users can customize the tag color based on their preferences.
Certification Tagset:
The certification tagset has only one tag:
Certified
On the Dataset detail page, Certify Button shows when the button is clicked, the user will be directed to the Classification section, and Certified Tag will also get selected automatically with a single click.
NOTE
:
Users can also select a Classified tag from the classification section.
Enabling "Require Note" prompts a text box for the Certified Tag note when clicking Apply. After entering the note, publishing the Dataset results in the certified tag displaying alongside the Dataset name on the listing reference page.
Dataset Quality Tagset:
Dataset Quality Tagset has the following tags:
Deprecated
Under Maintenance
Warning
Edit the classification section and any tag from available tags in Data Quality tags. As Require Note is enabled, click on Apply button a text box will appear to enter the note for Under Maintenance Tag.
Save the changes and publish the Dataset after adding the note. The Under Maintenance tag will be shown with the Dataset name and listing ref page
Watch this quick video tutorial on how to use Certification and Quality Tagset:

--------------------------------------------------------------------------------

================================================================================
TITLE: Catalog and Datasets/Reports Approval
URL: https://dvsum.zendesk.com/hc/en-us/articles/19390168472852-Catalog-and-Datasets-Reports-Approval
================================================================================

Introduction
Catalog Tables and Datasets/Reports can be scanned and edited using an approval workflow in the DvSum Data Catalog. Approval workflows are configured for each data sub-domain. When an approval workflow is configured, then Data Stewards are able to edit the tables and datasets/reports and submit the changes for approval. Approvers are then able to approve or reject the updates. The Approval flow already exists in the case of Business Glossary Terms. You can click
here
if you would like more information about the Glossary Approval flow.
Prerequisites
1. An administrator must enable data governance.
Administration â†’ Account â†’ Module Settings â†’ Governance
2. Data domains should already be defined.
The approval workflows will be defined for a data domain.
Let's check the step-by-step guide on how this functionality works. So, first of all, we will be adding the data domain and then attaching the data domain to a table or catalog/report.
Adding a Domain:
Go to the Administration tab, and select " Organizations", and then choose "Data Domains" from the dropdown. On the domain listing page, click the
Add
button.
On Add Domain form provide the Domain name, Description, and Data Steward. And then click on the "
Enable Workflow
" checkbox.
This will enable the workflow for this particular domain. Choose the approvers: There can be single or multiple approvers. Further, choose the workflow type, either "
Anyone can approve
" or "
All need to approve
".
Click on the Save button, and the domain will be saved. In the listing, it can be seen that the newly added domain will appear on the top with an icon that represents this domain has workflow enabled as shown below.
The steward who is selected while creating the Domain will be the one who will be editing the table/dataset/reports and making the changes. Then, changes will be sent for approval to the approver(s). The approver's job will be to review the changes made by the Steward and approve or reject them.
Workflow for Tables
Workflow can be enabled for Tables by attaching a Domain for which workflow was enabled. Users can add data domain by going on to the detail page of the table from data dictionary tab:
The Data domain can be added to the table by clicking on "Edit," which opens the Edit mode of the table. The Data domain for which workflow is enabled should be added in order for the workflow process to be started:
Note: The Domain for which workflow is enabled will have the workflow symbol in front of it
Once the Domain is added, the changes will be needed to publish so that the domain is now attached to the table:
Once the changes are published for the workflow to be initiated user is required to edit the table and make any changes in the table once the change is made then in order to publish the change, the table would be required to be published by the approver.
Once the changes are made, the user can click on the "Done" button. After clicking "Done," there will be an option of "Submit for Approval". Once the user clicks "Submit for Approval," then the workflow process can be initiated, and the table will be submitted for approval.
The Submitter has the option of Cancelling the submission or otherwise waiting from the approver's end.
The approver will receive an email once the table is submitted for approval:
On the Approver's screen, the approver can "Review changes" that have been made by the submitter.
All the fields will appear, and the edited ones will show up as highlighted.
If the approver wants to reject the changes, then the "
Reject
" button can be clicked. It will ask the user to add a reason for rejection as a comment. The changes will get rejected.
To approve the changes, the user needs to click on the "
Approve
" button, and the changes that the Steward made will be approved, and the term will get Published.
Submitting/Approving multiple Tables using Mass Update:
If there are more tables to be submitted for approval, then the user can submit them all for approval at once by using mass update:
Note: In order to initiate the workflow from listing first tables should have the workflow domain and must be edited (change the data domain, add entity from mass update, or add tags).
Once the Tables are submitted for approval, the approver can approve multiple tables as well at once from the Mass update. Go to the Data Dictionary listing again, and there will be a tab "Assigned to me," which will show all the tables that are assigned to the logged-in approver.
From the "Assigned to me" tab, select the tables and click on mass update and from the mass update modal, click on the status field and select "
Approve
" and apply. This will approve all the selected tables. That's how the approver can approve multiple tables at once.
Workflow for DQ Rules
Workflow for DQ Rules is automatically applied to any Rule associated with a Table that belongs to a Data Domain where workflow functionality is enabled. This ensures that any modifications to DQ Rules follow a controlled approval process, maintaining data governance and change auditability across governed domains.
For Tables or DQ Rules that belong to a workflow-enabled Data Domain, a workflow symbol appears on both the Database Table and DQ Rule tabs..
The newly created rule can be executed in either
Online
or
Offline
mode where both the
Data Steward
and
Data Owner
can initiate the rule execution.
After initial creation, the Steward can modify the rule definition by selecting
Edit option
. Changes can be made on theÂ  Rule Definition (Overview), Thresholds, and Workflow. However, any changes made to a workflow-enabled rule require
approver authorization
before being published.
For Example:
When changes are made to the Rule Definition, the
Submit for Approval
option automatically appears.
Once all modifications are complete, select Submit for Approval to initiate the workflow process.
The rule then enters the approval stage.
On the DQ Rule > Overview page, the submitter can view the current workflow status.
At this stage, the submitter can either Cancel the Submission or wait for the approverâ€™s response.
Approval Process
Once a rule is submitted for approval, the approver receives an email notification indicating that the DQ Rule is awaiting review.
The approver can click Review Changes to view all modifications made by the submitter. All updated fields are highlighted for easier comparison.
If the approver decides to reject the changes, the Reject option can be selected. A reason for rejection must be entered as a comment. Once rejected, the submission is discarded, and the rule remains in its previous state.
To approve the changes, click Approve. The updates made by the Data Steward are then accepted, and the DQ Rule is published.
Workflow for Dataset/Reports
Datasets and Reports can be found on the BI Models and reports tab, and just like the tables for the workflow to be started, a data domain for which workflow is enabled should be added for the dataset or report:
Once the workflow domain is added, the user will be required first to publish the changes and then again edit the dataset/report to initiate the workflow:
Once the Dataset/Report is submitted for approval, user can see the workflow on the top right, and email will be received by the approver about the dataset/report that is sent for approval:
The approver can "Compare changes" for the dataset/report and afterward approve or reject the dataset or report. Once the approver approves the dataset/report, the dataset/report will be published
Just like the Tables, if there are more than one dataset/reports then they can be submitted for approval through the mass update, and the approver can accept the dataset/report through the mass update:
Submitter's account
Approver's account
The overall functionality of the workflow is the same for Tables, Datasets, and reports, so for that reason, the dataset/report part is not covered like the Tables part.
How to check the Activities history of the Tables/Reports/Datasets?
On the Table/Report/Dataset detail page "
Managed by
" section, there will be a chat icon. Click on that, and it will show all the history of activities that have been performed on the table/report/dataset. Comments can also be added to the table/report/dataset activity.

--------------------------------------------------------------------------------

================================================================================
TITLE: Approving Glossary Terms
URL: https://dvsum.zendesk.com/hc/en-us/articles/15536676968724-Approving-Glossary-Terms
================================================================================

In this Article
Overview
Prerequisites
Adding a New Term
Approving multiple Terms using Mass Update
Workflow for Assets
Approver Review
Approving multiple Assets using Mass Update
Video Tutorial
Overview
Glossary Terms can be created and edited in DvSum Data Catalog using an approval workflow. Approval workflows are configured for each data sub-domain. When an approval workflow is configured, then Data Stewards are able to propose new terms and suggest edits to existing terms. Approvers are then able to approve or reject the updates.
Prerequisites
1. An administrator must enable data governance.
Administration â†’ Account â†’ Module Settings â†’ Governance
2. Data domains and sub-domains should already be defined. The approval workflows will be defined for a data sub-domain.
Let's check the step-by-step guide on how this functionality works. So, first of all, we will be adding the data domain, sub-domain, and then finally the term.
For detailed information on adding data domain and sub-domain refer article
"Data Domains & Data Sub-Domains"
Adding a New Term
From the left navigation bar select
Dictionaries >Glossary
Terms
which will navigate to the terms listing page. From the listing click on the "Add Asset" button.
From the Add Term modal, select the same sub-domain that we created above. Enter the Name, Title, and Description, and select the Term Type. Click on the Save button and the term will be saved redirecting the user to the term detail page.
On the term detail page, it can be seen that this newly created term is in the draft status. The term will only get published once it is submitted for approval and gets approved.
Note:
The terms in the Draft state will not appear in enterprise search results. Only Published terms will appear in enterprise search.
At the top of the Term detail page, the workflow stages will appear. Currently, it will be in the first stage as Steward would need to make some changes.
Now below is the Steward's view which they can edit in this term. The below image shows that the Steward has changed the definition.
To edit a term, click on the "Edit" button. Click on the pencil icon for whichever section needs to be edited. Once changes are made to the particular field, click on the circular check mark button on the same section and then the Done button.
Once the Steward edits the term, then the "
Submit for Approval
" button needs to be clicked which will send the term for approval to the approver.
Once the Steward submits the term for approval, the workflow will move to the second stage which will show the pending approval status along with the approver's name as shown below
Now next step is for Approver who needs to review the changes made by the Steward. The below image shows the approver's screen.
Click on the "
Compare Changes
" button to review the changes that have been made. All the fields will appear and the edited ones will show up as highlighted.
If the approver wants to reject the changes, then the "
Reject
" button can be clicked. It will ask the user to add a reason for rejection as a comment. The changes will get rejected.
To approve the changes, the user needs to click on the "
Approve
" button, and the changes that were made by the Steward will be approved and the term will get Published.
Approving multiple Terms using Mass Update
Other than that, Approver can approve multiple terms as well at once from the Mass update. Go to the Business glossary listing again and there will be a tab "Assigned to me" which will show all the terms that are assigned to the logged-in approver.
From the "Assigned to me" tab select some term and click on mass update.
From the Mass Update modal click on the status field and select "
Approve
" and apply. This will approve all the selected terms. That's how the approver can approve multiple terms at once.
How to check the Activities history of the term?
On the term detail page "
Managed by
" section, there will be a chat icon. Click on that and it will show all the history of activities that have been performed on the term. Comments can also be added to the term activity.
Workflow for Assets
For any asset associated with a workflow-enabled data domain, if the Data Steward makes changes to the asset, the
Submit for Approval
button must be clicked. This action sends the asset to the designated approver for validation and approve.
After the Steward submits the asset for approval, the workflow status is displayed at the top of the asset detail page, indicating
Pending Approval
along with the approverâ€™s name, as shown below..
Approver Review
The next step is for the Approver to review the changes made by the Steward. The image below shows the Approverâ€™s view.
Click the
Compare Changes
button to review the modifications. All fields will be displayed, with any edited fields highlighted for easy identification.
To reject the changes, the Approver can click the
Reject
button. A prompt will appear to provide a reason for the rejection as a comment, and the changes will then be declined.
To approve the changes, click the
Approve
button. The modifications made by the Steward will be approved, and the asset will be published.
Approving multiple Assets using Mass Update
The Approver can also approve multiple assets at once using the
Mass Update
feature. On the asset listing page, the
Assigned to Me
tab displays all assets assigned to the logged-in Approver.
From the
Assigned to Me
tab, select the desired assets and click
Mass Update
.
In the
Mass Update
modal, click the
Status
field, select
Approve
, and then click
Apply
. This action will approve all the selected assets, allowing the Approver to process multiple assets at once.
Video Tutorial
Watch this
Tutorial
to understand the Glossary Security Flow in which the following points are discussed:
1. Enable Workflow for Sub-Domain
2. Add New Term and Submit for Approval
3. Approve the Term
4. Edit the Term and submit for Approval again
5. Compare changes and Approve the term
6. Approve multiple terms

--------------------------------------------------------------------------------

================================================================================
TITLE: Execution Modes, Model & Workflow Configurations in Tool
URL: https://dvsum.zendesk.com/hc/en-us/articles/41054809831572-Execution-Modes-Model-Workflow-Configurations-in-Tool
================================================================================

In This Article:
1. Instruction-Based Logic
2. Flow-Based Logic
When configuring a Tool, you can choose between two execution modes:
Instruction-Based Logic
and
Flow-Based Logic
. Each mode defines how the assistant interprets data, applies logic, and integrates workflows.
1. Instruction-Based Logic
Overview
Instruction-Based Logic allows you to configure the assistant using natural language instructions along with dataset descriptions. The assistant interprets these instructions in real time to generate recommendations. This mode is ideal when flexibility and human-readable explanations are needed.
Configuration Fields
Execution Mode
Select
Instruction-Based Logic
to enable this mode.
AI Assistant Type
A text field to define the role or job title of the assistant.
Data Description
A text field to provide a detailed explanation of the dataset.
Logic
A plain-text field to describe the rules and conditions the assistant should apply.
Guidelines
A text area for specifying rules or constraints that the assistant should follow when generating outputs.
Model & Workflow Configuration
Model Version
â€“ Dropdown to select the model version for execution.
Workflow JSON
â€“ Editable JSON field that must include:
"source_configs"
"state_machine"
The JSON editor supports copy, download, and full-screen expand options for easier editing.
2. Flow-Based Logic
Overview
Flow-Based Logic requires a structured JSON flow to define all conditions, steps, and outcomes explicitly. This ensures deterministic execution where outputs are consistent and predictable.
Configuration Fields
Execution Mode
Select
Flow-Based Logic
to enable this mode.
AI Assistant Type
A text field to define the role or job title of the assistant.
Logic JSON
A structured JSON editor field to define the Flow-Based Logic (steps, conditions, outcomes).
This JSON determines the issue category and drives troubleshooting analysis.
UI Response Format
A text field specifying how the assistantâ€™s output should be presented in the UI.
Model & Workflow Configuration
Workflow JSON
â€“ Editable JSON field that must include:
"source_configs"
"state_machine"
The JSON editor supports expand-to-full-screen for easier editing.
Key Notes
Instruction-Based Logic
includes a
Model Version
dropdown, while
Flow-Based Logic
does not.
The
Workflow JSON
field is common to both modes and must always contain:
"source_configs"
"state_machine"
The Workflow editor supports expand/full-screen mode, copy, and download for ease of use.

--------------------------------------------------------------------------------

================================================================================
TITLE: Co-Pilot in Data Intelligence
URL: https://dvsum.zendesk.com/hc/en-us/articles/25996549175828-Co-Pilot-in-Data-Intelligence
================================================================================

In This Article:
DvSum configuration
1. Add Source
2. Configure New Chatbot
3. Analyzing Customer Data from Train Bot
How Chat History Works with Copilot
This article describes the steps needed to configure Rest API as a source in DvSum Data Intelligence (DI) to use CADDI Chatbots.
DvSum configuration:
1. Add Source
To create a data source, navigate to Administration â†’ Data Sources â†’ âŠ• Add Source.
Select Rest API.
Give the source a name, and save it.
2. Configure New Chatbot
Click on "New Chatbot" which will redirect the user to the new chatbot detail page.
From the Definition tab click on the "Download sample file" button to download the sample file. Configure the workflow JSON according to the requirements and upload the file.
There will be a default prompt already being set but the user can also update the prompt of the bot accordingly. The prompt directly impacts the quality of the results.
Once the workflow JSON file is uploaded and the prompt is updated, save the chatbot.
3. Analyzing Customer data from the Train bot
Navigate to the
Train bot
tab and type a question containing the customer ID.
When the question is executed, Co-Pilot displays a
progress indicator
showing the following four steps
Understanding your question
â€“ Co-Pilot interprets the query, identifies parameters, and validates inputs.
Selecting relevant tool
â€“ The most appropriate analysis tool is automatically chosen based on the question type and detected parameters.
Fetching data
â€“ Relevant records or data sets are retrieved for processing.
Analyzing data
â€“ The data is examined to extract key insights and generate a summarized result.
Once all four steps are completed,
insights are displayed automatically
, giving you a clear summary of findings and any key observations derived from the data.
The results display key facts, observations, and recommendations derived from the data. The prompt set from the Definition tab influences these results.
Insights from the data would show up in different grids that can be analyzed.
How Chat History Works with Copilot:
Users can ask a variety of questions related to telecom troubleshooting beyond analyzing customer IDs. Copilot utilizes a Q&A chain specifically designed for telecom troubleshooting queries. Users can also ask follow-up questions to receive recommendations or additional troubleshooting assistance for specific customer IDs.
Mult-Language Support in Copilot:
Copilot supports multiple languages. This functionality enables users to ask questions and receive answers and explanations in their preferred language, enhancing the platform's accessibility and usability for a diverse global user base.
The questions shown below were asked in Spanish, and the results and analysis also appear in Spanish.

--------------------------------------------------------------------------------

================================================================================
TITLE: CADDI Chat (Talk to your Data)
URL: https://dvsum.zendesk.com/hc/en-us/articles/18534685180948-CADDI-Chat-Talk-to-your-Data
================================================================================

In this Article:
Introduction
Viewing Execution Progress
Sample Questions
Detailed Questions
Profiling & Filtering
Saving the Question as Context
Generating Answers Based on Different Tools
Basic CRUD Operations
Left Navigation Bar
Introduction
Artificial Intelligence (AI) plays a crucial role in data analysis by providing a simple interface to extract insights from large and complex data sets. With its ability to process natural language quickly, AI helps people make better-informed decisions, identify patterns and trends, and uncover hidden insights that might otherwise go unnoticed. In DvSum users can talk to their data using
C
onversational
A
I for
D
ata-
D
riven
I
nsights (CADDI). DvSum CADDI is a user-friendly platform where users can ask different questions from their datasets and extract useful insights. The following article explains how CAADI chat can be used.
This article is linked to Agents. For more information on Agents, click
Configuring CADDI - Creating Agents
.
How to talk to the data in DvSum Data Catalog:
Before we start chatting with data, first, we need to create a agent and share it with some users.Â  Here I have a agent shown below, which I have shared with the user. On the agent go to "Manage Access" and share the agent with any user.
Once the agent is shared,
click on "Talk to Your Data" from the left Nav. It will open the Conversational page on a new tab.
Now in the Conversational tab, all the agents that are shared with some users will appear as agents. So we need to select some agent to start the conversation. I'm selecting the agent which I shared above.
Once the agent is selected, an About section of that chat will appear on the new chat, which will contain details of the agent i.e. the tables that were added to the agent .
Viewing Execution Progress
When a question is executed in CADDI Chat, a
progress indicator
appears showing the following four steps of processing:
Understanding the question
â€“ The query is interpreted, language detected, and validated for clarity.
Identification of relevant tables
â€“ CADDI determines which tables contain the required data.
Crafting SQL query
â€“ A SQL query is generated to ensure accurate and relevant data retrieval.
Explanation of how the query works
â€“ The system explains the query logic, including filters, joins, and calculations.
After execution, the
final output and query explanation
are displayed, offering transparency into how the results were derived.
On clicking the book icon located on the top right of the chat bar, some sample questions show up. Users can use these questions for analysis:
On the CADDI, two types of questions can be asked which are:
Sample Question
Detailed Question
Sample Questions
Click on the "Sample Data" button, and it will show the sample data of that data set. The Sample question will not have any SQL code generated, and there will be no visualized charts for it. The Sample Data will have the Pivot View.
From the sample data, users can have some basic insights into what data is about and what it looks like. Users can select different columns and apply filters to check the results.
Users can see the profiling of the different columns in sample questions by simply clicking on the column heading in the grid:
Users can apply some filters and set up that question as a context also by clicking on the context icon:
Note: For Sample Questions, context can only be set up when some filter is selected
The Concept of Profiling and Context will be explained further in this article.
Detailed Questions
A detailed question is one for which:
SQL Code is generated
Visualized Grid, Charts and Pivot Information is generated
Context can be set up, and Profiling Information is available
SQL Query
When the question is asked about any dataset, then for that question, SQL code is generated, it can be seen when users select the "Show Code" option below the answer output:
Different Generated Views
For the question that is not typed, 3 different views are generated which are:
Chart View (located next to Chart Settings)
Grid View
Pivot View
The Chart View, Chart Settings, and Pivot View are separate detailed topics, and they are not included in the scope of this article. For additional information, you can visit the articles below:
Chart Settings in CADDI
Pivot Tables in CADDI
The Grid View shows the data in tabular form of the columns that are fetched from the SQL query:
Along with the 3 different views there will be a gear icon of Settings which is present when this is clicked settings tab opens up with further three options:
General
Chart (Learn more in
Chart Settings in CADDI
)
Field (Explained in Profiling & Filtering below)
The General tab contains the toggle button of "Human readable format". By default, this option is turned off:
When this option is turned on then it helps in better reading of the data. For example, if there are large values like in thousands or millions then it will show in a much more readable format:
Profiling & Filtering
Profiling shows the distribution of different values of a column which helps in a better understanding of the data. Users can apply filters on the Grid through profiling by selecting particular values. The profiling of any column is opened by clicking the column heading in the grid. When the profiling of any column is opened then the "Settings ---> Field" tab is opened up.
When no column is selected and from "Show settings", the "Field" option is selected then no profiling of the column will be shown. Instead, it will show this:
2.3.1- Date type column
For any date type column user can set different date formats according to the requirement.
Once any date format is selected then in the grid that particular date format will be applied:
Note: When any date format is selected then it applies to every question that was previously asked or will be asked
The Grid can be filtered from the visual distribution section. A handle is provided which helps in applying filters according to different values. Once the values are selected then the grid on the left will be filtered accordingly:
Decimal type column
The Profiling of any column that has the data type can be opened by just clicking on the column heading in the grid. The difference is that there are some additional changes provided in its profiling. Users have the option to set decimal values after the point according to the requirement:
Filters can be applied from the visual distribution section. For this particular example, the handle is not present below the distribution bars because if the distinct values for the columns are less than 20 then there will be no handle provided, users can click on the bars to fill in "Minimum" and "Maximum" values.
Integer type column
The Profiling can be opened by clicking on the column heading. For the columns having data type there are no additional options, there will be just visual distribution present from where "Minimum" and "Maximum" values can be selected:
Here in the above example, the handle is provided below the distribution bars because, for this particular column, the distinct count exceeds 20. The logic that handle should be present or not is the same for columns having data type "Decimal" and "Integer".
String type column
The Profiling can be opened similarly by just clicking on the column heading present in the grid. The visual distribution for the string-type columns is a little different from other data types. Filters can be applied by selecting different bars and on the grid the filters will be applied accordingly:
Saving the Question as Context
The question that is asked that can be added as context or
any specific filter applied on the grid can also be added from the Context icon on the top right of the question:
The added context can be seen on the top left of the chat bar below. Users can add more than one context from different datasets. If the context is no longer needed, then it can be removed from Clear Context.
Next Time when any question related to the dataset is added as context this context will be incorporated into the questions also:
Generating Answers Based on Different Tools
Often times users will require some questions to be answered in a visualized form and there would be scenarios where users would want just typed answers. For this purpose before asking a question user can select the tool that will generate the answer based on the selected tool. The three tools are located at the top right of the chat bar:
There are 3 different types of Tools that can be selected before writing up any question:
SQL Query Tool
Data Understanding Tool
General Knowledge Tool
SQL Query Tool
By default when any CADDI chat is opened the selected tool is the "SQL Query tool":
When the selected tool is "SQL Query Tool" then when the user asks a question from any dataset then for that answer, SQL code and answer in visualized form (Charts and Grid) will be given:
Data Understanding Tool
The Data Understanding Tool can be used where the user wants some information about some columns present in the dataset e.g. range of values or different distributions of column. In this tool SQL Code and Visualized Grid will only be generated if necessary otherwise the answer is usually given in typed form where visualization is not required:
General Knowledge Tool
This tool can be used if the user wants to ask questions that are not related to the datasets present in the Data Catalog application. Any sort of general knowledge questions can be asked by using this tool. One use case of this tool can be for example there are some definitions for some columns for a particular dataset but user wants to know the industrial definition then this tool can be handy. This tool is located next to the Data Understanding tool:
Basic CRUD Operations
On the top left of the Chat, where the name is written, some basic crud operations can be performed:
Update Chat Name & Objective
Refresh Insights
Delete Chat
Collapse All/Expand All Questions
The Chat Name can be updated by clicking on the "Edit" icon. Users can update the chat name as well as the objective for that chat:
The "Refresh Insights" button will refresh all the questions present in the chat:
The "Delete" button will delete the Chat and all the questions that are asked in it:
The "Expand All/Collapse All" will expand and collapse all the questions present on the chat:
On the top left of the chat bar, the agent that is opened will be shown. When a user clicks on it the view is expanded, and all the agents that are shared are shown here. Users can switch from one agent to another:
Left Navigation Bar
When any agent is opened, if "New Chat" is clicked, then it takes to the Default CADDI window. Users can also scroll around the agents that were created:
The "Find Data" tab opens up the Home page of the Application on a new tab, and the "Logout" tab logs out of the Application:
Chat Preferences
Below the "Find Data" tab there will be a "Chat Preferences" tab. A modal opens up when "My Preferences" is clicked.
Users have the flexibility to configure a wide range of settings according to their preferences, allowing them to tailor the generated insights to their specific needs and requirements.
Note:
For more details, refer
"How to talk to your data".

--------------------------------------------------------------------------------

================================================================================
TITLE: How CADDI Agents Interpret Questions and Follow-Up Requests
URL: https://dvsum.zendesk.com/hc/en-us/articles/45239604841108-How-CADDI-Agents-Interpret-Questions-and-Follow-Up-Requests
================================================================================

Table of Contents
Overview
Types of Questions Supported
Intent Logic and Processing Chains
Analysis Chain Flow
QA Chain Flow
Action Chain Flow
Data Elements Extracted from Questions
Action Execution and Agentic Tools
Supported Actions
Overview
CADDI agents are designed to understand different types of user questions and respond appropriately based on intent. Depending on how a question is phrased, the agent may analyze customer data, answer a general question, or perform a system action.
This article explains the types of questions supported, how intent is evaluated, and when each processing chain is used.
Types of Questions Supported
CADDI agents support the following types of user questions.
Customer-specific questions that include a unique identifier (e.g. customer ID).
General or follow-up questions related to a domain or previous conversation.
Requests to perform actions such as resetting a modem or creating a Jira ticket.
The agent determines how to handle each question by evaluating intent flags.
Intent Logic and Processing Chains
Intent logic is defined within the agent configuration and controls how incoming questions are routed. Based on the detected intent, the agent selects one of three processing chains.
Analysis Chain Flow
The analysis chain is used for customer-specific data analysis.
When this chain is used
This chain is selected when the question contains a unique identifier. which are values. The question may also consist only of the unique identifier.
Rules
If no unique identifier is detected, this chain must not be used. This chain focuses on analyzing customer data across connected systems and does not perform actions.
Example inputs
Analyze
<unique identifier>
What issue is being faced by
<unique identifier>
QA Chain Flow
The QA chain is used for general questions and follow-up queries.
When this chain is used
This chain is selected when no unique identifier is present, no action is requested, and the question is informational or refers to previous conversation context.
Example inputs
What does HFC mean in the telecom context?
Can you identify the issue affecting customer
<unique identifier>
and suggest appropriate actions?
Action Chain Flow
The action chain is used when a user explicitly asks the agent to perform an action.
When this chain is used
This chain is selected when the user requests execution of a task that involves a real-world operation or a system update. If the request is a follow-up to a previous interaction, the agent internally rephrases the question to ensure the action is executed correctly.
When an action is triggered, the agent executes one action at a time and verifies the outcome before proceeding. If additional steps are required, the agent confirms the result and asks the user how they would like to continue.
Example actions
Create a Jira ticket.
Reset modem.
Check modem status.
Update CRM database.
Data Elements Extracted from Questions
To process requests correctly, the agent extracts specific data elements from user questions.
Required data elements
A unique identifier represents an identifier for a telecom customer.
The agent can extract a minimum of one data element and multiple data elements as needed, based on the userâ€™s question.
Example
For questions that require specific analysis, a unique identifier must be included in the request. This identifier allows the agent to identify the correct entity and retrieve the relevant data. Without it, the agent responds at a general level.
Action Execution and Agentic Tools
Actions performed by the agent are enabled through
Agentic Tools
.
These tools define what actions the agent can perform and how those actions interact with external systems such as APIs and databases.
Agentic Tools are built on predefined connections and commands. Connections define how the agent securely communicates with external systems, while commands define the specific actions that can be executed, such as rebooting a modem, checking modem status, updating customer records, or creating support tickets.
Each agentic tool includes details such as the tool name, purpose, required inputs, connector type, and execution method. Tools may interact with systems through API-based integrations or database operations, depending on the nature of the action.
Actions are executed only when explicitly requested by the user and follow controlled execution and verification logic to ensure safe and predictable behavior.
Supported Actions
The agent supports multiple predefined actions, including resetting a modem, checking modem status, creating Jira tickets, and updating CRM databases.
Each action is executed only when explicitly requested and follows strict execution and verification rules.

--------------------------------------------------------------------------------

================================================================================
TITLE: Feedback Analytics
URL: https://dvsum.zendesk.com/hc/en-us/articles/28138720331796-Feedback-Analytics
================================================================================

In this Article:
Overview
Analytics Tab Views
Review Feedback
Activities
Status
Overview
Feedback Analytics is designed to manage and understand user feedback easier and more efficiently. All user feedback is displayed in one organized table, making it easy to see everything in one place.
Analytics Tab Views
The Analytics tab in the agents offers two distinct views:
Chats Analytics
and
Feedback Analytics
. By clicking on
Feedback Analytics
, the system will display all questions that have received feedback in chats.
Here is what to expect from Feedback Analytics:
In
Feedback Analytics
, the top section includes a "View" selector that allows toggling between different views. By default, there is an
All Feedback Analytics view
, but users can create new views based on their preferences.
Below this, a comprehensive feedback table displays all the questions that have received feedback in chats.
To begin with, there are four tiles named as follows:
All Feedback:
It displays all the feedback
Approved:
It filters all the questions with Approved Feedback
Rejected:
It filters all questions with Rejected Feedback
To Review:
It displays all questions with the status set to New.
Following that, there are columns:
Question:
It displaysÂ the question text
Response:
It displaysÂ the generated query
User:
It displaysÂ user name who asked the question
Chat Name:
It displays the chat name in which the question is asked
Feedback Time:
It displays the time when the feedback was given
Feedback:
It displays the feedback on the question: Accepted or Rejected
Status:
It displays the current status of the question
Feedback Category:
It displays the category selected at the time of feedback
Feedback Text:
It displays the feedback text if provided
Review feedback
Clicking on any question within the table opens the
"Review Feedback"
modal, providing more details about the individual feedback items. This modal is divided into two panels:
The left panel displays a list of all questions within the current view, enabling users to navigate quickly between different feedback items.
The right panel offers information about the selected question:
Feedback Type
: Displays the type of feedback, with the full text available on hover.
Current Status
: Displays the current status of the feedback.
Chat and User Name
: Provides context by displaying the chat where the feedback was given and identifying the user who submitted it.
Question and Results
: Displays the specific question and the feedback results.
Activities
The activities icon displays a view of all actions taken regarding the feedback, including status updates and added comments. This ensures that users have a comprehensive history of interactions.
The activities icon is also accessible in the feedback listing table.
Status
The Update status dropdown allows users to update the feedback status directly from the modal.
Additionally, there is a mass update option that allows bulk status changes across multiple feedback items.

--------------------------------------------------------------------------------

================================================================================
TITLE: Agent Analytics
URL: https://dvsum.zendesk.com/hc/en-us/articles/27034936667924-Agent-Analytics
================================================================================

In this Article:
Overview
Visibility of the Analytics Tab
Analytics Tab
Agent Engagement
Top Users
Chats Listing
Copying and Exporting Data
Managing Custom Views
Overview
The new feature in our agent interface: the Analytics tab. This enhancement offers deep insights into agent engagement, top users, and detailed chat metrics.
Visibility of the Analytics Tab
The analytics tab appears only when the agent is shared with other users. To enable the analytics tab, share the agent with at least one other user. Once shared, the analytics tab becomes visible in the interface.
Analytics Tab
The Agent Analytics is divided into three main sections:
Agent Engagement
Top Users
Chats Listing
Agent Engagement
The Agent Engagement section provides a detailed overview of user interactions with the Agent. Here, you will find metrics on how many questions were asked by users and at what times. This section includes a graph that visualizes user activity over time and trends in user engagement.
Top Users
In the Top Users section, you can see a list of users ranked by their activity. This section displays the names of users and the total count of questions each has asked.
Chats Listing
The Chats Listing section is a overview of all chat interactions. It starts with four summary tiles:
Total Chats:
Total number of chats created.
Question:
Questions asked in the chats
With Feedback:
Questions with feedback
Users:
Users who have created the chat
Below these tiles is a detailed listing that includes the following columns:
Chat Name
Total Questions
Approved Feedback
Rejected Feedback
User
Latest Chat Time
Created On
Feedback Count
Objective
Copying and Exporting Data
The Analytics tab provides options to copy and export data from the Chats Listing section. Users can:
Copy:
Select and copy responses directly.
Copy with Headers:
Copy data along with column headers.
Export:
Download chat data for further analysis.
To use these options, right-click on any row in the Chats Listing table and choose the desired action from the context menu.
Managing Custom Views
The Analytics tab is designed to be customizable. By default, it features the Chats Analytics view, but users can create and manage custom views to suit their specific needs.
To create a new view click on the view, a drop-down will appear click on the Create View.
A modal will open. Add a name and details for the new view, and it will be created and displayed in the Analytics tab.
All the views created by the users will be shown in the drop-down.

--------------------------------------------------------------------------------

================================================================================
TITLE: Training of CADDI Agents
URL: https://dvsum.zendesk.com/hc/en-us/articles/27022713765524-Training-of-CADDI-Agents
================================================================================

Training of Agents:
In the CADDI Agent detail page, users can refine and enhance the agent's performance through the Training tab. This feature allows users to add and manage concepts that impact the agent's responses, as well as handle feedback to ensure continuous improvement. Here's a step-by-step guide on how to train your CADDI agent effectively.
In the CADDI agent detail page, there is a Training tab where users can train the agent. From the Training tab, users can add different concepts that impact the results of CADDI. Users can add concepts as shown below:
Go to the Training tab in the agent detail page and click on the 'Add Concept' button.
Once the 'Add Concept' button is clicked, a modal will open where the user can add and save the concept.
Once the user enters the concept, they need to set the scope of the concept. There are three scopes for concepts:
Agent Level:
By default, the concept is set at the Agent level. This means the concept will only be applied to questions asked within this specific agent.
Source Level:
This option can be selected if the concept needs to be added at the source level. This means the concept will be incorporated into all questions asked in any agent within this source.
Account Level:
If the concept needs to be added at the account level, this option can be selected. This will apply the concept to all agents within the entire account, making it a global scope.
Once the scope of the concept is set, click on the Save button, and the concept will be added to the Training tab listing.
.
Moreover, when the user provides negative feedback on any of the question results, that feedback shows up in the Training listing as well. Users can edit and delete the feedback and concepts.

--------------------------------------------------------------------------------

================================================================================
TITLE: Question level Menu items for CADDI Agents
URL: https://dvsum.zendesk.com/hc/en-us/articles/25334871881364-Question-level-Menu-items-for-CADDI-Agents
================================================================================

In Agents and chat interfaces, each question will be accompanied by a set of menu items, each featuring distinct action buttons for interaction. The functionality of each button is elaborated below.
Upon selecting a specific question block within the agent interface, a menu of items will be displayed below the results, as illustrated below:
The functionality of each menu item is explained below:
Approve icon
:
Clicking on this button signifies approval of the question's results, indicating user satisfaction with the generated outcomes. Additionally, a notification confirming the submission of feedback will be displayed upon clicking.
Reject icon
:
When the reject icon is clicked, a feedback modal will open, allowing the user to provide feedback on why the results appeared incorrect to them. Users can submit the feedback accordingly.
Refresh Data
:
Clicking on this button will trigger the re-execution of the generated query, providing the user with fresh data and insights.
Show code
:
When clicked, this icon will display the query above the results, representing the query generated for this question. If the query is already visible and the user clicks the icon again, the query block will hide.
Show Explanation
:
Clicking this button will display the explanation of the question and the query. Once the explanation appears, clicking the button again will hide the explanation section.
If the user seeks further information regarding the content displayed in the Query Explanation, they can refer to the following article:
Query Explanation in CADDI
Show log
:
The generated log will appear once this button is clicked, as shown below.
Delete question
:
The user can delete the question by clicking this button.
Save Context
:
Clicking this button allows the user to save the context, adding the question as a context.
If the user seeks further information on how context is added and its end-to-end functionality, they can refer to the following article:
Adding a context in CADDI and Agents
Users can configure default answer view settings from Chat Preferences, such as whether the Query code, Explanation, and Log will appear by default or not. All these settings can be adjusted within Chat Preferences. For detailed information on configuring these default settings, please refer to the following article:
Chat Preferences for CADDI Agents

--------------------------------------------------------------------------------

================================================================================
TITLE: Chat Preferences for Agents
URL: https://dvsum.zendesk.com/hc/en-us/articles/24639112445716-Chat-Preferences-for-Agents
================================================================================

Chat Preferences
The Chat Preferences option is available in the General Settings, allowing users to configure various settings that will impact the generated insights.
Navigate to My Settings from the top right corner.
Select "Chat Preference" from My settings page
Default Answer View
:
Within these settings, users can specify their preferred default view for generated insights: either
Chart
view or Table view. Note that if the data set cannot be displayed in a chart, then it will fall back to a table view of the data.
Default Chart Sorting
:
In the Default Chart Sorting settings, users have the option to set the default sorting order for generated insights on charts. They can choose between Ascending or Descending order, which will determine how charts are sorted by default. The image below shows these Default Chart Sorting.
By default, the Descending option will be set for users, which can be changed at any time. If the user selects the Ascending option, the chart will be sorted in Ascending order as shown below.
Ascending option was selected and settings were saved
Chart shows up sorted in Ascending order on first Y-Axis
If the user selects the Descending option, then the chart will be sorted in descending order as shown below.
Chart shows up sorted in Descending order on first Y-Axis
Conditions under which Default Chart Sorting works
:
There are certain conditions under which this default sorting will work. These default chart sortings will only apply to those questions for which the bar chart shows up by default and the generated query of the question doesn't have the Order By clause. Note that the bar chart will be sorted based on the first Y-axis which is Y1 axis. However, if a chart has been modified by the user, such as changing the sorting or altering the X or Y axis, then the default sorting will be removed.
These default sorting settings do not apply to line charts, as sorting is automatically applied to the time series column on the X-axis by default for line charts. Additionally, if a question displays a bar chart by default but the generated query includes an Order By clause, the default sorting settings will not apply to that question
Default Answer Outputs
:
In the default answer output, users can specify their preferred output settings, including '
Show Log
', '
Show Code
', and '
Show Explanation
'. By default, admins have 'Show Code' and 'Show Explanation' enabled, while other users default to 'Show Explanation' only.
Within these settings, users can select different Answer outputs. If a user toggles on the 'Show Log' option and saves the settings, logs will subsequently appear for each newly asked question.
Only log details are displayed for the question.
If a user wants the Query block to appear by default with insights, they can toggle on the 'Show Code' option
Only the Query code is displayed.
Likewise, if a user prefers the Query Explanation to appear by default for each answer, they can toggle on the 'Show Explanation' option.
Only Query Explanation is displayed.
Any changes made in the "Chat Preferences" will require the user to save the changes at the end.
Regardless of the settings chosen for Default Answer outputs, users can hide/show Logs, Query code, and Query Explanation at the individual question level. This means that users can adjust these settings for each specific question as per their need, without affecting the default settings. For example, even if Code is set to appear by default according to the chat preferences, a user can choose to hide Code for a particular question if desired. These changes are specific to each question and do not impact the settings for other questions or the default settings.
This chat preference page is also available in the Talk to Your Data interface.
The chat preferences pages in both 'My settings' and 'Talk to Your Data' are synchronized.

--------------------------------------------------------------------------------

================================================================================
TITLE: Agent Reliability; How Query generation re-attempt works
URL: https://dvsum.zendesk.com/hc/en-us/articles/23892690705684-Agent-Reliability-How-Query-generation-re-attempt-works
================================================================================

Sometimes in CADDI, we encounter issues where a question triggers an error state due to an incorrectly generated query. To address this problem, if a question enters an error state due to query generation issues, CADDI will attempt to rectify the error by identifying the mistake in the query and re-attempting query generation. If the initial attempt fails, it will make another try to generate a correct query. Once a correct query is successfully generated and executed, it will provide insights. If the question still fails, the user can provide feedback to help train the agent.
When example shows how query generation re-attempt will look on UI:
Once the question goes into a successful state then it will show the insights:

--------------------------------------------------------------------------------

================================================================================
TITLE: Query Explanation in CADDI
URL: https://dvsum.zendesk.com/hc/en-us/articles/23891989976084-Query-Explanation-in-CADDI
================================================================================

While we use CADDI to communicate with the database and gain meaningful insights from our data, there's an interesting feature that provides insights into the generated query. After a query is generated for any question, an Explanation section appears below the query block. This section explains the various keywords of the query and how they are used, as well as providing the expected output of the query.
The Query Explanation section will have the following details:
What we are looking at:
This section will indicate the action the query will perform, such as its purpose and the results it will produce upon execution.
Tables included in the Query:
This section will display all the tables that will be used in the query execution, which can range from one to many
Join Data:
If the generated query employs the "JOIN" keyword, indicating that a join operation is being utilized, this section will specify which tables are being joined.
Aggregation and Counting:
This section explains which aggregate functions are used in the query and how they are applied to specific columns.
Grouping Data:
This section will tell how data is being grouped in the generated query.
Sorting Order:
This section will specify the column for which the "ORDER BY" clause is being used and indicate whether the data will be sorted in ascending or descending order.
Filtering Conditions:
This section will detail any filtering conditions utilized in the query, specifying the conditions applied, such as which filters are being used.
Expected Output:
This section will provide an explanation of the expected output of the query, outlining the results it will return upon execution.
Following is an example of how the Query Explanation section will look like:

--------------------------------------------------------------------------------

================================================================================
TITLE: Adding a context in CADDI and Agents
URL: https://dvsum.zendesk.com/hc/en-us/articles/22955304388628-Adding-a-context-in-CADDI-and-Agents
================================================================================

In this Article:
Overview
Adding context to the questions
Removing the added context
Overview
In this article, we'll be covering how we can use contexts in the questions that we ask in CADDI. Adding the context will have a significant impact on our data analysis and insights. Let's discover more about the context in detail.
What is the importance of Contexts and how does it impact the insights which are generated s
ubsequently
?
When a user adds some context, the system stores this information. Subsequently, when a new question is asked, the system seamlessly integrates the contextual input into the generated queries, facilitating the generation of insights aligned with the provided context. This will help users get more precise insights from the data, contributing to an enhanced data analysis experience.
Adding context to the questions
Click on the "Saved Contexts" button which is located just underneath the Question text field as shown below.
Once the button is clicked, the Context panel will open from the right side of the screen. From that panel click on the "Add Context" button which will open up a text field where context can be added.
The example below shows the context that is added. Users can also add multiple contexts. Click on the Save button and the context will be saved.
From there on whatever question is asked, will incorporate the saved context. The below example shows how added context is used in the generated query
Results and insights as shown below are presented in alignment with the user's context.
Questions can also be used as a context. After a question is asked, click on the context button located with the actions buttons of the question.
After clicking the context button, the selected question will be added as a context. If any filters are applied from the grid view using profiling, these filters will also be incorporated into the context after the question is set as the context.
Note
: Users can add multiple contexts.
Removing the added context
The user has the option to remove a single context or clear all contexts. If a single context is to be removed, simply click on the cross icon located with each context as shown below.
Once it is clicked, that context will be removed from the "Context in Use" section but it will still appear in the "Saved Context" section as shown below.
An essential point to note is that only the contexts in the "Context in Use" section are active and will be included in the upcoming questions. Contexts that show up in the "Saved Context" section are only there to be added back to the active contexts section if needed.
The other option to remove the existing context from the "Context in Use" section is by clicking on the "Clear Context" button, it will remove all the contexts and move them to the "Saved Context" section.

--------------------------------------------------------------------------------

================================================================================
TITLE: Pivot Tables in CADDI
URL: https://dvsum.zendesk.com/hc/en-us/articles/18540310425748-Pivot-Tables-in-CADDI
================================================================================

In this Article
Introduction
Pivot View
Row Groups
Values
Column Labels
Introduction
Imagine that you have a dataset that contains some numerical values, and you want to know the sum of a column, the maximum value of a column, or the minimum value of a column. Then, for every answer, you are required to ask different questions on Agent.Â  Now, instead of asking different questions about maximum value, minimum value, or sum, if you can get all the answers in one place, then it can save a lot of time. This Article explains how Pivot View can be of great help. Now, for this article, you should have the know-how of Agents. The Article on Agents can be found here:
Configuring CADDI - Creating Agents.
Pivot View
When any question is asked on Agent from any dataset, there are different views generated, which are:
Chart View
Grid View
Pivot View
Our Focus in this article will be related to Pivot View only. The Pivot view can be found next to the Grid View:
Before analyzing the pivot view, let us see the grid view so we can see the data that has been brought up in the answer. The Grid View button is located to the left of the Pivot View button.
In the Grid, we have Transaction Amounts and Different Transactions. Now, Imagine that for different Transaction Names, I want to know the sum of the Transaction amount or maximum value, so for that, we will be required to ask different questions. Pivot View solves this problem.
When Pivot View is opened, then on top, we have the search bar and the columns that are fetched for that question:
Below that, we have 3 options which are:
Row Groups
Values
Column Groups
Row Groups
In the Row Groups, the columns that will be added will be shown as rows on the left side. The columns can be dragged from the dots present before the column name. In the Row groups, metric columns can not come under the Row Groups
Values
Under the "Values," only metric columns are allowed to be dragged. Metric columns are the columns whose data type is double, int, or decimal. Example of these types of columns is the amount, cost revenue, etc.
If dots are clicked on the columns under "Values," then there are multiple options like sum, average, count, max, min, etc. In this way, the user can shift between them, and the grid on the left side will be changed accordingly:
Here, in one place, the user can view the average, sum, minimum value, and maximum value of a metric instead of typing 3 to 4 different questions.
Column Labels
Just like in Row groups where we added columns as rows, in the column labels, columns added here will be added on top as columns:
Now, here for different dates and Transaction Names, the user can see the average, maximum, minimum, count, etc. of the transaction amount. That is how Pivot view saves the users from asking multiple questions. All of that information can be seen here. One thing is to be noted: just like Row Groups, metric columns are also not allowed to be dragged here.
The Pivot View is provided in both Agents and on CADDI chat. For more information on Agents and CADDI chat their articles can be found here:
Configuring CADDI - Creating Agents
CADDI Chat (Talk to your Data)
.
The example explained in this article is a simple one. Users can customize the pivot view according to their requirements.

--------------------------------------------------------------------------------

================================================================================
TITLE: Chart Settings in CADDI
URL: https://dvsum.zendesk.com/hc/en-us/articles/18506870411284-Chart-Settings-in-CADDI
================================================================================

In this Article:
Charts in CADDI
Charts Settings
Human-readable format
Attributes & Dates section
Metric Fields section
Charts in CADDI
Presenting data in charts or graphical forms enhances understanding and analysis. Visual representations simplify complex information, allowing users to grasp trends, patterns, and outliers quickly. This visual approach fosters more efficient decision-making and enables deeper insights from the data. In CADDI we show data results in the form of charts so that users can have more valuable insights of the data.
Within CADDI, three distinct chart types are available: Bar charts, Line charts, and Tile charts. When working with data results that include a Datetime column, the Line chart will be the default visualization, providing insights into temporal trends. Conversely, when the dataset lacks a Datetime column, the Bar chart will be automatically generated, aiding in the visualization of categorical comparisons. The Tile chart, a specialized representation, will exclusively manifest when the data output consists of a singular row and column intersection. Refer to the images below for a visual representation of each chart type.
Line chart
Bar chart
Tile chart
Charts Settings
Both Line and Bar charts feature customizable settings that empower users to conduct more comprehensive data analyses. To access these settings, simply click on the designated icon located on the charts, as depicted in the image below. Upon clicking, the chart settings panel will seamlessly appear on the right-hand side of the grid, offering users a convenient and intuitive way to fine-tune their analytical experience.
Chart settings panel will appear as shown below.
On the settings panel, X-axis and Y-axis fields appear which show which attribute is drawn on X-axis and which Metric fields are drawn on Y-axis. Remember that X-axis will have only one attribute whereas Y-axis can have up to 2 metric fields as there can be Y1 and Y2-axis.
If the attribute that is there on the X-axis is clicked, the data on the X-axis will get sorted in ascending order. If it is clicked for the second time, data will get sorted in descending order as shown below.
Click on the X-axis field.
Data will be sorted in ascending order on the x-axis and on the drawn chart as shown below:
If that attribute field is clicked again, data be sorted in descending order on the x-axis and on chart:
Now talking about the
Y-Axis field
on chart settings. There will be 2 metric fields for Y-Axis. One will be on the Y1 axis and the other one will be on the Y2 axis. So on the
Y-Axis field
on chart settings, if both the checkboxes are checked then both the fields will be drawn on the Y2 axis. And if both the checkboxes are unchecked, then both the fields will be drawn on the Y1 axis. If only one of the metric field's checkboxes is checked then that field will be drawn on Y2 whereas the one whose checkbox is not checked will be drawn on the Y1 axis.
Human-readable format
A toggle labeled "Human-readable format" is present. When enabled, it indicating that significant values within both the grid and chart will be presented in a user-friendly manner. This entails rendering values such as 10,000,000 as 10M and 24,000 as 24K for enhanced readability. Should users wish to revert to unformatted values for precise examination, they can conveniently deactivate this feature by toggling it off.
Attributes & Dates section
This section will have all the attributes and dates that can be drawn on X-axis. It will contain the string, date, and number type attributes. Users can click on any of the attributes and select it. Since there can only be one attribute on X-axis so if any of the attribute is selected from this section, then it will be drawn on the chart's X-axis.
Metric Fields section
This section will be disabled in case 2 metrics are already selected. In order to select some metric from this section, a maximum of one metric field should be pre-selected. Once it is enabled then user can select any of the metrics and it will be drawn on Y1-axis.
If 2 metric fields are selected then all the fields will be disabled as shown below:
So in order to select some metric field, first remove one or both fields from the Y-axis section:
Then it will get enabled and the user will be able to select any of it:
Once changes are made in the chart from settings and the user seeks to revert the chart to its original default configuration, the "Reset to default" button can be employed. By clicking this button, the chart will be reset to its initial state, restoring the original settings.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configuring CADDI - Creating Agents
URL: https://dvsum.zendesk.com/hc/en-us/articles/18497031217428-Configuring-CADDI-Creating-Agents
================================================================================

In this Article:
Introduction
Detailed Steps
1. Creating an Agent
2. Train Agent
3. Detailed Question
4. Add a Concept
5. Basic CRUD Operations
6. Manage Access
Introduction
Conversational AI for Data-Driven Insights (CADDI)
makes it effortless to interact with your data, understand it, and extract reliable insights
by using plain English. This article explains how to configure a CADDI Agent.
Detailed Steps
1. Creating a Agent
Agents can be found on the Sources Detail page. Any Source that will have Chat Enabled will have the Agent Tab. Chat Access can be given to any source through the Module Settings. For more information, click on
User Role and Module Settings
.
Agent Tab can be found on the detail page of the source for which Chat is enabled.
`
The "Agents" tab can be seen next to the "Settings" Tab. On clicking on Agents, a new Agent can be created:
When a new Agent is created, the user is landed on the Definition Tab of the Ageny. In a Data source, there can be several Tables. Users can either use all the tables in the agent or user can limit the agent to some specific sources by adding tables from the Available Tables.
Any added table shows up in "Tables For Chat". Users can also preview the Table to see the data inside it:
User can also see individual columns and their profiling by clicking on the "Manage Fields" option. Here Users can select or unselect columns of any Table:
After Adding the necessary Tables, the user can click on the "Done" and "Save" button to save the changes:
2. Train Agent
After the user has selected specific tables and saved the changes, along with the "Definition" Tab, the "Train Agent" tab can be seen. On clicking the "Train Agent," user can see the "Generate Questions" button up front and the Chat bar below that where questions can be typed:
On clicking Generate Questions, some sample questions show up. Users can use these questions for analysis:
On the Agent, two types of questions can be asked which are:
1- Typed Question
2- Detailed Question
3. Detailed Question
A detailed question is one for which:
SQL Code and Logs are generated
Visualized Grid, Charts, and Pivot Information are generated
Context can be set up and Profiling Information is provided
Positive or Negative feedback can be given for further improvement (Training)
Logs Generation & SQL Query
The Logs, in simpler words, are the information related to when a question is asked and what goes to the Backend in textual form. By default, the logs are not turned on. To turn the logs on, the user will have to enable them from the Chat Preferences:
Once the logs are turned on any question is asked the Logs will be printed, and SQL query generated for that question will be printed below the Logs:
The Log details and the SQL code can be hidden from the Hide Code/Log icon located below the answer output:
Different Generated Views
CADDI generates three different views:
Chart View (learn more in
Chart Settings in CADDI
)
Grid View
Pivot View (learn more in
Pivot Tables in CADDI
)
The Grid View shows the data in tabular form of the columns that are fetched from the SQL query:
Along with the 3 different views there will be a gear icon of Settings which is present when this is clicked settings tab opens up with further three options:
General
Chart (Learn more in
Chart Settings in CADDI
)
Field (Explained in Profiling & Filtering below)
The General tab contains the toggle button of "Human readable format". By default, this option is turned off:
When this option is turned on then it helps in better reading of the data. For example, if there are large values like in thousands or millions then it will show in much more readable format:
Profiling & Filtering
Profiling shows the distribution of different values of a column which helps in a better understanding of the data. Users can apply filters on the Grid through profiling by selecting particular values. The profiling of any column is opened by clicking the column heading in the grid. When the profiling of any column is opened then the "Settings ---> Field" tab is opened up.
When no column is selected and from "Show settings", the "Field" option is selected then no profiling of the column will be shown. Instead, it will show this:
Date type column
For any date type column user can set different date formats according to the requirement.
Once any date format is selected then in the grid that particular date format will be applied:
Note: When any date format is selected then it applies to every question that was previously asked or will be asked
The Grid can be filtered from the visual distribution section. A handle is provided which helps in applying filters according to different values. Once the values are selected then the grid on the left will be filtered accordingly:
Decimal type column
The Profiling of any column that has the data type can be opened by just clicking on the column heading in the grid. The difference is that there are some additional changes provided in its profiling. Users have the option to set decimal values after the point according to the requirement:
Filters can be applied from the visual distribution section. For this particular example, the handle is not present below the distribution bars because if the distinct values for the columns are less than 20 then there will be no handle provided, users can click on the bars to fill in "Minimum" and "Maximum" values.
Integer type column
The Profiling can be opened by clicking on the column heading. For the columns having data type there are no additional options, there will be just visual distribution present from where "Minimum" and "Maximum" values can be selected:
Here in the above example, the handle is provided below the distribution bars because, for this particular column, the distinct count exceeds 20. The logic that handle should be present or not is the same for columns having data type "Decimal" and "Integer".
String type column
The Profiling can be opened similarly by just clicking on the column heading present in the grid. The visual distribution for the string-type columns is a little different from other data types. Filters can be applied by selecting different bars and on the grid the filters will be applied accordingly:
Saving the Question as Context
The question that is asked that can be added as context or
any specific filter applied on the grid can also be added from the Context icon on the top right of the question:
The added context can be seen on the top left of the chat bar below. Users can add more than one context from different datasets. If the context is no longer needed, then it can be removed from Clear Context.
Next Time when any question related to the dataset is added as context this context will be incorporated into the questions also:
Giving Feedback
Often times there will be some questions that may not get answered, or the answer was not according to the user expectations. To tackle this problem, the concept of giving Feedback is introduced. The feedback can be positive if the user is satisfied with the answer:
But there will be times when the user will not be satisfied with the answer or the query generated for that question is wrong, due to which the question may give an error. In this case, the user can give negative feedback:
In the negative feedback, the user can select an option like whether the Didn't get the right data. Didn't visualize properly or Other. In addition, the user can explain in plain English about how the question should be approached, or the user can write the correct SQL query that should have been generated:
When the feedback is submitted, this feedback is taken into consideration when the next question will be asked. This is the kind of training that we are doing, so the accuracy of results can be improved over time.
Generating Answers Based on Different Tools
Often times users will require some questions to be answered in a visualized form and there would be scenarios where users would want just typed answers. For this purpose before asking a question user can select the tool that will generate the answer based on the selected tool. The three tools are located at the top right of the chat bar:
There are 3 different types of Tools that can be selected before writing up any question:
SQL Query Tool
Data Understanding Tool
General Knowledge Tool
SQL Query Tool
By default when any agent is opened the selected tool is the "SQL Query tool":
When the tool chosen is "SQL Query Tool" then when the user asks a question from any dataset then for that answer, SQL code and answer in visualized form (Charts and Grid) will be given. The questions asked by using the "SQL Query Tool" are detailed type questions that were explained in the article above.
For Technical users, they can verify which tool is being used. If the selected tool is "SQL Query Tool" then on the Log details it can be seen:
Data Understanding Tool
The Data Understanding Tool can be used where the user wants some information about some columns present in the dataset e.g. range of values or different distributions of column. In this tool SQL Code and Visualized Grid will only be generated if necessary otherwise the answer is usually given in typed form where visualization is not required:
General Knowledge Tool
This tool can be used if the user wants to ask questions that are not related to the datasets present in the Data Catalog application. Any general knowledge questions can be asked by using this tool. One use case of this tool can be for example there are some definitions for some columns for a particular dataset but user wants to know the industrial definition then this tool can be handy. This tool is located next to the Data Understanding tool:
4. Add a Concept
In the CADDI agent detail page, users can refine and enhance the agent's performance through the Training tab. This feature allows users to add and manage concepts that impact the agent's responses, as well as handle feedback to ensure continuous improvement. In the CADDI agent detail page, there is a Training tab where users can train the agent. From the Training tab, users can add different concepts that impact the results of CADDI. For a step-by-step guide on how to train your CADDI agent effectively go to the following article
Training a CADDI Agent
.
5. Basic CRUD Operations
On the top right of the Agent, some basic operations can be pretty handy for the users. There are two filters which are:
Asked By
Status
The "Asked By" gives the option of filtering the questions that are asked by different users in the agent. All the different users who have asked the questions in the agent can be seen in this filter:
There is another filter "Status" that will filter the questions according to different question types. The following are the filter values:
All
To Review
Failed to Generate
Approved
Rejected
Concepts
With the two filters, there are three dots, users have the option of "Refresh All", which refreshes all the questions and Expand/Collapse All, which will expand or collapse all the questions present in the agent.
The user has the option of Duplicating & Deleting the Agent. If the user wants to work on the same agent, but the user does not want to affect the original agent so the agent can be duplicated. If the user wants to delete the agent, then the option is available along with the duplicate button.
Some more basic information about the Source and Agent can be found below the Agent name, like questions asked, tables, and Chat exclusion criteria, which are mentioned in the Module Settings for this source. The link to the Module settings article can be found
here
6. Manage Access
By default, when any Agent is created, then it is private, and it is indicated by a "lock" icon present in front of the name of the agent . Users have the option of sharing the Agent with other users or user groups through Manage Access:
On clicking Manage Access, a modal opens up where the user can enter any user group or users with whom he wants to share the Agent. 2 types of Access can be given which are:
Chat Only
Edit
"Chat Only" access means that the user will only have access to CAADI chat (Talk to your Data). In "Chat Only", the user will not have any access to the agent, which is present on the Sources Detail page. Once the user shares the agent and gives "Chat Only" Access, another user will receive an email, and a link will be provided.
The Link provided in the email takes the user to CAADI chat (Talk to your Data). For more information about CAADI chat (Talk to your Data), go through the article
here
When the agent is shared with Access as "Edit," then the other user will have edit access to the Agent that is present on the source detail page. When the agent is shared with other users with access as "Edit", then the user will receive an email with a link.
The email link takes the user to the "Definition" Tab of the Agent. This user can also make changes in the original agent since he has edit access:
Once the user has given "Chat Only" access or "Edit" access, then on Manage access, there is another option of "Remove Access", which basically will remove the Chat and Edit access to the other user:
All the editors of the agent can change permissions for other users except for the creator of the agent.
For here the above user is an Editor of the agent, this user can add new users and remove access from already existing users but the permissions of the agent creator can never be changed

--------------------------------------------------------------------------------

================================================================================
TITLE: How to get a guided set of follow up questions to ask in CADDI
URL: https://dvsum.zendesk.com/hc/en-us/articles/16290044151828-How-to-get-a-guided-set-of-follow-up-questions-to-ask-in-CADDI
================================================================================

Before the user starts talking to the data, the user can get a guide of questions that can be asked. The guide depends upon what is the description of the topic and what objective is set by the user. Once you have selected the topic In CADDI, click on the icon on the top right of the question field as shown below.
Once the button is clicked, a modal will open which will have the objective field. Enter your objective and click Save.
Once the objective is set and saved, the user will get the summary along with the guided set of questions that will relate to the objective.
Users can get the guide at any time by clicking the guide icon. The guide questions will generate depending on the description of the chat topic, the objective that is set by the user, and the questions asked (if any) before triggering the guide.
Click on the "
Use this
" button and it will copy the guided question into the text field.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to talk to your data
URL: https://dvsum.zendesk.com/hc/en-us/articles/15168396488852-How-to-talk-to-your-data
================================================================================

In this Article:
Data Analysis and Artificial Intelligence
How Chat History Works with CADDI
Multi-Language Support in CADDI
Data Analysis and Artificial Intelligence
Artificial Intelligence (AI) plays a crucial role in data analysis by providing a simple interface to extract insights from large and complex data sets. With its ability to process natural language quickly, AI helps people make better-informed decisions, identify patterns and trends, and uncover hidden insights that might otherwise go unnoticed. In DvSum users have the ability to talk to their data using
C
onversational
A
I for
D
ata-
D
riven
I
nsights (CADDI). DvSum CADDI makes it effortless to interact with your data, understand it, and extract reliable insights.
Note:
For more details on CADDI chat refre "
CADDI Chat (Talk to your Data)
" article.
How to talk to the data in DvSum Data Intelligence
Before we start chatting with data, first we need to create a topic and share it with some users. Here I have a chat topic shown below which I have shared with the user. Go to the chat topic detail page and follow the steps below to see how we can share the topic with some other users.
After sharing the chat topic, click on "Talk to Your Data" from the left navigation. This will open the Conversational page in a new tab.
In the Conversational tab, all the analyses shared with users will appear as topics. Select the desired topic to begin the conversation.
Once the topic is selected, an About section of that chat will appear on the new chat which will contain details of the topic i.e. that chat topic is created for which dataset, who are the owners of that dataset, etc.
Click on the "Sample Data" button and it will show the sample data of that data set.
From the sample data user can have some basic insights of what data is about and how it looks like. Users can select different columns and apply filters to check the results.
Once sample data is explored generically, now users can move forward by asking different questions to have more to-the-point insights into the data. In the current example I have selected the Order History table so I will try asking questions to explore data further as shown below.
Here is the result:
The result shown above shows a line chart of the insights for which the question was asked. It shows the insights in graphical form of all the orders that were placed recently in the current month. In order to see the results in the form of a grid then click on the "Toggle View" button as shown below and it will also show a grid view of the results.
The type of chart depends on what query is generated from the question and what results are fetched. If datetime column and metric count in results is greater than or equal to 1 then the line chart will show up as shown in the case above. Whereas if datetime column count is 0 and the attribute and the metric count are greater than or equal to 1 then the Bar chart will show up as shown in the example below.
Whereas if results just show one row and column then the Tile chart will show up as shown below.
Users can also change the topic within the chat. Click on the topics button and it will show up all the topics. Select any and it will show its About section.
How Chat History Works with CADDI
CADDI supports chat history as well, allowing the system to use context from previous questions to provide more accurate and relevant responses to follow-up questions. The system first determines whether a question is a follow-up or a standalone query. If it is identified as a follow-up, it utilizes previous context to provide a more accurate and relevant response, similar to how other advanced agents, such as ChatGPT, operate. The example below illustrates how the system maintains the context of previously asked questions.
Firstly, the following question was asked: '
How many members are from the state of California?
', and it showed the results.
After that, a follow-up question was asked: '
Show me how many of them have diabetes and are in their 30s
.'
The second follow-up question didnâ€™t include any specific information about members from California, yet it still identified members from that state. This is because the system used the context from the first question, which mentioned California
Multi-Language Support in CADDI
CADDI also supports multiple languages, allowing users to query and receive insights in their preferred language. This feature enhances the platformâ€™s accessibility and usability for a diverse global audience.
Languages supported by CADDI include, but are not limited to, German, Spanish, Chinese, French, Portuguese, Japanese, Russian, Italian, and Korean, among others.
For example, if the query is submitted in Spanish, both the response and explanation are provided in the same language.

--------------------------------------------------------------------------------

================================================================================
TITLE: Why can I see only 300 exceptions in Analysis tab when Rule Run Result shows more ?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000991833-Why-can-I-see-only-300-exceptions-in-Analysis-tab-when-Rule-Run-Result-shows-more
================================================================================

If you see less records in cleansing workbench than the number of exceptions in the Rule Run Result, below is the reason why.
Please refer to the end of this article to know how to view all exceptions in Analysis tab.
You may have hundreds of records in your data source, but DvSum will process a limited number of records at a time. Similarly, there is a limited number of exceptions that are displayed in the cleansing workbench.
Where do these limits come from?
Every table of a source has its own staging configuration from where the administrator can control the number of records to be processed at a time and the number of exceptions which can be displayed at a time. This means if there are 1,241 exceptions (refer to the image above), you will only see 300 of the exceptions. Once these 300 are fixed and the rule is executed again, it will show 941 (1241 - 300) exceptions.
Can I show higher number of exceptions in extract?
Allowed Review limit: 300 - 10,000*
1. When theÂ rule is run in
online mode
(through online execution), it will give the correct exception count in Run Result, but the actual detail is 300 records in cleansing workbench.
2. WhenÂ the rule is run in
batch mode
(through scheduler), it will still have the same count, and the actual detail extract is up to 10,000 records in cleansing workbench.
*Note: These are the 2 boundary limits.Â Changing that number below 300 will not decrease and changing from 10,000 to a higher number will not increase the limit.
All exceptions in Online Run
This small button has the power to show you all exceptions on Analysis tab of rule detail page. By default this button is off and you will see 300 exceptions.
If you turn the button on, it will bring upto 10,000 exception records on Analysis tab.

--------------------------------------------------------------------------------

================================================================================
TITLE: How do you select the right Catalog View?
URL: https://dvsum.zendesk.com/hc/en-us/articles/115005116933-How-do-you-select-the-right-Catalog-View
================================================================================

You can determine the information you need in a view and then create one of the following DvSum Catalog views:
Scope View
RDBMS View
SAP View
Here is the detail of all three:
Scope View:
This view enables you to specify a logical scope of data relevant to you. You choose an existing data catalog object, set filtering criteria, and arrange fields in a layout that suits your preferences. It's a versatile view applicable to all types of data sources.
RDBMS View:
This view enables you to define a SQL-based view for joining and filtering multiple objects in a Relational Database (e.g., Oracle, SQL Server). It functions similarly to creating a database view, but without the obligation to do so. Subsequently, you can interact with this view in the same way you would with any table.
SAP View:
This view is designed for SAP-ECC and SAP-ISR data sources, allowing you to join various objects within SAP for data profiling and analysis.
Read more on
Joining tables to create Blend View

--------------------------------------------------------------------------------

================================================================================
TITLE: Add an IP Address to the White List
URL: https://dvsum.zendesk.com/hc/en-us/articles/115006294588-Add-an-IP-Address-to-the-White-List
================================================================================

Problem Statement:
When you sign in to DvSum Web Application and you see a warning sign that prevents you from connecting to the Web service.
Reason:
Not every IP can access the Web Service and use DvSum data quality management application because of the security risks. Your IP must exist in the White listed IP Addresses.
Resolution:
For a regular User/Administrator:
In case you are a regular User, please visit
https://www.whatsmyip.org/
and find out your IP Address. Next, request your administrator to add your IP to white list.
For an Owner:
In case you are an Owner, first you need to know the IP address to be white listed (your own/your companyâ€™s user) or you can simply click on "Get Current" and it will fetch the current IP for you.
Step 1:
On the left menu, go to Administrator >> Manage Accounts >> Data Security Tab
Step 2:
Enter the IP address in the field below and click on â€œAddâ€
Once the IP has been added to the white list, you can see the web service icon on top. You can now run the web service and access DvSum's data quality features.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to add Dataset Group?
URL: https://dvsum.zendesk.com/hc/en-us/articles/22706867079700-How-to-add-Dataset-Group
================================================================================

Dataset Groups serve as
tags
enabling you to categorize your datasets. These tags can be applied to organize datasets, functioning similarly to folders. However, unlike folders, a single dataset can have multiple Dataset Group tags, and conversely, a Dataset Group can include multiple datasets.
Create Dataset Group
A Dataset group can be created by users through Mass Update. Simply follow these steps:
Select Dataset -> More Action -> Mass Update.
As users input data into the "Value" field, previously established Dataset groups will become visible. If the desired Dataset group is not displayed, you can easily create a new one by pressing the Enter key.
Remove/ Replace Dataset Group
In
Mass Update
, users have the ability to remove or replace existing Dataset groups. You can do this by selecting the Dataset you want, going to "More Actions," and then choosing "Mass Update." From there, you can remove or replace the selected Dataset Group.
Here are the available choices related to Dataset groups:
Add the following Dataset Groups.
Replace existing Dataset Groups with the following Dataset Group.
Remove the following Dataset Groups.

--------------------------------------------------------------------------------

================================================================================
TITLE: Guide on using Address Validation API
URL: https://dvsum.zendesk.com/hc/en-us/articles/360056932812-Guide-on-using-Address-Validation-API
================================================================================

Overview
This document is a guide on how to use Dvsum Address Validation API. Dvsum Address Validation API helps the users to validate their address data thus getting a clean and more meaningful and understandable address.
In this document, we will learn the following things:
Dvsum Address Validation API endpoint: The REST endpoint to access the amazing Address Validation functionality
The HTTP method to ping this endpoint.
Request Format: The format in which the data is to be sent to the API.
Response Format: The format in which the API will return the validated results.
Error Codes: If some error occurs, error codes help to know what kind of error it is.
URL
https://apis.dvsum.com/address-validation
Method
The request type is
POST
.
Authentication Header
Request header should contain the following header.
x-api-key: <api-key-value>
Request Payload
The address data needs to be sent to API in the request body of the POST request. Following is the structure of how the request body is to be created.
{
"addresses":Â [
{
"address_line1":Â "",
"city":Â "",
"zip_code":Â "",
"country":Â "",
"company_name":Â "",
"address_line2":Â "",
"address_line3":Â "",
"state":Â "",
"other_address_lines":Â ""
}
],
"case_format":Â "",
"zip_code_format":Â ""
}
Now, let us dig deep into each of these fields.
â€œaddressesâ€
(mandatory)
contains the address data that is to be validated. It can contain the following fields.
â€œaddress_line1â€
mainly contains the data of the street i.e., street number and street name. However, it can also contain the data that one is supposed to enter in â€œaddress_line2â€ and â€œaddress_line3â€.
â€œcityâ€
contains name of the city.
â€œzip_codeâ€
contains zip code. It can be added either in US format or non-US format.
â€œcountryâ€
preferably contains abbreviated name of the country; however, full name can also be provided but in that case matched status will be
For example,
if user wants to enter United States, he/she should enter
US
rather than the full name.
â€œaddress_line2â€ (optional)
contains the data of Suite/Apartment/Building.
â€œaddress_line3â€ (optional)
contains c/o information and attention.
â€œstateâ€ (optional)
preferably contains abbreviated name of the state or province, however, full name can also be provided but in that case matched status will be
For example, if user wants to enter New York, he/she should enter
NY
rather than the full name.
â€œcase_formatâ€ (optional)
represents the case format in which the results are to be returned. It can have any of these three values,
â€œUPRâ€
: Converts the results in upper case.
â€œLWRâ€
: Converts the results in lower case.
case_format is an
optional
field. Hence, if not provided, its default value will apply which will return results in
InitCap
format. For example, the street address â€œ4000 KRESGE WAY REC DOCKâ€ will be converted to â€œ4000 Kresge Way Rec Dockâ€. Here, you can see that the first letter of each word is in uppercase and the rest are in lowercase.
For
OK
result API is doing case sensitive check which means input address is exactly same as matched one then it is considering as ok. Any case change or standardization (Road to Rd) consider as Standardize.
â€œzip_code_formatâ€ (optional)
represents the Zip code format in which the results are to be returned.
For US, zip code format can have any of these two values,
â€œFULLâ€
: Returns complete zip code which contains 5 initial letters and 4 additional letters in the following format â€œxxxxx-xxxxâ€.
â€œINITâ€
: Returns only the initial 5 letters of the zip code.
For non-US, zip code format might not be provided. API will validate it and might add a whitespace. For example, L5T2N7 will get converted to L5T 2N7. In this case, matched status will be standardized.
zip_code_format is an optional field. Hence, if not provided â€œ
FULL
â€ will be used as its value.
Note:
All the field names are to be provided in lower case.
Out of these fields, it is mandatory to provide â€œaddress_line1â€, â€œcountryâ€ and either â€œcityâ€ or â€œzip_codeâ€.
Response Format
The validated address is returned by the API in the form of JSON. Its format is as follows.
{
"results":Â [
{
"matched_address_line1":Â "",
"matched_address_line2":Â "",
"extra_information":Â "",
"matched_city":Â "",
"matched_state":Â "",
"matched_zip_code":Â "",
"matched_country":Â "",
"matched_complete_address":Â "",
"matched_location_type":Â "",
"matched_status":Â "",
"matched_status_name":Â "",
"longitude_coordinates":Â "",
"latitude_coordinates":Â "",
"street_number":Â "",
"street_name":Â "",
"zip_code":Â "",
"zip_code_suffix":Â "",
"input_complete_address":Â "",
"partial_match": "",
"matched_address_types":Â []
}
]
}
Let us, dig into each of these fields.
â€œresultsâ€
will have a list of matched addresses. Address contains the following fields
â€œmatched_address_line1â€
contains the validated data of the street i.e., street number and street name
â€œmatched_address_line2â€
contains the validated data of Suite/Apartment/Building.
â€œextra_informationâ€
contains extra address data, which is usually not part of standard address data, if input address contains any c/o information (phone number, person name etc.).
â€œmatched_cityâ€
contains validated name of the city.
â€œmatched_stateâ€
contains abbreviated name of the validated state or province.
Here, it will still contain abbreviated name even if full name of state or province was provided in the input address. And hence, match status in this case will be Standardized.
â€œmatched_zip_codeâ€
contains the validated zip code.
For
US
If
â€œFULLâ€
was provided as zip code format in the input address, full zip code will be provided in the following format 40207-4605.
If
â€œINITâ€
was provided as zip code format in the input address, only initial 5 letters of zip code will be provided. (only 40207 in this case)
For
non-US
a zip code will either be provided as it is.
or with an added whitespace will be provided. For example, L5T2N7 will get converted to L5T 2N7. In this case, matched status will be standardized.
â€œmatched_countryâ€
contains abbreviated name of the validated country.
Here, it will still contain abbreviated name even if full name of the country was provided in the input address. And hence, match status in this case will be Standardized.
â€œmatched_complete_addressâ€
contains the complete validated address. It is basically comma separated concatenation of all the above explained matched fields. There is no comma in between
â€œmatched_address_line1â€, â€œmatched_address_line2â€
and
â€œmatched_stateâ€, â€œmatched_zip_codeâ€
â€œmatched_location_typeâ€
stores additional data about the specified location. The following values are currently supported:
"ROOFTOP"
indicates that the returned result is a precise geocode for which we have location information accurate down to street address precision.
"RANGE_INTERPOLATED"
indicates that the returned result reflects an approximation (usually on a road) interpolated between two precise points (such as intersections). Interpolated results are generally returned when rooftop geocodes are unavailable for a street address.
"GEOMETRIC_CENTER"
indicates that the returned result is the geometric center of a result such as a polyline (for example, a street) or polygon (region).
"APPROXIMATE"
indicates that the returned result is approximate.
â€œmatched_status_nameâ€
contains the complete name of the matched status. It can contain one of these values:
Incomplete
OK
Standardized
Enrich
No match
Each of these statuses is explained next.
â€œmatched_statusâ€
contains the status of the matched address in abbreviated format. it may contain the following values based on intelligence:
INC, OK, SD, ER, NM
INC (Incomplete)
i
ndicates that the provided address data is not complete and is returned if any of the below conditions is fulfilled:
address_line1 is empty.
country is empty.
city and zip_code both are empty.
OK
i
s returned if following conditions are fulfilled:
matched_location_type is â€œ
ROOFTOP
â€.
matched_address_line1 exists.
matched_city exists.
matched_zip_code exists.
matched_country exists.
Input address and validated address are exactly same without any case difference or if
"
case_format"
is
"DEF"
then input address and validated address will be compared as case insensitive.
SD (Standardized)
i
s returned if following conditions are fulfilled:
matched_location_type is "
ROOFTOP"
.
matched_address_line1 exists.
matched_city exists.
matched_zip_code exists.
matched_country exists.
There is only
ZIP Code
deference between provided address and validated address (example: 63122-6604, 63122 or 63122, 63122-6604). If
"
zip_code_format"
value is
"INIT"
then it will compare the first 5 characters only.
There is
abbreviated form
found in the validated address (example: Henry Street, Henry St).
ER (Enrich)
i
s returned if following conditions are fulfilled:
matched_location_type is â€œ
ROOFTOP
â€.
matched_address_line1 exists.
matched_city or matched_zip_code exists.
matched_country exists.
Validated address is a bit changed from provided address (example: Henry Road, Henry St).
There is only
ZIP Code
deference between provided address, validated address (example: 63121, 63122).
NM (No match)
is returned if none of the above status is returned.
â€œlongitude_coordinatesâ€
contains the longitude coordinate of the provided location.
â€œlatitude_coordinateâ€
contains the latitude coordinate of the provided location.
â€œstreet_nameâ€
contains the name of the validate street.
â€œstreet_numberâ€
contains the number of the validated street.
â€œzip_codeâ€
contains the initial 5 letters of the validated zip code. For example, if 40207-4605 was provided as a zip code, then
â€œzip_codeâ€,
will contain only 40207.
â€œzip_code_suffixâ€
contains the last 4 letters of the validated zip code. For example, if 40207-4605 was provided as a zip code, then
â€œzip_code_suffixâ€,
will contain only 4605.
â€œinput_complete_addressâ€
contains complete address that was provided to the API. It is basically comma separated concatenation of all the input fields There is no comma in between
"address_line1â€, "address_line2", "address_line3", "other_address_lines"
and
"stateâ€, "zip_code"
â€œmatched_address_typesâ€
array indicates theÂ typeÂ of the returned result. This array contains a set of zero or more tags identifying the type of feature returned in the result. For example, a geocode of "Chicago" returns "locality" which indicates that "Chicago" is a city.
The following types are supported and returned in the matched_address_typeÂ  arrays:
street_address
indicates a precise street address.
route
indicates a named route (such as "US 101").
intersection
indicates a major intersection, usually of two major roads.
political
indicates a political entity. Usually, this type indicates a polygon of some civil administration.
country
indicates the national political entity and is typically the highest order type.
administrative_area_level_1
indicates a first-order civil entity below the country level. Within the United States, these administrative levels are states. Not all nations exhibit these administrative levels. In most cases, administrative_area_level_1 short name will closely match ISO 3166-2 subdivisions and other widely circulated lists; however, this is not guaranteed as our geocoding results are based on a variety of signals and location data.
administrative_area_level_2
indicates a second-order civil entity below the country level. Within the United States, these administrative levels are counties. Not all nations exhibit these administrative levels.
administrative_area_level_3
indicates a third-order civil entity below the country level. This type indicates a minor civil division. Not all nations exhibit these administrative levels.
administrative_area_level_4
indicates a fourth-order civil entity below the country level. This type indicates a minor civil division. Not all nations exhibit these administrative levels.
administrative_area_level_5
indicates a fifth-order civil entity below the country level. This type indicates a minor civil division. Not all nations exhibit these administrative levels.
colloquial_area
indicates a commonly used alternative name for the entity.
locality
indicates an incorporated city or town political entity.
sublocality
indicates a first-order civil entity below a locality. For some locations may receive one of the additional types:Â sublocality_level_1Â toÂ sublocality_level_5. Each sublocality level is a civil entity. Larger numbers indicate a smaller geographic area.
neighborhood
indicates a named neighborhood
premise
indicates a named location, usually a building or collection of buildings with a common name
subpremise
indicates a first-order entity below a named location, usually a singular building within a collection of buildings with a common name
postal_code
indicates a postal code as used to address postal mail within the country.
natural_feature
indicates a prominent natural feature.
airport
indicates an airport.
park
indicates a named park.
point_of_interest
indicates a named point of interest. Typically, these "POI"s are prominent local entities that do not easily fit in another category, such as "Empire State Building" or "Eiffel Tower".
In addition to the above, address components may include the types below.
floor
indicates the floor of a building address.
establishment
typically indicates a place that has not yet been categorized.
landmark
indicates a nearby place that is used as a reference, to aid navigation.
point_of_interest
indicates a named point of interest.
parking indicates
a parking lot or parking structure.
post_box
indicates a specific postal box.
postal_town
indicates a grouping of geographic areas, such as locality and sublocality, used for mailing addresses in some countries.
room indicates
the room of a building address.
street_number
indicates the precise street number.
bus_station, train_station and transit_station indicate the location of a bus, train or public transit stop.
An empty list of types indicates there are no known types for a particular address component, for example, Lieu-dit in France.
â€œpartial_matchâ€
indicates that the geocoder did not return an exact match for the original request, though it was able to match part of the requested address. You may wish to examine the original request for misspellings and/or an incomplete address.
Partial matches most often occur for street addresses that do not exist within the locality you pass in the request. Partial matches may also be returned when a request matches two or more locations in the same locality
Response Codes
Following are the response codes that will help user know what kind of response is returned by the API.
Code
Description
200
{
"results": []
}
It indicates successful processing of request. The results will have a list of matched addresses.
400
{
"message":Â "InvalidÂ request.Â MissingÂ the
'addresses' parameter.",
"status":Â 400,
"error": "Bad Request"
}
It indicates a parameter is missing from the request body.
parameter that will cause this issue is "addresses".
403
{
"message":Â "API key is missing or invalid in request header.",
"status":Â 403,
"error": "Forbidden"
}
It indicates that API key was missing or invalid in the request header.
406
{
"message":Â "Expected input format is JSON.",
"status":Â 406,
"error": "Invalid input format"
}
It indicates the request body format is not correct i.e., backend is expecting json and client is sending request in some other format.
500
{
"message":Â "Error while processing request.",
"status":Â 500,
"error": "Internal server error"
}
It indicates that something went wrong on server.
504
{
"message":Â "The server didnâ€™t respond in time.",
"status":Â 504,
"error": "Timeout"
}
It indicates that the request did not finish in time and request got timed-out.
API Request, Response Examples
Example 1
Let see an example where the address data is added separately in address line 1,2 and 3.
Request Body
{
"addresses":Â [
{
"address_line1":Â "4000Â KRESGEÂ WAY",
"address_line2":Â "RECÂ DOCKÂ 1",
"address_line3":Â "TAGÂ 0001238279",
"city":Â "LOUISVILLE",
"state":Â "KY",
"zip_code":Â "40219",
"country":Â "US",
"company_name":Â "BAPTISTÂ HEALTHÂ LOUISVILLE"
}
],
"case_format":Â "",
"zip_code_format":Â ""
}
Response Body
{
"results":Â [
{
"matched_city":Â "Louisville",
"matched_status":Â "ER",
"extra_information":Â "RECÂ DOCKÂ 1,Â TAGÂ 0001238279",
"matched_location_type":Â "ROOFTOP",
"input_complete_address":Â "4000Â KRESGEÂ WAYÂ RECÂ DOCKÂ 1Â TAGÂ 0001238279,Â LOUISVILLE,Â KYÂ 40219,Â US",
"matched_zip_code":Â "40207-4605",
"matched_state":Â "KY",
"matched_status_name":Â "Enrich",
"street_name":Â "KresgeÂ Way",
"zip_code":Â "40207",
"longitude_coordinates":Â "-85.6395549",
"matched_complete_address":Â "4000Â KresgeÂ Way,Â Louisville,Â KYÂ 40207-4605,Â US",
"matched_address_line2":Â null,
"zip_code_suffix":Â "4605",
"partial_match": true,
"matched_country":Â "US",
"street_number":Â "4000",
"latitude_coordinates":Â "38.2375702",
"matched_address_type": [
"street_address"
],
"matched_address_line1":Â "4000Â KresgeÂ Way",
}
]
}
Example 2
Let see an example where the address data is added only in address line 1.
Request Body
{
"addresses":Â [
{
"address_line1":Â "4000Â KRESGEÂ WAYÂ RECÂ DOCKÂ 1Â TAGÂ 0001238279",
"city":Â "LOUISVILLE",
"state":Â "KY",
"zip_code":Â "40219",
"country":Â "US",
"company_name":Â "BAPTISTÂ HEALTHÂ LOUISVILLE"
}
],
"case_format":Â "",
"zip_code_format":Â ""
}
Response Body
{
"results":Â [
{
"matched_city":Â "Louisville",
"matched_status":Â "ER",
"extra_information":Â "RECÂ DOCKÂ 1,Â TAGÂ 0001238279",
"matched_location_type":Â "ROOFTOP",
"input_complete_address":Â "4000Â KRESGEÂ WAYÂ RECÂ DOCKÂ 1Â TAGÂ 0001238279,Â LOUISVILLE,Â KYÂ 40219,Â US",
"matched_zip_code":Â "40207-4605",
"matched_state":Â "KY",
"matched_status_name":Â "Enrich",
"street_name":Â "KresgeÂ Way",
"zip_code":Â "40207",
"longitude_coordinates":Â "-85.6395549",
"matched_complete_address":Â "4000Â KresgeÂ Way,Â Louisville,Â KYÂ 40207-4605,Â US",
"matched_address_line2":Â null,
"zip_code_suffix":Â "4605",
"partial_match": true,
"matched_country":Â "US",
"street_number":Â "4000",
"latitude_coordinates":Â "38.2375702",
"matched_address_type": [
"street_address"
],
"matched_address_line1":Â "4000Â KresgeÂ Way",
}
]
}
Here, we can see that even if data was only added in address line 1, we were still able to get the expected validated address from the API and non-address related information is added in â€œ
extra-information
â€ property.
API Versioning
1) All new changes to API will be published as new API Version. API consumer will be able to use both i.e. latest and previous versions of Address validation API using same API key. Currently only one version is available i.e.
https://apis.dvsum.com/address-validation
2) API consumer will have existing and new versions available for testing on Prod environment
3) Every new version will be published with version number in URI i.e
https://apis.dvsum.com/address-validation/v1
https://apis.dvsum.com/address-validation/v2
Address Validation Rule
This is a
link
to Address Validation rules article.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Configure Email Notifications ?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360045674733-How-to-Configure-Email-Notifications
================================================================================

In DvSum, there are multiple modules which send emails to notify information about respective modules. Most popular modules are Job Scheduling (Rules, Profiling, Batch), Data Quality Workflows and Data Management workflows. Previously, there was no control defined at account level for these emails to be received. With Email Optimization,Â we will be giving the control on user profile page (click user name at top right corner + click Profile option in the drop-down to navigate to User Profile) to set the frequency of the Notification Alert type.
You will be provided with the notification settings on the profile page through which respective flags can be controlled.
Notifications settings are segregated into following parts
Notification Frequency
Notification Type
Notification Frequency
Instant:
Notifications will be received as they were previously. Emails will be received right away as the event gets completed.
Hourly:
Notifications will be received on the hourly basis i.e all the set of actions performed in previous hour will be consolidated in a single email and then sent to the respective user. No emails will be received instantly then.
Daily:
Notifications will be received on the daily basis i.e all the set of actions performed in previous day will be consolidated in a single email and then sent to the respective user. No emails will be received instantly or hourly then.
Never:
Notifications will never be received on setting Never as frequency from the list.
Notification Type
There are five standard notification types in DvSum application. Below are the details for each notification type.
Data Management Workflow Notifications
Enable to receive the following notifications for data management workflows
Workflow Initiation Request
:
Enable to receive notification whenever new Workflow request is started
Workflow Completion
:
Enable to receive notification whenever workflow gets finished
Workflow Cancellation
:
Enable to receive notification whenever workflow gets cancelled
Workflow Step Assignment
:
Enable to receive notification whenever step is assigned to you
Workflow Step Assignee Changed
:
Enable to receive notification whenever step is reassigned to someone else
Workflow Consecutive Steps
:
Enable to receive notification to your further next step
Workflow Step Rejection
:
Enable to receive notification whenever step assigned to you gets rejected
Data Quality WorkflowÂ Notifications
Enable to receive notifications for data quality workflows
On proceeding to your own next step
:
Enable to receive notification when you are assigned to in the next step
Scheduled Rule EmailÂ Notifications
Enable to receive scheduler rule email notifications
Receive Email only with Exceptions
:
Enable to receive notification with exception rule only
Scheduled Batch EmailÂ Notifications
Enable to receive scheduler batch email notifications
Receive Email only with Exceptions
:
Enable to receive batch notification with exception rules only
Scheduled Profiling Email
Enable to receive schedule profile email notifications
Add on Feature -Â Receive Job Email as Creator
There was a request from our customer that they do not want to receive the email if they have scheduled a job unless they are explicitly added in the job as an internal recipient. So if it is disabled and you schedule a job with yourself added as recipient in that job, you will not be receiving the job completion email.
Enable to receive schedule email if you create the job.
Note: If you are the job creator and you have turned off the notification and added external user in that job, the external user will not be receiving the job completion email. The resolution is that you must have to select at-least one internal user in that job.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Security Admin
URL: https://dvsum.zendesk.com/hc/en-us/articles/21781811462676-Data-Security-Admin
================================================================================

Overview
In an era where data privacy and security are paramount, the need to fortify sensitive information within applications is more critical than ever. This article explores the imperative task of enhancing data security and privacy within the DvSum application.
Let's start looking into this.
Add Data Security Admin
Owner Account can add Data security admin from Administration -> Manage Account -> User Security.
Only Data security admins will have access to Hide/ Unhide Data Catalog and apply masking on columns.
Note:
The user must have Admin permission to the Data Catalog at the Role level.
Now login with the Data Security Admin account, and you will see the Hide/unhide menu for the Data Catalog Grid.
Hide/ Unhide Table & Columns:
Table Dictionary:
Users can mark any table as hidden. The hidden tables won't show at:
Data Catalog - Table Dictionary
Profiling Listing page
Profiling Page search field
Elastic Search
On the Rules Creation form
Exported Excel File
To view hidden Tables select the checkbox
Show Hidden Tables
, all hidden tables in the grid will be shown.
Users can unhide the Table, by selecting the row and clicking on the unhide menu
Column Dictionary:
When a column is marked as hidden, The hidden column won't show at:
Data Catalog - Column Dictionary
Table Profiling Detail Page - Profiling Tab
Table Profiling Detail Page - Data Quality Tab
Table Profiling Detail Page - Field Configuration Tab
Table Detail Page - Field Tab
Table Detail Page - Sample Data
On the Rules Creation form
Rule Detail Page - Data Tab
Rule Detail Page - Definition Tab
Exported Excel File
To view hidden Columns select the checkbox
Show Hidden Columns
, all hidden Columns in the grid will be shown.
Users can unhide the column, by selecting the row and clicking on the unhide menu.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Masking
URL: https://dvsum.zendesk.com/hc/en-us/articles/21103301641748-Data-Masking
================================================================================

Overview
DvSum Data Quality (DvSum DQ) provides the ability to mask sensitive data through the use of tags. Tag values include Public, Internal, Restricted, and Sensitive. Details about the effect of different tags and how to set them are explained in this article.
Configuring Data Security Admins
In order for a user to specify the tag values that control whether data is masked, the user must be given the Data Security Admin privilege. Steps to enable this privilege:
While logged in as the account owner, navigate to Administration â†’ Manage Account â†’ User Security â†’ Data Security Admins
Select and add the relevant users. Likewise, remove users when appropriate.
Masking Columns
Users with the Data Security Admin privilege are able to view and set the value of the Data Classification property.
Data masking in DvSum DQ serves as a privacy shield, obscuring sensitive information with masked placeholder values of "X". A data security admin can mask any column using the Data Classification property.
Navigate to Data Catalog â†’ Column Dictionary.
Select the desired columns, then click "Mass Update".
Set the Data Classification property to the desired value.
Sensitive
: The column data will be masked as "XXX " all across the application.
Restricted
: The column will be marked as a Hidden Column, so users won't see the column.
Internal
/ Public
: The column data will be visible (default situation).
When the column is marked as sensitive, an icon will be shown with the column name and the data for the sensitive column will be masked as "XXX" throughout the application.
When the rule is executed on the table that has a masked column, the exported Excel file will show masked Data for the sensitive Column.

--------------------------------------------------------------------------------

================================================================================
TITLE: Manage Roles
URL: https://dvsum.zendesk.com/hc/en-us/articles/360052075952-Manage-Roles
================================================================================

There are different set of users who use DvSum application. If Admin wants to provide access to particular set of users/groups or if he wants to restrict the access of certain modules, here's how to do it.
Manage Users
Step 1:
In this section, there is a field "Module Access Role". In this column, the default roles will be displayed as same as Roles for existing users. For instance; if an existing user has an 'Admin' role then it will also appear as 'Admin' in Module access role column as shown below:
Step 2:
If you create a new user and wants to give it a different level of access, then, first select the user, click the 'Edit user' button. It will open up an Edit user interface where you can see the new field added as 'Manage Access Role'. It will have all the existing default roles, also the new roles that are added from which you can select. For instance; you select 'Analyst' role for user 'Admin' and hit Save.
Note:
Now it will have restricted access that is given at Analyst level. Please note that even though the user role is set as 'Admin' but the preference of access will be granted according to option set in 'Module Access Role'.
Manage Roles
Step 3: Default Roles and access
Please note that for existing users, their default roles i.e Owner, Admin, user, super user level will have no change. Also, the default roles (Owner, Admin, user, super user) cannot be edited or deleted from here and there will be no 'Created by' and 'Modified By' data in the cell. These roles will be generated by default here.
By default Owner will have access to all tabs. Admin will have access to Admin tab, Reference dictionary and Batch execution. Super user will have access to reference dictionary and Batch execution. Normal user will have no default access at all.
But for custom role(s) (custom role that is created by user), these permissions can be changed as explained in step 7
Owner
Admin
Super User
User
Administration tab
âœ“
âœ“
âœ•
âœ•
Manage Account Tab
âœ“
âœ•
âœ•
âœ•
Delete a user from Admin Tab
âœ“
âœ•
âœ•
âœ•
Reference Dictionary
âœ“
âœ“
âœ“
âœ•
Batch Execution
âœ“
âœ“
âœ“
âœ•
Step 4: View Users
If you want to view list of users that have a role set as as 'Admin' and their user 'Status' then click this 'View users' button and it will display a new interface consisting list of all users added as an Admin as shown below:
Step 5: Create Role
Additionally, we have provided user a facility to create more roles other than just default roles which later can be assigned to users with different level of role access. These custom roles can be edited or deleted based on the access given which later is explained in
Step 8
that shows how it can be set.
In the Create Role interface, there is 'Name', 'Clone from role' and 'description' field. The clone from role will enable you to select role from any existing roles and the permission for this new role will be set accordingly which later can be modified.
Step 6 : Role Detail Page
As soon as the role is created user will land on the Role detail page. User can navigate back to Manage Roles page by clicking it from top. Also, if you want to view list of users, then click 'View Users' link. Under this, all the modules and sub modules of DvSum application are displayed and from here, you can control permissions for these as shown below:
Note:
Please note that the access of Dashboard will always be enabled and cannot be changed. For instance; if some user has no access of any module or sub module in DvSum application then dashboard by default will always be visible.
Step 7 : Setting up Permission(s)
Let's say the role 'Access Module' that is just created, you turn off the switch for 'Manage Dashboard' and 'Advanced Analytics' for it. Click the Save button.
Step 7.1
Now any user that this role is assigned to will not have access to these two sub modules of Dashboard. Let's say, you select any user from Manage users section and assign this Role to it as shown below:
Note:
From here you can also verify that, the user role is set as 'Admin' but the permission of access will be preferred based on what role is set in 'Module Access Role'.
Step 7.2
To verify it, login from that user account and check the options right under Dashboard section. Those two sub modules will not be showing down there as shown below:
Step 7.3
If you switch off the main Module permission then that particular module will not be visible on main DvSum application menu. For instance; Administration tab is switched off for above user then on login from that account, it will be hidden as shown below:
Step 8Â  Manage Role: Viewing/Creating/Editing/Deleting Role
Furthermore, in this version v1.0 of manage access, we have enabled you to further set permissions of Viewing, Creating, Editing, and Deleting in 'Manage Role' under the Administration module. If you want to enable this role to be able to only view/create/edit/delete or do all of these, it can be done from here.
For instance; set the view permission in the Manage role for 'Access Module' and Save the changes as shown below
Step 8.1
Specifically, for Data Dictionary the Read/Write access can be done like this.
By selecting the view only access for Table DIctionary, it will allow user to only view all the details from listing view of table / column dictionary respectively. User can not make any changes from listing view pages e.g:
1.
Row Level Operations:
Mass Update, Hide/Unhide Rows, Refresh Relationships.
2.
Column Level Operations:
Inline editing for editable columns e.g:
a.
Table Dictionary:
Table Type. Table Entity, Data Domain, Subject Area, User Description & Comments.
b.
Column Dictionary:
Columnn Type, Entity Type, DQ Category, Sensitivity Level, CDE, User Description & Glossary Term.
3. Table Detail Page:
Publish/Unpublish, Overview section (User description), Stewards, Primary Attributes, Relationships & comments.
Step 8.2
Login from that user account OR if you have already logged in, just refresh the page. Select that role and you will see that above options do not get enabled. You will be able to only View it but cannot make any modification. That is the reason because we have restricted the access to modify it. Just like that you can set any other permission from Manage role i.e View, Edit/create/Delete.
Note:
This is for customer roles only not for the default roles (Admin, user, super user)
Step 9Â  Deleting Role
In case of Deleting a role, when you click Delete button then it will require you to transfer the control to any other role if that role has user(s) associated with it.
Note:
Once you transfer the role to a different role, keep in mind that permissions will also get updated based on new role assigned here.
Step 9.1
If any role does not have any user(s) associated here, then it will further ask you to confirm and then Delete the role as shown below:
Step 10 Access denied Message when accessing with in application from a page
To understand this, lets simply take an example. If you switch off 'Table dictionary' permission for this user.Â  Save the changes. Login from that account, click profiling and then click any table name.
Step 10.1
Since this user has no access to table dictionary, on clicking any table name, it will notify you that Permission is denied for accessing table detail page. If you click OK you will be redirected to Dashboard and if you 'Click here' then you will be redirected to main Profiling page.
Step 10.2 Access denied Message when accessing using URL
For example; you are not allowed to access Profiling module, if you copy and paste the URL "prod.dvsum.com/profiling" in new tab and hit enter then it will notify you that Access is denied
Step 10.3
If you click OK then it will redirect you to main Dashboard page as shown below:
Step 11: Other Associated Modules Permission and Permission Denied Messages
Here is the list of some other associated modules that will be operational same as above example.
Manage / Analyze Rules
Dashboard (Rule list widget)
Profiling (Table -> View Rules)
Profiling (Table -> Data Quality -> Anayze Rule)
Profiling (Table -> Data analysis -> Add business rule)
Profiling (Table -> Data analysis -> View business rule)
Table / Column Data Dictionary (Table detail -> Data Quality section)
Comply (Action Items Grid -> Rule Id)
Manage workflow (Add / edit task -> Rule Id)
Glossary (Term Detail -> Data policy -> Rule Id)
Table Dictionary - Table Detail
Profiling -> Table
Column Dictionary -> Dataset
Elastic Serach Results (Table Name)
Scheduler
Manage Sources
Profiling
Manage / Analyze Rules
Batch Execution
Data Prep workbench
Table / Column Data Dictionary (Table detail - > Data Preparation section)
Workflow
Glossary - Term Detail (Process Workflows -> Workflow Id)
Glossary - Term
Data Prep Workbench (Assigned asset columns -> Term)
Column Dictionary listing (Glossary Column -> Associated Term Name)
Elastic Serach Results (Term Name)

--------------------------------------------------------------------------------

================================================================================
TITLE: Schedule Cataloging
URL: https://dvsum.zendesk.com/hc/en-us/articles/360049346552-Schedule-Cataloging
================================================================================

The article below explains how users can schedule the cataloging process. Let's get started.
Catalog Scheduling
Step 1 :
Go to Administration -> Manage Sources
Choose a source and click the "
Schedule Cataloging
" button as shown below:
Note:
The scheduler catalog is applicable to SAWS version 2.4 and above.
The user will be directed to the Schedule page as shown below. The scheduler operates in the same manner as it does in the Dvsum tool.
Step 2 :
If you schedule a job and view the details in the Scheduler section, you'll notice a newly introduced Catalog icon. When hovered over, it displays as "Cataloging," as shown below:
Schedule Cataloging Email Notification
Step 3:
To customize the notification frequency or enable/disable scheduled cataloging notifications, visit your User profile. Click on Email Notification, then scroll down to find the "Scheduled Cataloging Email Notification" option, as shown below:
Note:
By default, this option will be enabled.
Step 4:
After scheduling a Catalog, the user will receive an email according to the set email frequency. Below is the template for the email:

--------------------------------------------------------------------------------

================================================================================
TITLE: User Access at DQ Rules Level
URL: https://dvsum.zendesk.com/hc/en-us/articles/360050817613-User-Access-at-DQ-Rules-Level
================================================================================

User Access at DQ Rules Level
Now, users and Super users can be restricted from using Edit functionality in Rule definition, Rule Summary, Users, Workflows, Script configuration, Column Sequence and Rule instruction tabs in Rule detail page. Only Admins, Owners or any authorized users will be able to Edit the rules. For that, this access is managed using the User Groups section. For getting started with it, let us dig into the detail bit by bit.
Step 1:
From Home page, click Administration section. In drop down options you will see a Manage User Groups option. This option is available for Admins and Owners to limit the access for users or super users at DQ Rule level. Select this option and Manage User Groups detailed page will be displayed. On this page you can Add, Edit and Delete Group(s) as shown below.
Step 2:
From there, click Add/Edit Group. An Add/Edit Group detailed page will be displayed, scroll down to bottom and there you will see a new section for
DQ
Rule
Access
is added. The access setting in this section will be applied to all exiting and new DQ-Rules that are present in application. It will have two options, View and Edit as shown below:
Scenario 1:
Edit Access at User/Super User Level
Step 3 :
Now on the same page, select user(s) or super user(s) that you want to limit or give the access to Edit the DQ Rule(s). For example, you select the Edit option from drop down for selected user(s). Save the option and now login into DvSum application using that user credentials. After login, select any rule to open rule detail page. Once rule detail paged is displaced, observe the following changes.
A- Rule Summary
User/Super user will be able to edit Rule summary as shown below:
B- Rule Definition
User/ Super user will be able to edit Rule definition as shown below:
C- Users
User/Super user will be able to remove or add user(s) as shown below:
D- Script Configuration
User/Super user will be able to edit Script configuration as shown below:
E- Workflow
User/Super user will be able to add workflow as shown below:
F- Rule Instruction
User/ Super user will be able to edit Rule instruction as shown below:
G- Column Sequence
User/Super user will be able to edit column sequence and save button will be visible as shown below:
Scenario 2:Â  Â View Access at User/Super User Level
Step 4 :
Now on the same page, select user(s) or super user(s) that you want to limit or give the access to View the DQ Rule(s). For example, you select the View option from drop down for selected user(s). Save the option and now login into DvSum application using that user credentials. After login, select any rule to open rule detail page. Once rule detail paged is displaced, observe the following changes.
A- Rule Summary
User/Super user will not see the Edit icon to edit the Rule summary as shown below:
B- Rule Definition
User/Super user will not see the Edit icon to edit the Rule definition as shown below:
C- Users
User/Super user will not be able to Add or Remove user(s) as shown below:
D- Workflow
User/Super userÂ will not be able to Add Workflow(s) as shown below:
E- Rule Instructions
User/Super userÂ will not be Edit in Rule instruction as shown below:
F- Script Configuration
User/ Super user will not be able to Edit Script configuration tab as shown below:
G- Column Sequence
User/ Super user will not be able to make any change in the column sequence tab. Save button will be hidden as shown below:
Scenario 3:
If a user/super is added in two different user groups and in each of those groups, user/super user has different access. For instance, user/super user in Group A has "View" access and same user/super user in Group B has "Edit" access. By business requirement, the least access which is only "View" access will be applicable to user/super user.Â  Therefore, user/super user will only be able to view the changes and won't be able to Edit anything.
Scenario 4:
If you select any user which has the role type "Admin/Owner" and you set the access as "View" for that particular user then by default as per Business requirement, it will still have the Edit access at the rule level. Therefore, this access limit will not be applicable on role type of Admin and owner.

--------------------------------------------------------------------------------

================================================================================
TITLE: DvSum Edge Gateway Installation (SAWS) for Data Quality
URL: https://dvsum.zendesk.com/hc/en-us/articles/360021928854-DvSum-Edge-Gateway-Installation-SAWS-for-Data-Quality
================================================================================

Overview
This document describes the installation process for the DvSum Edge Gateway. It includes everything needed for installation, such as required middleware and instructions for deploying the Gateway/Connector on your machine.
Note: Gateway, Connector, SAWS (Stand-Alone Web Service) and webservice are used interchangeably referring to the DvSum Edge Gateway.
System Requirements
Operating System
Windows XP, 7 - 10 or Windows Server 2008, 2012 and above.
Disk Space
1 GB (free)
Memory
4 GB
Browser
Google Chrome, Mozilla Firefox, Microsoft Edge, Apple Safari
Downloads
DvSum Data Quality Gateway Windows Installer
Network Configuration
If DvSum Edge Gateway is to be installed on a server where outside access is restricted by a firewall, you must white-list the following addresses so the gateway can communicate with the DvSum SaaS web application.
prod.dvsum.com port 443
email-smtp.us-west-2.amazonaws.com
Open port for gateway communication
Port 8183 (default port)
Network Diagram
Install System Prerequisites
Check and install Java Runtime (JRE)
1. On your machine, go to
Start
>
Run
> type "cmd" > "
Run as Administrator
"
2. Type "java -version" and hit enter. This will indicate what version of Java (if any) is installed.
3. If Java is not installed, please download and install required version of Java.
For other sources, download and Install
Java 8
Note: You may be asked to create an oracle.com account to download Java.
Install Microsoft Access database Engine 2010
(For EXCEL only)
1. Go to
www.microsoft.com
and search "
Microsoft Access database engineÂ 2010 Redistributable"
If 64 bit Java is installed on system make sure to install 64 bit Microsoft Access Database Engine 2010
If 32 bit java is installed on system make sure to install 32 bit Microsoft Access Database Engine 2010
For more details,
How to Configure EXCEL/CSV?
Install Python
Find the
Python 3.7 installation
files.
Download the relevant installer from the Files section (e.g. for Windows 64 bit, install Windows x86-64 executable installer)
Run the installer.
Select the checkbox "Add Python 3.7 to PATH"
Click on "customize installation"
Click next, confirm that "Install for all users" is checked.
The options pre-selected on this step may be left unchanged. Click "Install".
This will install Python on your system.
Confirm installations
Confirm that Java and Python are working well.
java -version
python --version
Install additional required Python libraries
To install python libraries, go to command prompt (
Start
â†’
Run
â†’ type "cmd" â†’
Run as administrator
)
Type (or paste):
pip install pandas XlsxWriter xlrd Flask beautifulsoup4 "SQLAlchemy<2.0"
This will install required Python libraries on your system.
For more details, read
Step-by-step Installation of Python
Install DvSum Edge Gateway
Download SAWS from DvSum
Log onto
https://prod.dvsum.com
Go to
Administration
â†’
Manage Account
â†’
SAWS tab
Next to the heading "Stand-alone web service information" click "Download". This will download "webservice.zip".
Unzip webservice.zip.
Optionally move the folder to a more convenient location.
This folder will be the webservice root directory.
On a machine dedicated to the DvSum Edge Gateway, this will typically be c:\dvsum\webservice or something similar.
Add SAWS to DvSum account
Go to
https://prod.dvsum.com
Go to
Administration
â†’
Manage
Account
â†’
SAWS tab
Click "âŠ•Add" to register a new Edge Gateway.
Name: any descriptive name
Host Name: enter either the hostname or the IP address where the Edge Gateway will run
Port: 8183 by default
Help: If installing the Edge Gateway on your own laptop/computer, enter the Host Name as 127.0.0.1
Update configuration.properties file with api.token
Go to
Administration
â†’
Manage
Account
â†’
SAWS tab
Copy the communication key.
Open configuration.properties in a text editor. You'll find this file in folder created earlier. For example: c:\dvsum\webservice\configuration.properties
Paste your communication key into configuration.properties as the value for api.token:
api.token=KnM15U6GsAcQtbXJMtQV51SRciAG8Het
Save configuration.properties.
For more details, read
Add and Install SAWS as a service
Install DvSum Edge Gateway as a service
Go to downloaded webservice root directory.
For example: c:\dvsum\webservice
Right click "saws_service_install.bat" run as administrator.
This will install the windows service "DVSUM SAWS" along with some closely related services.
Verify DvSum SAWS is running by
Start
â†’ type
Services
â†’ open Services â†’ scroll to "DVSUM SAWS" in the services list.
Client machine IP white-listing
In order to establish the communication channel between the DvSum Edge Gateway (SAWS) that you just installed and the DvSum SaaS Web Application, the end-user public IP must be white-listed in the DvSum Web Application.
Go to
Administration
â†’
Manage Account
â†’
Application Security
â†’ IP AddressÂ  White List
Enter the public IP address of the end-user machine. Click Add.
Add SAWS exception to your browser
Go to
https://prod.dvsum.com
Look at the SAWS icon on top right corner.
If the SAWS cloud icon is green, SAWS is successfully installed and running.
If the SAWS cloud icon is red, click on it.
This will open a window prompting you to add an browser exception for SAWS.
Click "Advanced".
Click "Proceed to [hostname or IP address] (unsafe)". You are connecting to the SAWS instance that you just installed. It is fully secure.
For more details, see
Why do I need to Add exception?
Now the SAWS cloud icon will turn green, and the SAWS installation is complete.
Copy SAP DLLS to c:/windows/system32
1. Go to webservice
root
directory > click on
Sap Dependencies
folder >
SAP JARS AND DLLS
For 32-bit operating system, click onÂ SAP 32bit jars and dll folder.Â Copy 32-bit sapjco3.dllÂ and paste in c:\windows or c:\windows\system32
For 64-bit operating system,Â click onÂ SAP 64bit jars and dll folder. Copy 64-bit sapjco3.dll and paste in c:\windows or c:\windows\system32
2. make sure folder address where dll has been pasted is added to system path variable i.e
right click on (
my computerÂ icon
or
Computer
or
this PC icon
) >
Properties
>
Advanced System Settings
>
Environment Variables
> find 'path' in System Variables.
If c:\windows and c:\windows\system32 is added in path leave as it is otherwise add following String in the beginning of path c:\windows;c:\windows\system32;
Here is a quick video on How to Install DVSUM Edge Gateway:
DvSum Edge Gateway Logs
The DvSum Edge Gateway maintains logs on the machine where it is running. This log file records all actions performed by SAWS. In case of an error, this log can facilitate troubleshooting to pinpoint the root cause of the issue.
The logs are found in the logs sub-folder. For example: c:\dvsum\webservice\logs
The server is configured to rotate logs daily. Rotated log files follow this naming convention: SAWS_2023-05-15.log.
The undated file,
SAWS.log
, is the latest.
SAWS Maintenance and Advanced settings
After the basic SAWS setup, some advanced configuration options are available. You can easily manipulate the settings from SAWS configuration file.
For details, read
SAWS Advanced Settings (Configuration.properties file)
SAWS Upgrade
When a newer version of SAWS is available on DvSum, you will see the notification for upgrade. It will update SAWS in one click.
For details, read
SAWS Update.
Other Articles Related to DvSum Edge Gateway (SAWS)
How does SAWS work?
Why is SAWS showing red cloud even though it is installed successfully?
Can I add multiple SAWS?
How to Schedule Jobs?
What are SAWS Alert notification controls?

--------------------------------------------------------------------------------

================================================================================
TITLE: Advanced Dashboards
URL: https://dvsum.zendesk.com/hc/en-us/articles/360052125833-Advanced-Dashboards
================================================================================

Dashboard Main Page:
When a user has a default Dashboard, then upon landing on main dashboard page, the default dashboard will be shown.
Note:
When a user creates an account for first time, by default, Advanced dashboard option will be enabled in user profile
If a user has more than 1 default dashboard then the first default dashboard will be selected and displayed
User can switch the dashboard from the list of shared default dashboards
When a user switches the dashboard, this newly selected dashboard will be set as default dashboard of the user.
User Groups in Share Dashboard
Now, in sharing feature of Dashboard, you will be able to add "User Groups/Users" as well as shown below;
User Groups in Create/Edit Dashboard
User will be able to share a dashboard with user groups and specific users as well from here. For admins/owners, when creating/editing dashboard, a checkbox will be shown to mark the dashboard as default as shown below:
Note:
Only admin/owner user will be able to make a dashboard as default
There can be no more than one default dashboard for one user group
Add New User:
If a new user is added from manage user and some group(s) are assigned to the user then the default dashboard(s) of groups will be assigned as default dashboard of the user.
If the user is not a part of any group then no default dashboard will be set.
Add New User in User Groups:
When a new user is made part of the group, theÂ default dashboard(s) of groups will be assigned as default dashboard of the user
Email and Bell Icon Notification On Sharing Dashboard
Upon sharing a dashboard, user(s) will be notified via Email as well as by bell icon notification as shown below;

--------------------------------------------------------------------------------

================================================================================
TITLE: History Trend Widget using Power BI Template Reports
URL: https://dvsum.zendesk.com/hc/en-us/articles/360049313234-History-Trend-Widget-using-Power-BI-Template-Reports
================================================================================

History Trend Widget - Dashboards
One of the requests from our clients, which added great value to the existing functionality for Dashboards, is to be able to see the Trend History Report by selecting the slicer 'ValGroup' as well. Previously, you were only able to see trend history widget report only on the basis of Rule ID as a slicer.
Now, we will have 3 different power bi template reports against each template widget. They will be different based on the slicer selection. The differentiation will be according to these slicers
Rule Id
Val Group
Val Group and Rule Id
1.1 Rule Id
On Add Widget form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.
Based on your selection as a slicer, as in this case it is 'Rule ID', you will be able toÂ view the Trend History Report.
You may then create a view where you can specify a criteria for a particular ValGroup. On saving a view, you will be navigated to Dashboard page where you can view the widget.
On clicking the widget, you will be navigated to a new tab where you would see the Report.
You will be seeing 2 different charts
- Average of RunResult by RunDate -
This will be the graphical representation of Run Result count (avg.) for all rules in the selected ValGroup. The time-series on the x-axis of the chart is on weekly basis. The count on y-axis is the Run Result count (avg.)
- RunDate -
This will be the list type that contains the following for each rule in the ValGroup.
- Rule ID
- Run Date
- Run Result
- Val Group
1.2 Val Group
On Add Widget form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.
Based on your selection as a slicer, as in this case it is 'Val Group', you will be able toÂ view the Trend History Report.
On clicking the widget, you will be navigated to a new tab where you would see the Report.
1.3 Val Group and Rule Id
On Add Widget form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.
Based on your selection as a slicer, as in this case it is 'Val Group, Rule ID Both', you will be able toÂ view the Trend History Report.
On clicking the widget, you will be navigated to a new tab where you would see the Report.
2. Color Persistence - Dashboards
Dashboard widgets, moving forward, will have same color across all widgets for specific data. Also, the color would persist across different widgets i.e if green color is assigned to â€˜Passedâ€™ run status, then it should be same across all the widgets.
2. Data Analysis (DAE) Rules - Creation From Manage Rules Tab
We have now given the flexibility to create DAE rule from Audit -> Manage Rules tab as well. Previously, they could only be created from Profiling -> Show Details -> Data Analysis

--------------------------------------------------------------------------------

================================================================================
TITLE: History Trend Widget â€“ Dashboards
URL: https://dvsum.zendesk.com/hc/en-us/articles/360045131774-History-Trend-Widget-Dashboards
================================================================================

On â€œAdd Widgetâ€ form, while selecting Rules History as a category, you will be able to select a template for Run Result or Readiness Score.
Based on your selection, you will be able to view the Trend History Report for a ValGroup.
You may then create a view where you can specify a criteria for a particular ValGroup.
Note: You can only create a view where selected ValGroup contains no more than 10 rules, else you will be notified to select a ValGroup having rules not more than 10.
On saving a view, you will be navigated to Dashboard page where you can view the widget.
On clicking the widget, you will be navigated to a new tab where you would see the Report for the selected ValGroup
You will be seeing 3 different charts
Average of RunResult by RunDate - This will be a graphical representation of Run Result count (avg.) for all rules in the selected ValGroup. The time-series on the x-axis of the chart is on weekly basis. The count on y-axis is the Run Result count (avg.)
Average of RunResult by RunDate and RuleID - This will be a graphical representation of Run Result count (avg.) for each rule in the selected ValGroup.
RunDate - This will be the list that contains the following details for each rule in the ValGroup.
** Run Date
** Run Result
** Val Group

--------------------------------------------------------------------------------

================================================================================
TITLE: Advanced Analytics - Integrating Power BI reports into DvSum
URL: https://dvsum.zendesk.com/hc/en-us/articles/360021231094-Advanced-Analytics-Integrating-Power-BI-reports-into-DvSum
================================================================================

We can create interactive reports using Power BI webapp tool and integrate them in DvSum for data visualization.
On Dashboards > Advanced Analytics tab, all reports will be visible. Upon clicking, it will open the detailed report with widgets. You can apply filters to the widgets and since they are all dependent, the data of all widgets will be refreshed according to the selected filter.
For each widget you will have a few more options like sort, have a spotlight on a certain widget.
You can also export data on your machine as an excel/csv file.

--------------------------------------------------------------------------------

================================================================================
TITLE: Workflow Digest Emails
URL: https://dvsum.zendesk.com/hc/en-us/articles/360021222614-Workflow-Digest-Emails
================================================================================

If you have turned on Workflow Email Digest from User profile for Daily or weekly, you will receive email with following information.
There are 2 sections in this email; Execution and Step summary will contain information of last 24 hours or last week depending on the setting you chose.
Workflow Execution Summary
Initiated Execution(s) - Executions that just launched
In-Progress Execution(s) - Currently taking place executions in which you are involved
Completed Execution(s) - Completed and closed by the day (or week)
CanceledÂ Execution(s) - Terminated executions
Workflow Step Summary
Submitted Step(s) - Steps which were completed by you today (or in current week)
Open/In-Progress Step(s) Steps which you are working on or have yet to work on
Re-assigned Step(s) - Steps re-assigned to you.
Rejected Step(s) - When an execution is rejected and a Step is assigned back to you.
Note: If there is no information regarding any steps or executions, you will not see the table. For example, if no steps were Re-assigned to you then this section will not appear.

--------------------------------------------------------------------------------

================================================================================
TITLE: DvSum export options for reports
URL: https://dvsum.zendesk.com/hc/en-us/articles/360007921813-DvSum-export-options-for-reports
================================================================================

DvSum supports export report capability for the following:
1. Dashboard (Views)
You can create customized dashboard with different criteria and filters. These dashboards appear in the drop down list. Here's how to proceed:
Select the dashboard you want to export
Click on export icon
An excel file consisting the data of all the widgets present in the selected dashboard will be downloaded on your computer
2. Audits (All rules)
To download the list of created rules, navigate to Manage Rules > More Actions > Export Rules.
3. Analyze Rules (Views)
You can create customized view of rules as follows:
Navigate to Review > Analyze Rules page.
From "Analyze Rules" drop down, click on "Create View".
Here you can apply specific criteria and choose columns to appear in a view.
Once the view is created, it appears in the drop-down list.
Select the view and click on "Export Results". An excel report of the selected view will be downloaded.
4. Exceptions report
Whenever a rule is executed and there are exceptions, DvSum generates an exception report which can be downloaded from the rule detail page. Go to Analysis tab and click on "Export".
5. Glossary Categories
Glossary category export downloads all the terms within that category. You can download multiple categories at a time. From Manage Categories page, click on "Export Results"
6. Assessment Report
DvSum provides the GDPR assessment report. You can take the
survey
and create a
data model
required to generate the
assessment report
.
Go to assessment report detail page. The "Download PDF" option will download the report.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Oracle as Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/23084323535124-Configure-Oracle-as-Source
================================================================================

Overview
This article describes the steps needed to configure Oracle as a source in DvSum Data Quality (DQ).
Detailed Steps
Oracle Configuration
Step 1:
Create a user to be configured in DvSum, if the user does not already exist.
Step 2
: Grant readonly access to the User for schemas and tables that you would like to catalog and profile. (The username and password will be used below when configuring the data source in DvSum)
Step 3:
Grant readonly access to the user for the following system tables:
ALL_USERS
ALL_OBJECTS
ALL_TABLES
ALL_SYNONYMS
ALL_TAB_COMMENTS
ALL_EXTERNAL_TABLES
ALL_TAB_COLS
ALL_COL_COMMENTS
ALL_CONSTRAINTS
ALL_CONS_COLUMNS
In order to grant access the below statement can be used:
GRANT SELECT ON (
TABLE_NAME
) TO (
USER
)
Reference Article
DvSum configuration
1. Add Source
To create a data source, navigate to Administration â†’ Manage Sources â†’ Add Source â†’ Select Oracle.
1.1 Basic Information
Add
Source Identifier and select the Webservice for the source.
To configure the web service, you can refer to this
Article
1.2 Host Information
Add the following information in host information section:
Host
Port
Database SID or Service Name
1.3 Credentials
Add DB login and DB password and click on the save button.Â  The source will be created successfully.
2. Test the Connection
The source will now appear in the Manage Sources Listing. Select the source and Click on Edit Source:
Click on the Source and in the bottom left corner click "Test Connection"
3. Catalogue the data source
Navigate to Manage Sources and click the "Run Cataloging" button. When the cataloging is completed, the status will change to "Tick Mark".
After the Catalog completion, click on Go to Catalog and it will open the Table Dictionary page of this source.
The Table dictionary page displays all the insights of the scan. It indicates how many new tables and columns are fetched in this catalog.
Your Oracle connection is now fully configured and functional.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Oracle ADW as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/18826674587796-Configure-Oracle-ADW-as-a-Source
================================================================================

In this article, we'll guide you through the process of leveraging DvSum's capabilities within Oracle ADW to transform your data into a valuable asset. Let's embark on this journey together and unlock the full potential of your data.
Adding Oracle ADW source in the DvSum tool
Step 1:
Open the Dvsum application, select Administration, and click on the Manage Sources option.
Click on Add Source, and Select Oracle ADW source as shown below
Step 2:
In the "Basic Information" section, provide the following details:
Source Name
Description
Select the web service on which the Oracle ADW source is enabled.
Other fields are optional, as shown below.
Step 3:
In the host information section, you will be required to provide:
TNS name: TNS names denote the available service levels and are accessible within the Database Connection Tab of the Autonomous Database Details.
Wallet Path: Absolute directory path of the extracted wallet folder located on the same machine as the SAWS.
Credentials:
Wallet Username:
The wallet username corresponds to the username of the database user.
Wallet password:
The wallet password refers to the password provided by the user during the download of the wallet zip file from the Database Connection Tab within the details of the Autonomous Database.
Step 4:
Click on Test Connection, and a success message will be displayed.
Now click on the save button.
Step 5:
Now Oracle ADW is added as a source, and the
user will be able to Catalog it, profile it, Execute Rules, and cleanse it.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Azure Synapse Analytics as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/12893404920980-Configure-Azure-Synapse-Analytics-as-a-Source
================================================================================

A data source refers to a specific location or system where data is stored and can be accessed. It can be a database, a file, an application, or any other entity that holds information. Data sources can be internal, such as a company's database, or external, such as a government website. The data from these sources can then be used to support decision-making, analyze trends, and perform other tasks.
Adding Azure Synapse Analytics as a Data Source:
To add a data source, you have to navigate to the â€œ
Administration
â€ tab on the left panel.
After clicking on the â€œ
Manage Sources
â€ tab, you will see â€œ
Add Source
â€ on the window as shown below in the screenshot:
Once you have clicked on
Add Source,
you will see all the supported sources by DvSum, you have to select â€œ
Azure Synapse Analytics
â€
After selecting â€œ
Azure Synapse Analytics
â€, you will be redirected to the configuration window where you have to enter the following details step by step as shown in the screenshot:
Step 1
: Enter Host
Step 2
: Enter Port
Step 3
: Enter Instance Name
Step 4
: Enter DB Login
Step 5
: Enter DB Password
Step 6
: Click on Authenticate
After entering the details, you need to click on â€œ
Authenticateâ€
. On Authentication, it should now prompt for adding the â€œ
Database
â€. After selecting the Database, the last step to successfully add the source is to Test the Connection. As shown in the screenshot below, you have to click on â€œ
Test Connection
â€.
Now that you have successfully added Azure Synapse Analytics as a source, it is time to Run the Cataloging. In the screenshot below you will see in the
Green
box â€“ â€œ
Run Cataloging
â€, it will take a few minutes to complete the cataloging of the source that you have added. You will also get a notification in the
Blue
box with the arrow on the top right corner after the cataloging is completed.
In order to view the cataloged source, you will have to go to the
Data Catalog
tab and select
â€œTable Dictionaryâ€
as shown in the
Red
box.
After the Cataloging is completed and you have received the notification, you can view the catalog in the Table Dictionary tab. In the screenshot below you can see the cataloged source details. It is now time for
Profiling
. In order to do that, you have to navigate to the
Profile
tab on the left panel and select â€œ
Profilingâ€
as shown in the
Green
box.
In the Profiling tab, you have to select the source and the table name respectively that you want to run the Profiler on. As shown in the screenshot below, you have to click on â€œ
Run Profiler
â€ in the
Green
box. It will take a few minutes to run the profiler, once completed you will receive a notification in the notification bar on the top right corner.
If you want to view the catalog, you can click on the table name or â€œ
View Catalog
â€ as shown in the
Red
box.
Congratulations, you have successfully:
Added Azure Synapse Analytics as a source.
Run the Cataloging.
Run the Profiler
Finally, this is how your Catalog will look like:

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Databricks as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/9830486369428-Configure-Databricks-as-a-Source
================================================================================

Azure Databricks is optimized for Azure and tightly integrated with Azure Data Lake Storage, Azure Data Factory, Azure Synapse Analytics, Power BI, and other Azure services to store all your data on a simple, open lakehouse and unify all your analytics and AI workloads.
Enabling Databricks Source in DvSum
Step 1:
Open the Dvsum application, select the Administration tab and click on, the Manage sources option,Â  Click on 'Add Source' and Select Data Bricks source'. Following error messages will be displayed if the DataBricks source is not enabled for users 'Owner' and 'Admin'.
Note
:
Only the owner is authorized to add a source.
Owner:
Admin:
Step 1.2
Owner will click the 'Manage account' link and gets redirected to this page from where the source can be enabled. On other hand, Admin will request the owner to get the source enabled for the account. Click the 'Saws' tab, select the saws, and click the 'Enable source' button.
Step 1.3
From the list of available sources, select DataBricks and click the
Upgrade
button as shown below
Step 1.4
On returning back in the SAWS tab, it will take some time to process and after that, Databricks Icon will appear in the enabled sources column which means that the source is successfully enabled as shown below
Scenario 1: SAWS Error
On upgrading, if there's any issue with SAWS, an error message will be displayed "Please check if your SAWS is working correctly".
Scenario 2:Â  Pending State
On upgrading, if any job(s) is running, it will go to a 'pending' state.
Adding Databricks source
Step 2.1
Open the Dvsum application, select 'Administration' and click on the 'Manage sources' option,Â  Click on 'Add Source' and Select Databricks source as shown below
Step 2.2
In the Basic information section, provide the source name, and description, and select web service on which Databricks source is made enabled, other fields are optional as shown below;
Step 2.3
In order to get the Server hostname, HTTP path, and personal access token go to Databricks Dashboard.
Step 2.3.1
Click on Compute >> Cluster name >> Cluster configuration >> Advance Options >> JDBC/ODBC. And Youâ€™ll get â€œServer Hostnameâ€ and â€œHTTP pathâ€.
Step 2.3.2
To get Personal Access Token, click on â€œUser settingsâ€. Add Name for Token and set Days limit for token and click on Generate.
Note
:
Make sure to copy the token now. You won't be able to see it again
.
Scenario 1: Authentication using Access Token
Step 1
Add server hostname, HTTP path and personal access token in Host information and click on Authenticate button.
Step 2
Database Name will be shown select the database from the dropdown and click on the save button.
Step 3
Edit the source and verify the â€œTest connectionâ€
Scenario 2: Authentication using Client Secret
~Prerequisites for Configuring Azure Databricks (
Service Principal Service
):
Please refer to the article to configure
Azure Databricks (Service Principal Service).
Step 1
For Authentication using
Client Secret
, enter the correct
Server Hostname
,
HTTP path
(
Server Hostname and HTTP path used for authenticating via Access Token will be the same here),
Azure Subscription ID
,
Azure Resource Group
,
Azure Workspace
,
Azure Tenant ID
,
Azure Client Id
,
Azure Client Secret
and
OAuth Secret
. Click the Authenticate button.
Note:
OAuth secret is a confidential key used to authenticate and authorize applications when integrating securely with external services. Admin can generate it from Service Principle's secret tab.
Note: OAuth Secret is a required field when the checkbox is enabled. Using OAuth secret, Databricks will use an updated driver which eliminates the need for an additional rule to wake up the cluster and scheduled jobs will be executed faster
Step 2
Database Name will be shown select the database from the dropdown and click on the save button and "Test Connection"

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Google BigQuery as a source
URL: https://dvsum.zendesk.com/hc/en-us/articles/4403282434324-Configure-Google-BigQuery-as-a-source
================================================================================

We continue to roll-out connectors to support bi-directional connection with Google Bigquery, where user can catalog, audit rules and the cleanse data without need for any external data pipelines. User will be able to add Google Bigquery as a source in DvSum and make customer's data insightful by Cataloging, Profiling , Executing rules and cleansing it.
Note:
To add Google bigquery as a source following are the required conditions to be fulfilled.
a) SAWS version should be 2.4.0 and above
b) Google bigquery as a source type should be enabled (explained below)
Step 1:Â  Enabling Google bigquery Source in DvSum
1
Open the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Google bigquery source. Following error messages will be displayed if the Google bigquery source is not enabled for users 'Owner' and 'Admin'.
Note:
Only owner is authorized to add the source.
Owner
:
Admin
:
1.2
Owner will click the Manage account link and gets re-directed to this page from where the source can be enabled. On other hand, Admin will request the owner to get the source enabled for the account. Click the Saws tab, select the saws and click
Enable
source
button;
1.3
From the list of available sources, select Google bigquery and click
Upgrade
button as shown below;
1.4
On returning back in SAWS tab, it will take some time to process and after that Google bigquery Icon will appear in enabled sources column which means that source got successfully enabled as shown below
Scenario 1 : SAWS Error
On upgrading if there's any issue with SAWS then an error message will be displayed "Please check if your SAWS is working correctly".
Scenario 2:Â  Pending State
On upgrading if there's any job(s) running then it will go in pending state.
Step 2 : Adding Google bigquery source in DvSum tool
2.1
Open the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Google bigquery source as shown below;
2.2
Once landed on source detail page, you will be required to provide following info to add Google bigquery as a source;
Choose and upload JSON file in
Service account key file
It will fetch
Project ID
Based on that
Data set
options will be populated in the drop down option
Note
: In order to set up service account, please follow this
link
:
2.3
After that Success message will be displayed and now click the save button;
2.4
Now Snowflake is added as a source and user will be able to Catalog it, profile it, Execute Rules and cleanse it.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Snowflake as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/360050674571-Configure-Snowflake-as-a-Source
================================================================================

We continue to roll-out connectors to support bi-directional connection with Snowflake, where user can catalog, audit rules and the cleanse data without need for any external data pipelines. User will be able to add Snowflake as a source in DvSum and make customer's data insightful by Cataloging, Profiling , Executing rules and cleansing it. Let's get started.
Note:
To add Snowflake as a source following are the required conditions to be fulfilled.
a) SAWS version should be 2.4.0 and above
b) Snowflake as a source type should be enabled (explained below)
Getting Started with Snowflake as a Source
Before starting the configuration process, ensure that query history is properly set up. Query history is essential for enabling seamless data lineage and effective data analysis within the application. For a detailed guide on enabling query history refer to the
Enabling
Query
History
for
Data
Sources
article.
Step 1:Â  Enabling Snowflake Source in DvSum
1
Open the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Snowflake source. Following error messages will be displayed if the Snowflake source is not enabled for users 'Owner' and 'Admin'.
Note:
Only owner is authorized to add the source.
Owner
:
Admin
:
1.2
Owner will click the Manage account link and gets re-directed to this page from where the source can be enabled. On other hand, Admin will request the owner to get the source enabled for the account. Click the Saws tab, select the saws and click
Enable
source
button;
1.3
From the list of available sources, select SnowflakeÂ and click
Upgrade
button as shown below;
1.4
On returning back in SAWS tab, it will take some time to process and after that Snowflake Icon will appear in enabled sources column which means that source got successfully enabled as shown below
Scenario 1 : SAWS Error
On upgrading if there's any issue with SAWS then an error message will be displayed "Please check if your SAWS is working correctly".
Scenario 2:Â  Pending State
On upgrading if there's any job(s) running then it will go in pending state.
Step 2 : Adding Snowflake source in DvSum tool
2.1
Open the Dvsum application, select Administration and click on Manage sources option,Â  Click on Add Source and Select Snowflake source as shown below;
2.2
Once landed on source detail page, you will be required to provide following info to add Snowflake as a source;
Host
Information
URL (will be used by user to access database. This link is given in the email that you will get on activating account for snowflake)
Database
Warehouse
Credentials
(Same as which are use to login into Snowflake account)
User
Password
2.3
After that Success message will be displayed and now click the save button;
2.4
Now Snowflake is added as a source and user will be able to Catalog it, profile it, Execute Rules and cleanse it.

--------------------------------------------------------------------------------

================================================================================
TITLE: Catalog Table Detail Page
URL: https://dvsum.zendesk.com/hc/en-us/articles/17457787570452-Catalog-Table-Detail-Page
================================================================================

Overview
DvSum DQ's Table Detail page provides an efficient look at table metadata, data, current data quality rules, suggested data quality rules, and more. Users can easily edit table details and manage rules, enabling seamless decision-making and data quality maintenance.
Sample View:
Finding the Table Detail Page
Data Catalog â†’ Table Dictionary:
Profile â†’ Profiling â†’ View Catalog:
Functionality
Run Profiling
Data profiling will query the data source to gather metrics about the data. It will also generate suggested DQ rules based on the data and metadata that it discovers.
Profile Table â†’ Run Online
This will promptly initiate the profiling process. A progress bar will display the ongoing progress. It's crucial to stay on the page during profiling, as it operates synchronously. Leaving the page will result in the cancellation of the profiling process.
Profile Table â†’ Run Offline
This will schedule a profiling job to run in the background. The job will typically start within the next minute, and this is the recommended approach for profiling large tables that may require several minutes to complete. You can navigate to other pages without affecting the job, and the updated statistics will be available once the job finishes. Check the job status here:
Review â†’ Scheduler.
The
Profile Table button will be
Yellow
if profiling has never run. Afterwards, it will change to
Blue.
Edit Table
Edit the table details by clicking on the
Edit Table
button.
The following fields can be edited:
Description
Table Type
Load profile
All Data - Select this load profile if data is fully refreshed regularly.
Incremental Data - Select this if data is normally loaded incrementally (appended).
If the load profile is set to incremental, then you should also set the Metric Time field.
Metric time - Select from the Data and Timestamp fields in the table. This field will be used in relevant DQ rules. For example, when calculating daily load volumes this field will be used to see which records were loaded in the last 24 hours.
Table Detail Page
The Table detail page primarily contains 4 sections as shown in the screenshot below:
Columns
Displays metadata (e.g. data types) and data (e.g. unique count).
Rules -Â Available Rules
Create new rules and review existing rules.
Select an existing rule to run it online or offline. For details refer to the article
Run Online/Offline.
More Actions:
Update Schedule -
update the existing schedules or add a new schedule.
Remove Schedule -
This option allows the user to remove any existing schedule of the rule(s)
Delete rule
- This option allows the user to Remove the rule permanently
Clone rule
- This option allows the user to clone the current rule
Mass Update
- Mass update allows the user to select multiple rules and update them at once
Rules - Recommended rules
Recommended Rules
These are the rules that are recommended based on the profiling of the source's tables
If the User wants to add a recommended rule they can simply click on the
ADD
button
The following rule types are
Recommended
:
Blank
Value Range
Count
Metric
Sample Data
This has info same as Profiling >> Data Analysis. The grid will have all columns that are added in that table and will display sample data for each column.
Relationship
It shows the relationship of the table between different tables, their cardinality/relationship types, Constraint/Key names, database status, recommended status, accepted status, and Zoom in/out. The rejected state will not be shown. If you click any table name from here then it will open up its table detail page separately.
View Comments
Users can view the comments and add a new comment

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Catalog
URL: https://dvsum.zendesk.com/hc/en-us/articles/360052123973-Data-Catalog
================================================================================

User will be able to see the data of all Sources/Columns/Tables in Data Dictionary section. There will be one enterprise dictionary for one account. Letâ€™s get started step by step.
A) Table Dictionary
B) Column Dictionary
C) Table Detail Page
Table Dictionary
Step 1
Click on Data dictionary and it will display sub item as "Table Dictionary" under it. Select this option and it will take user to detailed page of Table dictionary. On this page, all the table specific details will be displayed as below;
Source Name
Source type
Entity type
Table type
Table group
Data set
Data set description
User description
Record identifier exist
Record count
Profile status
No. of Rules
comments
Column Dictionary
Step 2
On the left side menu of DvSum application, click on "Data Dictionary" it will display sub item as "Column Dictionary" under it. Select this option and it will take user to detailed page of Column dictionary as shown below;
Note:
In Data dictionary page, only those sources will be visible that are connected and cataloged.
On this page, all the column specific details will be displayed as below;
Source
Entity type
Data set
Column Name
Column Description
Column type
Column group
Sensitivity level
Compliance Category
CDE
Data type
Length
Primary Key
Foreign Key
Check Constraints
Unique Constraints
User description
Profiling Info
Glossary Info
Step 3Â  Profiling Info and Distribution data
At the end of the grid view, you will see a field name as "Profile Info". If column is profiled, then a profiling icon will be shown. Otherwise it will not be shown. Refer to the screenshot below;
If user clicks the icon then it will show the profiling info for specific column or table that is being profiled. Also, if any source or column has distribution data info available then it will also be shown there on clicking the icon. Otherwise, only profiling info will be displayed i.e source name, table name, max value, min value, unique count and null count.
Note:
On default Data dictionary view page, data across all sources/Column will be displayed. Also, this default view cannot be shared or marked as favorite. The favorite and share icons on top right side will appear as disabled.
From here, following steps listed below are commonly used features in both Table and Column dictionary.
Step 4Â  Â Filters
Filters are also provide against all fields of columns. User can click on any field, apply filter and sort the data as shown below;
Step 5Â  Mass Update
Mass update will be applicable on Entity field. The purpose of this functionality is to be able to group the the different sources/columns available.
Select sources, click the Entity field, a new interface will open upfront. For the first time, create an entity, provide entity name and hit the apply button as shown below;
You will see that it gets mass updated against all the selected sources. User can apply filter on this field as well.
Step 6Â  Picker List
Now, user will be able to select an existing entity type from the picker list available in the Entity type field which are fetched from DB. Click on cell, picker list will appear that will contain all the available entities (already existing in DB) from which user can select.
Following are the picket lists:
1) Entity type
2) Table type
3) Table group
Step 7 : Hide/Unhide Rows
Now, in column dictionary, select the rows that you want to hide, click on the Hide Row(s) button and then rows will be hidden. In table dictionary, same functionality will be applicable on tables.
Click on Show Hidden Rows button on top right side, then the hidden rows will start appearing with slightly grayish background color to differentiate it. Select that row and click the Unhide Row(s) button given on top as shown below;
Note:
One hide/unhide Rows, the sequence of data won't change. After un-ride, the rows will appear in their current sequence from where they were hidden.
Following field is editable
User Description (Column + Table dictionary)
Step 8 : Drag/Drop and Copy/Paste
User can copy entity field from one cell and paste it into another cell by Ctrl+c and Ctrl+v. Also, drag and drop any entity type from one cell to another by selecting the cell(s) and pressing Ctrl+d.
Step 9 : User Description as Editable text field
This field is fully editable. User can delete, copy, paste, drag and drop text inside it.
Step 10Â  Add New or Clone Existing Data dictionary
User can also create a new Data dictionary as shown below;
User can also clone the default data dictionary as shown below;
Step 11Â  Cloning the Data Dictionary
On cloning detail page, provide name, select column fields, Specify criteria. In specific criteria section, user can also apply criteria for Entity field that will show the data w.r.t that particular entity. A 'Share with' section is also added to share this cloned Data dictionary with specific users or groups. Now, click the save button as shown below as shown below;
A cloned Data dictionary will be displayed w.r.t specified criteria. On the top right corner, Favorite and share icon will also become enabled as shown below;
Step 12Â  Fine Grain Control
User can create a view, add sources and give access to User, Super user and Admin. By default, Admin and Owner users will have complete access to the data of all sources added in a view. But for user and super user, data will be shown on column/table dictionary listing/detail page according to the user/super access given.
Step 13 Export View
Click on the "Export icon" to export records given in Table/Column grid (w.r.t record visibility on current page) as shown below;
Table Detail Page
Step 14
Dataset column is a hyperlink in column/table dictionary. By clicking the hyperlink, user will be navigated to Table Detail page.
Also, from
profiling page
, View Catalog on hover will appear as well as on top of the grid, it is added as button from where user will be navigated to Table detail page.
Step 15
On top it is displaying following;
Source name
Table/Column name
View comments and its count
Endorsing (Accept and Reject)
This is just dummy and not functional yet.
Mark Favorite
Publish/Unpublish:
Publish
(Unhide row)Â -
Button on Table detail page will appear as 'Publish' if this not published and its is hidden on Table/Column dictionary page. If user clicks on 'Publish' button, then it will automatically unhide the row on table/column dictionary page and text on button will be updated as 'Unpublished'. Similarly, if user unhides row from table/column dictionary page then on navigating to table detail page, text on button should be updated as 'unpublished'
UnPublish
(Hide row)-
Button on Table detail page will appear as 'UnPublish' if this is published and its is unhide on Table/column dictionary page. If you click 'UnPublish' button then it will automatically hide the row on table/column dictionary page and text on button will be updated as 'Publish'. Similarly, if you hide row from table/column dictionary page then on navigating to table detail page, text on button should be updated as 'Publish"
Then under this there are main navigation menus including
Overview
Fields
Sample data
Statistics
Relationships
Quality
Data preparation
Comments
Step 15Â  Overview
It has two parts;
System description: Defined at DB level and it cannot be Editable
User description : Defined by user just like at table/column listing page. It can be edited by clicking edit icon given.
Step 16Â  Fields
This represents that column names of table/column, Raw datatypes, data types, PK, sample value (min-max values), description (user description). Only Description field will be editable.
Step 17Â  Sample Data
This has info same as Profiling >> Data Analysis. The grid will have all columns that are added in that table and will display sample data for each column.
Step 18Â  Â Statistics
This section is same as main Profiling tab on Data profiling page. On clicking Distribution icon from here, it will open up column distribution data. Similarly, on clicking profiling pattern it will open up data pattern here.
Distribution Chart
Profiling Pattern
Step 19Â  Â Relationships
It shows relationship of table (whose table detail page is opened i.e CUSTOMER2) between different tables, their cardinality/relationship types, Constraint/Key names, database status, recommended status, accepted status, Zoom in/out. The rejected state will not be shown. If you click any table name from here then it will open up its table detail page separately.
View Types:
It has two views;
ERD view
Grid view
(Same information as ERD view). Action column will be functional. From here you can update their statuses.
Complexity Level:
It has two complexity types
Level 1 :
Main table (i.e CUSTOMER2) which has relation with 7 other tables as shown below;
Level 2 :
In case those 7 tables have relation with further other tables as shown below;
Step 20 Data Quality
It shows rule sets that are created on this table (CUSTOMER2) and this info will be as same as Analyze rule page.
Step 21Â  Data Preparation
It will show all datasets here that are created on this particular table/column.
Step 22Â  Â Comments
All comments will be shown here. If you click comments from top header, you will be directed down here. Also, you cannot delete other user's comment(s). You can delete your comment only.
Step 23Â  Stewards
You can add and delete stewards from here as shown below;
Step 24Â  Top Users
It will show list of users here based on most visited percentage.
Step 25Â  Primary Attributes
It will be as same as Mass update fields shown in table detail page. User can edit from here as well.
Step 26Â  Other Information
It will display same information as of Profiling detail page.

--------------------------------------------------------------------------------

================================================================================
TITLE: Blend Views
URL: https://dvsum.zendesk.com/hc/en-us/articles/360016874754-Blend-Views
================================================================================

Overview
A Blend View combines data from two or more tables of the same source. It's useful in cases the data model is normalized, and a data steward reviewing and correcting errors needs to see fields from multiple related tables. It can also be useful to aggregate data saved at different granularities in different tables.
Blend Views are available for most data sources, but the exact details will depend on the source technology. If your relational database technology does not support full outer joins, then you may not use a full outer join in your blend view--though DvSum will not prevent you from defining such a view.
Create a Blend View
Begin by selecting Profile â†’ Profiling â†’ "âŠ• Create View" (visible in the upper-right corner of the page)
In the
Create View
wizard, specify the View Type as shown below.
In Step 2 of the
Create View
wizard, select 2 or more tables. To create a join between them, click and drag from one table to the other.
The Join configuration pop up will provide you the option to select the join type and the columns. DvSum supports join types of inner, outer, left, or right joins, but you should confirm that your relational database technology supports your selected type. DvSum intelligently guesses which columns should be used for the join, but you may add columns or alter the columns used.
Once you save the join configurations, the translated join condition will appear.
Advanced Option:
The FUI interaction provides a simpler user experience in most cases. But an advanced user can provide the Join Query. As soon as you start typing query in the "Translated Join Condition" box, DvSum will try to determine the tables for you and automatically reverse engineer the specified join into graph above.
You may not see a success message. Sometimes DvSum will not be able to translate the join condition you have provided into a graph. It does not mean there is necessarily a mistake. If you are confident your join condition is correct, then these three steps will suffice to define the joins:
Select the Join Tables.
Enter the Translated Join Condition.
Validate the join condition.
In Step 2 of the
Create View
wizard, the fields of all tables will be displayed on the left. Select the desired fields for your view.
Save the Blend View.
The detail page for this view will open. From here chose "Run Profiler" to populate field values. You may use this Blend View as you would use any other table.

--------------------------------------------------------------------------------

================================================================================
TITLE: Reference Dictionary
URL: https://dvsum.zendesk.com/hc/en-us/articles/360005669534-Reference-Dictionary
================================================================================

A Reference Dictionary is like a pre-organized reference book where users can create their own dictionaries, associating useful information with sets of values. This is helpful for DvSum validation audits, allowing users to choose a reference dictionary with a set of data values instead of listing individual ones.
For example, a user might create a "Hired Personnel 2017" dictionary with names and hiring dates of employees who joined in that year.
When setting up a "Value Range" audit to ensure only hired personnel are included in a column, you can easily select the reference dictionary you've created, rather than manually entering the range.
When you run the audit, it checks if your data matches the values in the reference dictionary you selected.

--------------------------------------------------------------------------------

================================================================================
TITLE: Creating and editing a Catalog View
URL: https://dvsum.zendesk.com/hc/en-us/articles/115005116733-Creating-and-editing-a-Catalog-View
================================================================================

Data profiling provides you the basic information summary about a particular table of source which might not be enough for effective profiling and rule definition. Whereas, Catalog view generates customized information from multiple tables of sources. You can personalize these views depending on how you want to evaluate the data. That data is shown in the form of a table.Â You can then run profiling on that table just as on any other table of a source.
From Profile tab in the left Navigation, go to profiling and click on "Create View". There are 3 steps in creating a view.
Step 1:
Enter View Name, type, source and table name.
Read here to see
how to select the right Catalog View Type?
Step 2:
Now you can apply Conditions to filter the data in the required form.
Step 3:
Now you can select the table fields of your source. Only these selected fields will appear as columns of catalog view table.
You can change the name of the columns as well from the Fields Information section. That is it. Now you can "Save" the view.
This will take you to the View detail page where you can profile or create rules on this view.
Edit View
If you want to edit any view, you will go back to the Profiling page and select the check-box for the view. Go to "More Actions > Edit view"
This will open the Edit View dialogue where you can make changes and save.

--------------------------------------------------------------------------------

================================================================================
TITLE: Review Profiling Results
URL: https://dvsum.zendesk.com/hc/en-us/articles/202846854-Review-Profiling-Results
================================================================================

You can access Data Profiling from Left Navigation, and then selecting the Source and Tables you want to view, of simply selecting a View from your history that is automatically displayed. Profiling information is also available when reviewing nodes in the Data Exploration section.
The initial Table detail is a good view to identify basic data exceptions such as Blanks, or Values that may be out of the expected or desired range. As outlined above, Rules can be created within Profiling by clicking on the Data Quality and Data Analysis tabs.
Selecting the specific Column Name displays the distribution of the data to enable further analysis.
Selecting the specific Column Name displays the distribution of the data to enable further analysis.

--------------------------------------------------------------------------------

================================================================================
TITLE: Run Online and Run Offline
URL: https://dvsum.zendesk.com/hc/en-us/articles/17472157533076-Run-Online-and-Run-Offline
================================================================================

Overview
Users can execute many different tasks in DvSum Data Quality (DQ). Testing a DQ rule might take just a few seconds. But some rules could take several minutes or even longer. Therefore DQ offers the ability to "
Run Online
" and "
Run Offline
" to give users the flexibility to run jobs as required.
Executing a rule synchronously and waiting to see the results is often the most efficient way to test a new rule. Executing a long-running job in the background and reviewing the results later is sometimes preferred.
Details
The option to "
Execute Now
" or "
Run Offline"
applies in several places.
Source cataloging
Table profiling
Rule execution
The options are complementary and give developers the flexibility to work as they prefer.
Run
Online
- Will immediately execute the action. Note that if the user navigates to another page while the task is executing, then the task will be aborted.
Run Offline
- Will schedule the action to run soon. Typically the task will begin execution within a few seconds. The job will be visible in the
Scheduler
tab (Review â†’ Scheduler).
Source cataloging
Administration â†’ Manage Sources
Table Profiling
Table profiling can be executed from multiple places.
Data Catalog â†’ Table Dictionary
Profile â†’ Profiling
Table Detail Page:
Rule Execution
Audit â†’ Manage Rules
Rule Detail Page:
Review â†’ Analyze Rules

--------------------------------------------------------------------------------

================================================================================
TITLE: DQ Rule Details
URL: https://dvsum.zendesk.com/hc/en-us/articles/17468996404756-DQ-Rule-Details
================================================================================

The latest update to our rule management system introduces offline and online execution capabilities, providing users with greater flexibility. The alert status of each rule is displayed as either "Healthy" or "Alerting" based on specified thresholds. Users can now easily edit rule definitions, descriptions, priorities, and scope, including window types and lookback days. Data aggregation into buckets enables detailed data quality checks. Various threshold types allow tracking and alerting based on exceptions. Rule notifications, action statuses, and scheduling options have been enhanced for improved rule management. The new history tab provides execution charts and grids for comprehensive performance analysis. Overall, these updates enhance rule customization, monitoring, and analysis.
Some of the major changes that have been implemented in the User Interface are highlighted in the
the screenshot below for a quick review.
The Rule Detail page has undergone significant changes, both in its interface and the introduction of new functionalities. Let's explore these enhancements in detail:
Not Scheduled
-
A notification at the top of the page is shown which indicates whether the rule is scheduled. If the rule is not scheduled it will not run automatically.
Alert status
for the rule will be shown as
Healthy
- if the metric value is within specified threshold limits, then the alert status will be Healthy
Alerting
- If the metric value is not within specified threshold limits, then the status will be Alerting
Run
Online & Offline rules can be run directly from the Rule detail page
For Details please check the article
Run Online/Offline
Action
Users can set the Action status for non-cleansable rules using the Action button. The action status button will be available for all rules, but in data cleansing, the user can enable workflow from the workflow tab and there won't be an action button since we are tracking using workflow.
The
Action
button has the following statuses:
New -
When the rule is created, default status is set to New
Acknowledge
â€“ When the user acknowledges the rule execution/ exception details
Resolve
â€“Â Once the exception has been resolved, the user has the option to perform an action and mark it as resolved upon completion
Track action item
More
Update Schedule
- The User can now have the ability to update the existing schedules of the rule and able to create a new one.
Remove Schedule
â€“ This option allows the user to remove any existing schedule of the rule(s)
Delete
- This allows the user to permanently remove Rule
Clone
- This allows the user to clone the current Rule
Edit
This button will allow the user to update the following:
Overview
Rule Description
: Users can update the description for the rule
Priority
: Users can update the priority for the rule
Open Rule Definition:
Users can directly open the rule definition page from here
Scope
If the
Metric Time
field is selected on the table level. Selected field name will be inherited to the rule as well
Otherwise, the user can also select / update the metric time field at the rule level as well
Users can set the window type: - Used to define what is the scope of data to be selected.
All Data
- By Default all the rules will window type as All data, which will consider all the data in the table during theÂ  execution of the rule
Data Max Time
- In the case of incremental data, we can choose this option to run validation only on newly added data based on the timestamp available on the table
Clock Time
-Â  In the case of incremental data, we can choose this option to run validation only on newly added data based on the current timestamp available on the table
Users can set the
Lookback days
for Data max time & Clock time
Users can optionally aggregate data into
Buckets
and DQ checks will be performed on each bucket.
No bucket
1 Day
1 Hour
Users can select available
Slicer
options
Threshold
For the Metric type, we can choose whether we want to track and alert based on the number of exceptions or percentage of exceptions.
No Threshold
- Metric will not alert.
Constant
- Metric will be compared against constant thresholds.
User can set the Upper bound and Lower Bound
Relative
- Percentage change (increase or decrease) in metric compared to the previous bucket or execution. For DQ exception checks, the decrease will not alert.
User can set the percentage for the relative threshold type
Adaptive
- Thresholds auto-adjust based on observations using outlier detection techniques. It uses the Interquartile range technique to detect if the metric is an outlier.
User can choose the threshold bounds from the 3 available options:
Upper and Lower
Upper
Lower
Notifications
Assign a rule to the user - The user is able to assign a rule to any user
Add a rule group to the rule - The user is able to add a rule group to the current rule
Add/update the schedule for a rule -A new functionality is added to manage the scheduling of the rule
On the Lower Section of the Rule detail page, the user will be able to see only the 3 tabs by default:
One important change is the modification of the first tab in the lower panel:
Existing
: Currently the first tab is called the "
Analysis"
tab
New
: In the new update we have replaced the "
Analysis"
tab with "
Data
"
Users can click on
Show More
to view the other tabs available:
After clicking on the
Show More
tab, user will be see the following tabs and if the user wants to Hide them they simply need to click on
Hide Others
History
The history tab will show the execution history for the rule in the form of a chart
Existing:
In the existing case the Trend was shown in the Insights tab as shown in the screenshot below:
New:
A new tab has been introduced at the bottom of the Rule detail page named "
History
" which now shows two views that are as follows:
Chart View -
In this view, the data is shown to the user in the form of a chart
When I select the Slicer, the data in the History Chart will get distributed based on Slicer Field
Grid View -
In this view, the same data is shown to the user in a tabular form
Users can view execution history for:
Current
: It will show the metric value for the current execution
30 Days
: It will show metric value for 30 days
90 Days:
It will show metric value for 90 days
ALL
: It will show the metric value for All data

--------------------------------------------------------------------------------

================================================================================
TITLE: Rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/17457922936468-Rules
================================================================================

Introducing a range of exciting updates to our rule management and analysis features. We are thrilled to enhance your experience by introducing offline rule execution capabilities and expanding the Action Grid Menu with more options. Users can now run rules both online and offline, ensuring continuous rule execution even in disconnected environments. The updated menu offers functionalities like updating and removing schedules, deleting and cloning rules, and performing mass updates. Additionally, the system now highlights recommended rules, providing improved visibility. These updates empower users to efficiently manage and analyze rules, enabling better decision-making and data quality control.
Manage Rules
The Manage Rules page has undergone significant changes, both in its interface and the introduction of new functionalities. Let's explore these enhancements in detail:
In the User interface, the following changes have been introduced:
Available and Recommended Rules
There are 2 options regarding the rules:
Available Rules
- These are the existing rules that are created and can be run
Recommended Rules
- These are the rules that are recommended based on the profiling of the source's tables
Alert Status -
This column has been added in the new release to show the alerting status of the rule based on the threshold limits
There are 2 alerting statuses:
Healthy
- if the metric value is within specified threshold limits, then the alert status will be Healthy
Alerting
- If the metric value is not within specified threshold limits, then the status will be Alerting
New Rule -
Users can create new rules based on their requirements from the following rules available
Foundational DQ
Pipeline Checks
Cross-System DQ
MDM
Data Diffs
Run Rule
Run
Online
- This feature will immediately execute the reference action
Run Offline
- This feature will schedule the reference action to run ad-hoc and will be
For Details please check the article
Run Online/Offline
More Actions
Update Schedule
- The User can now have the ability to update the existing schedules of the rule as we can able to create a new one.
Remove Schedule
- This option allows the user to remove any existing schedule of the rule(s)
Delete rule
- This option allows the user to Remove the rule permanently
Clone rule
- This option allows the user to clone the current rule
Mass Update
- Mass update allows the user to select multiple rules and update them at once
Analyze Rules
ReRun Rule
Run
Online
- This feature will immediately execute the reference action
Run Offline
- This feature will schedule the reference action to run ad-hoc and will be
More Actions
Update Schedule
- The User can now have the ability to update the existing schedules of the rule as we can able to create a new one.
Remove Schedule
- This option allows the user to remove any existing schedule of the rule(s)
Delete rule
- This option allows the user to Remove the rule permanently
Clone rule
- This option allows the user to clone the current rule
Mass Update
- Mass update allows the user to select multiple rules and update them at once
Users can
Export
grid data - This allows the user to export the current grid
Favorite
- This allows the user to mark the current view as favorite
Share
the view - The user can share the current view with any user
More
:
Edit
- Allows the user to edit the current view settings
Clone
- Allows the user to clone the current view
Delete
- Allows the user to permanently remove the current view
Alert Status -
This column has been added in the new release to show the alerting status of the rule based on the threshold limits
There are 2 alerting statuses:
Healthy
- if the metric value is within specified threshold limits, then the alert status will be Healthy
Alerting
- If the metric value is not within specified threshold limits, then the status will be Alerting
Workflow Status
A workflow status column has been added to the Default Analyze Rules Page

--------------------------------------------------------------------------------

================================================================================
TITLE: Creating a CUSTOM QUERY DQ Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/15714519528980-Creating-a-CUSTOM-QUERY-DQ-Rule
================================================================================

Custom Query Summary
The "CUSTOM QUERY" Rule Type is a powerful feature that empowers users to apply custom logic using a SQL query. This query should be a SELECT statement designed to identify invalid records.
Details
Preparation
Outside of DvSum, users must create a SQL query to find invalid records. For instance, in a data model where STUDENTS can have multiple STUDENT_ADDRESSES, the query should identify cases where students have more than one address marked as primary (IS_PRIMARY set to true).
/* query identifying students with too many primary addresses */
select s.STUDENT_ID, GIVEN_NAME, FAMILY_NAME
from
DEMO_DB.EDUCATION.STUDENTS s
left outer join DEMO_DB.EDUCATION.STUDENT_ADDRESS sa on ( sa.STUDENT_ID = s.STUDENT_ID )
group by
s.STUDENT_ID, GIVEN_NAME, FAMILY_NAME
having
sum(case when sa.IS_PRIMARY then 1 else 0 end) > 1
Steps
Create the rule
Audit â†’ Manage Rules â†’ âŠ•Add Rule â†’ Process Quality â†’ CUSTOM QUERY
Define the details
Define the Rule Description.
Select the relevant Data Source and Table Name.
Paste in the Custom Query.
Set the Threshold Min and Max to 0.
Save.
In some advance cases you may use other thresholds. When the rule is intended to find all records in error, use zeros.
Run the rule
Initially, the summary information will be mostly empty.
Click "â–ºRun" in the top right corner.
After running the rule once, the summary information will contain more details.
View the errors
If your query found any errors, then these errors will be visible on the Analysis tab.
The fields available in Exceptions Deep Dive are determined by the fields returned in your query.
Next steps
Your CUSTOM QUERY Data Quality rule should now be working. Common next steps include:
Enable writeback
in the rule to allow data stewards to fix error records.
Set Rule Groups for this rule.
Schedule the rule to run regularly.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Create/Assign/Remove Rule Group or Val group & Batch Scheduling
URL: https://dvsum.zendesk.com/hc/en-us/articles/13070366457620-How-to-Create-Assign-Remove-Rule-Group-or-Val-group-Batch-Scheduling
================================================================================

What are Rule Groups?
Rule Groups serve as categorizing "labels" for organizing your audits into meaningful groups. By applying a Rule Group label, you can categorize your Audits according to your desired grouping. Unlike folders, Rule Groups allow for multiple labels to be assigned to a single audit and vice versa, where an audit can belong to multiple Rule Groups.
How to Create a new/ Assign existing Rule Group?
In case you want to add a new Rule Group or Assign an existing one to your Audit you can do that from multiple places.
1. Rule Detail Page
Adding a New Rule Group
Select any rule, Go To the Rule detail page, Click on the Edit Button, and navigate to the Notification Section. In the screenshot below you can see the option of "Rule Group". You can simply type in the name you want to give to the new Rule Group and press enter and the new rule will be added and assigned to your audit after you have saved it.
Adding single or multiple Existing Rule Groups
In case you want to add single or multiple rule groups that already exist to a new Audit you can simply select them from existing rule groups from the dropdown as shown in the screenshot below.
2. Mass Updating Rule Groups
Mass update is another way of adding/removing and replacing single/multiple Rule groups to a single or multiple Rules. There are multiple operations that you can perform with Mass Update as follows:
1. Adding one or multiple new Rule Groups to a single Rule
2. Adding one or multiple existing Rule Groups to a single Rule
3. Adding one or multiple new Rule Groups to multiple Rules
4. Adding one or multiple existing Rule Groups to multiple Rules
5. Replacing/ Removing one or multiple Rule Groups from a single or multiple Rules
Mass update operations can be performed from both Manage Rules (For Legacy Accounts only) and Analyze Rules:
Manage Rules:
Analyze Rule:
Batch Execution
Batch execution is an operation that allows a user to Run a single or more Rule Groups at the same time.
Details of the Run History are shown at the bottom of the page.
Execute Via a Schedule
You can create a
Schedule
by following the steps:
Select Group -> Add group -> Schedule Rule
Next, you need to enter the following details if you want to share an email alert:
1. Internal Recipients
2. Recipient Groups
3. External Recipients
4. Select the Repeat configuration from dropdown. (One-time/ Daily/ Weekly/ Monthly / Monitor)
5. Select the start date & time for the schedule
6. Click on the Save button in the bottom right corner:
Note
: "
Send schedule completion email" will send an email on job completion
"Send email only on alerts" will send email only rule is
Alerting
Execute Via a Script
You can also schedule them
Via a Script
by following the steps:
Select Group -> Add group -> Schedule Rule
Next, you need to enter the following details if you want to share an email alert:
1. Internal Recipients
2. Recipient Groups
3. External Recipients
4. Click on Generate Script in the bottom right corner:
Note
: "
Send schedule completion email" will send an email on job completion
"Send email only on alerts" will send email only rule is
Alerting
You will now receive the
Script information
and
Dynamic Source Script
in the respective blocks:
You can copy the script and execute it to run the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Executing the rule API via ADF
URL: https://dvsum.zendesk.com/hc/en-us/articles/10311781361556-Executing-the-rule-API-via-ADF
================================================================================

Rules in dvsum can be executed using the azure data factory. To run a rule or set of rules through a script, you can generate a script for those rules using the Schedule Rule Menu.
The script takes the form of
https://<saws
address>:<saws_port>/runJob?jobId=<jobid>
Users can generate the script for the rule and use the script URL to execute the rule. Let's get started step by step:
Step 1:
Open the Dvsum application, go to Audit >> Manage Rules >> select any existing rule >> click More action >> select Schedule Rule as shown below:
Step 1.1:
Dynamic Source Script Generation
Go to Generate Script tab and click on the button at the bottom to generate the script as shown below;
Step 1.2:
A job is created against the rule. Now go to the main scheduler page, search the rule, and a newly created job will be visible with a description and status as â€œstand byâ€.
Step 2:
Go to Azure Data Factory and paste the copied URL in api call. Click on Debug button to start the execution.
Step 2.1:
Once the execution is completed. it will print the remarks in API call output same as shown on the rule detail page in Dvsum.
On the Scheduler page, the job status changes to "Completed".
The return_code always contains 3 values
0Â  -> PASS if a rule has 0 exceptions or exceptions are less than the lower bound of the threshold
-1 -> WARN if the rule has exceptions but exceptions are greater than the lower bound but less than the upper bound
-2 -> FAIL if the rule has exceptions greater than the upper bound limit.
The return_remark contains text that will provide the name of the rule and additional information that you typically see in the system remark field on the Rule Detail page.
Related Articles:
Databricks as a Source
Integrating Rules into batch workflow

--------------------------------------------------------------------------------

================================================================================
TITLE: Execute/run Scheduled Jobs
URL: https://dvsum.zendesk.com/hc/en-us/articles/115000205853-Execute-run-Scheduled-Jobs
================================================================================

The jobs that are scheduled using DvSum Web Application are executed by SAWS. ItÂ continuously interrogates the web application using the API token to check the readiness or state of the scheduled jobs that are to be executed.
There are two prerequisites for running the scheduled jobs.
API token needs to be added in the configuration file when SAWS is downloaded on a system. You can ask your administrator to provide you the API key. (If required, please refer to
How to install and Configure SAWS file
)
Your IP address needs to be white listed as DvSum Application blocks connections from remote locations.

--------------------------------------------------------------------------------

================================================================================
TITLE: File upload to DvSum and creating rules on file
URL: https://dvsum.zendesk.com/hc/en-us/articles/360014113613-File-upload-to-DvSum-and-creating-rules-on-file
================================================================================

File upload requires user to create a folder where a file can be dropped. The idea behind file upload is the reuse-ability of rules on a dataset. In a real life scenario, a user uploads a file and create a source in DvSum which uses that file. That file is then profiled and certain rules are made and executed on it. The report generated by these rules provides important stats to make day-to-day business decisions.
When the data in the file changes, a user simply uploads the new file to the folder and runs the same rules again. This saves user the time of repeating the process of profiling and creating rules again.
Note:Â This folder should be in the same machine as the SAWS associated with it.
Follow the steps below to upload file and get started:
Step 1: Create Folder
From Administration tab, go to Manage Account > Folders and add a folder. You need to specify folder name, folder path and the associated SAWS.
Once a folder is added, it will show on the grid and on the
file upload page
.
Step 2: Upload File
File Upload page shows the folders added by all users. If you hover over the folder you created, it will show you the path set for it. Simply add the files in drop-zone and they will be uploaded to the folder. (only excel, csv and txt files are allowed)
Step 3: Add File as Source
Now you can create a source using this excel file from Administration > Manage Sources page.
Specify the path for where the file is (This could be remote location as well where remote SAWS is running). After saving, "Configure" the source and it will open all sheets of excel file.
Now that the source is added, you can profile it and create DvSum rules on it.
Advanced Option:
If you want to apply preprocess properties to file in order to cleanse it before creating rules on it, read the article
Excel Cleansing Upon File Upload
.

--------------------------------------------------------------------------------

================================================================================
TITLE: Excel Cleansing Upon File Upload
URL: https://dvsum.zendesk.com/hc/en-us/articles/360013799514-Excel-Cleansing-Upon-File-Upload
================================================================================

EXCEL Default Cleansing
When a file is uploaded, it is automatically cleansed to make it compatible for DvSum use. DvSum always reads the first row as "header" and rest of the rows as "data".
The default cleansing includes:
Removing blank rows
Replacing white spaces within a column name with underscore e.g. First Column will be changed to First_Column
Special characters from column name are removed
Duplicate columns are dropped
However in some cases, the default cleansing is not enough and the user needs to explicitly specify if any rows needs to be skipped or specify the format for number columns where the data automatically truncates the values up to required decimal precision, for that purpose, a properties file associated with every uploaded file is available for user to edit.
EXCEL Pre-process properties
When an excel file is uploaded to folder, properties file is created with the same name as the uploaded file name. A user can manually set the pre-process options to change file formatting. So whenever the same file is uploaded again to the folder, DvSum will automatically apply the pre-process options and cleanse the file accordingly.
For example, below is the un-processed file.
1. There is some extra information in row 1, 3 and 4 which cannot be read by DvSum. User manually needs to inform DvSum to skip these.
2. Any spaces in between column names will be replaced by underscore and special characters will be removed automatically.
3. Any blanks rows within the data will be removed automatically.
4. For number fields, User can inform DvSum to cleanse the values up to a specific decimal precision.
Once this file is uploaded to folder, basic cleansing (as mentioned before) will be applied by DvSum and pre-process properties for upload file will be created.
Open the pre-process properties file with the name as the original file (AES2test)
Save the properties file. An important step is to upload the file again to the target folder. This will format the excel source file according to the changes made in preprocess properties.
Now the file is cleansed and ready to use in DvSum. If you configure this excel source from Manage Sources page, it will show you the cleansed file.
Validate JSON - Verify syntax of pre-process.properties file
As a good practice, we recommend you to validate json of preprocess file to make sure it has no syntax errors. You can validate using
https://jsonlint.com/
Copy all the content of preprocess.properties file and paste it in text field of jsonlint.com and click on "Validate JSON". If there is no syntax error, it will say Valid JSON.
If JSON is invalid, it will show you the line where there is an error. Here, you can see there is error on line 11 because there is semi-colon (;) added instead of comma (,)
It is recommended to validate json every time you make changes to your pre-process file.

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Preparation Workbench
URL: https://dvsum.zendesk.com/hc/en-us/articles/360035713813-Data-Preparation-Workbench
================================================================================

Introducing
DvSum
Data Preparation Workbench
, a place where you can access data from source conveniently, prepare and perform cleansing actions and commit validated data back to source. Let's start.
On your DvSum homepage, you will see Data Preparation in left navigation. Go to "Datasets"
You can create multiple datasets on a single table. With every dataset you create, the idea is to filter some part of data for analysis that is according to your requirements.Â That filtered data will open in a workbench.
Note: For now, Data Preparation Workbench functionality is only applicable for
Oracle and SQL Server
sources only
Create Dataset
There are 3 simple steps to create a dataset.
Step 1
- Click on Create button and provide source and table name on which dataset is to be created.
Step 2
- Choose Columns and Apply filters (if required)
Here, you can choose the columns you want to see on workbench and the order by which you want to view them on Workbench. You can also apply filters to dataset according to your requirements.
Step 3
- Provide a Unique Identifier, Sync All Dataset User Views (optional), Execute Stored Procedure(optional) and Sharing.
1. A "
Unique identifier"
of the table needs to be selected.
2. "
Sync All Dataset User Views
", will update and sync all the views that you have created on the workbench that are linked.
3. "
Execute Stored Procedure
", this will execute stored procedure on commit or on consolidate, write procedure including parameter name.
3. If you want to "
Share"
this dataset with others, select the required option.
This is it. You will see the created dataset on the dataset page and when you click on the name, it will open a workbench fetching the data accordingly.
Workbench Operations
There are multiple operations you can perform on workbench. Here is a list of few:
1. Inline Editing
Click on a cell and start editing. The changes will automatically be saved and you can commit the changes to write-back to source.
2. Drag And Copy
You can select a cell and drag cell-selection outline to the place where you want the copy to appear and release the mouse button. Pressing CTRL + D will copy the value to all selected cells.
3. Find and Replace
This is a bulk-update operation performed on a single column. You can even choose to fill all blanks with a particular value across the data source.
Select a column and you will see Find and Replace Option.
4. Delete Rows
This is a row level operation. Once you select row(s), you will see the option to Remove Rows. You can remove single or multiple (at max 100) rows at a time. Please note that this will permanently remove the records form your data source and you will not be able to Undo after Committing.
5. Apply Validations for Write-back
Sometimes there might be certain validations required on a column for write-back operation. For example, Customer_ID should have no blanks, or for a string Column "Country" the input value should have only 2 characters (US, UK etc)
Such validations can be applied from Profiling page using Master Completeness Ruleset (MDC Ruleset) on a table.
There are 3 types of data constraints you can apply from MDC ruleset
1. Blanks Check
On a column where blanks check is applied, workbench will highlight null fields are red and it will not take "Blank" as an input
2. Value Range
Allow only a specific range of values as valid
3. Data Format
Constraints
Validations on string (character length limit), Number (only integers, whole numbers or decimal values) are to be allowed.
These rules are reflected on Workbench. You will see a small badge for each of the rules above (B, VR, DF) below the column name on which rule is applied.
When you hover on the DF badge, it will show you the validation and highlight the cells which do not meet the criteria.
6. Apply DvSum Rules as Filters
The DvSum rules, like Custom Query, Blanks, Value Range, MDC Ruleset, Orphan Records, Integrity Check created on a particular table can help you filter out the data on workbench. You can select one rule at a time on workbench and it will bring only those records which are shown as exception on rule detail page.
Click on the "Select a Rule" drop-down where you will see only the above mentioned rules. If the rule has not been executed, it will show as disabled. Similarly, rules other than the listed will be disabled.
The selected rule will start to show on top and it will fetch the records according to rule definition.Â  This is helpful when you want to perform cleansing actions on selected data.
7. Reference Column - Data Preparation Workbench
If a column has a reference column defined in Field configuration, when we update key column value in Prep workbench, its reference column would also get updated accordingly.
Profiling - Field Configuration Update -
We will now have a Reference Mapping section where user can define Reference column for a Key column
Reference Dictionary Update -
Currently, you were able to create reference dictionary by manually adding values from UI (by adding values for the fields) i.e name and description.
Now, there will be a checkbox 'Is Key Value Pair', if marked this as checked, user will be able to create a Key Value based dictionary.
On marking this checkbox checked, you will be able to add key, value and description. Else you can import values
On importing values for a column whose reference is not defined in Field Configuration, it will prompt you to select a key column for it and the Reference column will be set in the Field Configuration automatically
On importing values for a column whose reference is already defined, it will prompt you. On importing values, it will create a reference dictionary containing distinct values of both columns
On saving, it will create a Key Value Based reference dictionary with Is Key Value Pair
Note:
While creating a MDC rule, on Reference column defined in Field configuration, it will not allow you to create a Value Range rule with normal dictionaries. It will give you a warning message to select a Key Value Pair Dictionary from the list
Data Preparation Workbench Updates-
On updating a value of a reference column (if defined in Field Configuration) in the Workbench, it should automatically update the value in its key column accordingly.
All those columns having write back enabled will have icon 'Write Back' enabled for them.
The defined unique Identifier Column, Key Column will have write back disabled with grayed out column header.
Key Column will also have a chain icon in the column header. On hover, it will show the related information i.e the reference column defined for it.
On updating a value in the Reference Column, it will automatically update the value in the key column accordingly
The same operation can also be performed via Find and Replace. Clicking the reference column header checkbox, it will enable the Find and Replace button and values can be updated as required

--------------------------------------------------------------------------------

================================================================================
TITLE: Off-line Write-back to Source in DQ workflow
URL: https://dvsum.zendesk.com/hc/en-us/articles/360013797874-Off-line-Write-back-to-Source-in-DQ-workflow
================================================================================

Among the Workflow Configuration Steps, there is a setting of commit parameter as "Off-line Write-back to source".
By default, when a DQ Workflow executes, DvSum writes back the fixes of exceptions to the source. With off-line write-back, user can cleanse the exception records on DvSum cleanse workbench UI and save changes but it would be required to manually commit fixes to source outside DvSum.

--------------------------------------------------------------------------------

================================================================================
TITLE: Cleansing Work Bench and Audit Trail
URL: https://dvsum.zendesk.com/hc/en-us/articles/219786788-Cleansing-Work-Bench-and-Audit-Trail
================================================================================

Issue:
If I run a rule and immediately click on Cleanse to open workbench. I notice that the Write-back shows as disabled, even though I have it enabled. Additionally, my old cleansing audit trail shows up. If I want for a few seconds after running the rule and open it, I don't see the above issues. Why?
Reason:
The exception cleansing information experiences a brief delay in loading after the completion of rule execution due to the asynchronous generation of Cleansing data following audit execution. Typically, when the rule is run in the background, users are notified of rule completion, and upon logging in to resolve, the exception data is already loaded.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to create a Workflow with excel file as input ?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360033768933-How-to-create-a-Workflow-with-excel-file-as-input
================================================================================

Follow along the steps to use Excel file as an input in a Workflow;
1. Create a target Folder
From Administration > Manage Account > Folder tab, create a folder. Provide the path of the folder where you want to upload excel file.
2. Upload file to folder
From File upload on left navigation, select the folder you created. Upload the unprocessed excel file in this folder. This is how an unprocessed file will look like.
Once uploaded to target folder, DvSum will run pre-processing on the file and apply basic cleansing.
Basic cleansing includes removing blank rows, replacing white spaces within a column name with underscore e.g. First Column will be changed to First_Column. Special characters from column name are removed. Also, the duplicate columns are dropped. Below is the image how the file looks like after basic cleansing.
Pre-processing file is created against the uploaded excel source in target folder. To do so, go to the target folder and there will be a new folder called "ExcelPreProcessProps".
Open the folder to access the preprocess.properties file. It a note-pad file with the same name as the excel source file with "_preProcess" appended at the end.
3. Custom Cleansing with Pre-process.properties file
In some cases, the default cleansing is not enough. Sometimes a user needs to tell DvSum to skip rows which do not contain actual data. Or specify the format for number columns where the data automatically truncates the values up to required decimal precision. For that purpose, a properties file associated with every uploaded file is available for user to edit.
As an example, lets change the data type of one column to "str".Â A particular use case when your data source has numeric field values but you want DvSum to read it as string. For example, in the case below where you want the field value "0000000000000000043" to be read as it is and not as "43", you would have to mention the data type "str" in pre-process file. DvSum source requires to explicitly mention the dataType in such case.
For DvSum to read "Article_ID" as string and not as number, open pre-process propertiles file and set the dataType as "str". Save the pre-process properties files.
For more details, please read
EXCEL Pre-process properties
4. Upload file again to apply latest preprocess.properties
Once you have saved the changes in preprocess properties file, you will be required to upload the file again (same as step 2). This will apply the latest changes to the file.
This will clean and format the excel source the way it is required. Now it is ready to be configured in DvSum.
5. Add Excel in Memory Source
From Administration > Manage Sources > Add Source.
Provide a name and select the source type as Excel in Memory. You would be required to provide File System Path (root folder path where the excel file is placed)
Save to add this source in DvSum.
6. Configure Source
Click on "Configure" button on the bottom left corner. It will show you all the tabs/sheets of your excel source. Select a tab and select the option "My Data has Headers". This will show you the name of the columns. Select the columns you want to configure in DvSum.
Note: The columns that you specified as "str" in pre process.properties file are to be explicitly set as "String" from configure page.
When you click on "Add", those columns will show the status as "Cataloged". Click on Finish.
7. Profile Source
From Profile > Profiling page, select the source and table - Run profiler. Once completed, go to "Show Details" to view the data which will now be appearing according to the pre-process properties.
8. Create Rules
From Profiling, you can create MDC ruleset and DAE rules. You can also create any other rules like Value Range, Blanks check, Orphan Records or Integrity check rules on your excel source.
Execute the rules to extract exception records. The idea is to use these rules in a workflow and cleanse these there.
9. Create a Workflow
From Workflows > Manage Workflows, Create a workflow.
Once you Save the workflow, you will land on Workflow Designer page. Complete the Workflow by adding Step widgets and making their end to end connection.
Now in order to add tasks to a step, go to detail page of that step and click on "Add Task"
9.1 File Upload Task
Provide a name to task and choose type as File Upload Task. In the section File Upload Details, select the same folder that you created in Step 1 (where the file was uploaded). Also, make sure you have checked "Upload any file" and "Apply pre-processing" options.
9.2. Validation Task
Provide a name to task and choose type as "Validation Task". In the section Rule List, select the rules created on the configured excel source. You can also select them by filtering rules by source name or Rule group. Also, make sure you have enabled "Allow dynamic file selection" check in order to run rules against the dynamic file.
If you want the task to be Mandatory, enable Mandatory checkbox.
The mandatory step in rule validation can sometimes cause hard-stop in workflow execution. In some cases, it may be acceptable to have exceptions for rule and a user may want to be able to over-ride the failed and mark the validation step as complete by first getting them "Approved"
Enable "Select Approver" and select from the group of users or a single user for this role.
Note
: Approvers can be appointed only for mandatory Validation Tasks.
10. Publish workflow
Once you have defined the tasks within the steps of workflow, go back to the Workflow Designer window and click on "Publish" button. This will change the status of workflow from Draft to Published.
11. Execute Workflow
From New Workflow Request on the top right corner, create an execution for this workflow.
Provide the required details and click on Initiate.
12. Upload dynamic file from workflow
Once the execution starts, the step owner will be notified about progress. He/She will then complete the tasks and submit them. User is required to upload a dynamic file, execute validation rules against the uploaded file and cleanse the exceptions.
12.1 File Upload task
From the task detail page, Click on Upload file and select the file to be uploaded.
Upon clicking Save, DvSum will apply the pre-process properties to file and cleanse it. Now you can execute the validation rules on it.
12.2 Validation Rules Task
From the task detail page, Click on Execute. This will show you a dialogue to select a data source against which the rules are to be executed. The file uploaded in previous step will be automatically selected.
If the execution of rules passes, then you mark the step as complete. If it fails, you will have the option to cleanse the exceptions or assign step to "Approver"
13. Fix exceptions and assign Step to Approver
Click on the Validation rule task to see the Rules. You can go to the rule detail page and fix the required exceptions.
If there are still exceptions left and you want to be able to over-ride these exceptions and move to the next step of Workflow, assign the step to "Approver" to take a decision whether the exceptions fixed are acceptable or not.
The Approver can approve the changes and the step can now be marked as complete.
The execution will go back to the Step Owner to submit and move to next step of workflow.
Other Related Articles
How can I add new fields in a Excel upload Template?
Data Management Workflow Alerts
Workflow Digest Emails
How to appoint an Approver for Validation Task in a workflow?
Who are Watchers and how to add them in a workflow?
What are the types of tasks that can be created in a process step of a workflow?

--------------------------------------------------------------------------------

================================================================================
TITLE: Why is Data Analysis tab not showing for the profiled table?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360022912733-Why-is-Data-Analysis-tab-not-showing-for-the-profiled-table
================================================================================

Sometimes when you profile a table of your data source, Data Analysis tab does not appear in table detail page. For Data Analysis tab to be enabled, there must be at least 2 fields marked as attributes.
Step 1 -
From
Profiling pageÂ  > Show details of table you cannot see Data Analysis tab
Step 2 -
From Field Configuration tab, mark at least 2 fields as attributes.
Step 3 -
Go to previous page and profile the table again.
Step 4 -
Go to Show details and now Data Analysis tab will be enabled.

--------------------------------------------------------------------------------

================================================================================
TITLE: Staging Configuration: How to set up staging table ?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360021780333-Staging-Configuration-How-to-set-up-staging-table
================================================================================

For rules like Address Validation and Match-Merge, DvSum works on staging tables i.e copy of the original tables. The changes committed back to source will be written on staging table.
Step 1:
To create Staging Table, go to Profiling page and select source and table > Show Details
Step 2:
Enable Match Merge and enter description, click SAVE.
Step 3:
Click on "Generate Script"
Step 4:
Run the script on your Datasource to create staging table and then Refresh catalog. This table will start showing on profiling page.
You can profile it and create Address validation rules on it.

--------------------------------------------------------------------------------

================================================================================
TITLE: How can I send Scheduler job emails to other users?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360021367293-How-can-I-send-Scheduler-job-emails-to-other-users
================================================================================

When a job is scheduled, sometimes it is required to send email alerts containing summary of rule execution to multiple users. These users may or may not exist in DvSum.
Schedule a rule, you will see the option to send the email to Internal Recipients, Groups or External Recipients. This option is available for On demand scheduling, schedule for later and Generate Script.
Internal Recipients
- Users signed up on DvSum application
Recipient Groups
- Group of DvSum users from Administration > Manage Groups page
External Recipients
- Users not signed up on DvSum but added as external users from Administration > Manage Users > External tab
For detailed help, read our article on
How to add External Users as Recipients?
Note: Email is addressed to the person who creates the schedule and a carbon-copy is sent to all other users. This way it is easy for recipients to communicate on the same email thread.

--------------------------------------------------------------------------------

================================================================================
TITLE: Can I add new columns to my already cataloged and profiled EXCEL source?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360013703693-Can-I-add-new-columns-to-my-already-cataloged-and-profiled-EXCEL-source
================================================================================

If an excel source is already cataloged and profiled and a user wishes to add new columns, he can add, catalog and profile them individually and it will not have an impact on profiling of old columns or affect any rules created on source previously.
- Navigate to the Administration tab
- Configure the Excel source
- Add the desired column
- Click on Finish
This will catalog the new column:
Upon visiting the profiling details for this source, the added column will be visible. You can then individually profile and establish rules for this column without any impact on your existing rules.
Note:
Merge Catalog feature is available for both EXCEL and CSV.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to create Custom Governance Views for Rules?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360008059154-How-to-create-Custom-Governance-Views-for-Rules
================================================================================

You can create custom view in DvSum by selecting the fields and defining filtering criteria according to your requirement, once created, these views will appear in "Created by me" section.
This capability is available for Rules list and Workflow execution list. To create a custom view, follow the steps below.
1.
Go to Review > Analyze Rules > Analyze rules drop down > Create View
2.
A dialog will open where you enter relative information.
Enter name for your view and drag the available fields to selected. These field will appear in your custom view.
3.
Specify the criteria
4.
For "Shared with" section, choose who can see the view.
5.
Save this view and it will appear in "Created by me" section of drop down.
You can mark the view as favourite, edit, clone or delete a view from the ellipsis button on top tight corner.
You cannot edit a view that is created by someone else and shared with you. You can however clone it and then edit it to create your own version.
Note: You can create a custom view for Workflow execution list in a similar way from Workflows > Execution list > Workflow Exec List drop down > Create View.

--------------------------------------------------------------------------------

================================================================================
TITLE: How can I export/download Rule Exception Report?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360006879773-How-can-I-export-download-Rule-Exception-Report
================================================================================

DvSum rule execution allows you to see the potential bad records in your data source. It also saves these exceptions in to an excel file which can be downloaded from Analysis tab on the rule detail page.
The "Export" button will download an excel file containing exceptions for this rule but this button is only enabled when there are
exceptions
in the run result. If the executed rule passes, which means there are no bad records identified in your data source, then, there will not be any report to download. Hence, the Analysis tab will be disabled.

--------------------------------------------------------------------------------

================================================================================
TITLE: Why don't I see all users in list of owners in glossary?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360002074473-Why-don-t-I-see-all-users-in-list-of-owners-in-glossary
================================================================================

DvSum Owners can practice fine-grain or global access control over the categories of Business Glossary. This means only those users which have given access to a particular category will be able to
view
or
edit
the terms belonging to those category.
For example, if
Alice
manages (edit rights) the Category
Income. Bob
carries out some tasks over this category and he has edit rights for this category as well. However,
John
can only read (view rights) the terms defined under
Income.
If
Term1
is defined under
Income
. This means only Alice and Bob can be assigned the role of Term Owner or Term Steward. Bob can view the detail page of the terms but he wont be able to make any edit changes.
Note: You can learn more about
What is Fine-Grain and Global Glossary Access Control?
here.

--------------------------------------------------------------------------------

================================================================================
TITLE: Edge Gateway (SAWS) not accessible
URL: https://dvsum.zendesk.com/hc/en-us/articles/360001182373-Edge-Gateway-SAWS-not-accessible
================================================================================

Overview
The DvSum Edge Gateway (SAWS) always runs over HTTPS. It sometimes uses a self-signed certificate. In cases where it uses a self-signed certificate, you need to add an exception to allow the browser to communicate with the web service.
Troubleshooting
Client not white-listed
Symptom: Warning Icon
Only browser clients with a white-listed IP address are able to access the Edge Gateway Servers (SAWS). If your IP address is not white-listed, you will see this warning icon icon:
Solution:
Add your IP address to the White List
.
Edge Gateway (SAWS) not accessible
Symptom: Red Cloud Icon
The DvSum application running on prod.dvsum.com uses a valid, secure, trusted certificate. But the gateway server in use is using a self-signed certificate. This means that communication passing between the browser and the gateway server is still encrypted, but the certificate is not trusted by any certificate authority. The situation is indicated visually with the red cloud icon:
You may register an exception with your browser to allow this connection.
To do so, click on the Cloud icon. It will open the list of edge gateway servers (most customers use only a single server). Click on the web-service with the red cloud icon. It will open the web-service page which will warn you of connection not being private. Click on â€œShow me advanced optionsâ€ â†’ â€œProceed to <<IP Address>>â€.Â  This is an internal company connection, and it is fully secure.
Once you click â€œContinueâ€ or â€œProceedâ€, the next page should display the SAWS Management Console as shown below. This means that your browser can now connect to the web service over a secure connection. You do not need to login to DvSum SAWS Console. Simply close this page to return to the DvSum application.
In the DvSum Data Quality application, the web service icons will turn green now:
You can execute data quality functions on your data sources.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to create a Custom Dashboard?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000260754-How-to-create-a-Custom-Dashboard
================================================================================

Step 1: Create a Dashboard
Go to Dashboards >> Manage Dashboards Page. Click on "Add" button. Specify a name and description. You can also choose to
Share
your Dashboard with others or keep it to yourself.
When you "Save" this Dashboard, you will be navigated to "My Dashboard" tab where the newly created dashboard opens. Since you need to add widgets to your dashboard, an "Add Widget" dialogue will appear.
Step 2: Add widgets
You can add template widgets to your dashboard.
Upon selecting a template widget, it opens the widget dialogue which allows you to specify on which data source do you want the template widget is to display the information (
explained in next step
). These template widgets auto-populate the preferred choice for
chart type
related to
slicers
(Y-axis) and
metric
(X-axis). You can
sort
how the data appears on the chart and enable
Datalabels
(legends) on charts.
Step 3: Create a View
Click on "Create View". It opens an "Add Widget - Create View" dialogue. Based on this View, the widget will display the information.
Enter a Name for View. Specifying columns wont appear on the chart because thats what slicers and metric do. But that is required to create a view. However the conditions that you apply on a view under the "Specify Criteria" section filters the data accordingly before displaying it on the dashboard.
When you save this view, it will be populated in the "Add Widget" dialogue of step 2 (image shown below).
Save this widget to make it appear on Dashboard. This completes Step 2.
Step 4: Dashboard
The widget will start appearing on Dashboard. You can add more widget by clicking on "
Add Widget
" Button. You can mark this dashboard as
Favorite
or
Edit
,
Clone
and
Delete
it.
Once you mark the Dashboard as Favorite, it appears in the Dashboard drop-down under the "Favorites" section along with "Created by me" dashboards and "Shared with me" Dashboards.
Note: You can view the dashboards shared with you but you cannot edit them. You may however clone the dashboard and edit it according to your liking.

--------------------------------------------------------------------------------

================================================================================
TITLE: How do you select the right chart based on your data?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000024413-How-do-you-select-the-right-chart-based-on-your-data
================================================================================

First, identify the purpose of the chart. When you understand why you need to create a chart and with what information, youâ€™ll be able to select the best chart type to make it more readable. There are several chart types available to you while creating a widget.
If you want to compare things, you may choose a
bar chart
,
line chart
or
list
.
If you want relationship analysis,
table
is the best option.
If you want to show distribution, you may go with a
column chart
.
If you want to show composition,
stacked column
or
bar charts
are preferred.
If you want to show trends over time, a
line chart
is a great option.
if you want to represent parts of a whole, a
donut chart
would work well.

--------------------------------------------------------------------------------

================================================================================
TITLE: How do my Scheduled jobs execute with MultiSaws?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000018334-How-do-my-Scheduled-jobs-execute-with-MultiSaws
================================================================================

If you have scheduled a job where
Profiling tables
or
Rules
are associated with multiple SAWS, the scheduler will split this job into separate jobs. Each of these jobs execute independent of the other with the SAWS they are mapped to.
For e.g. if a job has 4 Rules to run.Â  2 Rules are associated with two SAWS S1 and the other with S2.
The scheduler will give this job a name and split them into two. Letâ€™s say the Job name is JBS â€“ 00951. It will split into JB â€“ 00951 001 and JB â€“ 00951 002.
JB â€“ 00951 001 is mapped to Saws S1
JB â€“ 00951 002 is mapped to Saws S2
This way even if S2 is not up and running, JB â€“ 00951 001 will have no impact and it will still be executed.
Similarly for profiling of two tables associated with two SAWS, the job breaks up into two.
The job JBS-000971001 and JBS-000971002 are executed independent of each other.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to reassign responsibilities of a user upon deletion?
URL: https://dvsum.zendesk.com/hc/en-us/articles/115001891934-How-to-reassign-responsibilities-of-a-user-upon-deletion
================================================================================

Once a user is added by an administrator, he can only be soft deleted. i.e. the status of the user will change to â€œDeletedâ€ and upon deletion, all of his responsibilities will be assigned to another user.
When you Delete a User; An administrator can reassign his responsibilities to another user, admin or owner of DvSum account
When you Delete an Administrator; An administrator, his responsibilities are reassigned to another administrator or owner of DvSum account.

--------------------------------------------------------------------------------

================================================================================
TITLE: What is Fine-Grain and Global Workflow Access Control?
URL: https://dvsum.zendesk.com/hc/en-us/articles/115001889873-What-is-Fine-Grain-and-Global-Workflow-Access-Control
================================================================================

Workflow Control
The two levels of control provided to users over Workflows are
Workflow fine-grain access control
Global (when the checkbox for Workflow fine-grain access control is disabled)
Fine Grain Disabled (Global Access)
When fine grain checkbox is disabled, it means the users of a particular group are given global access to â€œallâ€ the categories of Workflow.
On all categories, there are further 3 levels of access. i.e. Execute, Manage, No Access. An admin can set the access for a group from edit/add group dialog.
Execute Workflows
On Workflows Manage and Execution listing pages, all the WFs of all the categories will be visible to the users of that particular group. It means that the users can only view the WF list and details but cannot create or edit them.
Manage Workflows
On Workflows Manage and Execution listing pages, all the WFs of all the categories will be editable to the users of that particular group. It means that the users can edit the WF list and create and execute WFs.
No Access
For no access, User will be not able to see the WF listing on the Manage and Execution List pages.
Fine-Grain Enabled
When fine grain checkbox is enabled, it means the users of a particular group are given privileges to â€œselective or allâ€ categories of Workflow.
On all categories, there are further 2 levels of access. i.e. Manage Workflow and Execute Workflow. An admin can set the access for a group from edit/add group dialog.
Execute Workflow
On Workflows Manage and Execution listing pages, all the WFs of the selective categories will be visible to the users of that particular group. It means that the users can only view the WF list and details but cannot create or edit them.
Manage Workflow
On Workflows Manage and Execution listing pages, all the WFs of the selective categories will be editable to the users of that particular group. It means that the users can edit and create and execute WFs.
Workflow Behavior for both Fine Grain and Global Access
There are some actions that a user can perform regardless of the defined access control.
Workflow Creation:
Functional Owner: Only those users of a group can be Functional Owner for which they have â€œmanageâ€ or â€œexecuteâ€ right over a category of WF.
Data Stewards: Users of a group can Data Stewards for which they have â€œmanageâ€ right over a category of WF.
Step Owner: Only those users of a group can be assigned a step for which they have â€œmanageâ€ or â€œexecuteâ€ right over a category of WF.
Initiate, Cancel and Delete Workflow:
All these actions can be performed by the Functional Owner or Data Stewards of a particular WF.
Reassign, Reject and Close Reject:
All these actions can be performed by the Functional Owner, Data Stewards or Step Owners of a particular WF.

--------------------------------------------------------------------------------

================================================================================
TITLE: Why can User A see more Source Tables than User B?
URL: https://dvsum.zendesk.com/hc/en-us/articles/115000143513-Why-can-User-A-see-more-Source-Tables-than-User-B
================================================================================

Users can't add sources; only administrators can do that. Admins have the authority to grant users permission to work with specific tables on source and table levels. Essentially, users can only see tables that administrators give them access to.
Consider this example: the administrator adds a source, let's call it Source XYZ, which includes 20 tables. User A is granted "Source level" access, while User B is given "Table level" access to specific tables (10 out of the 20).
User A can view/configure* Source XYZ (20)
User B can view/configure* Source XYZ (10)
If you need access to a data source or its tables that you currently don't have, please reach out to your DvSum Administrator for assistance. The administrator is responsible for determining whether a user has the ability to view, write, or configure at the Source level or Table level.

--------------------------------------------------------------------------------

================================================================================
TITLE: Error - 'File is corrupted and cannot be opened'
URL: https://dvsum.zendesk.com/hc/en-us/articles/225558507-Error-File-is-corrupted-and-cannot-be-opened
================================================================================

Issue:
When attempting to open a downloaded report by clicking on the "Download report" link in Rule detail or via the link provided in the email alert, an error occurs where Excel displays the message "File is corrupt and cannot be opened."
Reason:
Excel prevents opening files downloaded from the internet, impacting not just DvSum but also files from colleagues or other websites.
Resolution:
Change the settings in Excel Options
Step 1: Open Excel --> Click on File --> Options
Step 2: Click on Trust Center on left menu
Step 3: Click on Trust Center Settings
Step 4: Click on Protected View
Step 5: Uncheck the first check-box
Step 6: Close Excel
Starting now, your files should open without any issues.
If changing your Trust Center settings isn't possible or you prefer not to, you can manually unblock the Excel file to open it.
Step 1: Navigate to the folder (e.g. Downloads) where the report is downloaded
Step 2: Right click on the excel file and click on Properties
Step3: Click on Unblock at the bottom of the dialog
Step4: Close dialog
Now this file can be opened in Excel.

--------------------------------------------------------------------------------

================================================================================
TITLE: Address Validation rule is becoming Invalid
URL: https://dvsum.zendesk.com/hc/en-us/articles/222595948-Address-Validation-rule-is-becoming-Invalid
================================================================================

Problem: When you run the address validation rule, it shows as invalid. The comments say, unknown error, contact administrator.
Common Reasons:
1. Incorrect configuration: Go to Configure tab of the table by clicking on configuration in the Rules Scorecard or from Profiling --> Show Details. Click to Edit and Save again so the system Â checks the configuration. If there are any errors, it would prompt for the same
2. Web Service not able to write changes to staging table. (Administrator) go to Web Service log (dvsum\webservice\SAWS.log) and look for any errors in processing the request.

--------------------------------------------------------------------------------

================================================================================
TITLE: Scheduler not executing jobs / error in SAWS.log related to SUN PKIX Path Building Failed
URL: https://dvsum.zendesk.com/hc/en-us/articles/221853427-Scheduler-not-executing-jobs-error-in-SAWS-log-related-to-SUN-PKIX-Path-Building-Failed
================================================================================

Root-cause
This is related to DvSum Web Service (SAWS) not able to make a secure HTTPS connection to DvSum Cloud. This issue occurs due to local java not recognizing the root certificate authority (CA). Â By default Java Security configuration exists under JRE and not under JDK.
Resolution
Execute SAWS using JRE instead of JDK
Steps
1. On the machine (laptop, server) where SAWS is running, open Computer Properties --> Advanced Properties --> Environment Variables
2. Add JAVA_HOME\jre\bin to the beginning of the path
3. If you are using SAWS as a service, stop and re-start the Service. If you manually started SAWS, end the Java Process in Task Manager and re-start by double-clicking on DvSum.jar

--------------------------------------------------------------------------------

================================================================================
TITLE: Why only certain fields are available in the Data Analysis screen
URL: https://dvsum.zendesk.com/hc/en-us/articles/221403867-Why-only-certain-fields-are-available-in-the-Data-Analysis-screen
================================================================================

In DvSum, Data Analysis is a part of the Data Profiling module. It helps discover how data is connected across different fields, such as business unit, product family, and product line. For example, it identifies which combinations exist and how many records are associated with them.
The system automatically makes those determinations by reading the statistics of the data. And it does it only for fields it considers as attributes. So for example if a table has 100,000 rows and there are 100,000 unique item ids, then that field will not show up in Data Analysis. On the other hand, if there is Â product family, for which there are 100 unique values, then those will show up in the analysis.
For this reason, you only see limited values in the data analysis section.

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Create Rules (Standard, Data Quality Ruleset, Business Rules DAE)
URL: https://dvsum.zendesk.com/hc/en-us/articles/202846784-How-to-Create-Rules-Standard-Data-Quality-Ruleset-Business-Rules-DAE
================================================================================

Rules are created and added to Audits to be run together with other rules.
Rules can also be run individually. They can be scheduled or run ad-hoc.
Rules in DvSum can be created from a number of different locations within the tool. They can also be added from the Rules Library, or if you have existing rules for your data they can also be uploaded to DvSum.
When a rule is created it is assigned a unique ID. The ID will have a form like "DQ-000167". DvSum generates a page at a url in the form https://prod.dvsum.com/rules/{rule_id} which includes the Rule Definition, Summary, Exceptions from the latest Audit run, and trends.
Standard Rule Creation
The majority of rules are created through the Rules Wizards provided within the tool. From the left navigation pane, access Audit â†’ Manage Rules, and click "âŠ• Add Rule".
Completing Rules Wizards
For each of the Rules wizards, information is completed within the following sections. Depending on the type of Rule, options will differ. If the Rule is applicable to a single data source, Reference Inputs will not apply. Doc-matching rules will also require different information.
For some rule types, under Basic Inputs, the Variance Check check-box indicates this is a rule that will compare results of the current run to the results of a previous run, and you will be asked to provide a Variance Threshold or %.
Data Quality Rulesets
Creating Rules from within Profiling â†’ Data Quality
When in Profiling, by clicking the Data Quality tab, you have the ability to create Rules and Rulesets for the data you are reviewing.
You must first click â€œEnable Ruleset,â€ complete the required information, and click â€œSaveâ€ before defining the specific checks.
Rules created from the Data Quality tab within Profiling are limited to Completeness and Value Range checks. For example, by selecting the â€œBlanks Checkâ€ check box, the system automatically creates a rule to identify blank or NULL values.
You also have the opportunity to establish Filter conditions, to ignore the special scenarios where a blank might be an acceptable value.
Within the same Ruleset, or as a standalone Rule, you can also define Value Range Checks. These can be defined based on a Min/Max Threshold, or based on values from a Picklist defined either within the Field or in a reusable Reference Dictionary.
Your specified configuration is automatically saved as a rule which can be viewed and edited under Audit â†’ Manage Rules.
Data Analysis Exceptions (DAEs)
Creating Rules from within Profiling â†’ Data Analysis
More complex data rules can be created from within Profiling, under the Data Analysis tab. When exploring data through the filtering analysis, you may identify combinations of data that you want to define as Exceptions or that you want to create a query for to review the record details.
For example if the combination is invalid and should not exist, a Rule can be created from this view. As seen below, for a Product/Item Category of Accessories (Accy) sourced from Asia, a user may know that the Plan Level (planning category or priority) should not be "4." You can create a rule by clicking "Add Business Rule", entering a Description, and clicking Create. A Rule template (or shell) is created, which you may then edit as desired.
Clicking Create takes you Audit â†’ Manage Rules with the Rule template created. From here you can Edit, add detail, Action Items, Create a Workflow, etc. before finalizing the rule.
You may also simply choose to click, "Add Rule" which takes you to the Rule Editor. Here you will need to recreate the filtering conditions for the Rule you want to create and click "Add Rule" which notates the condition under the "Exceptions Rule List".
Here you can specify whether the conditions are valid or invalid. If valid, the rule will be displayed under "Valid Rule List". It will consider the condition as valid and bring all other records as exceptions. If invalid, the rule will bring only those records as exception which meet the condition. This can be selected from top right corner.
When creating rules from this view, you are able to create multiple filtering conditions, within the selected Attributes, and also add them as Rules. This creates a compound rule that returns all qualifying exceptions or records within the query, and allows you to view them together. At the bottom, there is Cleanse Column list. Here you can check the columns which you want to make cleanse-able and unchecked any column which serves only as a filter.
In the cleanse workbench, the filtered columns with be selected by default and only the cleansable columns will be visible to be fixed.
Alternatively, DAE rules can also be created from Audit â†’ Manage Rules tab as well.
Select "âŠ• Add Rule" â†’ "Business Context" â†’ "DATA ANALYSIS EXCEPTIONS".

--------------------------------------------------------------------------------

================================================================================
TITLE: How to Install Python service for Excel, Excel in Memory Source?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360021118474-How-to-Install-Python-service-for-Excel-Excel-in-Memory-Source
================================================================================

Note: This is a part of
SAWS installation
. If your SAWS is already running, uninstall it and follow the steps below
To download and install python with its libraries, please make sure you follow all the steps below.
****************Python Installation****************
1
Go to https://www.python.org/downloads/release/python-370/
2
From "Files" section, install the executable file according to your operating system. (e.g. for windows 64 bit, install Windows x86-64 executable installer)
3
Right click and run as administrator the exe file.
4
Select the checkbox "Add Python 3.7 to PATH"
5
Click on "customize installation"
6
Select check box "Install for all users"
7
Click on Browse and select driectory accessible to all users i.e "C:/Program Files/Python" for python installation and Click on install
This will install Python on your system.
****************Python Libraries****************
1. To install python libraries, go to command prompt (Type "Cmd" on run, make sure to run cmd as administrator)
2. Type pip install pandas, press Enter.
3. Type pip install XlsxWriter, press Enter.
4. Type pip install xlrd, press Enter.
5. Type pip install Flask, press Enter.
6. Type pip install SQLAlchemy, press Enter.
This will install required Python libraries on your system.
Now go to SAWS folder on your machine and click on
saws_service_install
. Run it as an administrator. This will install webservice and you will be able to see DvSum Python Service running in your machine along with SAWS.
Note: This python service is required to avail features like excel preprocessing and for Excel in memory source.

--------------------------------------------------------------------------------

================================================================================
TITLE: Is there a limit of how much data we can extract to a file on azure blob/S3 bucket or to folder?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360018196733-Is-there-a-limit-of-how-much-data-we-can-extract-to-a-file-on-azure-blob-S3-bucket-or-to-folder
================================================================================

You can control the data extraction limit on Azure blob or s3 by specifying the number of records to be uploaded.
This is the storage level option provided on Cloud storage tab. From Administration > Manage Account > Cloud storage > Add/edit
The Export Limit field is optional. If a user does not specify a value, by default 75,000 records will be uploaded. Valid range for this is 1 to at max 10 million records.
Note: If there are more exceptions than the limit, exception count will show on rule detail page but at the max 10M exception file will be uploaded to Azure/s3.

--------------------------------------------------------------------------------

================================================================================
TITLE: On Premise Gateway (SAWS) Advanced Settings (Configuration properties file)
URL: https://dvsum.zendesk.com/hc/en-us/articles/360015361094-On-Premise-Gateway-SAWS-Advanced-Settings-Configuration-properties-file
================================================================================

Following the basic gateway setup, additional features are provided to enhance the experience for Dvsum users. You can conveniently adjust the settings through the gateway configuration file. This file is located within the webservice folder.
Note: Read here if you are not sure
how to install SAWS and run as a Service
The configuration file allows gateway on your machine to communicate with DvSum application.
SAWS Error Identification
The flag "enable.log.sync" is false by default. If you set it to true, the activity logs of SAWS will be uploaded on S3 bucket which makes it easier for the DvSum team to monitor SAWS logs in case of errors during rule/thread execution.
It is to be noted that your data remain private. We will not have access to your data but only the SAWS logs.
Scheduled SAWS Reboot
If you are concerned that SAWS process might be consuming a lot of memory, you can refresh/restart it to release un-used resources to ensure smooth running of SAWS.
For this purpose, configuration file has rebootTime flag. Here you can give time in 24 hour format i.e. 18:45.
Everyday at the specified time, SAWS will reboot and flush the memory.
Port
By Default, SAWS runs on port 8183. If you want to run it on some other port, you can simply change it from here and save file.
SAWS Auto-Restart
If for any rare reason SAWS crashes, it will restart on its own within 5 minutes. You may need to add SAWS exception in your browser.Â Otherwise the
SAWS cloud will appear red.

--------------------------------------------------------------------------------

================================================================================
TITLE: Deleting the data source. (Owner and Administrators)
URL: https://dvsum.zendesk.com/hc/en-us/articles/360005670514-Deleting-the-data-source-Owner-and-Administrators
================================================================================

In order to delete a data source, first, navigate to Administration, then proceed to Manage Sources in the left-hand navigation menu. Select the data source labeled as EXCEL_DEMODATASET, and click on "Delete Source."
Performing this action marks the source as deleted, and it will be removed from the Source list after 24 hours. Once deleted, it won't appear anywhere in the application, including profiling tables, rules, or related workflows, as they will also be deleted.
Administration section is only available for Owner and Administrator roles. If you are a user or Super-User, you will not be able to delete the demo dataset.
All data for the selected demo datasets will be deleted. This includes profiles, audits, results and action items. The data is also at an account level. So once the data sources are deleted, they are deleted at account level and no user can use it. It is recommended that you delete the demo dataset after confirming with any users in the system who might be using the demo dataset.

--------------------------------------------------------------------------------

================================================================================
TITLE: Export options for Rule Results (Administrators and Super Users)
URL: https://dvsum.zendesk.com/hc/en-us/articles/360005093774-Export-options-for-Rule-Results-Administrators-and-Super-Users
================================================================================

When you execute a rule on DvSum, you see the run result in Analysis tab. You can also download the exception file on your computer.
There are further advanced options available for you to manage rule result. You can export rule data to cloud storage like AWS S3 and AzureÂ  or return it to calling API. This can be done only on a Custom Query rule as it covers almost all rules.
On the custom rule detail page, there is a tab "Script Configuration". You can set the export configuration to send the data to either AWS S3 Bucket/Azure Bucket or send the data to calling API.
AWS S3 Storage/ Azure Storage
From Administration > Manage Account > Cloud Storage, click on Add and select the Storage type and the web-service. Depending on the Storage type, you will provide the Access key and fill out the mandatory fields.
Export limit allows you to set the number of records to be uploaded to Azure/s3 bucket. (Default limit 75,000)
Once you save this, you can test your connection with Cloud Storage.
Now from rule detail page you can select this storage, give a file name and type, say the file name is "S3 file" and the type selected is CSV.
When this rule is executed, the results will be uploaded to AWS S3 bucket in a file named "S3 file".
Similarly for Azure, you can set up Azure storage from Cloud Storage tab and select the same Azure Storage on rule detail page.
Send Data to Calling API
This option allows you to execute rule and fetch the results of that rule outside the DvSum domain. As the name suggests, the run result will be sent to the calling API.
To generate script, schedule a rule and go to Generate Script tab. Copy the script and run from command line. The results for this rule will be returned in command line.
Export to Folder
Another option to manage rule report is to export it to a folder. This folder can be on your local machine or wherever the webservice is running. Folder is created from Administration tab > Folder tab where user needs to specify folder path and SAWS associated with it.
When the option â€œExport to Fileâ€ is selected, you will be able to see the folders which are associated with the same SAWS as the source on which rule is created. i.e. The SAWS executing the rule and uploading the report to the folder should be same.
NOTE: when there are no exceptions in rule executions, no file will be created or uploaded.

--------------------------------------------------------------------------------

================================================================================
TITLE: SAWS Alert Notification controls (Owners only)
URL: https://dvsum.zendesk.com/hc/en-us/articles/360002717393-SAWS-Alert-Notification-controls-Owners-only
================================================================================

SAWS Alert is the Capability to Send Email Notification. These Notifications let Account Owner and optionally Administrators know when a specific SAWS is not able to communicate to DvSum web application and also when it resumes communication after failure.
These controls can be changed from Add/Edit SAWS dialogue.
Enable SAWS Alert ( Default is Yes)
Also Notify Admins ( Default is No )
If
Enable SAWS Alert
is set to Yes and
Also Notify Admins
is set to No Notification will only be sent to Owner of the account.
If
Enable SAWS Alert
is set to Yes and
Also Notify Admins
is set to Yes Notification will only be sent to Owner as well as Admins of the account.
If
Enable SAWS Alert
is set to NO, Application won't send alert Notification for this specific SAWS.

--------------------------------------------------------------------------------

================================================================================
TITLE: What is a "Not-Connected DataSource"?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360002610433-What-is-a-Not-Connected-DataSource
================================================================================

Not-connected sources are the ones which are not connected to the DvSum Web-service for the purposes of cataloging, profiling, executing rules or cleansing data.
If you want to create a blueprint of a source or may require to create lineage, you can select this option. This means you would manually add the tables and the columns of your source.
Once you have added the Not-Connected Source, you can go to Profile tab >> Profiling and select your source to define the tables manually. Only for a not-connected source you will see the "Add Table" button.
When you add the table, it starts appearing in the tables drop down.
Select this table and go to "show details" where you can define the columns manually.
You can give name to your columns and select their data-type
This completes the blueprint of your Database.

--------------------------------------------------------------------------------

================================================================================
TITLE: DQ SAWS Update (For Owner Account)
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000764854-DQ-SAWS-Update-For-Owner-Account
================================================================================

Whenever there is a new version available for SAWS, a pop up will appear informing you about it after you log on to DvSum. You will also see a notification on your SAWS cloud icon.
If you decide to upgrade right away by clicking on "Upgrade Now", it will take you to the Administration >> Manage Account >> SAWS tab.
The "Upgrade" button is
enabled
for the SAWS that is running and the communication key matches with the api token in the configuration file.
The "Upgrade" button is
disabled
for the SAWS that is not currently running.
If your SAWS is busy in running a job, it will not be able to upgrade right away but don't worry. We have got it covered. The application will show "Upgrade Pending" status and the SAWS will upgrade itself automatically once the SAWS is free.
After the successful download of the latest version, you will see a blue tick and the version will be updated. If your already have the latest version installed, you will not see the "Upgrade" button. (like for the "Remote ca ws" in the image below)
Note:
1. Please be patient while the SAWS upgrades. It takes around 1-2 minutes.
2. . SAWS upgrade notification appears only when SAWS is running and showing green cloud
3. SAWS upgrade from UI is supported by SAWS version 1.1.0 and above. If you have older version of SAWS running, you will manually have to download SAWS from Administration > Manage Account > SAWS tab > Download.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure one Data Source with Multiple SAWS
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000247253-Configure-one-Data-Source-with-Multiple-SAWS
================================================================================

One Data source can only be configured with one web-service
at a time.
However, You can edit the source and map it to another web-service by clicking on "Edit Source"

--------------------------------------------------------------------------------

================================================================================
TITLE: Updates to database source data catalog
URL: https://dvsum.zendesk.com/hc/en-us/articles/225102967-Updates-to-database-source-data-catalog
================================================================================

Overview
From time to time, your database source data definition changes. The metadata changes could be as small as the data type being modified or as big as new tables being added. It can include dropping tables and adding or removing columns. All of these changes can be seamlessly synched to DvSum.
Steps to Refresh the Data Catalog
Step 0.
Make changes in your source systems (outside of DvSum).
Step 1.
Login to
https://prod.dvsum.com
and navigate to
Administration â†’ Manage Sources.
Note: Only users with Admin or Owner roles have access to this.
Step 2.
Select a source and click "Run Cataloging" or "Schedule Cataloging". DvSum will
read your current database definition and synchronize it with the catalog in DvSum.
Note: "Schedule Cataloging" can be executed immediately and runs in the background. This is what most users should select.
"Run Cataloging" runs while you have the page open. It's better for smaller environments.
Results of Data Catalog Updates
The synchronization will result in following possible updates.
Tables or Views
If new tables or views are added, they will automatically be imported.
If tables or views that previously existed were dropped, then they will be
marked for deletion
in DvSum. Under the catalog list in Profiling page, they will show up with a prefix (deleted). The administrator can manually review and delete them from Profile â†’ Profiling.
Columns or Fields
New fields: automatically imported and become visible.
Dropped fields: automatically removed from DvSum.
Note that if there were rules that used those fields, those rules will become invalid and may have to be re-configured.
Renamed fields: treated as combination of drop and add. So the new fields must be re-profiled to generate statistics.
Extra steps when using staging configuration
These steps are needed only when you are using the staging configuration and you added new columns or changed the names of any columns.
New fields are not automatically included in the staging configuration. Therefore the domain data type and pre-processing and post-processing logic must be configured. Go to Staging Configuration
Step 1.
Go to Staging configuration of your data source by navigating from Profiling Main Page and clicking onÂ Show Details.
Step 2.
Select the Staging Configuration tab and click Edit. Then press Save.
This action will re-initialize the configuration. Click Edit a second time.
Step 3.
Scroll to the bottom of the screen. The new fields are available, but they are not included in the mapping.
Step 4.
Check the box to include the field in staging workflow. Click on the pencil icon to set its match data type and any pre-process validation or standardization settings.
Step 5.
Click Save. The system will warn you if there are any rules associated with the configuration; those rules will be reset due to the change in structure. Click OK.
Object maintenance is complete and you can continue using DvSum with the fully updated Data Catalog metadata.

--------------------------------------------------------------------------------

================================================================================
TITLE: Change of language in DvSum
URL: https://dvsum.zendesk.com/hc/en-us/articles/360003300593-Change-of-language-in-DvSum
================================================================================

DvSum application can adapt according to the language preferences by translating the source language (English) in to the targeted language.
You can change the display language to your preferred language from Profile Page
Under the Change Settings section, choose your language from the drop down box and Save.

--------------------------------------------------------------------------------

================================================================================
TITLE: Dashboard templates and blank widgets
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000024353-Dashboard-templates-and-blank-widgets
================================================================================

Template Widgets
Template WidgetsÂ are dashboard charts pre-designed to show the meaningful information that you can otherwise view by going through the DvSum application. There is a button "Add Widget" on dashboard which shows a list of template widgets you can select from.
For example, when you add a template Rule widget â€œRule Count by Source and Statusâ€, it populates the input fields related to the widget with the preferred Chart type better suited to display the information. All you need to do is provide the Data Source so the widget can fetch the data.
Blank Widget
Blank widget is the most powerful widget because it allows you to create a widget from scratch with any information and any chart.

--------------------------------------------------------------------------------

================================================================================
TITLE: What do I see on Advanced Dashboard based on user roles?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000017313-What-do-I-see-on-Advanced-Dashboard-based-on-user-roles
================================================================================

DashboardÂ gives you quick access to frequently used info.
On a
User level
, dashboard widgets help you see your involvement in the action items of DvSum application without going to individual functionality sections. For e.g. you can create a dashboard with a Rule Widget showing â€œRule Countâ€ by â€œRule Statusâ€ in the form of a list. This shows all Rules created by you with their status: Pass, Fail or Exceptions.
On an
Admin/Owner level
, it is easier to track all user activity over the DvSum application on a single interface. Admins can filter out the data for a single user or view it for everyone using
Data for:
filter. (Shown below)

--------------------------------------------------------------------------------

================================================================================
TITLE: What is new in Advanced Dashboards?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360000018374-What-is-new-in-Advanced-Dashboards
================================================================================

DvSum Advanced Dashboards
are made up of tables, charts and lists used to view, analyze and track the overall activity on DvSum application.
These easy-to-read customized Dashboards help you to organize and present the information the way it is required.
Unlike the basic data quality dashboard, now you can create a dashboard of your own, share it with others or clone someone elseâ€™s dashboard and edit it accordingly.
You can add multiple widgets to a single dashboard or create multiple dashboards. You can also add the widgets for
Data Management
Workflows
and
Business Glossary
to your advanced dashboard.
Here is what the old dashboard looks like:
This is our revamped Dashboard:

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Salesforce as a Source
URL: https://dvsum.zendesk.com/hc/en-us/articles/360049829631-Configure-Salesforce-as-a-Source
================================================================================

Overview
DvSum Data Quality (DQ) supports a bi-directional connection with Salesforce, where a user can catalog data assets, author data quality rules, and even cleanse data without need for any external data pipelines.
Detailed Steps
Salesforce configuration
1. Connected App configuration
Open browser, navigate to salesforce.com, and log in.
https://login.salesforce.com/
Navigate to Setup â†’ Apps â†’ AppManager to create an app.
Tip: use the Quick Find box.
In the Connected Apps section, click "New Connected App".
Enter a name to be displayed to users when they log in to grant permissions to your app, along with a contact email address.
Enable OAuth Settings, and enter the relevant value in the Callback URL box:
For Data Quality:
https://prod.dvsum.com/cdata/saveVerifierCode
For Data Catalog:
https://apis.dvsum.ai/data-sources/sources/salesforce/saveVerifierCode
Select the scope of permissions that your app should request from the user. Save the changes and Continue to the next screen.
Click your app name to open a page with information about your app. Note the OAuth client credentials. These properties are needed to add Salesforce as a source in DvSum DQ:
Consumer Key
Consumer Secret
2. Permission Set configuration
The user connecting to Salesforce must have the ability to call Salesforce APIs. This is done by setting the relevant properties for the relevant Permission Set.
Access the relevant Permission Set via either of these 2 paths (the GUI layout will differ depending on whether Lightning Force or Classic view is active, but the logical path remains identical):
ADMINSTRATION â†’ Users â†’ Permission Sets â†’ <permission_set>
or
ADMINSTRATION â†’ Users â†’ Users â†’ <user> â†’ Permission Set Assignments â†’ <permission_set>
From here you need to activate System Permissions â†’ API Enabled (Access any Salesforce.com API.)
With the Connected App and Permission Set configured, then you're ready to configure DvSum.
DvSum Configuration
Create connection in DvSum
Open
DvSum DQ
. Navigate to Administration â†’ Manage Sources â†’ âŠ•Add Source. Select Salesforce.
Paste the copied values:
Consumer
Key
â†’ Client ID
Consumer
Secret
â†’
Client
Secret
Then Authenticate.
While authenticating, following message will be displayed.
You will be re-directed to Salesforce. Provide user credentials and log in.
Depending on your Salesforce configuration, a verification code will typically be sent to you via email. Paste the code here and verify.
Now, click "Allow". This will allow the DvSum application to access Salesforce data.
It will trigger authentication of DvSum's connection to Salesforce.
A success message should be displayed. Then click "Save".
Your Salesforce connection is now fully configured and functional.
Details when using a Salesforce Sandbox
Be sure to enable the checkbox
Use Sandbox
as shown in the screenshot below.
Once you have enabled that, you must enter the
Sandbox Login URL
. then enter the
Client ID
and the
Client Secret
.
Then
Authenticate
the connection. You will be redirected to the Salesforce Sandbox login page andÂ will need to follow these steps:
Enter Login credentials.
Enter the Verification code received on the registered email address.
Allow Access.
After the
Authentication
is done, you can Test the connection to confirm that the source has been added successfully.

--------------------------------------------------------------------------------

================================================================================
TITLE: SQLEXPRESS connection to SAWS
URL: https://dvsum.zendesk.com/hc/en-us/articles/360033961454-SQLEXPRESS-connection-to-SAWS
================================================================================

Sometimes DvSum Gateway is unable to make connection to SQLEXPRESS. If you are sure you have provided the correct information and credentials and "Test Connection" still fails, please check the following settings on your computer.
Step 1:
From Windows button, click on SQL Server Configuration Manager.
Step 2:
Click on Network Configuration in the Manager
Step 3:
Double click on Protocols. Confirm if TCP/IP is enabled.
If not enabled, right click and enable. (Donâ€™t re-start service yet)
Step 4:
Double click TCP/IP and in the dialog box, click on IP Address tab
Step 5:
Scroll to bottom of page. Confirm that under IPAll, TCP Port isÂ  set to 1433.
If not, add it and apply.
Step 6:
Close and get back to Configuration Manager. Click on SQL Server Services on left side.
Step 7:
Click on SQL Server (Running) and Click on Restart button in action bar.
Step 8:
Once re-started, DvSum will connect to this database. Confirm the following in Manage Sources page.

--------------------------------------------------------------------------------

================================================================================
TITLE: Configure Excel/CSV files?
URL: https://dvsum.zendesk.com/hc/en-us/articles/115000641314-Configure-Excel-CSV-files
================================================================================

Prerequisites:
Java version 7
You can configure Excel files after creating a new Source on DvSum app. But before that can be done, you have to make sure that you have â€œAccess Database Engineâ€ Drivers installed wherever SAWS is running (local/cloud). If you donâ€™t, following error message will appear when trying to configure Excel/CSV file.
Follow the steps below if you require ADE drivers installed on your system
Step 1:
Go to
www.microsoft.com
and download Microsoft Access Database Engine exe file.
Step 2:
Install Access Database Engine
Step 3:
Search Windows for ODBC Data Source Administrator and Click on â€œDriversâ€ tab to see if the Access and Excel drivers are installed.
Now go to DvSum Web Application and follow the steps below to configure Excel/CSV files
Step 1:
Go to Manage Sources >> Add Source
Step 2:
Fill out the mandatory fields with Source Type as Excel or CSV
Step 3:
Enter the File System Path. It can be local or Cloud file system location. If your files are on Cloud, please ask your administrator to provide you the path.
Step 4:
Click on â€œSaveâ€ to add the source
Step 5:
Edit the source you just added
Step 6:
Click on the â€œConfigureâ€ button appearing on the bottom left corner.
Now you can Configure the columns of the Excel sheet.

--------------------------------------------------------------------------------

================================================================================
TITLE: Job Execution Using Dynamic Schema
URL: https://dvsum.zendesk.com/hc/en-us/articles/360061493952-Job-Execution-Using-Dynamic-Schema
================================================================================

We have introduced a feature where user can execute rule created in Dvsum on another source just by providing information of source with same metadata. A new field has been added up 'Generate Script form'. User have to copy curl Script from Dynamic source Script section given on Generate Script tab of create Scheduler page. Let's get started step by step.
Step 1 :
Prerequisite - Installation of curl
Click the
link
here.
Download curl w.r.t machine's compatibility
Extract the files from zip folder and double click on application main file to run the curl
Step 2 :
Rules, Profiling, Cataloging Execution using Dynamic schema
Open Dvsum application, go to Audit >> Manage Rules >> select any existing rule >> click More action >> select Schedule Rule as shown below;
Step 3 :
Dynamic Source Script Generation
Go to Generate Script tab and click on the button at the bottom to generate the script as shown below;
Step 3.1
Copy the Dynamic source script and click OK as shown below;
Step 3.2
Job is created against the rule. Now go to the main scheduler page, search the rule and a new created job will be visible with description and status as 'stand by' as shown below;
Step 3.3
Open notepad and paste the copied source script there. It is recommended to make changes in the script using notepad. Replace the information which is different or which you may want to update i.e. Schema name/port/sourceID/Host. Once the modifications are done, copy the script.
Step 3.4
Open command prompt, run it as Administrator and paste the copied script and hit enter. It starts the execution and at Dvsum application side, the Job description and its status gets updated as shown below;
Step 3.5
Once the execution is completed then in cmd, it will print the remarks as same as shown in rule detail page in Dvsum.
and job status changes to completed after execution
Step 3.6
Similarly for Profiling and Batch execution same steps can be performed to execute jobs using dynamic schema.

--------------------------------------------------------------------------------

================================================================================
TITLE: Address Validation Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/360054484771-Address-Validation-Rule
================================================================================

Address validation, or address verification, is a rule ensuring that street and postal addresses are valid. It can be done upfront when a user searches for an incomplete or incorrect address, or by comparing and formatting data in a database with reference postal information.
In the application, there are changes in:
Staging Configuration (Profiling section)
Rule Detail (Workbench)
Staging Configuration for Address Validation Rules
Note
:
This setup is necessary only for the initial configuration on Oracle source. It's mandatory to complete this step; otherwise, during the execution of Address Validation rules, the user will receive an error message in the rule remarks stating "MM configurations missing."
Step 1 :
Navigate to Profile -> Profiling -> Select an Oracle data source, select main table and staging configuration table. In the below screenshot, main table is CUSTOMER_ADDRESSES and staging table is CUSTOMER_ADDRESSES_DVSUM.
Note:
It is required to make sure that Staging table exists for Address Validation rule to be run.
Step 2 :
Select main table and click on show details
Step 3 :
Select Staging configuration tab and click on Edit
Steps 4 :
Setting up Configuration, Additional Configuration, Staging Configuration fields
Configuration setup
Choose the Subject Area, set Golden Record Criteria, tick the Enable Validation checkbox, and provide a Description.
Additional Configuration setup
Verify the "Write-back Post Process" option, meaning enable it if the user intends to activate write-back on the main table. Additionally, specify the "Case type" preference, indicating whether the user wants to maintain the format of the Suggested Address as fetched by default, either in lower or UPPER case.
Staging Configuration
setup
Choose the Staging table, Match key, Match count, Match rule, Record identifier, Golden record ref, Record type, Merge status, Time stamp identifier, and then click the Save button located at the top, as illustrated below:
Step 5 :
Upon saving the changes, a success notification will inform the user that the modifications have been saved successfully. Simultaneously, an error message will appear, indicating that "Fields are not mapped to address data types Address Line 1 or Country." At the bottom, the Edit Mapping grid will be displayed as illustrated below:
Edit Mapping
Step 6 :
After configuring the staging settings, click on the Edit button to modify the mapping. Initially, Match Data Types are assigned randomly to column names, so the user needs to update these Address Match Data Types based on the required columns. The user can choose all columns by clicking Select All or include individual columns, as depicted below:
Step 7 :
Columns are mandatory to be mapped against following Address Match Data Types
City
Country
State
US ZIP code
ADDRESS LINE 1
Following address match data types will be mapped as:
Address Line 1 (Street Address)
Address Line 2 (Suit/Apartment)
Address Line 3 (Info/Atten)
Other Address Lines
Google response of street address will writeback to Address Line 1. Likewise suit/apartment, Info/Atten will writeback to Address line 2 and Address Line 3 respectively. In this way, the quality of addresses data will be improved.
Step 8 :Â  Completeness Rule
The default completeness rule is as follows:
"address_line1" is mandatory.
"country" is mandatory.
Either "city" or "zip_code" must be mandatory.
Users have the option to add this rule for all records or apply it selectively to specific filtered records. For instance, in the example provided, the `CUST_STATE` column is chosen as a mandatory field for all addresses where the country is set to 'US'.
Address Validation Rule
Step 9 :
Navigate to "Manage Rules," click on "Add Rule," and choose the Address Validation rule from the Process Quality category.
Step 10 :
Provide all the required fields here.
Note
: Table name field should be Main table not the staging configuration table as Rule will be created on Main table.
Step 11:
Visit the Rule Detail page and execute the rule, as shown below:
Step 12:
After the rule is executed, the following data is retrieved:
Run Status
Scanned Records (Total records got scanned)
Review Status
a. Pending (requires review)
b. Modified (if user accepts suggested address OR current address fetched from DB gets updatedÂ  Â  Â  Â  and Saved)
c. Skipped (If user skips suggested address)
Commit Status
a. committed
b. commit failed
Readiness Score
Remarks
Step 13 :
Visit the Addresses tab, where the matched addresses will be visible in the grid. To edit or update any record, simply click on the Review button, as shown below:
Address Line 1, Address Line 2, Address Line 3, Other address lines, City, state, country are mapped together in Complete Address column.
Complete Address detail information is fetched from User's DB
Match Statuses
Step 14
Following are the Match statuses
INC (Incomplete)
i
ndicates that the provided address data is not complete and is returned if any of the below conditions is fulfilled:
address_line1 is empty.
country is empty.
city and zip_code both are empty.
OK
i
s returned if following conditions are fulfilled:
matched_location_type is â€œ
ROOFTOP
â€.
matched_address_line1 exists.
matched_city exists.
matched_zip_code exists.
matched_country exists.
Provided address and validated address are exactly same without any case difference.
SD (Standardized)
i
s returned if following conditions are fulfilled:
matched_location_type is not empty.
matched_address_line1 exists.
matched_city exists.
matched_zip_code exists.
matched_country exists.
There is only
Case
difference between provided address and validated address (example: HENRY STREET, Henry Street).
There is only
ZIP Code
deference between provided address and validated address (example: 63122-6604, 63122 or 63122, 63122-6604).
There is
abbreviated form
found in the validated address (example: Henry Street, Henry St).
ER (Enrich)
i
s returned if following conditions are fulfilled:
matched_location_type is â€œ
ROOFTOP
â€.
matched_address_line1 exists.
matched_city exists.
matched_zip_code exists.
matched_country exists.
There is at the very least 50% match difference between provided address_line1 + address_line2 and matched_address_line1+ matched_address_line2.
Validated address is a bit changed from provided address (example: Henry Road, Henry St).
There is only
ZIP Code
deference between provided address, validated address (example: 63121, 63122).
NM (No match)
is returned if none of the above status is returned.
Record Statuses
Step 15
There are following record statuses:
Update
(If user Accept Suggestion)
Skip
(If user Skips Suggestion)
Modified update
(Once current address fetched from DB gets updated and Saved by user)
Step 16 :
In the presented screenshot, this icon indicates the specific address chosen for writeback from the suggested or complete address.
Users can edit the current address in the review window, validate it, and receive a success or error message accordingly. The label next to the current address will show "Edited," and users can also revert to the original value if needed.
Review Window:
Step 17
When the record statuses are modified to Update, Skip, or Update, the count for pending records decreases, while the count for reviewed records increases.
Step 18
Users have the option to select all records and take actions such as accepting or skipping suggestions. They can also clear the selection for all records or choose to discard the changes.
Step 19
Once user clicks Finish button given on bottom, then
Commit
button appears in Address tab on rule detail page.
Step 20
Once commit is done, then Fixed records and Reviewed records count gets updated in Rule summary
Step 21
User can also create view
Step 22
Users can add comments to the Match Status column by clicking on any status, entering a comment, and saving it. A comment icon with a count will start to appear on the main grid, and the count increases with each new comment. If there are no comments, hovering over the comment icon will display it.
Postal Addressing Standards supported in Address Validation rule (Bag of words)
Primary Street Suffix Name
Commonly Used Street Suffix or Abbreviation
Postal Service Standard Suffix Abbreviation
AVENUE
AV
AVE
AVE
AVEN
AVENU
AVENUE
AVN
AVNUE
BOULEVARD
BLVD
BLVD
BOUL
BOULEVARD
BOULV
CAUSEWAY
CAUSEWAY
CSWY
CAUSWA
CSWY
CIRCLE
CIR
CIR
CIRC
CIRCL
CIRCLE
CRCL
CRCLE
COURT
COURT
CT
CT
DRIVE
DR
DR
DRIV
DRIVE
DRV
HIGHWAY
HIGHWAY
HWY
HIGHWY
HIWAY
HIWY
HWAY
HWY
LANE
LANE
LN
LN
PARKWAY
PARKWAY
PKWY
PARKWY
PKWAY
PKWY
PKY
PLACE
PL
PL
ROAD
RD
RD
ROAD
STREET
STREET
ST
STRT
ST
STR
TRAIL
TRAIL
TRL
TRAILS
TRL
TRLS
Full Form
Abbreviation
Apartment
APT
Building
BLDG
Department
DEPT
Floor
FL
Unit
UNIT
Suite
STE
Room
RM
Guide on using Address Validation API
Here
is a link to an article for a complete guide on address validation API.

--------------------------------------------------------------------------------

================================================================================
TITLE: Allow Blanks in SAP Writeback
URL: https://dvsum.zendesk.com/hc/en-us/articles/360051535313-Allow-Blanks-in-SAP-Writeback
================================================================================

This enhancement in the DvSum application enables the write-back of blank values to the source data. Let's proceed with the following steps to comprehend how this functionality operates.
Users need to ensure that two checks are applied:
Rule Level
Table level
Note:
Please note that this configuration will be applicable to all rules except
BLANKS RULE.
Rule Level Check
Step 1 :
Value Range Rule
Lets consider an example of VR rule in which
first the rule definition will be checked for blanks and then table level configurations will be checked.
For the rest of
rules,
only table level configurations will be checked to allow blanks.
1. Go to the "Manage Rules" tab in the DvSum application.
2. Search for any VR (Value Range) rule and click on the rule to open the rule detail page.
3. Once on the rule detail page, navigate to the "Definition" tab.
4. Click the "Edit" button.
5. Under the "Specify Range value" section, find the "Include Blanks" checkbox.
6. Check the "Include Blanks" box.
7. Ensure that the "Valid" option is selected for values.
8. Click the "Save" button to apply the changes.
By following these steps, you can configure the VR rule to include blanks and ensure the "Valid" option is selected for values.
Note:
If "Include Blanks" is not checked and "Valid" option is selected above, still it won't allow to write back for blank values.
Step 2 :
Now click Analysis tab and see on which table the exceptions are showing up as shown below;
Table Level Check
Step 3 :
Go to Profiling section, select source, table name and click the Select button. Once the record is displayed, select it and click Show details option as shown below;
Step 4 :
Once the details page is displayed, select Field Configuration tab. In this tab, "Allow Blanks" column is added. Now, search for the table name and verify Allow blanks option is checked for instance; in this case it was COUNTRY, re-run the rule as shown below;
Step 5 :
Once the rule is re-run, go back to rule detail page and click Cleanse button. On Cleanse detail page, note the Remaining Exception count shown above, click on Edit option for any field, remove the value, leave it blank and apply the change as shown below;
Step 6 :
As soon as you apply the change, observe that it allows the blank value and Remaining Exception count also gets reduced. Save it and Commit the change. as shown below;
Step 7 :
After change is committed, the field color turns Green which confirms that the write back was successful as shown below;
Scenario 1 :
When Rule level check is enabled but Table level check is disabled, in this case the Blank won't be write back-able and user will be notified at Cleanse detail page as shown below;
Scenario 2 :
When Table level check is enabled but Rule level check is disabled,Â in this case the Blank won't be write back-able and user will be notified at Cleanse detail page as shown below;

--------------------------------------------------------------------------------

================================================================================
TITLE: Clone Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/360050083754-Clone-Rule
================================================================================

In the past, the functionality of cloning rules did not exist, requiring users to manually create rules each time. However, users can now easily duplicate existing rules with minor adjustments and assign them to other users. This eliminates the need for manual rule creation, saving time and allowing users to create rules quickly using the new clone functionality. For more information on this feature, follow the steps outlined below.
Cloning From Manage Rules Page
Single Rule Cloning
Steps 1 :
Navigate to the "Manage Rules" section and choose a specific rule. Click on "More Actions" at the top, then select "Clone Rule" from the drop-down menu, as illustrated below:
Step 2 :
Upon selecting "Clone Rule," the user will be taken to the Clone Rule detail page. At the top, there will be a prefix and all the clone properties, which will be inherited by default. Users have the flexibility to unselect any property as desired. The cloned rule will have a target source, initially set to the same as the current source by default. However, users can modify it to any other source of their choice, as depicted below:
Step 3 :
Now, let's assume the user chooses a different target source for this rule and clicks the "Clone" button, as demonstrated below:
Step 4:
After cloning the rule, the user will be taken to the Clone Summary detail page. In the grid below, you will find the Rule ID, the new ID for the cloned rule (which remains blank in case of failed cloning), the default source of the rule, the source on which the rule has been cloned, the status indicating whether the cloning was successful or failed, any remarks in case of unsuccessful cloning, and finally, a description. In the top right corner, there is a "Download Summary" button to download a report of this screen, as illustrated below:
Multiple Rules Cloning From Different Sources
Step 5:
If the user selects multiple rules belonging to different sources, the same step 1 will be executed. Upon redirecting to the clone detail page, you will observe the prefix, all clone properties, and below that, a target sources section. In the case of multiple rule sources, there will be both "From Source" and "To Source" options. By default, the "To Source" will be the same as the "From Source," but users can modify it later, as depicted below:
Step 6:
On the Clone Summary detail page, you will be able to see the number of rules that encountered errors and those that were successfully cloned, as illustrated below:
Multiple Rules With Same Source
Step 7:
Choose rules that belong to the same source. Opt for the "Clone Rule" option, as demonstrated below:
You'll encounter a familiar screen as before, but this time, as the rules belong to the same source, there's only one target source displayed. Users can modify the target source if necessary. Simply click the "Clone" button, as indicated below:
Upon successful cloning, you will observe that the two rules have been duplicated, and their details will be displayed as shown below:
Note:
A rule that has been cloned can be treated independently from the original (parent) rule on which the cloning was performed. This implies that any configurations made in the cloned rule will not affect the parent rule.
Cloning From Rule Detail Page
Step 8:
Choose any rule and navigate to its detail page, where you will find a "Clone Rule" button on the top right side, as illustrated below:
Step 9:
After clicking the "Clone" button, you will be directed to the clone detail page. In this specific flow, the information remains consistent, with the addition of displaying the target table on which the rule was created. Users have the option to modify both the target source and target table as needed.
Step 10:
If the user selects a different target table to clone the rule and there's an issue with that target table, the user will receive a notification before the cloning process. For example, if there are data type mismatches between the source and target tables or if there is a structural mismatch, the user will be informed about these errors in advance. Various types of errors may be communicated to the user beforehand, as illustrated below:
S
tep 11:
After cloning the rule, the user will be redirected to the rule detail page of the rule on which the cloning was performed. Additionally, a prominent glitter message/ confirmation message will be displayed at the top, indicating the new cloned rule ID. Clicking on the ID will open the detail page in a new tab, as depicted below:

--------------------------------------------------------------------------------

================================================================================
TITLE: Rule Status
URL: https://dvsum.zendesk.com/hc/en-us/articles/360050837013-Rule-Status
================================================================================

Cleansing Information Visibility On Listing Page
Previously, when user used to run a rule then it showed the X number of exceptions and it's status which was also exception status. Later on, after cleansing and write-back operation, there was no such visibility of this action on analyze rule listing page and also in rule detail page unless user had opened cleanse workbench page. There was as such no indication that exceptions were getting fixed or write-back was performed.
In order to have visibility, without opening the work-bench, we introduced this new enhancement with the help of which now users will have more visibility about the rule(s) on listing/Rule detail page. To get familiar with this enhancement, let us get started step by step.
Manage Rules >> Rule Detail Page
Step 1:
Go to Manage Rule section, select any rule and click on it to open Rule detail page. On this page, you will see a new field added as 'Commit Status'. This field has three new statuses asÂ shown below.
'Modified' : To be committed
'Committed' : Write-back has done successfully
'Commit Failed' : Write-back has failed
Cleanse Bench Detail Page
Step 2 :
Now, go to Analysis tab, hit the cleanse button there which opens up the cleanse bench detail page. On top you will see three different sections, 'Suggestions', 'Progress', 'Change History'. In Progress box, you will see total number of Exceptions, remaining ones and under that modified count, committed count and commit failed count is displaying as shown in image below:
Step 3 : Modified Rule Status
Now, update any value and you will see that Modified count gets updated from 0 to 1. Also, remaining Exceptions count reduces from 54 to 53 as shown below:
Step 4: Committed Rule Status
Save the changes and then hit commit button. After commit gets successful then modified count changes from 1 to 0 and commit count increases from 0 to 1 as shown below:
Step 5 : Commit Failed Rule Status
Now, we will try to perform this status scenario. For that, provide any input which is invalid and Save the changes. The modified count gets updated again from 0 to 1Â  and Remaining count reduces from 53 to 52 as shown below:
Step 6 :
After saving, now hit the Commit button and observe the following change. Remaining count stays as 52 but modified count changes again from 1 to 0 and since it was an invalid input so commit failed count gets increased by 1 as shown below:
Step 7 :
Now we will modify few more records to see the visibility of its status on other different screens. Make sure that you just Save the changes but do not commit it. After this, Modified count gets updated as 5 and Remaining count reduces to 47. The sum of these 3 status and remaining count will be equal to total Exception count as shown below:
Step 8 :
Navigate back to Rule detail page and check the count update there. It will be same as it was on cleanse bench detail page. Also, Run status field will show Modified status and icon since we did last modify it as shown below:
Analyze Rules >> Rule Listing Page
Step 9:
Go to analyze rule page and observe the count update there as well which will be as same as shown above. See image below :
Dashboard Page
We have added 2 more widget under Rule History category
Step 10 :
Same rule statuses will be shown in all existing widgets on Dashboard created on any particular rule. A new widget is added
Exception detail History
as shown below: It will show the commutative count for run result and other 3 rule statuses.
Step 11 :
Now click on "
Run status of Rules by weekly Run date
" to view report. The weekly filter is applied. Select the rule on which cleanse was done. You will see the recent rule statues count in the top record which lies with in the current week. The other records shown under it are previous weeks rule status results:

--------------------------------------------------------------------------------

================================================================================
TITLE: SAP Writeback Configuration Setup For Writeback from Flat to Classification table Mapping
URL: https://dvsum.zendesk.com/hc/en-us/articles/360047799993-SAP-Writeback-Configuration-Setup-For-Writeback-from-Flat-to-Classification-table-Mapping
================================================================================

Scenario: Writeback from Flat to Classification table Mapping
Step 1:
In Order to Writeback to SAP Table using DB Source, First Verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below
Now Verify the Record Identifierâ€™s in the below location. If the Record Identifierâ€™s are not set, please set them here and Save.
Step 2:
Now, navigate to the DB Source Table (Ex: DVSUM.VW_ZMAT_ATTRIBUTE_FLAT) as shown below for which User wants to create a rule and do the writeback to SAP Table (Ex: ZMAT_ATTRIBUTE).
Step 3:
Verify the Record Identifiers are same as the SAP Table. If not, please select the record Identifiers for DB Source Table (Ex: DVSUM.VW_ZMAT_ATTRIBUTE_FLAT)
Step 4:
Select the SAP Table (Ex: ZMAT_ATTRIBUTE) To which user needs to writeback using DB source Table (Ex: DVSUM.VW_ZMAT_ATTRIBUTE_FLAT) as shown below, and Click on â€œNextâ€
Step 5:
Map the Key Identifier columns and Write-back columns of DB Table and SAP Table by using below screen. Perform the mapping as shown the below screens.
Here, As there are two Key Identifierâ€™s(MATNR, ATTRIBUTE) in the Target (ZMAT_ATTRIBUTE) table, and in the source we have only one key identifier (MATNR_ARTICLE), The Field â€œATTRIBUTEâ€ is by default selected as Qualifier.
In Case of ZMAT_ATTRIBUTE_FLAT table, there will be only One Writeback Column. That Field Needs to be selected under â€œWrite Back Column Mapping Selectionâ€.
The â€œQualifier Column Valueâ€ will be the â€œATTRIBUTEâ€ Column Value Where the Writeback will be performed in ZMAT_ATTRIBUTE table.
Once the mapping is completed for the Required/All columns that are used in the rule, Save the mapping by clicking on â€œSaveâ€ button.
Addition configuration for Custom Query
Step 6:
Starting from creating a custom query rule, go to Manage Rules, click on Add Rule button, select Process Quality from drop down options and lastly click on Custom Query
as shown in image below
Step 6:
User will be redirected to Add Rule detail page. In Basic Input section, under Custom Query box you will see the
EnableÂ Write-back
check box
as shown in image below:
Provide all the details for the Rule and Enable Write-back by clicking on the check box. Save the rule by clicking on â€œSaveâ€ button
Step 7:
Run the Rule First, then to setup the Writeback Configuration Fields.
After running the rule, User will see the rule as below.
Step 8:
Setup the Record identifiers mapping by navigating to â€œWriteback Configurationâ€ tab as shown below and click on â€œNextâ€
Step 9:
Setup the mapping for remaining columns as shown below. Click on check box under â€œEnable Write-backâ€ column for the columns where we need to writeback the data. After Enabling Writeback, User can verify the configuration where the field is going to writeback if SAP configuration is available.
Click on â€œSaveâ€ to save the configuration as shown below
Step 10:
Re-Run the Rule after Saving to see the Editable Fields and Cleanse Button.
After the running the rule, now user can see the Cleanse button and Writeback Fields marked with colour.
Step 11:
Now, click on Cleanse Button, Modify the Data as shown below.
After Modifying, Save the data. Here, user can see the Writeback is going to Perform on SAP Table (ZMAT_ATTRIBUTE)
Step 12:
Once the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.
As soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.
Wait for the Jobs to complete by Navigating to Scheduler Page.
13:
Verify the changes on the Rule Page by refreshing the page Once the Job(s) are completed.

--------------------------------------------------------------------------------

================================================================================
TITLE: Column Sequencing
URL: https://dvsum.zendesk.com/hc/en-us/articles/360046573114-Column-Sequencing
================================================================================

Now, users can control the sequence of columns when profiling a source's table/view. Additionally, the sequence of columns can be customized at the rule level too. To learn more about this feature and understand how it works, let's start step by step.
Table Level Column Sequencing
Step 1
Navigate to Profiling. On the Profiling detail page, choose a table associated with any source where you want to apply sequencing. If you observe the Configuration column, you'll notice that no icon is currently displayed, indicating that no column sequencing is applied. Once you apply customized sequencing at the table level, an icon will appear (which we will verify in the next steps). Finally, click on the "Show Details" button, as depicted in the image below:
The user will be redirected to the Table's detail page. In the Profiling tab, you will notice a
"Default Sequence"
label right next to the Table's name in the lower grid, serving as a secondary identification to indicate that no customized sequencing has been applied. Now, click on the "Field Configuration" tab, as shown in the image below:
Step 2
It's important to note that the "
Field Configuration"
is the main tab where column sequencing will be applied. Similar to the previous steps, the user will observe a "Default" label displayed next to the Table's name. Directly below that, in the lower main grid, two buttons will appear: "
Column Sequencing
" and "
Default Sequencing
," as illustrated in the image below.
The "Default Sequence" button will initially appear disabled. This button is designed to reset the sequence of columns to its default behavior. It becomes enabled once the user applies customized sequencing to the table. Hover over the button to view additional information, as depicted in the image below:
The "Column Sequencing" button is where the user will click to set the sequence of columns. Hover over the button to read the following information, as illustrated in the image below:
Step 3
Now, click on the "Column Sequencing" button. This will open up a new interface listing all the column names. Begin arranging the sequence of column(s) through drag and drop. Once the sequence is set, click the "Save" button, as shown in the image below:
Customized Sequencing Confirmation I
In the image shown above, the user has placed the "CUSTOMER_ID" column at the top. Upon saving the changes, the user will be redirected to the Table's detail page, where the following updates will be visible, as shown in the image below:
A - In the "Field Configuration" tab, "CUSTOMER_ID" is now displayed at the top in the Column name.
B - The "Default Sequence" label is updated to "Custom Sequence."
C - The "Default Sequence" button is now enabled.
Clicking the "Default Sequence" button will reset the applied customized sequencing to its default state.
Step 4
Customized Sequencing Confirmation II
After setting the customized sequence in the "Field Configuration" tab, when you go to the "Profiling" tab, you'll see the same sequence applied. The "Custom Sequence" label will be visible on this screen as well.
Customized Sequencing Confirmation III
Click on "Profiling" from the left navigation menu, and you will see an icon appearing against the table on which the user has applied a Custom Sequence, as depicted in the image below:
Step 5:
If the user wants to reset the custom sequencing to its default sequencing, there are two ways to do that, as shown in the image below:
Step 5.1:
- Go to the Profiling main page.
- Select the table on which custom sequencing is applied.
- Click the "More Actions" button.
- Finally, click on the "Reset Column Sequence" option.
Step 5.2:
Users can also apply default sequencing on multiple records with the following actions:
- Select multiple records on which custom sequencing is applied.
- Click the "More Actions" button.
- Then select the "Reset Column Sequence" option.
Note: In case of multiple records, if there exists any single record with the default sequence, then the "
Reset Column Sequence
" option in the "More Actions" button will remain disabled until the user un-selects that particular record.
The second way to reset custom sequencing to default sequencing is as follows:
1. Select any record on the Profiling detail page.
2. Click "Show Details."
3. Navigate to the "Field Configuration" tab.
4. Click the "Default Sequence" button, as shown in the image below:
Once the user has reset the custom sequence to the default sequence, the applied changes will revert back to their default settings on the following sections:
A - The "Custom Sequence" label will be replaced back with the "Default Sequence" label.
B - The "Default Sequence" button will be disabled again.
C - The column(s) sequence will be set to the default sequence.
D - The custom sequence icon will disappear.
Rule Level Column Sequencing
Note:
Column Sequencing at Rule level is only applicable for following rules as listed below:
A- Blank Rule
B- Value Range Rule
C- Data Analysis Exception Rule
D- Master Data-set Rule
E- Data Format Rule
F- Orphan Records Rule
G- Integrity Check Rule
Step 6
To begin with Column Sequencing at the Rule Level, we'll consider three scenarios, starting with Scenario I. For each of these scenarios, let's create a rule from scratch. Follow these steps:
1. Go to "Manage Rules."
2. Click on the "Add Rule" button.
3. Select the "Business Context" option.
4. Then, click on "Value Range" rule.
Scenario IÂ  -Â  Table Specific :
When the user selects the data source and table name on which custom sequencing is applied, follow these steps:
1. Provide a rule description.
2. Select the data source.
3. Choose the table name.
4. Select the field name.
5. Enter the maximum and minimum threshold values.
6. Hit the "Save" button.
In Scenario I, when the user lands on the Rule detail page, a new tab called "Column Sequence" becomes visible. This tab appears for rules where Rule-level column sequencing is enabled. Under this section, three options are displayed.
A- Suggested
B- Table Specific
C- Rule Specific
In the current scenario, Option B will be selected by default. This represents the logic that when a rule is created on a Table/Source that already has Custom Sequencing applied, the sequence of columns in the rule will be the same as the Table-level sequencing. In the "Selected Columns" section, the column(s) on which the rule is created or the Primary Key column will always appear as disabled. Additionally, the "Analysis" tab initially appears as disabled. Re-run the rule to enable the Analysis tab.
After the user has re-run the rule, the "Analysis" tab gets enabled. Click on the "Analysis" tab and verify that the sequence of columns is the same as it was on the table level, as shown in the image below. Click on the "Cleanse" button, and you will see the same sequence as it is on the "Analysis" tab and "Column Sequence" tab.
Step 7
If the user changes the column sequence at the Table level, upon navigating back to the rule(s) in the "Column Sequence" tab, a warning message will notify the user to re-run the rule in order to see the updated sequence in the "Analysis" tab, as shown in the image below. Once the user re-runs the rule, the sequence gets updated in the "Column Sequence" tab, "Analysis" tab, and the "Cleanse" section.
Scenario II - Rule Specific :
When user apply custom sequence at Rule level
Now, go back to the "Column Sequence" tab. Change the sequence of the column(s). As soon as you change the sequence, the option changes automatically from Table specific to Rule Specific. Hit the "Save" button. The user will be notified through a Warning message to re-run the rule to see the updated sequence in the "Analysis" tab, as shown in the image below. Once the user re-runs the rule, the sequence gets updated in the "Column Sequence" tab, "Analysis" tab, and the "Cleanse" section.
Scenario III - Suggested :
User creates a rule on Table on which No custom sequencing is applied.
Certainly! Here are the steps to complete Step 6:
1. Provide a rule description.
2. Select the data source.
3. Choose the table name.
4. Select the field name.
5. Enter the maximum and minimum threshold values.
6. Hit the "Save" button.
In Scenario III, when the user lands on the Rule detail page, in the "Column Sequence" tab, the "Suggested" option will be selected by default. However, the "Table Specific" option will appear as disabled.
A- Suggested
B- Table Specific
C- Rule Specific
Additionally, the "Analysis" tab appears as disabled initially. Re-run the rule to enable the "Analysis" tab.
Note:
Columns that has exceptions will always appear upfront in case of Suggested option.
Meanwhile, if the user applies custom sequencing at the Table level for the same table on which the above rule is created, then upon navigating back to the Column Sequencing section, the "Table" option gets enabled, as shown in the image below:
Step 8
Now, from here, select the "Table Specific" option. The column sequencing below gets updated with respect to the Table level. Also, if rule level sequencing is already applied to this rule, then on selecting the "Rule Specific" option, column sequencing gets updated with respect to the Rule level. However, to see the results in the "Analysis" and "Cleanse" tabs for Table level and Rule level changes, the user will always have to re-run the rule.
Case 1:
For instance, if the user switches the option from 'Suggested' to the 'Table Specific' option, the sequence gets updated. After clicking the "Save" button, it notifies the user through a Warning message to re-run the rule in order to see the updated sequence in the "Analysis" and "Cleanse" tabs.
Case 2:
Now, in "Table Specific," if the user changes the column sequence further, then the option switches from 'Table Specific' to 'Rule Specific' instantly. After clicking the "Save" button, it notifies the user through a Warning message to re-run the rule in order to see the updated sequence in the "Analysis" and "Cleanse" tabs.
Note:
If the user has created multiple rules against a single table, each rule can have its own unique column sequencing.
Step 9
As per the functionality, a maximum of 30 columns can be displayed on the Analysis tab. For instance, if a user selects a table that has 50 columns and creates a rule on the column that comes at the 40th number, the following behavior will be observed:
A-
Suggested:
The column name gets displayed upfront in the Column Sequencing/Analysis/Cleanse tab.
B-
Table/Rule Specific:
The column name gets displayed on the 31st number.

--------------------------------------------------------------------------------

================================================================================
TITLE: Enable Writeback on Custom Query DQ Rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/360045992074-Enable-Writeback-on-Custom-Query-DQ-Rules
================================================================================

Custom Query Rules
Custom Query data quality rules are powerful rules that give users the ability to monitor any sort of data quality rules that they require. The article
Creating a CUSTOM QUERY DQ Rule
explains how to create a rule. This article builds on that foundation and explains how to support writeback so that data stewards can take corrective action.
Enabling writeback prerequisites
You have a working DQ rule of type "CUSTOM QUERY".
The table that you want to allow writeback on has an ID field configured.
Confirm that an ID field is configured for the table
Profile â†’ Profiling â†’ [tick relevant table] â†’ More Actions â†’ Edit Configuration
Confirm the value of "Field(s) that can uniquely identify a data record".
Confirm that "Allow Write-Back" is ticked.
Enabling writeback steps
Overview
Edit rule definition to enable writeback.
Run the rule.
Configure writeback fields.
Run the rule.
Details
Edit the rule definition and tick "Enable Writeback".
Notes:
The tab "Write-back Configuration" is grayed out when writeback is not enabled.
The Analysis tab displays error records. They may be exported, but there is no "Cleanse" option to correct the errors.
After saving the change, a warning is displayed.
Note: the tab "Write-back Configuration" should now be enabled. But you must run the rule before it's possible to configure the writeback details.
Run the rule.
Configure the Key Identifier on the "Write-back Configuration" tab. Then click Next.
In this example the Key Identifier is CUSTOMER_ID, and the field returned by the query is also CUSTOMER_ID. So DvSum has automatically guessed the mapping. This is typical, and it's a best practice to use field names that match. But in some cases you'll need to map fields with different names.
Configure the Write-back Column Mapping.
Enable Write-back for the columns which data stewards should be allowed to make changes.
Run the rule.
Cleanse is now enabled in the Analysis tab.
The fields enabled for writeback will be highlighted in red. Clicking "Cleanse" will start a workflow allowing a data steward to fix the data.

--------------------------------------------------------------------------------

================================================================================
TITLE: SAP Writeback - Overview
URL: https://dvsum.zendesk.com/hc/en-us/articles/360045900614-SAP-Writeback-Overview
================================================================================

User will be able to perform write-back for SAP source using DvSum application. On opening cleanse workbench for the rules configured with SAP source, resolving the exception and performing commit, data will be written back to SAP source.
SAP User Based Credentials
DvSum application allows users to cleanse the datasets and write back directly on the source database. SAP database is a new addition as a source in DvSum application. For allowing user to write back on SAP source, the user needs to configure the SAP user credentials in DvSum Application. DvSum application will use those credentials for connecting to the source and enabling user to perform different actions on the source data.
There is a section in user profile page where user can setup the sap credentials for sources which have user-based write-back enabled. User will select the source and add the username and password for that source. User can also use Test connection button to test the connection with sap source with entered credentials.
For enabling, user will have to enable user based writeback in writeback parameters.
When a user enters the credentials, these will be saved. Only one credentials can be added for one sap source for each user.
Authorization needs to be provided for a user to be able to do writeback. He should have access to to ZDVSUM* functional module in SAP.
SAP Writeback Configuration Setup
User will now be able to Writeback Data into SAP Application System using Database Source (Ex: SAP MD) by doing certain configuration in DvSum Application at Source Level and Table Level. To know how to setup the configuration one by one, let us get started step by step.
Source Level Configuration:
Step 1:
To link an existing source connection to SAP, user should navigate to 'Administration' > 'Manage Sources' > select the source to be linked to SAP and click on 'Edit Source'
Step 2:
On Clicking 'Edit Source' (Other than SAP), user will be able to see the following page.
In the 'Source Write back Mapping' section, Select the SAP Source to which Data should be Writeback. 'Write back On parameter' must be selected as "Linked SAP source" in order for write-back to be performed on SAP source.
After selecting the â€œLinked SAP Sourceâ€ and â€œWriteback Onâ€, Save the Details by clicking on Save button.
Note: Â 'Write back On' parameter selected as "Original Source", DvSum will write back to Original source.
Table Level Configuration:
There can be multiple scenarios for setting the Table level configurations and do the writeback
Scenario A: Reading from SAP and writing to SAP
Step 1:
In Order to Writeback to SAP Table using SAP Source, First Verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below
Step 2:
In Order to Writeback to SAP Table using DB Source, First Verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below
Now Verify the Record Identifierâ€™s in the below location. If the Record Identifierâ€™s are not set, Please set them here and Save.
Step 3:
Starting from creating a rule, Value Range rule type in this case, go to Manage Rules, click on Add Rule button, hover on Business Context from drop down options and lastly click on Value Range
as shown in image below
:
After the running the rule, now we can see the Cleanse button and Writeback Fields marked with colour.
Step 4:
Now, click on Cleanse Button, Modify the Data as shown below.
Step 5:
Once the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.
As soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.
Wait for the Jobs to complete by Navigating to Scheduler Page.
Step 6:
Verify the changes on the Rule Page by refreshing the page, once the Job(s) are completed.
Scenario B: Reading from MSSQL Database and writing to SAP
Step 1:
Now, navigate to Profile > Profiling page. Select the DB Source Table (Ex: SAP_MD QA , DBO>MARADBO.TVARTICLEPURCHASING_GROUP) as shown below for which User wants to create a rule and do the writeback.
Step 2:
Verify the Record Identifiers are same as the SAP Table. If not, please select the record Identifiers for DB Source Table and click Next.
Make sure the key identifier is set against Source Identifier and the column which we want to use for cleansing purposes, enable writeback must be checked for it in Column Mapping section as shown in the image below. After setting the configuration, press Save.
Note: Please make sure to set the Key identifier for the Target SAP source as well like we set for the above data source
Step 3:
Starting from creating a rule, Value Range rule type in this case, go to Manage Rules, click on Add Rule button, hover on Business Context from drop down options and lastly click on Value Range
as shown in image below
:
Step 4:
After the running the rule, now we can see the Cleanse button and Writeback Fields marked with colour.
Step 5:
Now, click on Cleanse Button, Modify the Data as shown below. You will also be able too see the target source in the cleanse workbench where writeback would happen
Step 6:
Once the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.
As soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.
Wait for the Jobs to complete by Navigating to Scheduler Page.
Step 7:
Verify the changes on the Rule Page by refreshing the page, once the Job(s) are completed.
Scenario C: Reading from MSSQL Database and writing to SAP using Custom Query Rule
Step 1:
In Order to Writeback to SAP Table using DB Source via Custom Query rule, first verify the Record Identifiers are set for the SAP Table by Navigating to Profiling > Selected SAP Table > More Actions > Edit Configuration as shown below
Now Verify the Record Identifierâ€™s in the below location. If the Record Identifierâ€™s are not set, please set them here and Save.
Step 2:
Now, navigate to the DB Source Table (Ex: DBO.TVARTICLEPURCHASING_GROUP) as shown below for which User wants to create a rule and do the writeback to SAP Table (Ex: MARC).
Step 3:
Verify the Record Identifiers are same as the SAP Table. If not, please select the record Identifiers for DB Source Table (Ex: DBO.TVARTICLEPURCHASING_GROUP)
Step 4:
Select the SAP Table (Ex: MARC) To which user needs to writeback using DB source Table (Ex: DBO.TVARTICLEPURCHASING_GROUP) as shown below, and Click on â€œNextâ€
Step 5:
If the field names are same, they will be mapped automatically. If not, user will have to manually map the Key Identifier columns and Write-back columns of DB Table and SAP Table by using below screen.
Performing the mapping for the Fields as shown the below screens. Here you can choose the fields you want to perform writeback on by selecting the Enable Write-back as checked for the respective fields
Once the mapping is completed for the Required/All columns that are used in the rule, Save the mapping by clicking on â€œSaveâ€ button.
Addition configuration for Custom Query
Step 6:
Starting from creating a custom query rule, go to Manage Rules, click on Add Rule button, select Process Quality from drop down options and lastly click on Custom Query
as shown in image below
:
Step 7:
User will be redirected to Add Rule detail page. In Basic Input section, under Custom Query box you will see the
EnableÂ Write-back
check box
as shown in image below:
Provide all the details for the Rule and Enable Write-back by clicking on the check box. Save the rule by clicking on â€œSaveâ€ button.
Step 8:
Run the Rule First, then to setup the Writeback Configuration Fields.
After running the rule, User will see the rule as below.
Step 9:
Setup the Record identifiers mapping by navigating to â€œWriteback Configurationâ€ tab as shown below and click on â€œNextâ€
Step 10:
Setup the mapping for remaining columns as shown below.
Click on check box under â€œEnable Write-backâ€ column for the columns where we need to writeback the data
.
After Enabling Writeback, User can verify the configuration where the field is going to writeback if SAP configuration is available.
Click on â€œSaveâ€ to save the configuration as shown below
Step 11:
Re-Run the Rule after Saving to see the Editable Fields and Cleanse Button.
After the running the rule, now we can see the Cleanse button and Writeback Fields marked with colour.
Step 12:
Now, click on Cleanse Button, Modify the Data as shown below.
After Modifying, Save the data. Here, user can see the Writeback is going to Perform on SAP Table (MARC)
Step 13:
Once the Save is done. Click on â€œCommitâ€ button to writeback data into SAP Table.
As soon as commit is pressed, user can see Job(s) are scheduled to perform the operation.
Wait for the Jobs to complete by Navigating to Scheduler Page.
Step 14:
Verify the changes on the Rule Page by refreshing the page, Once the Job(s) are completed.
How the Commit Works
On setting up the configurations are set based on any of the above mentioned four scenarios, user will run the rule and see the pink colour for exceptions. User will click on Cleanse button and make the changes. While making changes, there is an ability where you can group the fields like the one mentioned below
On saving the changes and pressing commit, it will create a job. User will have to wait for the job to get completed. On job completion, user will receive the email template like below
If user wants to see the status of ongoing job, user can navigate to Review > Scheduler section to view the current status of the job.
Control Options
User can run the mass update in multiple batches. In order to configure batches, go to Manage Sources and define Batch Size.
Batch Size Variable
There is another variable called Writeback Batch Size that will limit the number of records that can be included in a single Batch Job at any given time.Â  Currently, in SAP QA1, this is set to 1,000.Â  Thus, if the data teams were to submit a job to cleanse the 75,000 records mentioned above, the DVSum system would create 75 executable jobs under that Batch Job, each with 1,000 changes.
the DvSum process to commit the jobs for changing in SAP, the 75 jobs would be placed in DvSumâ€™s job queue with 1 job marked with the status â€˜Scheduledâ€™ and 74 jobs marked with the status â€˜Stand byâ€™.Â  The job in status â€˜Scheduledâ€™ will wait until the scheduled time to begin and then will switch to status â€˜Runningâ€™ until the time that SAP returns after the completion of that work process.
Upon receiving the return from SAP, DvSum will mark that job with the status â€˜Completedâ€™ and will mark the next job with the status â€˜Scheduledâ€™ with a start time set for the next change in minute. This process will continue until all 75 jobs have been sequentially executed until completion or until one of the following events occurs:
DvSum user places the Job on
hold
â€“ This action in DvSum will place all jobs
not currently in execution within SAP
to the status â€˜On Holdâ€™ and will prevent DvSum from sending the remaining jobs to SAP for execution until such a time as the DvSum user releases the jobs to continue their execution.Â  This action will not stop the job currently being executed by the SAP System, but it will prevent any subsequent jobs from being sent.
DvSum user deletes the Job in Running
â€“ This action in DvSum should place the current job to the status â€˜Completed (Aborted)â€™, and all subsequent Stand by jobs for this batch will be put on â€˜On Holdâ€™ job status.
SAP Basis user cancels the work process
â€“ This action should terminate the current work process in SAP, and set the status of running job to errored and any subsequent jobs scheduled for that Batch should be changed to status â€˜On Holdâ€™ until such a time that the SAP Basis team allows the continuation of Batch Job processing.
When the status of all the subsequent jobs get completed, user will go to the respective rule details page and refresh it.
If the colour of the submitted exceptions is green, this means the writeback happened successfully.
If the colour of the submitted exceptions gets orange, this means the writeback operation got failed. To know the possible error, click on cleanse button and hover exclamation mark in Change History section. It will tell you the possible reason of failure and analyse with your SAP team to resolve the error.

--------------------------------------------------------------------------------

================================================================================
TITLE: SAP Connector Installation and Configuration Guide
URL: https://dvsum.zendesk.com/hc/en-us/articles/360043550373-SAP-Connector-Installation-and-Configuration-Guide
================================================================================

For making connections to SAP ECC, DvSum provides an RFC-enabled BAPI that is optimized to handle data query type of requests utilized in DvSum.
The attachments contains the Transport Files and the installation instructions.

--------------------------------------------------------------------------------

================================================================================
TITLE: How is Readiness Score calculated for rules?
URL: https://dvsum.zendesk.com/hc/en-us/articles/360035985334-How-is-Readiness-Score-calculated-for-rules
================================================================================

Readiness Score Definition
Readiness Score is a measure that identifies data quality in terms of percentage of clean, meaningful data among all records. It is visible on the "Data Quality" tab for profiled data sources, and it is displayed as "R Score(%)" in tabular views. It is listed as "
DQ Score" for individual Data Quality rules.
It gives a precise overview to users of the quality of their data and helps them to make relevant decisions.
Readiness Score Calculation
Readiness score is calculated slightly differently for different types of rules. The chart below describes how the score is calculated for each type of DvSum rule.
Rule Type
Readiness Score Calculation
Count
Count distinct
SUM
MANUAL
If the threshold minimum is 0 and the result is less than or equal to 0, then readiness score is 100%.
If the threshold minimum is not 0 and the result is less than or equal to the threshold minimum, then the readiness score is:
result / threshold minimum
If the result is greater than or equal to 2 times the threshold maximum, then the readiness score is zero.
If the result is greater than or equal to the threshold maximum but less than 2 times the threshold maximum, then the readiness score is:
(result - threshold maximum) / threshold maximum
Blank Check
Value Range
DAE â€“ Data Analysis Exceptions
Size Check
MDC Ruleset
Orphan Records
Cross Dataset
Integrity Check
Orphan Keys
RCL -X-System Reconciliation
Document
Matching
Address Validation
Data Format
If there are no exceptions, then the Readiness Score is 100%.
If there are exceptions, then the Readiness Score is calculated as:
(total records - exception records) / total records
CUSTOM QUERY
If there are no exceptions, then readiness score is 100%.
If there are exceptions, then readiness score is:
(total records - exception records) / total records
If the total record count is less than the custom query count, then readiness score is 0%.
Compare count
Compare Table
Compare Custom Query
Metric Value not selected on either Target or Reference
If the calculated result is less than or equal to the reference result, then the readiness score is:
calculated result / reference result
If the calculated result is greater than or equal to 2 times the reference result, then the readiness score is 0%.
If the calculated result is greater than or equal to the reference result, then the readiness score is:
(calculated result - reference result) / reference result
Rule Count Readiness Score at Dashboard Level:
This shows up in widgets including "Readiness Score by Val Group" and other similar widgets. It is analogous to the calculations above. But it is different because this is a calculation about how many rules have no exceptions rather than how many records have no exceptions.
The "Rule Count Readiness Score" calculation:
(Total rule count - Rules which fail or have exceptions) / Total rule count

--------------------------------------------------------------------------------

================================================================================
TITLE: Data Format Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/360013705873-Data-Format-Rule
================================================================================

The Data Format rule is useful for examining the data types, length, and patterns of input data, as well as determining whether a numerical field is a whole number, an integer, or a decimal.
For instance, a basic test helps find irregular records in an Excel dataset when a specific column doesn't match the required data format. In Data Format, you can set the data type (String or Number), character length (minimum and maximum column width), numeric precision (for decimals), and specify if white spaces are allowed.
This rule is accessible both independently in Manage Rules and as part of the Rule Set, which includes criteria like blanks, value range, and data format rules.
This rule created on column SIZE_DIM will bring exception records where field values are less than 5 or greater than 15 characters.
Likewise, for numeric fields, you have the flexibility to select the data type as decimal, whole numbers, or integers.
For the "WEIGHT" column, values with a decimal precision greater than 1 will be treated as exceptions. Similar to other exception rules, you have the option to cleanse and write-back fixes to the Excel source.

--------------------------------------------------------------------------------

================================================================================
TITLE: Add an Orphan Records check between two datasets
URL: https://dvsum.zendesk.com/hc/en-us/articles/360009027514-Add-an-Orphan-Records-check-between-two-datasets
================================================================================

Overview
Orphan records are records that exist in one dataset but not in a corresponding dataset. For example, consider these three datasets: ORDERS_PLACED, ORDERS_SHIPPED, INVOICES. We may have a business requirement that any orders found in ORDERS_SHIPPED or in INVOICES must also be found in ORDERS_PLACED. If we ship an order but have no record of receiving the order, there is a discrepancy. The converse could also apply: if we have an order in ORDERS_PLACED but no corresponding record in INVOICES, this could also indicate a problem.
You can perform this type of check using Orphan Records or Orphan Keys rules in DvSum.
Detailed Steps
Step 1. Manage Rules
Login to DvSum and navigate to Audit â†’ Manage Rules.
Step 2. Orphan Records
Select "âŠ• Add Rule" â†’ Foundational â†’ Orphan Records.
Step 3: Basic Input
In the Rules Wizard, enter a description, e.g. "Validate that all ORDERS_SHIPPED appear in ORDERS_PLACED." Then fill in the Basic Input section with Data Source, Table Name, and Field Name corresponding to the table in which you want to search for orphan records.
Step 4: Reference Input
Fill in the Reference Input with Data Source, Table Name, and Field Name that you want to check against. The system will automatically select the columns with the same names. You can change them if appropriate.
Step 5: Validate
Save the rule. The rule definition will be displayed. Click Run to execute and test the rule.

--------------------------------------------------------------------------------

================================================================================
TITLE: Field Dictionary
URL: https://dvsum.zendesk.com/hc/en-us/articles/360007001653-Field-Dictionary
================================================================================

The term field dictionary appears in basic Value Range rule (or DQ ruleset, Exploration). Value rangeÂ identifies exception records within a dataset where a specific field is not within a desired range. Range can be a continue range between min and max or could be a set of specific values.
These Specific values can be selected from the field's own profile called Field Dictionary. In a Value Range rule, select your data source, table and field. Specify Field dictionary to define valid/invalid values.
This will show you all existing values of the field "Ending_Date". This makes it easy for a user to pick out the values for fixing data.

--------------------------------------------------------------------------------

================================================================================
TITLE: Features of Value Range Rule
URL: https://dvsum.zendesk.com/hc/en-us/articles/360006916294-Features-of-Value-Range-Rule
================================================================================

Value Range is one of the most widely used rule to cleanse the data. It helps you to identify the exceptions by specifying valid range or giving a list of invalid records.Â Range can be a continuous range between min and max or could be a set of specific values. Specific values can be selected from the field's own profile or as a reference dictionary.
Let's look into the detail of this rule. First you select your Data source, table and field. Now you can select the kind of values as valid or invalid.
Valid/Invalid Range
From the second drop down you can specify whether the range is for Valid values or invalid. If you select "Valid", the rule execution will show any value that is
outside
the range as an exception. If you select "Invalid", the rule execution will show any value is
within
the stated range as an exception.
Single Value
As the name suggests, for a single value there is a single threshold. From the list of operators, you can define the threshold on a column name.
Considering an example, where we want to create a VR rule on "Ending_Date" which should be greater than equal to the "Starting_Date". Specifying this as "Valid" range, this rule will show the records where Ending_Date is less than Starting_Date as exceptions.
The same case can be started the other way where a user can create a rule on "Ending_Date" and provide threshold as less than the "Starting_Date" to define "Invalid" records. Both the use cases will return same exceptions.
Range Value
For the Range values, you need to provide the minimum and maximum threshold.
Here you have 3 options. You can provide an absolute value, like 10 or you can use Reference column of the table. Threshold Max would be less than equal to the column Title. You can also opt for a custom expression.
Field Dictionary
Field dictionary
provides you with a list of values in the selected column which makes it easier to pick out the records you want as valid or invalid. Lets say we select as 2001-02 as invalid. This rule execution will bring records with Starting_Date as 2001-02 as exception in result.
Reference Dictionary
For reference dictionary with Valid values, the rule execution will bring the records with values that don't exist in dictionary as exceptions in result. For the Field "is_current' any value other than the ref dictionary will be exceptions.
Note: You can create your own Reference Dictionary from Profile tab or you can see
How to create and use Reference Dictionary
Include Blanks
Another small yet powerful check you can integrate with this rule is "Include Blanks". As an example below, a user can specify "Valid Range" of values and check-mark the Include Blanks. This mean the blank fields in the column "is_current" will not be shown as exceptions in rule result.
For the same case, if "Invalid" is selected,Â blank fields in the column "is_current" will be shown as exceptions along with any value within the range 10-20.

--------------------------------------------------------------------------------

================================================================================
TITLE: Integrating Rules into batch workflow
URL: https://dvsum.zendesk.com/hc/en-us/articles/360006820634-Integrating-Rules-into-batch-workflow
================================================================================

Rules in DvSum can be executed from the UI, scheduled through the scheduler and can also be called programmatically using an API. To run a rule or set of rules through a script, you can generate a script for those rules using the Schedule Rule Menu.
The script takes the form of
https://<saws
address>:<saws_port>/runJob?jobId=<jobid>
The script runs the rule exactly like running from the UI or the Scheduler, with the difference being what is returned back from the script. Below is a screenshot of what is returned from the script. The 2 elements areÂ  return_code and return_remark.
The return_code always contains 3 values
0Â  - PASS if rule has 0 exceptions or exceptions are less than lower bound of threshold
-1 - WARN if the rule has exceptions but exceptions are greater than lower bound but less than upper bound
-2Â  - FAIL if the rule has exceptions that are greater than upper bound limit.
The return_remark contains text that will provide the name of the rule additional information that you typically see in system remark field in the Rule Detail page.
Example
This rule has threshold set between 0 - 300
Since exception are 256 which is > 0 but less than <300, the return_code is -1 as a Warning.
Executing the rule via API:
Users can generate the script for the rule and use the script URL to execute the rule. Lets get started step by step:
Step 1:
Open the Dvsum application, go to Audit >> Manage Rules >> select any existing rule >> click More action >> select Schedule Rule as shown below:
Step 2:
Dynamic Source Script Generation
Go to Generate Script tab and click on the button at the bottom to generate the script as shown below;
Step 2.1:
A job is created against the rule. Now go to the main scheduler page, search the rule, and a newly created job will be visible with a description and status as â€œstand byâ€.
Step 3:
Copy the script url and paste it into Google Chrome and press Enter. Execution starts and on dvsum application, the job description and status get executed as well.
Step 3.1:
Once the execution is completed. it will print the remarks same as shown on the rule detail page in Dvsum.
On the Scheduler page, the job status changes to "Completed".
Executing the rule API via ADF:
Step 1:
For the existing Rule, select schedule to Generate the script URL as shown above, and copy the url.
Step 1.1:
A job is created against the rule. Now go to the main scheduler page, search the rule, and a newly created job will be visible with a description and status as â€œstand byâ€.
Step 2:
Go to Azure Data Factory and paste the copied URL in api call. Click on Debug button to start the execution.
Step 2.1:
Once the execution is completed. it will print the remarks in API call output same as shown on the rule detail page in Dvsum.
On the Scheduler page, the job status changes to "Completed".

--------------------------------------------------------------------------------

================================================================================
TITLE: Use Rule Groups as tags for better management of Rules
URL: https://dvsum.zendesk.com/hc/en-us/articles/115000728633-Use-Rule-Groups-as-tags-for-better-management-of-Rules
================================================================================

What are Rule Groups?
Rule Groups are
tags
that allow you to organize your rules into categories. (They were formerly known as "Val Groups", so you'll see this terminology in some documentation.) You can apply a Rule Group tag in order to categorize DvSum Rules. Rule groups are like folders, but unlike folders you can apply more than one Rule Group tag to a single rule. Likewise, a rule can belong to multiple Rule Groups.
Using Rule Groups
For instance, the user "Dani DataSteward" wants to group together a few rules she created. She can create a Rule Group called "Dani DataSteward Rules". She can then filter the "Analyze Rules" grid by Rule Group. This gives her the flexibility to tag rules that were not created by her as part of this Rule Group.
Additionally, under Review â†’ Batch Execution an administrator can execute multiple rules tagged with a specified set of Rule Groups very easily.
Creating Rule Groups
While creating a rule
A user can add Rule Groups from the "Add Rule" wizard when creating a new rule. As the user starts typing in the field "Rule Group", the list of existing Rule Groups appears. If the desired rule group does not already exist, then
hit Enter to create the new Rule Group
. (Note that the screenshot below shows the historic
label
"Val Group" rather than the current label "Rule Group".)
While editing a rule
You can also add New Rule Groups from rule detail page. Edit the rule summary information. The Rule Group drop down displays the list of existing rule groups. To create a new one, simply type the name and press Enter.
While performing a Mass Update
On the pages "Manage Rules" and "Analyze Rules", you have the option to perform a Mass Update. Under the field "Rule Group", you can select from the existing list or type to create a new rule group.
Remove Rules from a Rule Group
While editing a rule summary it's possible to remove any currently assigned rule groups.
Additionally, on the pages "Manage Rules" and "Analyze Rules" you may select rules and click on More Actions â†’ Mass Update. This interface provide the ability to remove (or add) rule groups.
You have the following options regarding Rule Groups:
Add following Rule Groups
Replace existing Rule Groups with following
Remove following Rule Groups (pictured above)

--------------------------------------------------------------------------------

================================================================================
TITLE: Types of Rules Wizards
URL: https://dvsum.zendesk.com/hc/en-us/articles/230178488-Types-of-Rules-Wizards
================================================================================

Types of Rules Wizards
We encourage customers to begin by identifying whatever data errors are causing impacts to their business and start with rules to identify those exceptions. Some of the rule types are straightforward, and their relevance is obvious. Others were created for specific customer use cases, and the potential application to your environment may not be readily apparent. Over time, potential rules and use cases become more apparent as you build out a more complete data governance approach.
The rules wizards are organized into the following categories:
Foundational DQ
Basic, logical checks you would typically do in any environment. Includes completeness, consistency, and validity checks.
Pipeline Checks
Validates data pipelines by verifying data quantities, assessing data quality and performance, and monitoring data timeliness within the data processing workflows. Enhances data flow, quality, and efficiency.
Cross System DQ
Validations and checks based on a specific business understanding of what the data should be.
MDM
Is the data valid or what is expected based on the process or purpose. Includes accuracy checks.
Data Diffs
Compare and reconcile details and records across systems â€“ without having to consolidate or pull the data. Includes consistency and validity checks.
Within these categories are various rule types that can be configured to validate data and generate exceptions. There may also be different approaches to start with different rule types that generate the Exceptions youâ€™re looking for. We encourage users to explore different rules and approaches, as there is an art to determining what works best within your environment and business processes.
Type of Audit Rule
Definition
Example (s)
Foundational DQ
BLANKS
Availability and completeness of data. Are there missing values?
All the records where address field is missing in customer ship-to information.
VALUE RANGE
Are the values in data within the expected range? Either within a continuous range, within a pick list, or one of a specific value.
Manufacturing yield should always be between 0.01 and 1
Order type in sales orders should only be one of ZOST, ZOCO, ZOFR.
Item_category in shipments should only be one of the values in the item_category reference
UNIQUENESS
Are the values within a data element unique in the dataset?
Duplicate customers in customer table. Data is not unique. Duplicate keys and their count are displayed.
UNIQUE VALUES
Are the Count of Unique Values within an acceptable range or equal to a defined number?
Expect unique locations to be between 95-100 based on 100 ship-from locations. Over 100 indicates error. Under 95 indicates potential error.
CUSTOM QUERY
Ability to write custom SQL.
Any custom SQL query.
DATA FORMAT
Are the values in the data conforming to predefined formats, or users defining their own formats or patterns, such as SSN, Email etc. ensuring consistency and accuracy.
Expect 2 values after decimal point in employee IDs field , if employee ID violates the rules's expected format, it would be considered a potential error.
Defining pattern and then providing regex expression in the filter modal.
Pipeline Checks
COUNT
Is the count of records within the expected range?
Count of sales orders in open order extract should be between 800k and 1M records.
Freshness
Monitors if data is not present or updated by a certain time.
In a weather forecasting app:
Data initially fetched at "2023-09-30 08:00:00."
Next execution timestamp: "2023-09-30 14:30:00."
Time difference: 6 hours.
Defined threshold: 3 hours.
if the difference is more than the defined difference, it should be alerting.
Metric
Is the aggregated quantity within the expected range?
Sum of open order quantity should be between 75M and 100M units.
Cross System DQ
ORPHAN RECORDS
Record exists in one systemâ€™s set of records, but not in another systemâ€™s set of what should be the same records.
Order A exists in the ERPâ€™s list of Orders, but not in the Transportation Systemâ€™s list of Planned Orders.
ORPHAN KEYS
Group of Records of a certain type exist in one system but not in another. Summarized by type.
Shipments for Customer X not found in list of Shipments by Customer.
Product Category B is found in the list of shipments, but it is not found in list of Products.
INTEGRITY CHECK
Does valid data exist in reference systems to execute a process?
Does each SKU have a valid Bill-of-Distribution?
Does each Item have weight/measure populated?
MDM
ADDRESS VALIDATION
Validates existing address against Google Maps. Also enables standardization.
123 Main Rd should actually be 123 W Main St.
DOC-MATCHING
Compare records at a field level and highlight discrepancies.
For Planned Orders in ERP vs Planned Orders in Warehouse System: validate dates, quantity, and price and show discrepancies
Data Diffs
COMPARE COUNT
Compare the count of records between 2 different data sources. The test may be at same or different granularities.
Compare total Orders or Shipments in ERP at transactional level to Order or Shipment summary loaded in data warehouse at aggregated level.
COMPARE CUSTOM QUERY
Create your own custom validation rule within one data set and compare to the same query in a separate data set or system.
Compare totals in the operational system of record to a downstream system and to an enterprise data warehouse.
COMPARE METRIC
Compare aggregated quantity between 2 different data sources.
Compare the total shipment volume for last 3 months in ERP with shipment volume loaded in data warehouse at aggregated level.
COMPARE TABLE
Holistic comparison of the count of records and volume for metric fields across different attributes.
Compare Open Sales Orders in SAP with Sales Order extract in JDA for count, total of orderquantity, qtyopen by order_type, item_category, plant, key customer accounts, product line.
COMPARE SCHEMA
Compare the schema structure between two sources, highlight differences in tables and columns, include disparities in data types where applicable.
Compare schema definitions between the operational system of record, a downstream system, and an enterprise data warehouse to ensure consistency and identify any discrepancies in table structures and column definitions.
Ruleset
Ruleset is a predefined set of data constraints, It ensures data accuracy by enforcing rules such as disallowing blank values, restricting values to a specified range, and validating data formats for columns like strings or numbers.
Example:
Customer_ID can't be left blank, and the "Country" column should only have two-character inputs like US or UK.

--------------------------------------------------------------------------------

================================================================================
TITLE: Validation Rule Reference Information
URL: https://dvsum.zendesk.com/hc/en-us/articles/203082320-Validation-Rule-Reference-Information
================================================================================

Overview
Reference information about validation rules (DQ Rules).
Rule Types
Category
Rule Type
Foundational DQ
DATA FORMAT
BLANKS
VALUERANGE
UNIQUENESS
UNIQUE VALUES
DATA ANALYSIS EXCEPTIONS
CUSTOM QUERY
Pipeline Checks
COUNT
FRESHNESS
METRIC
Cross-System DQ
ORPHAN RECORDS
ORPHAN KEYS
INTEGRITY CHECK
MDM
ADDRESS VALIDATION
DOC-MATCHING
Data Diffs
COMPARE COUNT
COMPARE METRIC
COMPARE TABLE
COMPARE CUSTOM QUERY
Run Status
A new rule will have an implicit Run Status of
Valid
. After the rule is executed, there are a variety of Run Results that the rule may take. "Run Status" and "Run Result" are used interchangeably in DvSum DQ.
Status
Icon
Definition
Passed
Rule is valid, and no exceptions were found in the data in the most recent run.
Failed
Rule is valid, and exceptions were found in the most recent run.
This applies only to rules which do not identify specific invalid records. For example, a COUNT rule result falls outside of the allowed range.
Exception
Rule is valid, and at least one exception was found in the data in the most recent run.
This applies to rules which identify invalid records. For example, a failed BLANKS or UNIQUENESS check will result in this status.
Invalid
Rule is not valid.
This does not indicate a problem with the data. Rather, it indicates a problem with the rule definition which must be solved before the rule can be executed.
Matched
For example, a ADDRESS VALIDATION will indicate Matched after successful execution.
Modified
After data steward performs cleansing, but data is not yet committed back to source.
Committed
Valid
Run Status & Readiness Score
Along with a status, a Readiness Score is also generated. Readiness score provides a second degree of information on the quality of data. If the data rule is failing, how bad is it. Readiness score creates a common unit for measuring the quality of data and allows result of audits calculating the overall data quality score across various data elements and types of audits.
Sample Audit Result
Audit Result Icon
Readiness Score
Count of finished good items in item master is 6000 which is within the tolerance of 5500 and 7000
100%
Count of finished good items in item master is 5000 which is not within the tolerance of 5500 and 7000
91%
Explanation: 5000 is 9% short of 5500.
Calculation:
1 - (500/5500)
Forecast Name is 100% unique in SALES_FORECAST extract
100%
Run over run variance of qtyplanned in PURCHASE_PLAN is 5% for Supplier X which is more than the limit of 3%
33%
Calculation:
1 - (.02/.03)
There are 2 routing records where yield is not between 0 and 1. (total records are 100)
98%
Calculation:
1 - (2/100)
Following insights are available with your audit results
Icon
Insight Type
Definition
History
History for all audits
Exception List
List of Exceptions for master data audits
Drill-down Analytical
Drill-down with Variance for aggregation audits
History Trend Insights
With history, you can get insights into the changes to the audit result from last run and also the trend of that audit over time. History insights can be useful to identify what audits to focus on.
History Trend
Definition
Focus Required
Flat. No change
Status Quo
or
Positive Up
â€“ Test was a fail before and now pass. Test has been failing because results too low but now trending up.
Positive Down
â€“ Test has been failing because results were too high and now trending down.
Things are improving even if the tests are failing. May not need to focus.
or
Negative Down
â€“ Test was passing before and is now failing. Or it has been passing but values are trending down and will cross the lower tolerance soon. Or it has been failing and things are getting worse
Negative Up
â€“ Test was passing before and is not failing. Or it has been passing but values are trending up to cross the upper tolerance soon
Things are getting worse for failed tests â€“ requires top priority focus.
Things are trending in the wrong direction for passing tests â€“ heads-up for future issues

--------------------------------------------------------------------------------

